{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"nightWolf Linux System Performance \uf0c1 System performance is one of the major and reoccurring issue in IT infrastructure and Linux performance troubleshooting is one of the most tedious task in system administration. We have consolidated few major issues and bottlenecks for you to understand the root cause of the performance issues. This Article includes: Troubleshooting Linux Perfomance isssues due to high CPU Usage Troubleshooting Linux Performance issues due to high Memory Usage Troubleshooting Linux Performance issues due to high Disk IO Usage Troubleshooting Linux OS Network Performance Issues Interview Preparations \uf0c1 We have created a small interview question database and preparation material which will help you to prepare for your interviews. Please utilise these questions and tips for your interview preparation. This Article include: Linux and Troubleshooting Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting Interview Questions and Answers OS Networking OS Network Interview Questions Google Cloud Certification Practice Questions GCP ACE Practice Questions - set 1 GCP ACE Practice Questions - set 2 GCP ACE Practice Questions - set 3 AWS AWS Certified SysOps Administrator Questions - 1st AWS Certified SysOps Administrator Questions - 2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd DevOps - [GIT, Terraform, Jenkins & Docker] DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Docker Interview Questions for DevOps Roles Terraform Interview Questions for DevOps Roles - 1st Terraform Interview Questions for DevOps Roles - 2nd Terraform Interview Questions for DevOps Roles - 3rd Ansible Interview Questions and Answers Kubernetes Interview Questions and Answers JAVA Java/OOPs Interview Questions and Answers Java/OOPs Interview Questions and Answers - part 2 Manual Testing / Quality Assurance Manual Testing/QA Interview Questions and Answers Database Management System [DBMS] DBMS Interview Questions and Answers Managerial Interview Managerial Interview Questions and Answers Interview materials Cheatsheets \uf0c1 Cheatsheets for different techs for your references: This include: Kubernetes cheatsheet GIT cheatsheet Linux YUM cheatsheet Yum vs APT cheatsheet gcloud Commands Cheatsheet TLS Version difference Articles \uf0c1 We have below articles in our bucket: What is Artificial Intelligence(AI) ? Unleashing the Power of Artificial Intelligence What is Machine Learning ? What is Neural Network ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? Linux \uf0c1 Linux OS Basics Filesystem Basics Packsage Management OS Networking and Security OS Security and Hardening RAIDs Database \uf0c1 mysql mongoDB Network \uf0c1 Cloud \uf0c1 Service Comparison between GCP, AWS, Azure Contribute to nightWolf \uf0c1 (adsbygoogle = window.adsbygoogle || []).push({});","title":"Home"},{"location":"#linux-system-performance","text":"System performance is one of the major and reoccurring issue in IT infrastructure and Linux performance troubleshooting is one of the most tedious task in system administration. We have consolidated few major issues and bottlenecks for you to understand the root cause of the performance issues. This Article includes: Troubleshooting Linux Perfomance isssues due to high CPU Usage Troubleshooting Linux Performance issues due to high Memory Usage Troubleshooting Linux Performance issues due to high Disk IO Usage Troubleshooting Linux OS Network Performance Issues","title":"Linux System Performance"},{"location":"#interview-preparations","text":"We have created a small interview question database and preparation material which will help you to prepare for your interviews. Please utilise these questions and tips for your interview preparation. This Article include:","title":"Interview Preparations"},{"location":"#cheatsheets","text":"Cheatsheets for different techs for your references: This include: Kubernetes cheatsheet GIT cheatsheet Linux YUM cheatsheet Yum vs APT cheatsheet gcloud Commands Cheatsheet TLS Version difference","title":"Cheatsheets"},{"location":"#articles","text":"We have below articles in our bucket: What is Artificial Intelligence(AI) ? Unleashing the Power of Artificial Intelligence What is Machine Learning ? What is Neural Network ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Articles"},{"location":"#linux","text":"Linux OS Basics Filesystem Basics Packsage Management OS Networking and Security OS Security and Hardening RAIDs","title":"Linux"},{"location":"#database","text":"mysql mongoDB","title":"Database"},{"location":"#network","text":"","title":"Network"},{"location":"#cloud","text":"Service Comparison between GCP, AWS, Azure","title":"Cloud"},{"location":"#contribute-to-nightwolf","text":"(adsbygoogle = window.adsbygoogle || []).push({});","title":"Contribute to nightWolf"},{"location":"articles/","text":"Articles in our bucket !! \uf0c1 We are working on wide variety of articles for the readers. Here is a list of articles in our Artificial Intelligence (AI) \uf0c1 What is Artificial Intelligence(AI) ? \uf0c1 Unleashing the Power of Artificial Intelligence \uf0c1 What is Machine Learning ? \uf0c1 What is Neural Network ? \uf0c1 What is Internet of Things (IoT) ? \uf0c1 What is Natural Language Processing (NLP) ? \uf0c1 What is Robotics and Automation ? \uf0c1 Quantum Computing: The Next Frontier \uf0c1 Blockchain: A Primer for Beginners \uf0c1 What is Virtual Reality (VR) ? \uf0c1 Cybersecurity: Protecting our digital world \uf0c1 What is AI Chat Open Assistant Chatbot ? \uf0c1 What is Bard chatbot ? \uf0c1 (adsbygoogle = window.adsbygoogle || []).push({});","title":"Articles in our bucket !!"},{"location":"articles/#articles-in-our-bucket","text":"We are working on wide variety of articles for the readers. Here is a list of articles in our","title":"Articles in our bucket !!"},{"location":"articles/#artificial-intelligence-ai","text":"","title":"Artificial Intelligence (AI)"},{"location":"articles/#what-is-artificial-intelligenceai","text":"","title":"What is Artificial Intelligence(AI) ?"},{"location":"articles/#unleashing-the-power-of-artificial-intelligence","text":"","title":"Unleashing the Power of Artificial Intelligence"},{"location":"articles/#what-is-machine-learning","text":"","title":"What is Machine Learning ?"},{"location":"articles/#what-is-neural-network","text":"","title":"What is Neural Network ?"},{"location":"articles/#what-is-internet-of-things-iot","text":"","title":"What is Internet of Things (IoT) ?"},{"location":"articles/#what-is-natural-language-processing-nlp","text":"","title":"What is Natural Language Processing (NLP) ?"},{"location":"articles/#what-is-robotics-and-automation","text":"","title":"What is Robotics and Automation ?"},{"location":"articles/#quantum-computing-the-next-frontier","text":"","title":"Quantum Computing: The Next Frontier"},{"location":"articles/#blockchain-a-primer-for-beginners","text":"","title":"Blockchain: A Primer for Beginners"},{"location":"articles/#what-is-virtual-reality-vr","text":"","title":"What is Virtual Reality (VR) ?"},{"location":"articles/#cybersecurity-protecting-our-digital-world","text":"","title":"Cybersecurity: Protecting our digital world"},{"location":"articles/#what-is-ai-chat-open-assistant-chatbot","text":"","title":"What is AI Chat Open Assistant Chatbot ?"},{"location":"articles/#what-is-bard-chatbot","text":"(adsbygoogle = window.adsbygoogle || []).push({});","title":"What is Bard chatbot ?"},{"location":"cheatsheet/","text":"Kubernetes cheatsheet \uf0c1 GIT cheatsheet \uf0c1 Linux YUM cheatsheet \uf0c1 Yum vs APT cheatsheet \uf0c1 gcloud commands Cheatsheet \uf0c1 TLS Version key differences \uf0c1 (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Cheatsheet"},{"location":"cheatsheet/#kubernetes-cheatsheet","text":"","title":"Kubernetes cheatsheet"},{"location":"cheatsheet/#git-cheatsheet","text":"","title":"GIT cheatsheet"},{"location":"cheatsheet/#linux-yum-cheatsheet","text":"","title":"Linux YUM cheatsheet"},{"location":"cheatsheet/#yum-vs-apt-cheatsheet","text":"","title":"Yum vs APT cheatsheet"},{"location":"cheatsheet/#gcloud-commands-cheatsheet","text":"","title":"gcloud commands Cheatsheet"},{"location":"cheatsheet/#tls-version-key-differences","text":"(adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"TLS Version key differences"},{"location":"cloud-comp/","text":"Service Comparision between GCP, AWS and Azure: \uf0c1 Below table consists of names of service offered by 3 major public clouds for different requirements. This table will be very useful when you are comparing services offered by GCP, Azure and AWS. A detailed comparison is discussed in Article . Please checkout if you need more details. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Service Google Cloud Platform AWS Azure Computing Google Compute Engine Elastic Compute Cloud - EC2 Virtual Machines Analytics - Data Stream Cloud Dataflow Amazon Kinesis Azure Stream Analytics Application Hosting Google App Engine Amazon Elastic BeanStalk Azure Cloud Service Block Storage Persistent Disk Amazon Elastic Block Storage Azure Managed Storage Compliance Google Cloud Platform Security AWS Cloud HSM Azure Trust Center Cloud Specific Containers Google Cloud Run EC2 Container Service Azure Container Service Cloud Kubernetes containers Google Kubernetes Engine (GKE) Elastic Kubernetes Service (EKS) Azure Kubernetes Service Content Delivery Network (CDN) Google Cloud CDN Amazon CloudFront Azure CDN DNS Service Google Cloud DNS AWS Route53 Azure Traffic Manager Identity and Access Management Google Cloud IAM AWS IAM Azure Active Directory Key Management Service Google Cloud KMS AWS KMS Azure Key Vault Load Balancing Cloud Load Balancing Elastic Load Balancing Load Balancing for Azure Log Monitoring Cloud Logging Amazon CloudTrail Azure Operational Insights NoSQL Databases Cloud Datastore/Firestore AWS DynamoDB Azure DocumentDB Relational Databases Cloud SQL / Cloud Spanner Amazon RDS Azure Relational Database Notification Services Cloud Pub-Sub Amazon Simple Notification Service Azure notification Hub Object Storage Cloud Storage Amazon Simple Storage - S3 Azure Blob Storage Performance Monitoring StackDriver Monitoring Amazon CloudWatch Azure Application Insights Private Connectivity Cloud InterConnect AWS Direct Connect Azure Express Route Automatic Scaling Auto Scaler Auto Scaling Azure AutoScale Serverless Computing Google Cloud Functions AWS Lambda Azure Functions Virtual Network Cloud Virtual Private Network - VPC Amazon VPC Azure Virtual Network Automation Cloud Anthos Config Management AWS OpsWorks Azure Automation (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Service Comparison between GCP, AWS, Azure"},{"location":"cloud-comp/#service-comparision-between-gcp-aws-and-azure","text":"Below table consists of names of service offered by 3 major public clouds for different requirements. This table will be very useful when you are comparing services offered by GCP, Azure and AWS. A detailed comparison is discussed in Article . Please checkout if you need more details. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Service Google Cloud Platform AWS Azure Computing Google Compute Engine Elastic Compute Cloud - EC2 Virtual Machines Analytics - Data Stream Cloud Dataflow Amazon Kinesis Azure Stream Analytics Application Hosting Google App Engine Amazon Elastic BeanStalk Azure Cloud Service Block Storage Persistent Disk Amazon Elastic Block Storage Azure Managed Storage Compliance Google Cloud Platform Security AWS Cloud HSM Azure Trust Center Cloud Specific Containers Google Cloud Run EC2 Container Service Azure Container Service Cloud Kubernetes containers Google Kubernetes Engine (GKE) Elastic Kubernetes Service (EKS) Azure Kubernetes Service Content Delivery Network (CDN) Google Cloud CDN Amazon CloudFront Azure CDN DNS Service Google Cloud DNS AWS Route53 Azure Traffic Manager Identity and Access Management Google Cloud IAM AWS IAM Azure Active Directory Key Management Service Google Cloud KMS AWS KMS Azure Key Vault Load Balancing Cloud Load Balancing Elastic Load Balancing Load Balancing for Azure Log Monitoring Cloud Logging Amazon CloudTrail Azure Operational Insights NoSQL Databases Cloud Datastore/Firestore AWS DynamoDB Azure DocumentDB Relational Databases Cloud SQL / Cloud Spanner Amazon RDS Azure Relational Database Notification Services Cloud Pub-Sub Amazon Simple Notification Service Azure notification Hub Object Storage Cloud Storage Amazon Simple Storage - S3 Azure Blob Storage Performance Monitoring StackDriver Monitoring Amazon CloudWatch Azure Application Insights Private Connectivity Cloud InterConnect AWS Direct Connect Azure Express Route Automatic Scaling Auto Scaler Auto Scaling Azure AutoScale Serverless Computing Google Cloud Functions AWS Lambda Azure Functions Virtual Network Cloud Virtual Private Network - VPC Amazon VPC Azure Virtual Network Automation Cloud Anthos Config Management AWS OpsWorks Azure Automation (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Service Comparision between GCP, AWS and Azure:"},{"location":"contribute/","text":"Your contributions and feedbacks are highly appreciated. This will help us to help people like you to enhance their knowledge and score in their current and future roles. \uf0c1 You can contribute through nightwolf-cotribution github repo page by updating the content and creating a pull request. That pull request will be reviewd thoroughly and then committed, which will display on our website nightwolf.in. You can use below command to contribute through git cli: mkdir nightwolf cd nightwolf git clone git@github.com:v-nightwolf/nightwolf-cotribution.git You will start seeing multiple files there. Contribute in appropriate file and commit & push your changes back to git repo. To read more about how to work with remote repost, please read the Article . Otherwise you can submit your suggestions through below form. \uf0c1 Loading\u2026","title":"Contribute"},{"location":"contribute/#your-contributions-and-feedbacks-are-highly-appreciated-this-will-help-us-to-help-people-like-you-to-enhance-their-knowledge-and-score-in-their-current-and-future-roles","text":"You can contribute through nightwolf-cotribution github repo page by updating the content and creating a pull request. That pull request will be reviewd thoroughly and then committed, which will display on our website nightwolf.in. You can use below command to contribute through git cli: mkdir nightwolf cd nightwolf git clone git@github.com:v-nightwolf/nightwolf-cotribution.git You will start seeing multiple files there. Contribute in appropriate file and commit & push your changes back to git repo. To read more about how to work with remote repost, please read the Article .","title":"Your contributions and feedbacks are highly appreciated. This will help us to help people like you to enhance their knowledge and score in their current and future roles."},{"location":"contribute/#otherwise-you-can-submit-your-suggestions-through-below-form","text":"Loading\u2026","title":"Otherwise you can submit your suggestions through below form."},{"location":"db/","text":"mysql \uf0c1 mongoDB \uf0c1","title":"Db"},{"location":"db/#mysql","text":"","title":"mysql"},{"location":"db/#mongodb","text":"","title":"mongoDB"},{"location":"gcloud_cheatsheet/","text":"gcloud CLI commands Cheatsheet \uf0c1 This Article consists of gcloud commands which can be handy when working on google cloud shell to manage/automate the infrastructure components. Official documentation can be found at gcloud official doc All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Get going with the gcloud \uf0c1 Initialize, authorize, and configure gcloud gcloud init Display version + installed components gcloud version Install specific components gcloud components install COMPONENT_NAME Update your Cloud SDK to the latest version gcloud components update Set a default Google Cloud project to work on gcloud config set project Display current gcloud environment details gcloud info * For apt-get and yum, see https://cloud.google.com/sdk/install Google Compute Engine commands \uf0c1 List zones in Region: gcloud compute zones list | grep us-central1 Set default zone for new vm: gcloud config set compute/zone us-central1-c To create a new VM: gcloud compute instances create \"my-vm-2\" \\ --machine-type \"n1-standard-1\" \\ --image-project \"debian-cloud\" \\ --image-family \"debian-10\" \\ --subnet \"default\" To create a Cloud Storage Bucket: gsutil mb -l US gs://$PROJECT_ID To list the items in Cloud Storage Bucket: gsutil ls gs://$PROJECT_ID To list the set project property in the core section: gcloud config list $PROJECT_ID To list gcloud named configurations: gcloud config configurations list To create a new named configuration: gcloud config configurations create my-second-config To activate sepcific named configuration: gcloud config configurations activate my-default-configuration To list the properties of the specified section using the active configuration: gcloud config list To see your currently available accounts used for gcloud authentication: gcloud config list account To describe a named configuration by listing its properties: gcloud config configurations describe my-second-configuration To display all Compute Engine instances in a project: gcloud compute instances list To create Compute Engine virtual machine instances: gcloud compute instances create To display all data associated with a Compute Engine virtual machine instance: gcloud compute instances describe my-first-instance-from-gcloud To delete Compute Engine virtual machine instances: gcloud compute instances delete my-first-instance-from-gcloud To display all Google Compute Engine zones in a project: gcloud compute zones list To display all Google Compute Engine regions in a project: gcloud compute regions list To display all Google Compute Engine machine types in a project: gcloud compute machine-types list To display all Google Compute Engine machine types in Zone asia-southeast2-b : gcloud compute machine-types list --filter zone:asia-southeast2-b To filter the Google Compute Engine machine types in multiple zones: gcloud compute machine-types list --filter \"zone:(asia-southeast2-b asia-southeast2-c)\" To display all data associated with a Compute Engine region: gcloud compute regions describe $REGION To list Google Compute Engine instance templates: gcloud compute instance-templates list To create a Compute Engine virtual machine instance template: gcloud compute instance-templates create instance-template-from-command-line To delete one or more Compute Engine virtual machine instance templates: gcloud compute instance-templates delete instance-template-from-command-line To display all data associated with a Google Compute Engine virtual machine instance template: gcloud compute instance-templates describe my-instance-template-with-custom-image To list Google Compute Engine managed instance groups: gcloud compute instance-groups managed list To delete Compute Engine managed instance groups: gcloud compute instance-groups managed delete my-managed-instance-group To create a Compute Engine managed instance group: gcloud compute instance-groups managed create my-mig --zone us-central1-a --template my-instance-template \\ --size 1 To set autoscaling parameters of a managed instance group: gcloud compute instance-groups managed set-autoscaling my-mig --max-num-replicas=2 --zone us-central1-a To stop autoscaling a managed instance group: gcloud compute instance-groups managed stop-autoscaling my-mig --zone us-central1-a To set managed instance group size: gcloud compute instance-groups managed resize my-mig --size=1 --zone=us-central1-a To recreate instances managed by a managed instance group: gcloud compute instance-groups managed recreate-instances my-mig --instances=my-mig-85fb --zone us-central1-a To delete Compute Engine managed instance groups: gcloud compute instance-groups managed delete my-managed-instance-group --region=us-central1 Troubleshooting instance ssh issue \uf0c1 recommendations, rerun the ssh command with the --troubleshoot option. gcloud compute ssh instance-1 --project=smitis-dev --zone=us-central1-a --troubleshoot Or, to investigate an IAP tunneling issue: gcloud compute ssh instance-1 --project=smitis-dev --zone=us-central1-a --troubleshoot --tunnel-through-iap Google App Engine commands \uf0c1 cd default-service gcloud app deploy gcloud app services list gcloud app versions list gcloud app instances list gcloud app deploy --version=v2 gcloud app versions list gcloud app versions list --hide-no-traffic ==> DO not show version receiving no traffic. gcloud app browse gcloud app browse --version 20210215t072907 gcloud app deploy --version=v3 --no-promote gcloud app browse --version v3 gcloud app services set-traffic split=v3=.5,v2=.5 gcloud app services set-traffic splits=v3=.5,v2=.5 watch curl https://melodic-furnace-304906.uc.r.appspot.com/ gcloud app services set-traffic --splits=v3=.5,v2=.5 --split-by=random cd ../my-first-service/ gcloud app deploy gcloud app browse --service=my-first-service gcloud app services list gcloud app regions list gcloud app browse --service=my-first-service --version=20210215t075851 gcloud app browse --version=v2 gcloud app open-console --version=v2 gcloud app versions list --hide-no-traffic Google Kubernetes Engine commands \uf0c1 gcloud config set project my-kubernetes-project-304910 gcloud container clusters get-credentials my-cluster --zone us-central1-c --project my-kubernetes-project-304910 kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE kubectl get deployment kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080 kubectl get services kubectl get services --watch curl 35.184.204.214:8080/hello-world kubectl scale deployment hello-world-rest-api --replicas=3 gcloud container clusters resize my-cluster --node-pool default-pool --num-nodes=2 --zone=us-central1-c kubectl autoscale deployment hello-world-rest-api --max=4 --cpu-percent=70 kubectl get hpa kubectl create configmap hello-world-config --from-literal=RDS_DB_NAME=todos kubectl get configmap kubectl describe configmap hello-world-config kubectl create secret generic hello-world-secrets-1 --from-literal=RDS_PASSWORD=dummytodos kubectl get secret kubectl describe secret hello-world-secrets-1 kubectl apply -f deployment.yaml gcloud container node-pools list --zone=us-central1-c --cluster=my-cluster kubectl get pods -o wide kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.2.RELEASE kubectl get services kubectl get replicasets kubectl get pods kubectl delete pod hello-world-rest-api-58dc9d7fcc-8pv7r kubectl scale deployment hello-world-rest-api --replicas=1 kubectl get replicasets gcloud projects list kubectl delete service hello-world-rest-api kubectl delete deployment hello-world-rest-api gcloud container clusters delete my-cluster --zone us-central1-c kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE Create GKE cluster using below gcloud command: gcloud beta container clusters create test-cluster-1 \\ --zone=us-central1-a \\ --num-nodes=3 \\ --machine-type=n1-standard-1 \\ --disk-size=100 \\ --disk-type=pd-standard \\ --image-type=COS \\ --enable-autorepair \\ --enable-autoupgrade \\ --enable-autoscaling \\ --max-nodes=3 \\ --min-nodes=0 Google IAM commands \uf0c1 gcloud compute project-info describe gcloud auth list gcloud projects get-iam-policy glowing-furnace-304608 gcloud projects add-iam-policy-binding glowing-furnace-304608 --member=user:in28minutes@gmail.com --role=roles/storage.objectAdmin gcloud projects remove-iam-policy-binding glowing-furnace-304608 --member=user:in28minutes@gmail.com --role=roles/storage.objectAdmin gcloud iam roles describe roles/storage.objectAdmin gcloud iam roles copy --source=roles/storage.objectAdmin --destination=my.custom.role --dest-project=glowing-furnace-304608 Cloud SQL \uf0c1 gcloud sql connect my-first-cloud-sql-instance --user=root --quiet gcloud config set project glowing-furnace-304608 gcloud sql connect my-first-cloud-sql-instance --user=root --quiet use todos create table user (id integer, username varchar(30) ); describe user; insert into user values (1, 'Ranga'); select * from user Cloud BigTable \uf0c1 bq show bigquery-public-data:samples.shakespeare gcloud --version cbt listinstances -project=glowing-furnace-304608 echo project = glowing-furnace-304608 > ~/.cbtrc cat ~/.cbtrc cbt listinstances Pub/Sub \uf0c1 gcloud config set project glowing-furnace-304608 gcloud pubsub topics create topic-from-gcloud gcloud pubsub subscriptions create subscription-gcloud-1 --topic=topic-from-gcloud gcloud pubsub subscriptions create subscription-gcloud-2 --topic=topic-from-gcloud gcloud pubsub subscriptions pull subscription-gcloud-2 gcloud pubsub subscriptions pull subscription-gcloud-1 gcloud pubsub topics publish topic-from-gcloud --message=\"My First Message\" gcloud pubsub topics publish topic-from-gcloud --message=\"My Second Message\" gcloud pubsub topics publish topic-from-gcloud --message=\"My Third Message\" gcloud pubsub subscriptions pull subscription-gcloud-1 --auto-ack gcloud pubsub subscriptions pull subscription-gcloud-2 --auto-ack gcloud pubsub topics list gcloud pubsub topics delete topic-from-gcloud gcloud pubsub topics list-subscriptions my-first-topic (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"gcloud commands Cheatsheet"},{"location":"gcloud_cheatsheet/#gcloud-cli-commands-cheatsheet","text":"This Article consists of gcloud commands which can be handy when working on google cloud shell to manage/automate the infrastructure components. Official documentation can be found at gcloud official doc All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({});","title":"gcloud CLI commands Cheatsheet"},{"location":"gcloud_cheatsheet/#get-going-with-the-gcloud","text":"Initialize, authorize, and configure gcloud gcloud init Display version + installed components gcloud version Install specific components gcloud components install COMPONENT_NAME Update your Cloud SDK to the latest version gcloud components update Set a default Google Cloud project to work on gcloud config set project Display current gcloud environment details gcloud info * For apt-get and yum, see https://cloud.google.com/sdk/install","title":"Get going with the gcloud"},{"location":"gcloud_cheatsheet/#google-compute-engine-commands","text":"List zones in Region: gcloud compute zones list | grep us-central1 Set default zone for new vm: gcloud config set compute/zone us-central1-c To create a new VM: gcloud compute instances create \"my-vm-2\" \\ --machine-type \"n1-standard-1\" \\ --image-project \"debian-cloud\" \\ --image-family \"debian-10\" \\ --subnet \"default\" To create a Cloud Storage Bucket: gsutil mb -l US gs://$PROJECT_ID To list the items in Cloud Storage Bucket: gsutil ls gs://$PROJECT_ID To list the set project property in the core section: gcloud config list $PROJECT_ID To list gcloud named configurations: gcloud config configurations list To create a new named configuration: gcloud config configurations create my-second-config To activate sepcific named configuration: gcloud config configurations activate my-default-configuration To list the properties of the specified section using the active configuration: gcloud config list To see your currently available accounts used for gcloud authentication: gcloud config list account To describe a named configuration by listing its properties: gcloud config configurations describe my-second-configuration To display all Compute Engine instances in a project: gcloud compute instances list To create Compute Engine virtual machine instances: gcloud compute instances create To display all data associated with a Compute Engine virtual machine instance: gcloud compute instances describe my-first-instance-from-gcloud To delete Compute Engine virtual machine instances: gcloud compute instances delete my-first-instance-from-gcloud To display all Google Compute Engine zones in a project: gcloud compute zones list To display all Google Compute Engine regions in a project: gcloud compute regions list To display all Google Compute Engine machine types in a project: gcloud compute machine-types list To display all Google Compute Engine machine types in Zone asia-southeast2-b : gcloud compute machine-types list --filter zone:asia-southeast2-b To filter the Google Compute Engine machine types in multiple zones: gcloud compute machine-types list --filter \"zone:(asia-southeast2-b asia-southeast2-c)\" To display all data associated with a Compute Engine region: gcloud compute regions describe $REGION To list Google Compute Engine instance templates: gcloud compute instance-templates list To create a Compute Engine virtual machine instance template: gcloud compute instance-templates create instance-template-from-command-line To delete one or more Compute Engine virtual machine instance templates: gcloud compute instance-templates delete instance-template-from-command-line To display all data associated with a Google Compute Engine virtual machine instance template: gcloud compute instance-templates describe my-instance-template-with-custom-image To list Google Compute Engine managed instance groups: gcloud compute instance-groups managed list To delete Compute Engine managed instance groups: gcloud compute instance-groups managed delete my-managed-instance-group To create a Compute Engine managed instance group: gcloud compute instance-groups managed create my-mig --zone us-central1-a --template my-instance-template \\ --size 1 To set autoscaling parameters of a managed instance group: gcloud compute instance-groups managed set-autoscaling my-mig --max-num-replicas=2 --zone us-central1-a To stop autoscaling a managed instance group: gcloud compute instance-groups managed stop-autoscaling my-mig --zone us-central1-a To set managed instance group size: gcloud compute instance-groups managed resize my-mig --size=1 --zone=us-central1-a To recreate instances managed by a managed instance group: gcloud compute instance-groups managed recreate-instances my-mig --instances=my-mig-85fb --zone us-central1-a To delete Compute Engine managed instance groups: gcloud compute instance-groups managed delete my-managed-instance-group --region=us-central1","title":"Google Compute Engine commands"},{"location":"gcloud_cheatsheet/#troubleshooting-instance-ssh-issue","text":"recommendations, rerun the ssh command with the --troubleshoot option. gcloud compute ssh instance-1 --project=smitis-dev --zone=us-central1-a --troubleshoot Or, to investigate an IAP tunneling issue: gcloud compute ssh instance-1 --project=smitis-dev --zone=us-central1-a --troubleshoot --tunnel-through-iap","title":"Troubleshooting instance ssh issue"},{"location":"gcloud_cheatsheet/#google-app-engine-commands","text":"cd default-service gcloud app deploy gcloud app services list gcloud app versions list gcloud app instances list gcloud app deploy --version=v2 gcloud app versions list gcloud app versions list --hide-no-traffic ==> DO not show version receiving no traffic. gcloud app browse gcloud app browse --version 20210215t072907 gcloud app deploy --version=v3 --no-promote gcloud app browse --version v3 gcloud app services set-traffic split=v3=.5,v2=.5 gcloud app services set-traffic splits=v3=.5,v2=.5 watch curl https://melodic-furnace-304906.uc.r.appspot.com/ gcloud app services set-traffic --splits=v3=.5,v2=.5 --split-by=random cd ../my-first-service/ gcloud app deploy gcloud app browse --service=my-first-service gcloud app services list gcloud app regions list gcloud app browse --service=my-first-service --version=20210215t075851 gcloud app browse --version=v2 gcloud app open-console --version=v2 gcloud app versions list --hide-no-traffic","title":"Google App Engine commands"},{"location":"gcloud_cheatsheet/#google-kubernetes-engine-commands","text":"gcloud config set project my-kubernetes-project-304910 gcloud container clusters get-credentials my-cluster --zone us-central1-c --project my-kubernetes-project-304910 kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE kubectl get deployment kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080 kubectl get services kubectl get services --watch curl 35.184.204.214:8080/hello-world kubectl scale deployment hello-world-rest-api --replicas=3 gcloud container clusters resize my-cluster --node-pool default-pool --num-nodes=2 --zone=us-central1-c kubectl autoscale deployment hello-world-rest-api --max=4 --cpu-percent=70 kubectl get hpa kubectl create configmap hello-world-config --from-literal=RDS_DB_NAME=todos kubectl get configmap kubectl describe configmap hello-world-config kubectl create secret generic hello-world-secrets-1 --from-literal=RDS_PASSWORD=dummytodos kubectl get secret kubectl describe secret hello-world-secrets-1 kubectl apply -f deployment.yaml gcloud container node-pools list --zone=us-central1-c --cluster=my-cluster kubectl get pods -o wide kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.2.RELEASE kubectl get services kubectl get replicasets kubectl get pods kubectl delete pod hello-world-rest-api-58dc9d7fcc-8pv7r kubectl scale deployment hello-world-rest-api --replicas=1 kubectl get replicasets gcloud projects list kubectl delete service hello-world-rest-api kubectl delete deployment hello-world-rest-api gcloud container clusters delete my-cluster --zone us-central1-c kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE Create GKE cluster using below gcloud command: gcloud beta container clusters create test-cluster-1 \\ --zone=us-central1-a \\ --num-nodes=3 \\ --machine-type=n1-standard-1 \\ --disk-size=100 \\ --disk-type=pd-standard \\ --image-type=COS \\ --enable-autorepair \\ --enable-autoupgrade \\ --enable-autoscaling \\ --max-nodes=3 \\ --min-nodes=0","title":"Google Kubernetes Engine commands"},{"location":"gcloud_cheatsheet/#google-iam-commands","text":"gcloud compute project-info describe gcloud auth list gcloud projects get-iam-policy glowing-furnace-304608 gcloud projects add-iam-policy-binding glowing-furnace-304608 --member=user:in28minutes@gmail.com --role=roles/storage.objectAdmin gcloud projects remove-iam-policy-binding glowing-furnace-304608 --member=user:in28minutes@gmail.com --role=roles/storage.objectAdmin gcloud iam roles describe roles/storage.objectAdmin gcloud iam roles copy --source=roles/storage.objectAdmin --destination=my.custom.role --dest-project=glowing-furnace-304608","title":"Google IAM commands"},{"location":"gcloud_cheatsheet/#cloud-sql","text":"gcloud sql connect my-first-cloud-sql-instance --user=root --quiet gcloud config set project glowing-furnace-304608 gcloud sql connect my-first-cloud-sql-instance --user=root --quiet use todos create table user (id integer, username varchar(30) ); describe user; insert into user values (1, 'Ranga'); select * from user","title":"Cloud SQL"},{"location":"gcloud_cheatsheet/#cloud-bigtable","text":"bq show bigquery-public-data:samples.shakespeare gcloud --version cbt listinstances -project=glowing-furnace-304608 echo project = glowing-furnace-304608 > ~/.cbtrc cat ~/.cbtrc cbt listinstances","title":"Cloud BigTable"},{"location":"gcloud_cheatsheet/#pubsub","text":"gcloud config set project glowing-furnace-304608 gcloud pubsub topics create topic-from-gcloud gcloud pubsub subscriptions create subscription-gcloud-1 --topic=topic-from-gcloud gcloud pubsub subscriptions create subscription-gcloud-2 --topic=topic-from-gcloud gcloud pubsub subscriptions pull subscription-gcloud-2 gcloud pubsub subscriptions pull subscription-gcloud-1 gcloud pubsub topics publish topic-from-gcloud --message=\"My First Message\" gcloud pubsub topics publish topic-from-gcloud --message=\"My Second Message\" gcloud pubsub topics publish topic-from-gcloud --message=\"My Third Message\" gcloud pubsub subscriptions pull subscription-gcloud-1 --auto-ack gcloud pubsub subscriptions pull subscription-gcloud-2 --auto-ack gcloud pubsub topics list gcloud pubsub topics delete topic-from-gcloud gcloud pubsub topics list-subscriptions my-first-topic (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Pub/Sub"},{"location":"interview/","text":"Linux \uf0c1 Linux Interview Questions and Answers for Freshers \uf0c1 Linux Interview Questions and Answers for Freshers-2 \uf0c1 Linux L1 interview Questions and Answers \uf0c1 Linux L2 interview Questions and Answers \uf0c1 Advanced Linux L3 interview Questions and Answers \uf0c1 Solved list of Linux interview Questions \uf0c1 (adsbygoogle = window.adsbygoogle || []).push({}); OS Networking \uf0c1 OS Network interview Questions and Answers \uf0c1 Google Cloud Platform(GCP) \uf0c1 GCP ACE Practice Questions - set 1 \uf0c1 GCP ACE Practice Questions - set 2 \uf0c1 GCP ACE Practice Questions - set 3 \uf0c1 AWS \uf0c1 AWS Certified SysOps Administrator Questions and Answers - 1 \uf0c1 AWS Certified SysOps Administrator Questions and Answers - 2 \uf0c1 AWS Interview Questions and Answers - 1 \uf0c1 AWS Interview Questions and Answers - 2 \uf0c1 DevOps \uf0c1 DevOps Interview Questions and Answers for Freshers and Experienced-1 \uf0c1 DevOps Interview Questions and Answers for Freshers and Experienced-2 \uf0c1 Terraform \uf0c1 Terraform Interview Questions and Answers - 1 \uf0c1 Terraform Interview Questions and Answers - 2 \uf0c1 Terraform Interview Questions and Answers - 3 \uf0c1 GIT \uf0c1 GIT Interview Questions and Answers \uf0c1 Jenkins \uf0c1 Jenkins Interview Questions and Answers \uf0c1 Docker \uf0c1 Docker Interview Questions and Answers \uf0c1 Azure \uf0c1 Azure Interview Questions and Answers \uf0c1 (adsbygoogle = window.adsbygoogle || []).push({}); Ansible \uf0c1 Ansible Interview Questions and Answers \uf0c1 Kubernetes \uf0c1 Kubernetes Interview Questions and Answers \uf0c1 Shell-Scripting \uf0c1 Shell-Scripting Interview Questions and Answers \uf0c1 JAVA \uf0c1 Java/OOPs Interview Questions and Answers \uf0c1 Java/OOPs Interview Questions and Answers - part 2 \uf0c1 Manual Testing / Quality Assurance \uf0c1 Manual Testing/QA Interview Questions and Answers \uf0c1 Database Management System [DBMS] \uf0c1 DBMS Interview Questions and Answers \uf0c1 Managerial \uf0c1 Managerial Interview Questions and Answers \uf0c1 (adsbygoogle = window.adsbygoogle || []).push({});","title":"Linux"},{"location":"interview/#linux","text":"","title":"Linux"},{"location":"interview/#linux-interview-questions-and-answers-for-freshers","text":"","title":"Linux Interview Questions and Answers for Freshers"},{"location":"interview/#linux-interview-questions-and-answers-for-freshers-2","text":"","title":"Linux Interview Questions and Answers for Freshers-2"},{"location":"interview/#linux-l1-interview-questions-and-answers","text":"","title":"Linux L1 interview Questions and Answers"},{"location":"interview/#linux-l2-interview-questions-and-answers","text":"","title":"Linux L2 interview Questions and Answers"},{"location":"interview/#advanced-linux-l3-interview-questions-and-answers","text":"","title":"Advanced Linux L3 interview Questions and Answers"},{"location":"interview/#solved-list-of-linux-interview-questions","text":"(adsbygoogle = window.adsbygoogle || []).push({});","title":"Solved list of Linux interview Questions"},{"location":"interview/#os-networking","text":"","title":"OS Networking"},{"location":"interview/#os-network-interview-questions-and-answers","text":"","title":"OS Network interview Questions and Answers"},{"location":"interview/#google-cloud-platformgcp","text":"","title":"Google Cloud Platform(GCP)"},{"location":"interview/#gcp-ace-practice-questions-set-1","text":"","title":"GCP ACE Practice Questions - set 1"},{"location":"interview/#gcp-ace-practice-questions-set-2","text":"","title":"GCP ACE Practice Questions - set 2"},{"location":"interview/#gcp-ace-practice-questions-set-3","text":"","title":"GCP ACE Practice Questions - set 3"},{"location":"interview/#aws","text":"","title":"AWS"},{"location":"interview/#aws-certified-sysops-administrator-questions-and-answers-1","text":"","title":"AWS Certified SysOps Administrator Questions and Answers - 1"},{"location":"interview/#aws-certified-sysops-administrator-questions-and-answers-2","text":"","title":"AWS Certified SysOps Administrator Questions and Answers - 2"},{"location":"interview/#aws-interview-questions-and-answers-1","text":"","title":"AWS Interview Questions and Answers - 1"},{"location":"interview/#aws-interview-questions-and-answers-2","text":"","title":"AWS Interview Questions and Answers - 2"},{"location":"interview/#devops","text":"","title":"DevOps"},{"location":"interview/#devops-interview-questions-and-answers-for-freshers-and-experienced-1","text":"","title":"DevOps Interview Questions and Answers for Freshers and Experienced-1"},{"location":"interview/#devops-interview-questions-and-answers-for-freshers-and-experienced-2","text":"","title":"DevOps Interview Questions and Answers for Freshers and Experienced-2"},{"location":"interview/#terraform","text":"","title":"Terraform"},{"location":"interview/#terraform-interview-questions-and-answers-1","text":"","title":"Terraform Interview Questions and Answers - 1"},{"location":"interview/#terraform-interview-questions-and-answers-2","text":"","title":"Terraform Interview Questions and Answers - 2"},{"location":"interview/#terraform-interview-questions-and-answers-3","text":"","title":"Terraform Interview Questions and Answers - 3"},{"location":"interview/#git","text":"","title":"GIT"},{"location":"interview/#git-interview-questions-and-answers","text":"","title":"GIT Interview Questions and Answers"},{"location":"interview/#jenkins","text":"","title":"Jenkins"},{"location":"interview/#jenkins-interview-questions-and-answers","text":"","title":"Jenkins Interview Questions and Answers"},{"location":"interview/#docker","text":"","title":"Docker"},{"location":"interview/#docker-interview-questions-and-answers","text":"","title":"Docker Interview Questions and Answers"},{"location":"interview/#azure","text":"","title":"Azure"},{"location":"interview/#azure-interview-questions-and-answers","text":"(adsbygoogle = window.adsbygoogle || []).push({});","title":"Azure Interview Questions and Answers"},{"location":"interview/#ansible","text":"","title":"Ansible"},{"location":"interview/#ansible-interview-questions-and-answers","text":"","title":"Ansible Interview Questions and Answers"},{"location":"interview/#kubernetes","text":"","title":"Kubernetes"},{"location":"interview/#kubernetes-interview-questions-and-answers","text":"","title":"Kubernetes Interview Questions and Answers"},{"location":"interview/#shell-scripting","text":"","title":"Shell-Scripting"},{"location":"interview/#shell-scripting-interview-questions-and-answers","text":"","title":"Shell-Scripting Interview Questions and Answers"},{"location":"interview/#java","text":"","title":"JAVA"},{"location":"interview/#javaoops-interview-questions-and-answers","text":"","title":"Java/OOPs Interview Questions and Answers"},{"location":"interview/#javaoops-interview-questions-and-answers-part-2","text":"","title":"Java/OOPs Interview Questions and Answers - part 2"},{"location":"interview/#manual-testing-quality-assurance","text":"","title":"Manual Testing / Quality Assurance"},{"location":"interview/#manual-testingqa-interview-questions-and-answers","text":"","title":"Manual Testing/QA Interview Questions and Answers"},{"location":"interview/#database-management-system-dbms","text":"","title":"Database Management System [DBMS]"},{"location":"interview/#dbms-interview-questions-and-answers","text":"","title":"DBMS Interview Questions and Answers"},{"location":"interview/#managerial","text":"","title":"Managerial"},{"location":"interview/#managerial-interview-questions-and-answers","text":"(adsbygoogle = window.adsbygoogle || []).push({});","title":"Managerial Interview Questions and Answers"},{"location":"linux/","text":"Work in Progress \uf0c1 This is a dummy file for now which will act as placeholder for later.","title":"Linux"},{"location":"linux/#work-in-progress","text":"This is a dummy file for now which will act as placeholder for later.","title":"Work in Progress"},{"location":"mongo/","text":"mongoDB \uf0c1 We will use this article to post interesting mongoDB concepts. MongoDB University is a good place to start. If you are looking for official docs, please refer to MongoDB Docs (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"mongoDB"},{"location":"mongo/#mongodb","text":"We will use this article to post interesting mongoDB concepts. MongoDB University is a good place to start. If you are looking for official docs, please refer to MongoDB Docs (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"mongoDB"},{"location":"mysql/","text":"MySql Tuner \uf0c1 Below script can help you in tunning your MYSQL database for best performance. Please note that do not blindly trust what this script says. Always think before you impliment the recommendations. wget https://raw.github.com/major/MySQLTuner-perl/master/mysqltuner.pl; chmod +x mysqltuner.pl; ./mysqltuner.pl The output of this script will look like: [root@linux~]# ./mysqltuner.pl >> MySQLTuner 1.4.0 - Major Hayden <major@mhtx.net> >> Bug reports, feature requests, and downloads at http://mysqltuner.com/ >> Run with '--help' for additional options and output filtering [OK] Currently running supported MySQL version 5.1.73 [OK] Operating on 64-bit architecture -------- Storage Engine Statistics ------------------------------------------- [--] Status: +CSV +InnoDB +MRG_MYISAM [!!] InnoDB is enabled but isn't being used [OK] Total fragmented tables: 0 -------- Security Recommendations ------------------------------------------- [!!] User '@bash-test' has no password set. [!!] User '@localhost' has no password set. -------- Performance Metrics ------------------------------------------------- [--] Up for: 44m 11s (13 q [0.005 qps], 7 conn, TX: 16K, RX: 662) [--] Reads / Writes: 100% / 0% [--] Total buffers: 34.0M global + 2.7M per thread (1000 max threads) [!!] Maximum possible memory usage: 2.7G (280% of installed RAM) [OK] Slow queries: 0% (0/13) [OK] Highest usage of available connections: 0% (1/1000) [OK] Key buffer size / total MyISAM indexes: 8.0M/90.0K [!!] Query cache is disabled [OK] Temporary tables created on disk: 0% (0 on disk / 3 total) [!!] Thread cache is disabled [OK] Table cache hit rate: 53% (8 open / 15 opened) [OK] Open file limit used: 0% (16/5K) [OK] Table locks acquired immediately: 100% (18 immediate / 18 locks) -------- Recommendations ----------------------------------------------------- General recommendations: Add skip-innodb to MySQL configuration to disable InnoDB MySQL started within last 24 hours - recommendations may be inaccurate Reduce your overall MySQL memory footprint for system stability Enable the slow query log to troubleshoot bad queries Set thread_cache_size to 4 as a starting value Variables to adjust: *** MySQL's maximum memory usage is dangerously high *** *** Add RAM before increasing MySQL buffer variables *** query_cache_size (>= 8M) thread_cache_size (start at 4) (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Mysql"},{"location":"mysql/#mysql-tuner","text":"Below script can help you in tunning your MYSQL database for best performance. Please note that do not blindly trust what this script says. Always think before you impliment the recommendations. wget https://raw.github.com/major/MySQLTuner-perl/master/mysqltuner.pl; chmod +x mysqltuner.pl; ./mysqltuner.pl The output of this script will look like: [root@linux~]# ./mysqltuner.pl >> MySQLTuner 1.4.0 - Major Hayden <major@mhtx.net> >> Bug reports, feature requests, and downloads at http://mysqltuner.com/ >> Run with '--help' for additional options and output filtering [OK] Currently running supported MySQL version 5.1.73 [OK] Operating on 64-bit architecture -------- Storage Engine Statistics ------------------------------------------- [--] Status: +CSV +InnoDB +MRG_MYISAM [!!] InnoDB is enabled but isn't being used [OK] Total fragmented tables: 0 -------- Security Recommendations ------------------------------------------- [!!] User '@bash-test' has no password set. [!!] User '@localhost' has no password set. -------- Performance Metrics ------------------------------------------------- [--] Up for: 44m 11s (13 q [0.005 qps], 7 conn, TX: 16K, RX: 662) [--] Reads / Writes: 100% / 0% [--] Total buffers: 34.0M global + 2.7M per thread (1000 max threads) [!!] Maximum possible memory usage: 2.7G (280% of installed RAM) [OK] Slow queries: 0% (0/13) [OK] Highest usage of available connections: 0% (1/1000) [OK] Key buffer size / total MyISAM indexes: 8.0M/90.0K [!!] Query cache is disabled [OK] Temporary tables created on disk: 0% (0 on disk / 3 total) [!!] Thread cache is disabled [OK] Table cache hit rate: 53% (8 open / 15 opened) [OK] Open file limit used: 0% (16/5K) [OK] Table locks acquired immediately: 100% (18 immediate / 18 locks) -------- Recommendations ----------------------------------------------------- General recommendations: Add skip-innodb to MySQL configuration to disable InnoDB MySQL started within last 24 hours - recommendations may be inaccurate Reduce your overall MySQL memory footprint for system stability Enable the slow query log to troubleshoot bad queries Set thread_cache_size to 4 as a starting value Variables to adjust: *** MySQL's maximum memory usage is dangerously high *** *** Add RAM before increasing MySQL buffer variables *** query_cache_size (>= 8M) thread_cache_size (start at 4) (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"MySql Tuner"},{"location":"network/","text":"Work in Progress \uf0c1 This is a dummy file for now which will act as placeholder for later.","title":"Network"},{"location":"network/#work-in-progress","text":"This is a dummy file for now which will act as placeholder for later.","title":"Work in Progress"},{"location":"reference/","text":"References \uf0c1 Some questions are 'borrowed' from other great references like: https://github.com/darcyclarke/Front-end-Developer-Interview-Questions https://github.com/kylejohnson/linux-sysadmin-interview-questions/blob/master/test.md http://slideshare.net/kavyasri790693/linux-admin-interview-questions","title":"Reference"},{"location":"reference/#references","text":"Some questions are 'borrowed' from other great references like: https://github.com/darcyclarke/Front-end-Developer-Interview-Questions https://github.com/kylejohnson/linux-sysadmin-interview-questions/blob/master/test.md http://slideshare.net/kavyasri790693/linux-admin-interview-questions","title":"References"},{"location":"articles/ai_chatbot/","text":"AI Chat Open Assistant Chatbot: Transforming Conversations with AI-Powered Interaction \uf0c1 What is AI Chat Open Assistant Chatbot ? In the age of technological marvels, artificial intelligence (AI) has emerged as a driving force, reshaping industries and revolutionizing the way we interact with machines. One such innovation that stands out is the AI Chat Open Assistant Chatbot\u2014a versatile and intelligent conversational agent that is redefining human-computer interactions in unprecedented ways. With its advanced capabilities, contextual understanding, and user-centric approach, this chatbot is changing the game of online communication. Understanding the AI Chat Open Assistant Chatbot: At its core, the AI Chat Open Assistant Chatbot is an embodiment of AI's potential in natural language processing and understanding. Powered by cutting-edge technologies like deep learning and neural networks, it can engage users in human-like conversations. What sets it apart is its ability to comprehend context, discern user intent, and generate relevant responses that mirror genuine human interaction. Key Features and Capabilities: Contextual Intelligence: The AI Chat Open Assistant Chatbot excels in maintaining context throughout a conversation. This unique trait enables it to remember past interactions, leading to coherent and meaningful dialogues. Whether discussing complex topics or casual banter, the chatbot responds with impressive consistency and relevance. Multilingual Proficiency: Breaking language barriers, the chatbot boasts multilingual capabilities that allow it to converse effectively with users from diverse linguistic backgrounds. This inclusivity significantly broadens its reach and user base. Diverse Domain Expertise: The chatbot isn't confined to a narrow knowledge niche. It has been meticulously trained on a wide spectrum of subjects, ranging from general knowledge to specialized fields such as science, technology, literature, and more. This comprehensive knowledge base empowers the chatbot to assist users across a vast array of queries. Real-time Updates: An invaluable feature is the chatbot's ability to provide real-time information updates, making it a reliable source for news, current events, and developments. Personalization at Scale: Through continuous user interactions, the chatbot learns about individual preferences and tailors its responses accordingly. This personalized touch creates a sense of familiarity, leading to a more enriching user experience. (adsbygoogle = window.adsbygoogle || []).push({}); Applications Across Industries: Elevated Customer Support: Businesses are capitalizing on the AI Chat Open Assistant Chatbot to provide seamless and 24/7 customer support. The chatbot efficiently addresses common queries, guides users through troubleshooting processes, and seamlessly escalates intricate issues to human agents when necessary. Education Reinvented: In the realm of education, the chatbot doubles as a virtual tutor, aiding students by answering queries and explaining concepts. It offers supplementary learning materials and engages in interactive learning experiences, fostering a dynamic learning environment. Instant Information Access: The chatbot serves as a quick and dependable source for information retrieval. Whether users are seeking definitions, historical insights, or scientific explanations, the chatbot acts as a valuable wellspring of knowledge. Tailored Content Recommendations: Leveraging its understanding of user preferences, the chatbot can provide tailored content recommendations, from suggesting books and movies to music and articles. Ethical Considerations and Accountability: As AI-driven technology progresses, ethical considerations are paramount. Concerns about data privacy, response biases, and potential misuse must be proactively addressed. Ensuring transparency about the chatbot's abilities and limitations, alongside mechanisms that allow users to identify the interaction as AI-mediated, is crucial. Final Thoughts: The AI Chat Open Assistant Chatbot exemplifies a pivotal stride in AI-driven conversational technology. Its prowess in context retention, multilingualism, and multi-domain expertise marks a new era in human-computer interactions. While innovation is exciting, maintaining ethical responsibility and user trust is equally important. As the AI chatbot continues to shape our digital interactions, it holds the potential to enhance our lives while upholding transparency and ethical values. For more information, visit AI Chat Open Assistant Chatbot . Disclaimer: This article is for informational purposes only. The AI Chat Open Assistant Chatbot may not be directly represented as described here. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"What is AI Chat Open Assistant Chatbot ?"},{"location":"articles/ai_chatbot/#ai-chat-open-assistant-chatbot-transforming-conversations-with-ai-powered-interaction","text":"What is AI Chat Open Assistant Chatbot ? In the age of technological marvels, artificial intelligence (AI) has emerged as a driving force, reshaping industries and revolutionizing the way we interact with machines. One such innovation that stands out is the AI Chat Open Assistant Chatbot\u2014a versatile and intelligent conversational agent that is redefining human-computer interactions in unprecedented ways. With its advanced capabilities, contextual understanding, and user-centric approach, this chatbot is changing the game of online communication. Understanding the AI Chat Open Assistant Chatbot: At its core, the AI Chat Open Assistant Chatbot is an embodiment of AI's potential in natural language processing and understanding. Powered by cutting-edge technologies like deep learning and neural networks, it can engage users in human-like conversations. What sets it apart is its ability to comprehend context, discern user intent, and generate relevant responses that mirror genuine human interaction. Key Features and Capabilities: Contextual Intelligence: The AI Chat Open Assistant Chatbot excels in maintaining context throughout a conversation. This unique trait enables it to remember past interactions, leading to coherent and meaningful dialogues. Whether discussing complex topics or casual banter, the chatbot responds with impressive consistency and relevance. Multilingual Proficiency: Breaking language barriers, the chatbot boasts multilingual capabilities that allow it to converse effectively with users from diverse linguistic backgrounds. This inclusivity significantly broadens its reach and user base. Diverse Domain Expertise: The chatbot isn't confined to a narrow knowledge niche. It has been meticulously trained on a wide spectrum of subjects, ranging from general knowledge to specialized fields such as science, technology, literature, and more. This comprehensive knowledge base empowers the chatbot to assist users across a vast array of queries. Real-time Updates: An invaluable feature is the chatbot's ability to provide real-time information updates, making it a reliable source for news, current events, and developments. Personalization at Scale: Through continuous user interactions, the chatbot learns about individual preferences and tailors its responses accordingly. This personalized touch creates a sense of familiarity, leading to a more enriching user experience. (adsbygoogle = window.adsbygoogle || []).push({}); Applications Across Industries: Elevated Customer Support: Businesses are capitalizing on the AI Chat Open Assistant Chatbot to provide seamless and 24/7 customer support. The chatbot efficiently addresses common queries, guides users through troubleshooting processes, and seamlessly escalates intricate issues to human agents when necessary. Education Reinvented: In the realm of education, the chatbot doubles as a virtual tutor, aiding students by answering queries and explaining concepts. It offers supplementary learning materials and engages in interactive learning experiences, fostering a dynamic learning environment. Instant Information Access: The chatbot serves as a quick and dependable source for information retrieval. Whether users are seeking definitions, historical insights, or scientific explanations, the chatbot acts as a valuable wellspring of knowledge. Tailored Content Recommendations: Leveraging its understanding of user preferences, the chatbot can provide tailored content recommendations, from suggesting books and movies to music and articles. Ethical Considerations and Accountability: As AI-driven technology progresses, ethical considerations are paramount. Concerns about data privacy, response biases, and potential misuse must be proactively addressed. Ensuring transparency about the chatbot's abilities and limitations, alongside mechanisms that allow users to identify the interaction as AI-mediated, is crucial. Final Thoughts: The AI Chat Open Assistant Chatbot exemplifies a pivotal stride in AI-driven conversational technology. Its prowess in context retention, multilingualism, and multi-domain expertise marks a new era in human-computer interactions. While innovation is exciting, maintaining ethical responsibility and user trust is equally important. As the AI chatbot continues to shape our digital interactions, it holds the potential to enhance our lives while upholding transparency and ethical values. For more information, visit AI Chat Open Assistant Chatbot . Disclaimer: This article is for informational purposes only. The AI Chat Open Assistant Chatbot may not be directly represented as described here. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"AI Chat Open Assistant Chatbot: Transforming Conversations with AI-Powered Interaction"},{"location":"articles/ai_intro/","text":"What is Artificial Intelligence(AI) ? \uf0c1 I am sure, the question \"What is Artificial Intelligence (AI)?\" has come up in front of us many times and we must be wondering to find more about this AI thing. Let me help your query with below short introduction to AI. Artificial Intelligence (AI) is a term that has gained significant attention and popularity in recent years. It refers to the development and implementation of intelligent systems that can perform tasks requiring human-like intelligence. AI aims to simulate human cognition, reasoning, problem-solving, and decision-making processes, using algorithms and data analysis to achieve these objectives. AI systems are designed to think and learn like humans, enabling them to process vast amounts of information, analyze patterns, and make predictions or decisions based on the available data. By leveraging advanced algorithms, AI systems can identify hidden insights and correlations that might not be apparent to human observers. Within the field of AI, there are several subfields that focus on specific aspects of intelligent systems. Machine learning, for example, is a critical component of AI, enabling systems to learn from data and improve their performance over time without explicit programming. Machine learning algorithms can recognize patterns in data and use them to make accurate predictions or classifications. Another subfield of AI is natural language processing (NLP), which focuses on enabling machines to understand and interact with human language. NLP algorithms enable chatbots, virtual assistants, and other AI systems to comprehend human commands, respond to inquiries, and generate human-like text. This has opened up new avenues for human-machine communication and interaction. Computer vision is yet another subfield of AI that involves teaching machines to interpret and understand visual information. With computer vision, AI systems can analyze images and videos, enabling applications such as facial recognition, object detection, and autonomous vehicles. This capability has transformed industries such as security, healthcare, and transportation. (adsbygoogle = window.adsbygoogle || []).push({}); Robotics is a branch of AI that combines intelligence with physical systems. AI-powered robots are capable of performing tasks autonomously or in collaboration with humans. They can handle repetitive or dangerous tasks in industries like manufacturing and logistics, freeing up human workers for more complex and creative endeavors. The applications of AI are vast and span across various industries. In healthcare, AI is being used for diagnosis assistance, drug discovery, and personalized medicine. In finance, AI algorithms aid in fraud detection, algorithmic trading, and risk assessment. AI is revolutionizing transportation with self-driving cars and optimizing supply chain management in manufacturing. Furthermore, AI is enhancing entertainment experiences through recommendation systems and immersive technologies. While the potential of AI is immense, it is essential to consider the ethical implications of its development and deployment. Privacy concerns, bias in algorithms, and the impact on employment are among the challenges that need to be addressed to ensure responsible and beneficial use of AI. In conclusion, Artificial Intelligence represents the development and application of intelligent systems that mimic human-like intelligence. Through machine learning, natural language processing, computer vision, and robotics, AI systems are revolutionizing various industries and reshaping the way we live and work. As AI continues to advance, it is crucial to balance its potential with ethical considerations to create a future where human intelligence and artificial intelligence can coexist and thrive. (adsbygoogle = window.adsbygoogle || []).push({}); If I have to write above in crisp bullet points, this is how I would write: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. AI involves the development of intelligent systems capable of performing tasks that typically require human intelligence, such as perception, reasoning, problem-solving, and decision-making. AI systems use algorithms and data to analyze, interpret, and extract meaningful insights from vast amounts of information. AI encompasses various subfields, including machine learning, natural language processing, computer vision, robotics, and expert systems. Machine learning is a key component of AI, allowing systems to learn from data and improve their performance over time without explicit programming. AI systems can process and understand human language, enabling communication with humans through chatbots and virtual assistants. Computer vision enables AI systems to analyze and interpret visual information, leading to applications such as facial recognition and object detection. Robotics combines AI and automation to create intelligent machines that can perform physical tasks and interact with the environment. AI has widespread applications across industries, including healthcare, finance, manufacturing, transportation, and entertainment. Ethical considerations surrounding AI, such as privacy, bias, and job displacement, require careful attention to ensure responsible development and deployment. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"What is Artificial Intelligence(AI) ?"},{"location":"articles/ai_intro/#what-is-artificial-intelligenceai","text":"I am sure, the question \"What is Artificial Intelligence (AI)?\" has come up in front of us many times and we must be wondering to find more about this AI thing. Let me help your query with below short introduction to AI. Artificial Intelligence (AI) is a term that has gained significant attention and popularity in recent years. It refers to the development and implementation of intelligent systems that can perform tasks requiring human-like intelligence. AI aims to simulate human cognition, reasoning, problem-solving, and decision-making processes, using algorithms and data analysis to achieve these objectives. AI systems are designed to think and learn like humans, enabling them to process vast amounts of information, analyze patterns, and make predictions or decisions based on the available data. By leveraging advanced algorithms, AI systems can identify hidden insights and correlations that might not be apparent to human observers. Within the field of AI, there are several subfields that focus on specific aspects of intelligent systems. Machine learning, for example, is a critical component of AI, enabling systems to learn from data and improve their performance over time without explicit programming. Machine learning algorithms can recognize patterns in data and use them to make accurate predictions or classifications. Another subfield of AI is natural language processing (NLP), which focuses on enabling machines to understand and interact with human language. NLP algorithms enable chatbots, virtual assistants, and other AI systems to comprehend human commands, respond to inquiries, and generate human-like text. This has opened up new avenues for human-machine communication and interaction. Computer vision is yet another subfield of AI that involves teaching machines to interpret and understand visual information. With computer vision, AI systems can analyze images and videos, enabling applications such as facial recognition, object detection, and autonomous vehicles. This capability has transformed industries such as security, healthcare, and transportation. (adsbygoogle = window.adsbygoogle || []).push({}); Robotics is a branch of AI that combines intelligence with physical systems. AI-powered robots are capable of performing tasks autonomously or in collaboration with humans. They can handle repetitive or dangerous tasks in industries like manufacturing and logistics, freeing up human workers for more complex and creative endeavors. The applications of AI are vast and span across various industries. In healthcare, AI is being used for diagnosis assistance, drug discovery, and personalized medicine. In finance, AI algorithms aid in fraud detection, algorithmic trading, and risk assessment. AI is revolutionizing transportation with self-driving cars and optimizing supply chain management in manufacturing. Furthermore, AI is enhancing entertainment experiences through recommendation systems and immersive technologies. While the potential of AI is immense, it is essential to consider the ethical implications of its development and deployment. Privacy concerns, bias in algorithms, and the impact on employment are among the challenges that need to be addressed to ensure responsible and beneficial use of AI. In conclusion, Artificial Intelligence represents the development and application of intelligent systems that mimic human-like intelligence. Through machine learning, natural language processing, computer vision, and robotics, AI systems are revolutionizing various industries and reshaping the way we live and work. As AI continues to advance, it is crucial to balance its potential with ethical considerations to create a future where human intelligence and artificial intelligence can coexist and thrive. (adsbygoogle = window.adsbygoogle || []).push({}); If I have to write above in crisp bullet points, this is how I would write: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. AI involves the development of intelligent systems capable of performing tasks that typically require human intelligence, such as perception, reasoning, problem-solving, and decision-making. AI systems use algorithms and data to analyze, interpret, and extract meaningful insights from vast amounts of information. AI encompasses various subfields, including machine learning, natural language processing, computer vision, robotics, and expert systems. Machine learning is a key component of AI, allowing systems to learn from data and improve their performance over time without explicit programming. AI systems can process and understand human language, enabling communication with humans through chatbots and virtual assistants. Computer vision enables AI systems to analyze and interpret visual information, leading to applications such as facial recognition and object detection. Robotics combines AI and automation to create intelligent machines that can perform physical tasks and interact with the environment. AI has widespread applications across industries, including healthcare, finance, manufacturing, transportation, and entertainment. Ethical considerations surrounding AI, such as privacy, bias, and job displacement, require careful attention to ensure responsible development and deployment. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"What is Artificial Intelligence(AI) ?"},{"location":"articles/ai_trends/","text":"Unleashing the Power of Artificial Intelligence \uf0c1 Exploring the Latest Trends and Applications \uf0c1 Introduction \uf0c1 Artificial Intelligence (AI) has rapidly transformed from a futuristic concept to an integral part of our daily lives. With breakthroughs in technology and the development of sophisticated algorithms, AI is revolutionizing industries and driving innovation across various sectors. In this article, we will delve into the latest trends and applications of AI, addressing the most searched keywords in the field. Let's embark on a journey to explore the potential and impact of AI in our ever-evolving world. Machine Learning \uf0c1 Machine Learning (ML) lies at the heart of AI, enabling systems to learn and improve from experience without being explicitly programmed. ML algorithms analyze vast amounts of data to identify patterns and make predictions or decisions. From personalized recommendations on streaming platforms to fraud detection in financial institutions, ML has become an indispensable tool for businesses. The advancements in deep learning algorithms have further propelled ML to new heights, allowing for complex tasks such as image and speech recognition, natural language processing, and autonomous vehicles. ML enables systems to learn and improve from experience without explicit programming. ML algorithms analyze data to identify patterns and make predictions. Applications: personalized recommendations, fraud detection, image and speech recognition, natural language processing, autonomous vehicles. (adsbygoogle = window.adsbygoogle || []).push({}); Neural Networks \uf0c1 Neural Networks mimic the structure and functioning of the human brain, consisting of interconnected layers of artificial neurons. This architecture enables the network to process and interpret complex data. Convolutional Neural Networks (CNNs) have revolutionized image and video analysis, playing a vital role in facial recognition, object detection, and medical imaging. Recurrent Neural Networks (RNNs) have enhanced natural language processing tasks, enabling accurate text generation, language translation, and sentiment analysis. The continuous advancements in neural network architectures are fueling innovation across numerous AI applications. Applications: facial recognition, object detection, medical imaging, text generation, language translation, sentiment analysis. Internet of Things (IoT) \uf0c1 The Internet of Things (IoT) refers to the network of interconnected devices embedded with sensors, software, and other technologies, enabling them to collect and exchange data. When combined with AI, IoT opens up a world of possibilities. AI algorithms can analyze the massive influx of data generated by IoT devices, leading to enhanced automation, predictive maintenance, and intelligent decision-making. Smart homes, connected vehicles, and industrial automation are just a few examples of how the convergence of AI and IoT is reshaping our lives. Natural Language Processing (NLP) \uf0c1 Natural Language Processing (NLP) focuses on enabling machines to understand, interpret, and respond to human language. Chatbots and virtual assistants have become commonplace, with NLP algorithms understanding and generating human-like text. Sentiment analysis algorithms are employed to gauge public opinion, and machine translation systems bridge language barriers. NLP's potential extends to healthcare, where AI-powered systems analyze medical records and assist in diagnosis, revolutionizing patient care. Robotics and Automation \uf0c1 Robotics and Automation are transforming industries by streamlining processes, increasing efficiency, and reducing human error. From manufacturing and logistics to healthcare and agriculture, robots equipped with AI capabilities are taking on repetitive tasks, freeing up human resources for more complex endeavors. Collaborative robots, or cobots, work alongside humans, enhancing productivity and safety. AI-powered automation is expected to create new job opportunities and foster economic growth in the coming years. Conclusion \uf0c1 Artificial Intelligence is driving innovation across multiple domains, revolutionizing the way we live and work. Machine Learning, Neural Networks, IoT, Natural Language Processing, and Robotics are among the most searched keywords in the AI landscape. The rapid advancement of these technologies is reshaping industries, transforming business models, and improving the quality of life. As AI continues to evolve, it is crucial to explore its ethical implications, ensuring responsible development and deployment. Embracing AI's potential while addressing its challenges will pave the way for a future where human intelligence combines harmoniously with artificial intelligence. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Unleashing the Power of Artificial Intelligence"},{"location":"articles/ai_trends/#unleashing-the-power-of-artificial-intelligence","text":"","title":"Unleashing the Power of Artificial Intelligence"},{"location":"articles/ai_trends/#exploring-the-latest-trends-and-applications","text":"","title":"Exploring the Latest Trends and Applications"},{"location":"articles/ai_trends/#introduction","text":"Artificial Intelligence (AI) has rapidly transformed from a futuristic concept to an integral part of our daily lives. With breakthroughs in technology and the development of sophisticated algorithms, AI is revolutionizing industries and driving innovation across various sectors. In this article, we will delve into the latest trends and applications of AI, addressing the most searched keywords in the field. Let's embark on a journey to explore the potential and impact of AI in our ever-evolving world.","title":"Introduction"},{"location":"articles/ai_trends/#machine-learning","text":"Machine Learning (ML) lies at the heart of AI, enabling systems to learn and improve from experience without being explicitly programmed. ML algorithms analyze vast amounts of data to identify patterns and make predictions or decisions. From personalized recommendations on streaming platforms to fraud detection in financial institutions, ML has become an indispensable tool for businesses. The advancements in deep learning algorithms have further propelled ML to new heights, allowing for complex tasks such as image and speech recognition, natural language processing, and autonomous vehicles. ML enables systems to learn and improve from experience without explicit programming. ML algorithms analyze data to identify patterns and make predictions. Applications: personalized recommendations, fraud detection, image and speech recognition, natural language processing, autonomous vehicles. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Machine Learning"},{"location":"articles/ai_trends/#neural-networks","text":"Neural Networks mimic the structure and functioning of the human brain, consisting of interconnected layers of artificial neurons. This architecture enables the network to process and interpret complex data. Convolutional Neural Networks (CNNs) have revolutionized image and video analysis, playing a vital role in facial recognition, object detection, and medical imaging. Recurrent Neural Networks (RNNs) have enhanced natural language processing tasks, enabling accurate text generation, language translation, and sentiment analysis. The continuous advancements in neural network architectures are fueling innovation across numerous AI applications. Applications: facial recognition, object detection, medical imaging, text generation, language translation, sentiment analysis.","title":"Neural Networks"},{"location":"articles/ai_trends/#internet-of-things-iot","text":"The Internet of Things (IoT) refers to the network of interconnected devices embedded with sensors, software, and other technologies, enabling them to collect and exchange data. When combined with AI, IoT opens up a world of possibilities. AI algorithms can analyze the massive influx of data generated by IoT devices, leading to enhanced automation, predictive maintenance, and intelligent decision-making. Smart homes, connected vehicles, and industrial automation are just a few examples of how the convergence of AI and IoT is reshaping our lives.","title":"Internet of Things (IoT)"},{"location":"articles/ai_trends/#natural-language-processing-nlp","text":"Natural Language Processing (NLP) focuses on enabling machines to understand, interpret, and respond to human language. Chatbots and virtual assistants have become commonplace, with NLP algorithms understanding and generating human-like text. Sentiment analysis algorithms are employed to gauge public opinion, and machine translation systems bridge language barriers. NLP's potential extends to healthcare, where AI-powered systems analyze medical records and assist in diagnosis, revolutionizing patient care.","title":"Natural Language Processing (NLP)"},{"location":"articles/ai_trends/#robotics-and-automation","text":"Robotics and Automation are transforming industries by streamlining processes, increasing efficiency, and reducing human error. From manufacturing and logistics to healthcare and agriculture, robots equipped with AI capabilities are taking on repetitive tasks, freeing up human resources for more complex endeavors. Collaborative robots, or cobots, work alongside humans, enhancing productivity and safety. AI-powered automation is expected to create new job opportunities and foster economic growth in the coming years.","title":"Robotics and Automation"},{"location":"articles/ai_trends/#conclusion","text":"Artificial Intelligence is driving innovation across multiple domains, revolutionizing the way we live and work. Machine Learning, Neural Networks, IoT, Natural Language Processing, and Robotics are among the most searched keywords in the AI landscape. The rapid advancement of these technologies is reshaping industries, transforming business models, and improving the quality of life. As AI continues to evolve, it is crucial to explore its ethical implications, ensuring responsible development and deployment. Embracing AI's potential while addressing its challenges will pave the way for a future where human intelligence combines harmoniously with artificial intelligence. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Conclusion"},{"location":"articles/bard/","text":"Bard Chatbot : Redefining Conversations through AI-Powered Storytelling \uf0c1 What is bard chatbot ? In the digital age, as technology continues to advance, artificial intelligence (AI) has found its way into various aspects of our lives, including the way we communicate. The emergence of chatbots has transformed how we interact with machines, making tasks easier and more efficient. Among these innovative chatbots, the \"Bard Chatbot\" stands out as a creative and imaginative AI-driven conversational agent that brings the art of storytelling into the digital realm. Unveiling the Bard Chatbot: The Bard Chatbot is not just your ordinary conversational AI. It's a virtual storyteller designed to engage users in immersive and captivating narratives. Built on sophisticated natural language processing (NLP) and machine learning technologies, this chatbot goes beyond providing information or answering questions \u2013 it weaves stories, creates adventures, and offers users a unique form of interactive entertainment. Key Features and Capabilities: Storytelling Expertise: At the heart of the Bard Chatbot lies its ability to craft stories that capture the imagination. Whether it's fantasy, mystery, romance, or science fiction, the chatbot generates narratives that transport users to new worlds and experiences. Interactive Engagement: Unlike reading a traditional story, the Bard Chatbot encourages user participation. It allows users to make decisions and choices that influence the direction of the narrative, creating a personalized and engaging storytelling experience. Natural Language Understanding: The chatbot's advanced NLP capabilities enable it to understand user inputs, prompts, and context. This ensures that the generated stories remain coherent and relevant to the user's preferences. Diverse Genres: From classic tales to contemporary genres, the Bard Chatbot offers a wide range of storytelling options. Users can explore different themes and styles, catering to their individual interests. Continuous Learning: The more users interact with the Bard Chatbot, the better it becomes at tailoring its stories to their preferences. This learning process enhances the overall user experience by providing increasingly immersive narratives. (adsbygoogle = window.adsbygoogle || []).push({}); Applications and Impact: Entertainment and Creativity: The Bard Chatbot brings a new level of creativity and entertainment to the world of AI. It serves as a source of amusement, allowing users to escape into fictional realms and play an active role in shaping the story's outcome. Educational Tool: Beyond entertainment, the chatbot can serve as an educational tool. Teachers can use it to engage students in language arts activities, encouraging them to explore storytelling techniques and concepts. Personalized Experiences: In a world where personalization is valued, the Bard Chatbot stands out by tailoring its narratives to individual preferences. This customization enhances user engagement and satisfaction. Looking Ahead: As technology continues to evolve, the Bard Chatbot holds the potential to expand its capabilities even further. With advancements in AI, it could incorporate more dynamic storytelling elements, such as voice recognition and natural language generation, creating an even more immersive and lifelike experience for users. Conclusion: The Bard Chatbot is a testament to the creative fusion of AI and storytelling. By allowing users to participate in narrative creation, it transforms passive consumption into an interactive adventure. As technology advances and the boundaries of AI-driven creativity are pushed, the Bard Chatbot offers a glimpse into the future of digital storytelling \u2013 a future where imagination and technology merge to create unforgettable experiences. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"What is Bard chatbot ?"},{"location":"articles/bard/#bard-chatbot-redefining-conversations-through-ai-powered-storytelling","text":"What is bard chatbot ? In the digital age, as technology continues to advance, artificial intelligence (AI) has found its way into various aspects of our lives, including the way we communicate. The emergence of chatbots has transformed how we interact with machines, making tasks easier and more efficient. Among these innovative chatbots, the \"Bard Chatbot\" stands out as a creative and imaginative AI-driven conversational agent that brings the art of storytelling into the digital realm. Unveiling the Bard Chatbot: The Bard Chatbot is not just your ordinary conversational AI. It's a virtual storyteller designed to engage users in immersive and captivating narratives. Built on sophisticated natural language processing (NLP) and machine learning technologies, this chatbot goes beyond providing information or answering questions \u2013 it weaves stories, creates adventures, and offers users a unique form of interactive entertainment. Key Features and Capabilities: Storytelling Expertise: At the heart of the Bard Chatbot lies its ability to craft stories that capture the imagination. Whether it's fantasy, mystery, romance, or science fiction, the chatbot generates narratives that transport users to new worlds and experiences. Interactive Engagement: Unlike reading a traditional story, the Bard Chatbot encourages user participation. It allows users to make decisions and choices that influence the direction of the narrative, creating a personalized and engaging storytelling experience. Natural Language Understanding: The chatbot's advanced NLP capabilities enable it to understand user inputs, prompts, and context. This ensures that the generated stories remain coherent and relevant to the user's preferences. Diverse Genres: From classic tales to contemporary genres, the Bard Chatbot offers a wide range of storytelling options. Users can explore different themes and styles, catering to their individual interests. Continuous Learning: The more users interact with the Bard Chatbot, the better it becomes at tailoring its stories to their preferences. This learning process enhances the overall user experience by providing increasingly immersive narratives. (adsbygoogle = window.adsbygoogle || []).push({}); Applications and Impact: Entertainment and Creativity: The Bard Chatbot brings a new level of creativity and entertainment to the world of AI. It serves as a source of amusement, allowing users to escape into fictional realms and play an active role in shaping the story's outcome. Educational Tool: Beyond entertainment, the chatbot can serve as an educational tool. Teachers can use it to engage students in language arts activities, encouraging them to explore storytelling techniques and concepts. Personalized Experiences: In a world where personalization is valued, the Bard Chatbot stands out by tailoring its narratives to individual preferences. This customization enhances user engagement and satisfaction. Looking Ahead: As technology continues to evolve, the Bard Chatbot holds the potential to expand its capabilities even further. With advancements in AI, it could incorporate more dynamic storytelling elements, such as voice recognition and natural language generation, creating an even more immersive and lifelike experience for users. Conclusion: The Bard Chatbot is a testament to the creative fusion of AI and storytelling. By allowing users to participate in narrative creation, it transforms passive consumption into an interactive adventure. As technology advances and the boundaries of AI-driven creativity are pushed, the Bard Chatbot offers a glimpse into the future of digital storytelling \u2013 a future where imagination and technology merge to create unforgettable experiences. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Bard Chatbot: Redefining Conversations through AI-Powered Storytelling"},{"location":"articles/blockchain/","text":"Blockchain: A Primer for Beginners \uf0c1 How It Works and Why It Matters \uf0c1 What is blockchain? A blockchain is a distributed database that maintains a continuously growing list of records called blocks. [Image of Blockchain diagram] Each block contains a number of transactions, and every time a new transaction occurs on the blockchain, a record of that transaction is added to every participant's ledger. The decentralized database managed by multiple participants is known as Distributed Ledger Technology (DLT). Blockchain is a type of DLT in which transactions are recorded with an immutable cryptographic signature called a hash. Blockchains are typically used as a secure and transparent way to record transactions and track assets. How does blockchain work? A blockchain is a chain of blocks, each of which contains a number of transactions. When a new transaction occurs, it is added to a block. The block is then encrypted and linked to the previous block in the chain. This process continues, creating a chain of blocks that cannot be tampered with or altered. To add a new block to the chain, a miner must solve a complex mathematical problem. The miner who solves the problem first is rewarded with a cryptocurrency, such as Bitcoin. Once a block is added to the chain, it is visible to all participants in the network. This ensures that all participants have access to the same information and that the blockchain is tamper-proof. Benefits of blockchain Security: Blockchain is a very secure way to store data. Transparency: Blockchain is a transparent way to track transactions. Immutability: Blockchain is an immutable way to store data. Efficiency: Blockchain can be more efficient than traditional methods of record-keeping. Cost-effectiveness: Blockchain can be more cost-effective than traditional methods of record-keeping. (adsbygoogle = window.adsbygoogle || []).push({}); Potential applications of blockchain Finance: Blockchain can be used to track financial transactions, such as payments and loans. [Image of Finance blockchain diagram] Supply chain management: Blockchain can be used to track the movement of goods and materials through a supply chain. [Image of Supply chain management blockchain diagram] Healthcare: Blockchain can be used to track patient records and medical data. [Image of Healthcare blockchain diagram] Government: Blockchain can be used to track government records and transactions. Real estate: Blockchain can be used to track real estate transactions. Intellectual property: Blockchain can be used to track intellectual property rights. Challenges of blockchain Complexity: Blockchain is a complex technology. Scalability: Blockchain can be difficult to scale to large numbers of participants. Regulation: Blockchain is a new technology, and there is limited regulation around its use. Security: Blockchain is a secure technology, but it is not immune to attack. Conclusion Blockchain is a promising new technology with the potential to revolutionize many industries. However, it is important to remember that blockchain is still in its early stages of development, and there are some challenges that need to be addressed before it can be widely adopted. Overall, blockchain is a powerful tool that can be used to improve security, transparency, and efficiency. It has the potential to revolutionize many industries, but it is important to use it responsibly and ethically. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Blockchain"},{"location":"articles/blockchain/#blockchain-a-primer-for-beginners","text":"","title":"Blockchain: A Primer for Beginners"},{"location":"articles/blockchain/#how-it-works-and-why-it-matters","text":"What is blockchain? A blockchain is a distributed database that maintains a continuously growing list of records called blocks. [Image of Blockchain diagram] Each block contains a number of transactions, and every time a new transaction occurs on the blockchain, a record of that transaction is added to every participant's ledger. The decentralized database managed by multiple participants is known as Distributed Ledger Technology (DLT). Blockchain is a type of DLT in which transactions are recorded with an immutable cryptographic signature called a hash. Blockchains are typically used as a secure and transparent way to record transactions and track assets. How does blockchain work? A blockchain is a chain of blocks, each of which contains a number of transactions. When a new transaction occurs, it is added to a block. The block is then encrypted and linked to the previous block in the chain. This process continues, creating a chain of blocks that cannot be tampered with or altered. To add a new block to the chain, a miner must solve a complex mathematical problem. The miner who solves the problem first is rewarded with a cryptocurrency, such as Bitcoin. Once a block is added to the chain, it is visible to all participants in the network. This ensures that all participants have access to the same information and that the blockchain is tamper-proof. Benefits of blockchain Security: Blockchain is a very secure way to store data. Transparency: Blockchain is a transparent way to track transactions. Immutability: Blockchain is an immutable way to store data. Efficiency: Blockchain can be more efficient than traditional methods of record-keeping. Cost-effectiveness: Blockchain can be more cost-effective than traditional methods of record-keeping. (adsbygoogle = window.adsbygoogle || []).push({}); Potential applications of blockchain Finance: Blockchain can be used to track financial transactions, such as payments and loans. [Image of Finance blockchain diagram] Supply chain management: Blockchain can be used to track the movement of goods and materials through a supply chain. [Image of Supply chain management blockchain diagram] Healthcare: Blockchain can be used to track patient records and medical data. [Image of Healthcare blockchain diagram] Government: Blockchain can be used to track government records and transactions. Real estate: Blockchain can be used to track real estate transactions. Intellectual property: Blockchain can be used to track intellectual property rights. Challenges of blockchain Complexity: Blockchain is a complex technology. Scalability: Blockchain can be difficult to scale to large numbers of participants. Regulation: Blockchain is a new technology, and there is limited regulation around its use. Security: Blockchain is a secure technology, but it is not immune to attack. Conclusion Blockchain is a promising new technology with the potential to revolutionize many industries. However, it is important to remember that blockchain is still in its early stages of development, and there are some challenges that need to be addressed before it can be widely adopted. Overall, blockchain is a powerful tool that can be used to improve security, transparency, and efficiency. It has the potential to revolutionize many industries, but it is important to use it responsibly and ethically. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"How It Works and Why It Matters"},{"location":"articles/cyber_security/","text":"Cybersecurity: Protecting our digital world \uf0c1 Introduction \uf0c1 Cybersecurity is the practice of protecting computer systems and networks from unauthorized access, use, disclosure, disruption, modification, or destruction. It is a broad term that encompasses a wide range of security measures, from technical security controls to security awareness training. Understanding Cybersecurity \uf0c1 Cybersecurity is important because our reliance on computers and networks is increasing. We use computers and networks to store our personal information, conduct financial transactions, and communicate with others. If these systems are not secure, our personal information, financial assets, and reputations could be at risk. (adsbygoogle = window.adsbygoogle || []).push({}); There are many different types of cyberattacks, including: Phishing: This is a type of social engineering attack where the attacker sends an email or text message that appears to be from a legitimate source, such as a bank or credit card company. The email or text message will often contain a link that, when clicked, will take the victim to a fake website that looks like the real website. Once the victim enters their personal information on the fake website, the attacker can steal it. Malware: This is software that is designed to harm a computer system. Malware can be installed on a computer through a variety of ways, such as clicking on a malicious link, opening an infected attachment, or downloading a file from an untrusted source. Once malware is installed on a computer, it can steal personal information, damage files, or disrupt operations. Ransomware: This is a type of malware that encrypts the victim's files and demands a ransom payment in order to decrypt them. If the ransom is not paid, the victim may lose access to their files permanently. Cybersecurity is a critical issue for businesses and individuals alike. Businesses need to protect their computer systems and networks from cyberattacks in order to protect their data, intellectual property, and financial assets. Individuals need to protect their personal information from cyberattacks in order to protect their identity and financial security. There are a number of things that businesses and individuals can do to improve their cybersecurity posture, including: Use strong passwords and two-factor authentication. Keep their software up to date. Install security software on their computers and devices. Be careful about what links they click on and what attachments they open. Be aware of phishing scams. Back up their data regularly. Train their employees on cybersecurity best practices. Cybersecurity is an important and ever-evolving field. By taking steps to protect themselves, businesses and individuals can help to mitigate the risks of cyberattacks. (adsbygoogle = window.adsbygoogle || []).push({}); Applications of cybersecurity \uf0c1 Cybersecurity has a wide range of applications, including: Protecting critical infrastructure: Cybersecurity is essential for protecting critical infrastructure, such as power grids, water treatment plants, and transportation systems. These systems are essential to our everyday lives, and they are vulnerable to cyberattacks. Securing financial transactions: Cybersecurity is also essential for securing financial transactions. When we make online payments or use our credit cards, we need to be confident that our information is secure. Protecting personal information: Cybersecurity is also important for protecting our personal information. This includes our social security numbers, credit card numbers, and medical records. If this information is stolen, it could be used to commit identity theft or other crimes. Defending against cyberattacks: Cybersecurity is also important for defending against cyberattacks. These attacks can be used to steal data, disrupt operations, or even cause physical damage. Challenges in cybersecurity \uf0c1 Cybersecurity is a challenging field, and there are a number of challenges that need to be addressed, including: The increasing sophistication of cyberattacks: Cyberattacks are becoming increasingly sophisticated, and it is becoming more difficult to defend against them. The growing number of connected devices: The number of connected devices is growing rapidly, and this creates new security challenges. These devices are often not as secure as traditional computers, and they can be easily compromised. The shortage of cybersecurity professionals: There is a shortage of skilled cybersecurity professionals, and this makes it difficult to protect against cyberattacks. The lack of awareness of cybersecurity risks: Many people are not aware of the cybersecurity risks that they face, and this makes them more vulnerable to attack. Conclusion \uf0c1 Cybersecurity is an important and ever-evolving field. By understanding the risks and taking steps to protect ourselves, we can help to mitigate the risks of cyberattacks. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Cybersecurity"},{"location":"articles/cyber_security/#cybersecurity-protecting-our-digital-world","text":"","title":"Cybersecurity: Protecting our digital world"},{"location":"articles/cyber_security/#introduction","text":"Cybersecurity is the practice of protecting computer systems and networks from unauthorized access, use, disclosure, disruption, modification, or destruction. It is a broad term that encompasses a wide range of security measures, from technical security controls to security awareness training.","title":"Introduction"},{"location":"articles/cyber_security/#understanding-cybersecurity","text":"Cybersecurity is important because our reliance on computers and networks is increasing. We use computers and networks to store our personal information, conduct financial transactions, and communicate with others. If these systems are not secure, our personal information, financial assets, and reputations could be at risk. (adsbygoogle = window.adsbygoogle || []).push({}); There are many different types of cyberattacks, including: Phishing: This is a type of social engineering attack where the attacker sends an email or text message that appears to be from a legitimate source, such as a bank or credit card company. The email or text message will often contain a link that, when clicked, will take the victim to a fake website that looks like the real website. Once the victim enters their personal information on the fake website, the attacker can steal it. Malware: This is software that is designed to harm a computer system. Malware can be installed on a computer through a variety of ways, such as clicking on a malicious link, opening an infected attachment, or downloading a file from an untrusted source. Once malware is installed on a computer, it can steal personal information, damage files, or disrupt operations. Ransomware: This is a type of malware that encrypts the victim's files and demands a ransom payment in order to decrypt them. If the ransom is not paid, the victim may lose access to their files permanently. Cybersecurity is a critical issue for businesses and individuals alike. Businesses need to protect their computer systems and networks from cyberattacks in order to protect their data, intellectual property, and financial assets. Individuals need to protect their personal information from cyberattacks in order to protect their identity and financial security. There are a number of things that businesses and individuals can do to improve their cybersecurity posture, including: Use strong passwords and two-factor authentication. Keep their software up to date. Install security software on their computers and devices. Be careful about what links they click on and what attachments they open. Be aware of phishing scams. Back up their data regularly. Train their employees on cybersecurity best practices. Cybersecurity is an important and ever-evolving field. By taking steps to protect themselves, businesses and individuals can help to mitigate the risks of cyberattacks. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Understanding Cybersecurity"},{"location":"articles/cyber_security/#applications-of-cybersecurity","text":"Cybersecurity has a wide range of applications, including: Protecting critical infrastructure: Cybersecurity is essential for protecting critical infrastructure, such as power grids, water treatment plants, and transportation systems. These systems are essential to our everyday lives, and they are vulnerable to cyberattacks. Securing financial transactions: Cybersecurity is also essential for securing financial transactions. When we make online payments or use our credit cards, we need to be confident that our information is secure. Protecting personal information: Cybersecurity is also important for protecting our personal information. This includes our social security numbers, credit card numbers, and medical records. If this information is stolen, it could be used to commit identity theft or other crimes. Defending against cyberattacks: Cybersecurity is also important for defending against cyberattacks. These attacks can be used to steal data, disrupt operations, or even cause physical damage.","title":"Applications of cybersecurity"},{"location":"articles/cyber_security/#challenges-in-cybersecurity","text":"Cybersecurity is a challenging field, and there are a number of challenges that need to be addressed, including: The increasing sophistication of cyberattacks: Cyberattacks are becoming increasingly sophisticated, and it is becoming more difficult to defend against them. The growing number of connected devices: The number of connected devices is growing rapidly, and this creates new security challenges. These devices are often not as secure as traditional computers, and they can be easily compromised. The shortage of cybersecurity professionals: There is a shortage of skilled cybersecurity professionals, and this makes it difficult to protect against cyberattacks. The lack of awareness of cybersecurity risks: Many people are not aware of the cybersecurity risks that they face, and this makes them more vulnerable to attack.","title":"Challenges in cybersecurity"},{"location":"articles/cyber_security/#conclusion","text":"Cybersecurity is an important and ever-evolving field. By understanding the risks and taking steps to protect ourselves, we can help to mitigate the risks of cyberattacks. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Conclusion"},{"location":"articles/iot/","text":"Unveiling the Wonders of the Internet of Things (IoT) \uf0c1 A Guide to a Connected World \uf0c1 What is Internet of Things (IoT) ? \uf0c1 The Internet of Things (IoT) is a technological marvel that has revolutionized the way we live, work, and interact with our surroundings. In this article, we will explore the concept of IoT in simple terms, shedding light on its impact and providing learning materials to delve deeper into this exciting field. Let's embark on a journey to understand how IoT is transforming our world into a connected ecosystem. Understanding the Internet of Things (IoT) \uf0c1 Imagine a world where everyday objects are interconnected and can communicate with each other over the internet. That's the essence of the Internet of Things. IoT refers to the network of physical devices, vehicles, appliances, and other objects embedded with sensors, software, and connectivity, enabling them to collect and exchange data. (adsbygoogle = window.adsbygoogle || []).push({}); The key elements of IoT are as follows: \uf0c1 Devices: IoT is built upon a vast array of devices that are equipped with sensors and connected to the internet. These devices can be anything from smart home appliances like thermostats and refrigerators to wearables like fitness trackers and smartwatches. Sensors: Sensors play a crucial role in IoT devices as they capture and monitor various types of data from the surrounding environment. They can detect temperature, humidity, motion, light, and much more. Sensors act as the eyes and ears of IoT devices, collecting valuable information. Connectivity: The devices in an IoT network communicate with each other and with the cloud through various connectivity options such as Wi-Fi, Bluetooth, cellular networks, or even satellite connections. This enables seamless data exchange and remote control of IoT devices. Data and Analytics: IoT generates massive amounts of data through the sensors embedded in devices. This data is collected, stored, and analyzed to extract valuable insights. Advanced analytics techniques, such as machine learning, are applied to make sense of the data and drive intelligent decision-making. Impact and Applications of IoT \uf0c1 The impact of IoT is far-reaching and encompasses various aspects of our lives: Smart Homes: IoT enables homeowners to control and automate their home appliances, lighting, security systems, and more through smart devices and voice assistants. This enhances convenience, energy efficiency, and security. Healthcare: IoT devices can monitor patients' vital signs, track medication adherence, and enable remote patient monitoring. This enables personalized healthcare, early detection of health issues, and improved patient outcomes. Industrial IoT (IIoT): In industries, IoT is transforming manufacturing, logistics, and supply chain management. IIoT enables real-time monitoring of machinery, predictive maintenance, and optimization of operations for improved efficiency and productivity. Smart Cities: IoT technologies are being employed to create smarter and more sustainable cities. Smart parking systems, intelligent traffic management, and energy-efficient infrastructure are just a few examples of how IoT is revolutionizing urban living. Learning Materials to Explore \uf0c1 To delve deeper into the realm of IoT, here are some learning materials that can help you understand the subject better: \"A Gentle Introduction to IoT\" by The IoT Learning Initiative: This online guide provides a beginner-friendly introduction to IoT, covering its core concepts, applications, and challenges. IoT Courses on Coursera: Platforms like Coursera offer a variety of IoT courses from top universities and institutions. These courses cover topics such as IoT architecture, security, and data analytics, providing a comprehensive understanding of IoT. IoT Developer Platforms: Platforms like Arduino, Raspberry Pi, and Microsoft Azure IoT offer resources, tutorials, and projects to help you get hands-on experience with building IoT solutions. (adsbygoogle = window.adsbygoogle || []).push({}); Conclusion \uf0c1 The Internet of Things (IoT) has ushered in a new era of connectivity and transformed the way we interact with the world around us. By interconnecting devices, sensors, and data analytics, IoT has brought immense convenience, efficiency, and innovation to our lives. Exploring learning materials like the recommended guide, online courses, and developer platforms will enable you to further comprehend the intricacies of IoT and its applications. So, embrace the connected world of IoT and witness firsthand how it is shaping the future of technology and society. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"What is Internet of Things (IoT) ?"},{"location":"articles/iot/#unveiling-the-wonders-of-the-internet-of-things-iot","text":"","title":"Unveiling the Wonders of the Internet of Things (IoT)"},{"location":"articles/iot/#a-guide-to-a-connected-world","text":"","title":"A Guide to a Connected World"},{"location":"articles/iot/#what-is-internet-of-things-iot","text":"The Internet of Things (IoT) is a technological marvel that has revolutionized the way we live, work, and interact with our surroundings. In this article, we will explore the concept of IoT in simple terms, shedding light on its impact and providing learning materials to delve deeper into this exciting field. Let's embark on a journey to understand how IoT is transforming our world into a connected ecosystem.","title":"What is Internet of Things (IoT) ?"},{"location":"articles/iot/#understanding-the-internet-of-things-iot","text":"Imagine a world where everyday objects are interconnected and can communicate with each other over the internet. That's the essence of the Internet of Things. IoT refers to the network of physical devices, vehicles, appliances, and other objects embedded with sensors, software, and connectivity, enabling them to collect and exchange data. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Understanding the Internet of Things (IoT)"},{"location":"articles/iot/#the-key-elements-of-iot-are-as-follows","text":"Devices: IoT is built upon a vast array of devices that are equipped with sensors and connected to the internet. These devices can be anything from smart home appliances like thermostats and refrigerators to wearables like fitness trackers and smartwatches. Sensors: Sensors play a crucial role in IoT devices as they capture and monitor various types of data from the surrounding environment. They can detect temperature, humidity, motion, light, and much more. Sensors act as the eyes and ears of IoT devices, collecting valuable information. Connectivity: The devices in an IoT network communicate with each other and with the cloud through various connectivity options such as Wi-Fi, Bluetooth, cellular networks, or even satellite connections. This enables seamless data exchange and remote control of IoT devices. Data and Analytics: IoT generates massive amounts of data through the sensors embedded in devices. This data is collected, stored, and analyzed to extract valuable insights. Advanced analytics techniques, such as machine learning, are applied to make sense of the data and drive intelligent decision-making.","title":"The key elements of IoT are as follows:"},{"location":"articles/iot/#impact-and-applications-of-iot","text":"The impact of IoT is far-reaching and encompasses various aspects of our lives: Smart Homes: IoT enables homeowners to control and automate their home appliances, lighting, security systems, and more through smart devices and voice assistants. This enhances convenience, energy efficiency, and security. Healthcare: IoT devices can monitor patients' vital signs, track medication adherence, and enable remote patient monitoring. This enables personalized healthcare, early detection of health issues, and improved patient outcomes. Industrial IoT (IIoT): In industries, IoT is transforming manufacturing, logistics, and supply chain management. IIoT enables real-time monitoring of machinery, predictive maintenance, and optimization of operations for improved efficiency and productivity. Smart Cities: IoT technologies are being employed to create smarter and more sustainable cities. Smart parking systems, intelligent traffic management, and energy-efficient infrastructure are just a few examples of how IoT is revolutionizing urban living.","title":"Impact and Applications of IoT"},{"location":"articles/iot/#learning-materials-to-explore","text":"To delve deeper into the realm of IoT, here are some learning materials that can help you understand the subject better: \"A Gentle Introduction to IoT\" by The IoT Learning Initiative: This online guide provides a beginner-friendly introduction to IoT, covering its core concepts, applications, and challenges. IoT Courses on Coursera: Platforms like Coursera offer a variety of IoT courses from top universities and institutions. These courses cover topics such as IoT architecture, security, and data analytics, providing a comprehensive understanding of IoT. IoT Developer Platforms: Platforms like Arduino, Raspberry Pi, and Microsoft Azure IoT offer resources, tutorials, and projects to help you get hands-on experience with building IoT solutions. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Learning Materials to Explore"},{"location":"articles/iot/#conclusion","text":"The Internet of Things (IoT) has ushered in a new era of connectivity and transformed the way we interact with the world around us. By interconnecting devices, sensors, and data analytics, IoT has brought immense convenience, efficiency, and innovation to our lives. Exploring learning materials like the recommended guide, online courses, and developer platforms will enable you to further comprehend the intricacies of IoT and its applications. So, embrace the connected world of IoT and witness firsthand how it is shaping the future of technology and society. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Conclusion"},{"location":"articles/machine_learning_intro/","text":"What is Machine Learning ? \uf0c1 Unlocking the Power of Intelligent Algorithms \uf0c1 (adsbygoogle = window.adsbygoogle || []).push({}); Introduction \uf0c1 Machine Learning (ML) is a revolutionary technology that has gained widespread attention in recent years. It is a subset of Artificial Intelligence (AI) that empowers computers to learn and make predictions or decisions without being explicitly programmed. In this article, we will delve into the world of Machine Learning, explaining its concepts in layman terms and providing useful links to explore further. Let's embark on a journey to demystify this fascinating field. Understanding Machine Learning \uf0c1 Imagine teaching a computer to perform a task by showing it examples instead of giving it explicit instructions. That's the fundamental idea behind Machine Learning. Instead of traditional programming, where every rule needs to be predefined, Machine Learning algorithms learn from data and experiences to improve their performance over time. At its core, Machine Learning involves three essential components: data, models, and predictions. Data: Data is the fuel that powers Machine Learning algorithms. It can be any kind of information, such as numbers, text, images, or even sound. For example, in a spam email detection system, the data would consist of thousands of emails, both spam and non-spam, to help the algorithm learn to distinguish between them. Models: Models are the mathematical representations that Machine Learning algorithms create based on the provided data. They act as virtual \"brains\" that learn patterns and relationships within the data. Think of models as the knowledge gained by the algorithm from the examples it has been trained on. Predictions: Once a model has been trained, it can make predictions or decisions when presented with new, unseen data. Going back to our spam email example, the trained model can predict whether an incoming email is spam or not based on its learned patterns. Types of Machine Learning \uf0c1 There are several types of Machine Learning algorithms, but we'll focus on two fundamental categories: Supervised Learning: This type of Machine Learning involves training a model with labeled examples. Labeled data means that each example is associated with a known output. For instance, in a supervised learning model for predicting housing prices, the algorithm is trained on historical data where each house has a known price. The model learns patterns from this labeled data to predict prices for new, unseen houses. Unsupervised Learning: In contrast, unsupervised learning involves training models with unlabeled data, where there are no predetermined outputs. The goal is to discover hidden patterns and structures within the data. For instance, an unsupervised learning algorithm analyzing customer purchase history may identify distinct groups of customers based on their buying behavior. (adsbygoogle = window.adsbygoogle || []).push({}); Exploring Further \uf0c1 If you're curious to dive deeper into Machine Learning, here are some useful links to explore: Coursera's Machine Learning Course: A comprehensive online course by Stanford University's Andrew Ng, covering the basics of Machine Learning and its applications. Kaggle: A platform where you can find datasets and participate in Machine Learning competitions. It's a great way to learn by practicing and collaborating with a vibrant community. TensorFlow Playground: An interactive web-based tool that allows you to experiment with neural networks, a powerful Machine Learning technique. Conclusion \uf0c1 Machine Learning is an exciting field that enables computers to learn from data and make predictions or decisions without explicit programming. By harnessing the power of data, models, and predictions, Machine Learning is transforming industries and revolutionizing our lives. Whether it's predicting customer behavior, detecting diseases, or driving autonomous vehicles, Machine Learning has immense potential to shape the future. So, dive in, explore the links provided, and join the incredible journey of Machine Learning. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"What is Machine Learning ?"},{"location":"articles/machine_learning_intro/#what-is-machine-learning","text":"","title":"What is Machine Learning ?"},{"location":"articles/machine_learning_intro/#unlocking-the-power-of-intelligent-algorithms","text":"(adsbygoogle = window.adsbygoogle || []).push({});","title":"Unlocking the Power of Intelligent Algorithms"},{"location":"articles/machine_learning_intro/#introduction","text":"Machine Learning (ML) is a revolutionary technology that has gained widespread attention in recent years. It is a subset of Artificial Intelligence (AI) that empowers computers to learn and make predictions or decisions without being explicitly programmed. In this article, we will delve into the world of Machine Learning, explaining its concepts in layman terms and providing useful links to explore further. Let's embark on a journey to demystify this fascinating field.","title":"Introduction"},{"location":"articles/machine_learning_intro/#understanding-machine-learning","text":"Imagine teaching a computer to perform a task by showing it examples instead of giving it explicit instructions. That's the fundamental idea behind Machine Learning. Instead of traditional programming, where every rule needs to be predefined, Machine Learning algorithms learn from data and experiences to improve their performance over time. At its core, Machine Learning involves three essential components: data, models, and predictions. Data: Data is the fuel that powers Machine Learning algorithms. It can be any kind of information, such as numbers, text, images, or even sound. For example, in a spam email detection system, the data would consist of thousands of emails, both spam and non-spam, to help the algorithm learn to distinguish between them. Models: Models are the mathematical representations that Machine Learning algorithms create based on the provided data. They act as virtual \"brains\" that learn patterns and relationships within the data. Think of models as the knowledge gained by the algorithm from the examples it has been trained on. Predictions: Once a model has been trained, it can make predictions or decisions when presented with new, unseen data. Going back to our spam email example, the trained model can predict whether an incoming email is spam or not based on its learned patterns.","title":"Understanding Machine Learning"},{"location":"articles/machine_learning_intro/#types-of-machine-learning","text":"There are several types of Machine Learning algorithms, but we'll focus on two fundamental categories: Supervised Learning: This type of Machine Learning involves training a model with labeled examples. Labeled data means that each example is associated with a known output. For instance, in a supervised learning model for predicting housing prices, the algorithm is trained on historical data where each house has a known price. The model learns patterns from this labeled data to predict prices for new, unseen houses. Unsupervised Learning: In contrast, unsupervised learning involves training models with unlabeled data, where there are no predetermined outputs. The goal is to discover hidden patterns and structures within the data. For instance, an unsupervised learning algorithm analyzing customer purchase history may identify distinct groups of customers based on their buying behavior. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Types of Machine Learning"},{"location":"articles/machine_learning_intro/#exploring-further","text":"If you're curious to dive deeper into Machine Learning, here are some useful links to explore: Coursera's Machine Learning Course: A comprehensive online course by Stanford University's Andrew Ng, covering the basics of Machine Learning and its applications. Kaggle: A platform where you can find datasets and participate in Machine Learning competitions. It's a great way to learn by practicing and collaborating with a vibrant community. TensorFlow Playground: An interactive web-based tool that allows you to experiment with neural networks, a powerful Machine Learning technique.","title":"Exploring Further"},{"location":"articles/machine_learning_intro/#conclusion","text":"Machine Learning is an exciting field that enables computers to learn from data and make predictions or decisions without explicit programming. By harnessing the power of data, models, and predictions, Machine Learning is transforming industries and revolutionizing our lives. Whether it's predicting customer behavior, detecting diseases, or driving autonomous vehicles, Machine Learning has immense potential to shape the future. So, dive in, explore the links provided, and join the incredible journey of Machine Learning. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Conclusion"},{"location":"articles/natural_language_processing/","text":"What is Natural Language Processing (NLP) ? \uf0c1 A Layman's Guide to Understanding Human Language by Machines \uf0c1 (adsbygoogle = window.adsbygoogle || []).push({}); Introduction \uf0c1 Natural Language Processing (NLP) is an incredible field of Artificial Intelligence (AI) that focuses on enabling computers to understand and interact with human language. In this article, we will explore NLP in simple terms, uncovering its wonders and providing learning materials to further explore this fascinating subject. Let's embark on a journey to understand how NLP empowers machines to comprehend and communicate in our language. Understanding Natural Language Processing (NLP) \uf0c1 Think about how effortlessly we communicate with each other using words, sentences, and conversations. NLP aims to replicate this ability in machines, allowing them to understand, interpret, and respond to human language. NLP combines linguistics, computer science, and AI to bridge the gap between human communication and computer understanding. The key aspects of NLP are as follows: Language Understanding: NLP focuses on teaching computers to comprehend human language by breaking it down into its components. This includes understanding the meaning of words, sentence structures, grammar, context, and even nuances like sarcasm and sentiment. Text Analysis: NLP algorithms analyze written text to extract valuable information and insights. These algorithms can identify entities (names, places, organizations), perform sentiment analysis (determining emotions expressed in text), and extract relevant information from documents or articles. Language Generation: NLP also enables computers to generate human-like text. This includes tasks like chatbots that engage in conversational interactions, language translation systems, and even automated content generation. Applications and Impact of NLP \uf0c1 NLP has transformed various domains and industries, making a significant impact on our lives: Virtual Assistants: Virtual assistants like Siri, Alexa, and Google Assistant utilize NLP to understand and respond to voice commands, providing information, performing tasks, and offering recommendations. Sentiment Analysis: NLP algorithms can analyze text on social media platforms, customer reviews, and news articles to determine public opinion, sentiment trends, and brand reputation. Machine Translation: NLP enables the development of machine translation systems like Google Translate, making it possible to translate text from one language to another with reasonable accuracy. Text Summarization: NLP algorithms can automatically generate concise summaries of lengthy articles or documents, making it easier to extract key information and save time. (adsbygoogle = window.adsbygoogle || []).push({}); Learning Materials to Explore \uf0c1 To delve deeper into NLP, here are some learning materials and web links that can help you understand the subject better: \"Natural Language Processing for Beginners\" by the University of Washington: This introductory guide provides an easy-to-understand overview of NLP concepts, techniques, and applications. \"NLP Crash Course\" by Stanford University: Stanford offers an online crash course on NLP, providing video lectures, exercises, and assignments to gain hands-on experience with NLP fundamentals. NLP Libraries and Tools: Libraries like NLTK (Natural Language Toolkit) and spaCy provide resources and tutorials for NLP implementation in various programming languages. Conclusion \uf0c1 Natural Language Processing (NLP) is an awe-inspiring field that empowers machines to understand, interpret, and generate human language. By combining linguistics, AI, and computer science, NLP has transformed the way we interact with technology, enabling voice assistants, sentiment analysis, machine translation, and text summarization. Exploring the learning materials and web links provided will enable you to dive deeper into the world of NLP, unraveling its intricacies and applications. So, embrace the marvels of NLP and witness firsthand how machines can comprehend and communicate in our language, bridging the gap between humans and computers. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"What is Natural Language Processing (NLP) ?"},{"location":"articles/natural_language_processing/#what-is-natural-language-processing-nlp","text":"","title":"What is Natural Language Processing (NLP) ?"},{"location":"articles/natural_language_processing/#a-laymans-guide-to-understanding-human-language-by-machines","text":"(adsbygoogle = window.adsbygoogle || []).push({});","title":"A Layman's Guide to Understanding Human Language by Machines"},{"location":"articles/natural_language_processing/#introduction","text":"Natural Language Processing (NLP) is an incredible field of Artificial Intelligence (AI) that focuses on enabling computers to understand and interact with human language. In this article, we will explore NLP in simple terms, uncovering its wonders and providing learning materials to further explore this fascinating subject. Let's embark on a journey to understand how NLP empowers machines to comprehend and communicate in our language.","title":"Introduction"},{"location":"articles/natural_language_processing/#understanding-natural-language-processing-nlp","text":"Think about how effortlessly we communicate with each other using words, sentences, and conversations. NLP aims to replicate this ability in machines, allowing them to understand, interpret, and respond to human language. NLP combines linguistics, computer science, and AI to bridge the gap between human communication and computer understanding. The key aspects of NLP are as follows: Language Understanding: NLP focuses on teaching computers to comprehend human language by breaking it down into its components. This includes understanding the meaning of words, sentence structures, grammar, context, and even nuances like sarcasm and sentiment. Text Analysis: NLP algorithms analyze written text to extract valuable information and insights. These algorithms can identify entities (names, places, organizations), perform sentiment analysis (determining emotions expressed in text), and extract relevant information from documents or articles. Language Generation: NLP also enables computers to generate human-like text. This includes tasks like chatbots that engage in conversational interactions, language translation systems, and even automated content generation.","title":"Understanding Natural Language Processing (NLP)"},{"location":"articles/natural_language_processing/#applications-and-impact-of-nlp","text":"NLP has transformed various domains and industries, making a significant impact on our lives: Virtual Assistants: Virtual assistants like Siri, Alexa, and Google Assistant utilize NLP to understand and respond to voice commands, providing information, performing tasks, and offering recommendations. Sentiment Analysis: NLP algorithms can analyze text on social media platforms, customer reviews, and news articles to determine public opinion, sentiment trends, and brand reputation. Machine Translation: NLP enables the development of machine translation systems like Google Translate, making it possible to translate text from one language to another with reasonable accuracy. Text Summarization: NLP algorithms can automatically generate concise summaries of lengthy articles or documents, making it easier to extract key information and save time. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Applications and Impact of NLP"},{"location":"articles/natural_language_processing/#learning-materials-to-explore","text":"To delve deeper into NLP, here are some learning materials and web links that can help you understand the subject better: \"Natural Language Processing for Beginners\" by the University of Washington: This introductory guide provides an easy-to-understand overview of NLP concepts, techniques, and applications. \"NLP Crash Course\" by Stanford University: Stanford offers an online crash course on NLP, providing video lectures, exercises, and assignments to gain hands-on experience with NLP fundamentals. NLP Libraries and Tools: Libraries like NLTK (Natural Language Toolkit) and spaCy provide resources and tutorials for NLP implementation in various programming languages.","title":"Learning Materials to Explore"},{"location":"articles/natural_language_processing/#conclusion","text":"Natural Language Processing (NLP) is an awe-inspiring field that empowers machines to understand, interpret, and generate human language. By combining linguistics, AI, and computer science, NLP has transformed the way we interact with technology, enabling voice assistants, sentiment analysis, machine translation, and text summarization. Exploring the learning materials and web links provided will enable you to dive deeper into the world of NLP, unraveling its intricacies and applications. So, embrace the marvels of NLP and witness firsthand how machines can comprehend and communicate in our language, bridging the gap between humans and computers. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Conclusion"},{"location":"articles/neural_networks/","text":"What is Neural Network ? \uf0c1 A Layman's Guide to Understanding the Brain-Inspired Technology \uf0c1 Introduction \uf0c1 Neural Networks are a fascinating branch of Artificial Intelligence (AI) that draw inspiration from the complex structure and functioning of the human brain. In this article, we will embark on a journey to demystify Neural Networks in layman's terms, exploring their concepts and providing learning materials to delve deeper into this captivating field. Let's dive into the world of Neural Networks and discover how they revolutionize AI. Understanding Neural Networks \uf0c1 Imagine a vast network of interconnected brain cells working together to process information. Neural Networks aim to mimic this intricate network to perform intelligent tasks. Just like the human brain, a Neural Network is composed of interconnected layers of artificial neurons, also known as artificial neural units. The key elements of a Neural Network are as follows: Artificial Neurons: These are the building blocks of a Neural Network. Each artificial neuron receives input signals, performs a computation, and produces an output signal. These artificial neurons collectively process information, just like the neurons in our brains. Weights and Bias: Each connection between artificial neurons in a Neural Network has an associated weight. These weights determine the strength of the connection and influence the information flow through the network. Additionally, each artificial neuron has a bias term that adjusts the output signal. Activation Function: The activation function defines the output of an artificial neuron based on its input. It introduces non-linearities to the network, enabling it to learn complex patterns and make sophisticated decisions. (adsbygoogle = window.adsbygoogle || []).push({}); Training a Neural Network \uf0c1 To make a Neural Network learn and perform tasks, it undergoes a process called training. Training involves providing the network with a set of labeled examples, allowing it to adjust its internal parameters (weights and biases) to minimize errors and improve its predictions. During training, the Neural Network learns to recognize patterns and relationships within the data, just like how our brain learns from experiences. It adjusts the weights and biases of its artificial neurons, gradually improving its ability to make accurate predictions or decisions. Learning Materials to Explore \uf0c1 If you're intrigued by Neural Networks and want to dive deeper into the subject, here are some learning materials to get you started: \"Neural Networks and Deep Learning\" by Michael Nielsen: This online book provides an in-depth introduction to Neural Networks and covers the basics of their operation. It includes interactive exercises to reinforce your understanding. \"Deep Learning Specialization\" on Coursera: Offered by deeplearning.ai, this specialization by Andrew Ng explores Neural Networks, deep learning, and their applications. It provides a comprehensive understanding of the field and hands-on experience with building Neural Networks. TensorFlow: TensorFlow is an open-source deep learning framework that provides tools and resources for building and training Neural Networks. The official TensorFlow website offers tutorials, documentation, and sample code to help you get started. (adsbygoogle = window.adsbygoogle || []).push({}); Conclusion \uf0c1 Neural Networks are powerful AI models inspired by the human brain. By leveraging interconnected artificial neurons and learning from labeled data, Neural Networks can recognize patterns, make predictions, and perform complex tasks. Exploring learning materials like the recommended book, online courses, and deep learning frameworks like TensorFlow will help you delve deeper into the fascinating world of Neural Networks. So, embrace the opportunity to understand and harness this brain-inspired technology, and discover how it is reshaping the landscape of Artificial Intelligence. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"What is Neural Network ?"},{"location":"articles/neural_networks/#what-is-neural-network","text":"","title":"What is Neural Network ?"},{"location":"articles/neural_networks/#a-laymans-guide-to-understanding-the-brain-inspired-technology","text":"","title":"A Layman's Guide to Understanding the Brain-Inspired Technology"},{"location":"articles/neural_networks/#introduction","text":"Neural Networks are a fascinating branch of Artificial Intelligence (AI) that draw inspiration from the complex structure and functioning of the human brain. In this article, we will embark on a journey to demystify Neural Networks in layman's terms, exploring their concepts and providing learning materials to delve deeper into this captivating field. Let's dive into the world of Neural Networks and discover how they revolutionize AI.","title":"Introduction"},{"location":"articles/neural_networks/#understanding-neural-networks","text":"Imagine a vast network of interconnected brain cells working together to process information. Neural Networks aim to mimic this intricate network to perform intelligent tasks. Just like the human brain, a Neural Network is composed of interconnected layers of artificial neurons, also known as artificial neural units. The key elements of a Neural Network are as follows: Artificial Neurons: These are the building blocks of a Neural Network. Each artificial neuron receives input signals, performs a computation, and produces an output signal. These artificial neurons collectively process information, just like the neurons in our brains. Weights and Bias: Each connection between artificial neurons in a Neural Network has an associated weight. These weights determine the strength of the connection and influence the information flow through the network. Additionally, each artificial neuron has a bias term that adjusts the output signal. Activation Function: The activation function defines the output of an artificial neuron based on its input. It introduces non-linearities to the network, enabling it to learn complex patterns and make sophisticated decisions. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Understanding Neural Networks"},{"location":"articles/neural_networks/#training-a-neural-network","text":"To make a Neural Network learn and perform tasks, it undergoes a process called training. Training involves providing the network with a set of labeled examples, allowing it to adjust its internal parameters (weights and biases) to minimize errors and improve its predictions. During training, the Neural Network learns to recognize patterns and relationships within the data, just like how our brain learns from experiences. It adjusts the weights and biases of its artificial neurons, gradually improving its ability to make accurate predictions or decisions.","title":"Training a Neural Network"},{"location":"articles/neural_networks/#learning-materials-to-explore","text":"If you're intrigued by Neural Networks and want to dive deeper into the subject, here are some learning materials to get you started: \"Neural Networks and Deep Learning\" by Michael Nielsen: This online book provides an in-depth introduction to Neural Networks and covers the basics of their operation. It includes interactive exercises to reinforce your understanding. \"Deep Learning Specialization\" on Coursera: Offered by deeplearning.ai, this specialization by Andrew Ng explores Neural Networks, deep learning, and their applications. It provides a comprehensive understanding of the field and hands-on experience with building Neural Networks. TensorFlow: TensorFlow is an open-source deep learning framework that provides tools and resources for building and training Neural Networks. The official TensorFlow website offers tutorials, documentation, and sample code to help you get started. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Learning Materials to Explore"},{"location":"articles/neural_networks/#conclusion","text":"Neural Networks are powerful AI models inspired by the human brain. By leveraging interconnected artificial neurons and learning from labeled data, Neural Networks can recognize patterns, make predictions, and perform complex tasks. Exploring learning materials like the recommended book, online courses, and deep learning frameworks like TensorFlow will help you delve deeper into the fascinating world of Neural Networks. So, embrace the opportunity to understand and harness this brain-inspired technology, and discover how it is reshaping the landscape of Artificial Intelligence. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Conclusion"},{"location":"articles/quantum_computing/","text":"Quantum Computing \uf0c1 The Next Frontier in Computing \uf0c1 What is quantum computing? Quantum computing is a type of computing that uses the principles of quantum mechanics to solve problems that are intractable for classical computers. Quantum computers are able to perform calculations in parallel and can use superposition and entanglement to represent and process information in ways that are not possible with classical computers. This makes quantum computers potentially capable of solving problems that are far too complex for classical computers, such as breaking encryption codes, simulating complex molecules, and optimizing financial portfolios. [Image of Quantum computing diagram] Some potential applications of quantum computing: Breaking encryption codes: Quantum computers could be used to break the encryption codes that protect our financial data, government secrets, and personal communications. [Image of Encryption codes diagram] Simulating complex molecules: Quantum computers could be used to simulate the behavior of complex molecules, such as proteins and drugs. This could help us to design new medicines and materials. Optimizing financial portfolios: Quantum computers could be used to optimize financial portfolios, such as by finding the best combination of stocks and bonds to invest in. Other potential applications include: Artificial intelligence Drug discovery Materials science Weather forecasting Logistics (adsbygoogle = window.adsbygoogle || []).push({}); The challenges of quantum computing: Quantum computers are still in their early stages of development. They are very difficult to build and operate, and they are susceptible to noise and errors. There is a lack of quantum algorithms that can be used to solve real-world problems. The future of quantum computing: Quantum computing is a rapidly developing field, and there is a lot of excitement about its potential. However, it is important to remember that quantum computers are still in their early stages, and it is not clear when they will be able to solve real-world problems. Nevertheless, quantum computing is a promising technology with the potential to revolutionize many industries. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Quantum Computing"},{"location":"articles/quantum_computing/#quantum-computing","text":"","title":"Quantum Computing"},{"location":"articles/quantum_computing/#the-next-frontier-in-computing","text":"What is quantum computing? Quantum computing is a type of computing that uses the principles of quantum mechanics to solve problems that are intractable for classical computers. Quantum computers are able to perform calculations in parallel and can use superposition and entanglement to represent and process information in ways that are not possible with classical computers. This makes quantum computers potentially capable of solving problems that are far too complex for classical computers, such as breaking encryption codes, simulating complex molecules, and optimizing financial portfolios. [Image of Quantum computing diagram] Some potential applications of quantum computing: Breaking encryption codes: Quantum computers could be used to break the encryption codes that protect our financial data, government secrets, and personal communications. [Image of Encryption codes diagram] Simulating complex molecules: Quantum computers could be used to simulate the behavior of complex molecules, such as proteins and drugs. This could help us to design new medicines and materials. Optimizing financial portfolios: Quantum computers could be used to optimize financial portfolios, such as by finding the best combination of stocks and bonds to invest in. Other potential applications include: Artificial intelligence Drug discovery Materials science Weather forecasting Logistics (adsbygoogle = window.adsbygoogle || []).push({}); The challenges of quantum computing: Quantum computers are still in their early stages of development. They are very difficult to build and operate, and they are susceptible to noise and errors. There is a lack of quantum algorithms that can be used to solve real-world problems. The future of quantum computing: Quantum computing is a rapidly developing field, and there is a lot of excitement about its potential. However, it is important to remember that quantum computers are still in their early stages, and it is not clear when they will be able to solve real-world problems. Nevertheless, quantum computing is a promising technology with the potential to revolutionize many industries. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"The Next Frontier in Computing"},{"location":"articles/robotics_automation/","text":"Exploring Robotics and Automation \uf0c1 Revolutionizing Industries with Intelligent Machines \uf0c1 (adsbygoogle = window.adsbygoogle || []).push({}); What is Robotics and Automation ? \uf0c1 Robotics and Automation are dynamic fields that have reshaped industries by combining advanced technologies and intelligent machines. In this article, we will delve into the world of Robotics and Automation, exploring their concepts, applications, and providing learning materials to understand the transformative power of these technologies. Let's embark on a journey to unravel how Robotics and Automation are revolutionizing various sectors. Understanding Robotics and Automation \uf0c1 Robotics involves the design, development, and operation of mechanical devices, often called robots, that can perform tasks autonomously or with minimal human intervention. These robots are equipped with sensors, actuators, and programming that enable them to interact with their environment and accomplish specific tasks. Automation, on the other hand, focuses on the use of technology to perform tasks without human involvement. It entails automating processes using machines, software, and systems to enhance efficiency, productivity, and precision. Key Aspects of Robotics and Automation \uf0c1 Intelligent Machines: Robots in the field of Robotics and Automation are designed to be intelligent machines. They can sense and perceive their environment using sensors such as cameras, lasers, or touch sensors. They use this information to make decisions and take actions accordingly. Programming and Control: Robots are programmed to execute specific tasks or follow predefined instructions. This programming can be done using various approaches, such as traditional programming languages, visual programming interfaces, or even artificial intelligence techniques like machine learning. Human-Robot Interaction: One crucial aspect of Robotics and Automation is the interaction between humans and robots. Collaborative robots, also known as cobots, work alongside humans in shared workspaces, assisting in tasks and enhancing productivity and safety. (adsbygoogle = window.adsbygoogle || []).push({}); Applications and Impact of Robotics and Automation \uf0c1 Robotics and Automation have transformed various industries, providing numerous benefits and opening up new possibilities: Manufacturing and Industrial Automation: Robots are extensively used in manufacturing processes to perform repetitive tasks, assembly, welding, and quality control. Automation systems streamline production lines, enhance precision, and increase productivity. Logistics and Warehousing: Robots are employed in logistics and warehousing to automate inventory management, order fulfillment, and material handling. Autonomous guided vehicles (AGVs) and drones are utilized for efficient movement and delivery of goods. Healthcare and Medical Robotics: Robots are assisting in healthcare settings for tasks such as surgery, patient monitoring, and rehabilitation. Robotic exoskeletons and prosthetics are improving mobility and enhancing the quality of life for individuals with disabilities. Agriculture and Farming: Automation technologies are revolutionizing agriculture with autonomous tractors, robotic harvesters, and precision farming techniques. This improves crop yield, optimizes resource usage, and reduces manual labor. Learning Materials to Explore \uf0c1 To delve deeper into the world of Robotics and Automation, here are some learning materials and web links to help you understand these fields better: \"Robotics: Aerial Robotics\" by the University of Pennsylvania: This online course on Coursera provides a comprehensive understanding of robotics, focusing on aerial robots and their applications. \"Automation, Robotics, and the Future of Work\" by Massachusetts Institute of Technology (MIT): This edX course explores the impact of automation and robotics on the future of work and society. ROS (Robot Operating System): ROS is an open-source framework for developing robotic applications. The official ROS website provides extensive documentation, tutorials, and resources for learning and implementing robotics projects. Conclusion \uf0c1 Robotics and Automation have brought forth a new era of intelligent machines that enhance productivity, efficiency, and safety across industries. By combining advanced technologies with human ingenuity, these fields continue to revolutionize manufacturing, healthcare, logistics, and agriculture. Exploring the learning materials and web links provided will enable you to delve deeper into the world of Robotics and Automation, unveiling the transformative power of these technologies. So, embrace the wonders of Robotics and Automation and witness firsthand how intelligent machines are shaping the future of industries and societies. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"What is Robotics and Automation ?"},{"location":"articles/robotics_automation/#exploring-robotics-and-automation","text":"","title":"Exploring Robotics and Automation"},{"location":"articles/robotics_automation/#revolutionizing-industries-with-intelligent-machines","text":"(adsbygoogle = window.adsbygoogle || []).push({});","title":"Revolutionizing Industries with Intelligent Machines"},{"location":"articles/robotics_automation/#what-is-robotics-and-automation","text":"Robotics and Automation are dynamic fields that have reshaped industries by combining advanced technologies and intelligent machines. In this article, we will delve into the world of Robotics and Automation, exploring their concepts, applications, and providing learning materials to understand the transformative power of these technologies. Let's embark on a journey to unravel how Robotics and Automation are revolutionizing various sectors.","title":"What is Robotics and Automation ?"},{"location":"articles/robotics_automation/#understanding-robotics-and-automation","text":"Robotics involves the design, development, and operation of mechanical devices, often called robots, that can perform tasks autonomously or with minimal human intervention. These robots are equipped with sensors, actuators, and programming that enable them to interact with their environment and accomplish specific tasks. Automation, on the other hand, focuses on the use of technology to perform tasks without human involvement. It entails automating processes using machines, software, and systems to enhance efficiency, productivity, and precision.","title":"Understanding Robotics and Automation"},{"location":"articles/robotics_automation/#key-aspects-of-robotics-and-automation","text":"Intelligent Machines: Robots in the field of Robotics and Automation are designed to be intelligent machines. They can sense and perceive their environment using sensors such as cameras, lasers, or touch sensors. They use this information to make decisions and take actions accordingly. Programming and Control: Robots are programmed to execute specific tasks or follow predefined instructions. This programming can be done using various approaches, such as traditional programming languages, visual programming interfaces, or even artificial intelligence techniques like machine learning. Human-Robot Interaction: One crucial aspect of Robotics and Automation is the interaction between humans and robots. Collaborative robots, also known as cobots, work alongside humans in shared workspaces, assisting in tasks and enhancing productivity and safety. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Key Aspects of Robotics and Automation"},{"location":"articles/robotics_automation/#applications-and-impact-of-robotics-and-automation","text":"Robotics and Automation have transformed various industries, providing numerous benefits and opening up new possibilities: Manufacturing and Industrial Automation: Robots are extensively used in manufacturing processes to perform repetitive tasks, assembly, welding, and quality control. Automation systems streamline production lines, enhance precision, and increase productivity. Logistics and Warehousing: Robots are employed in logistics and warehousing to automate inventory management, order fulfillment, and material handling. Autonomous guided vehicles (AGVs) and drones are utilized for efficient movement and delivery of goods. Healthcare and Medical Robotics: Robots are assisting in healthcare settings for tasks such as surgery, patient monitoring, and rehabilitation. Robotic exoskeletons and prosthetics are improving mobility and enhancing the quality of life for individuals with disabilities. Agriculture and Farming: Automation technologies are revolutionizing agriculture with autonomous tractors, robotic harvesters, and precision farming techniques. This improves crop yield, optimizes resource usage, and reduces manual labor.","title":"Applications and Impact of Robotics and Automation"},{"location":"articles/robotics_automation/#learning-materials-to-explore","text":"To delve deeper into the world of Robotics and Automation, here are some learning materials and web links to help you understand these fields better: \"Robotics: Aerial Robotics\" by the University of Pennsylvania: This online course on Coursera provides a comprehensive understanding of robotics, focusing on aerial robots and their applications. \"Automation, Robotics, and the Future of Work\" by Massachusetts Institute of Technology (MIT): This edX course explores the impact of automation and robotics on the future of work and society. ROS (Robot Operating System): ROS is an open-source framework for developing robotic applications. The official ROS website provides extensive documentation, tutorials, and resources for learning and implementing robotics projects.","title":"Learning Materials to Explore"},{"location":"articles/robotics_automation/#conclusion","text":"Robotics and Automation have brought forth a new era of intelligent machines that enhance productivity, efficiency, and safety across industries. By combining advanced technologies with human ingenuity, these fields continue to revolutionize manufacturing, healthcare, logistics, and agriculture. Exploring the learning materials and web links provided will enable you to delve deeper into the world of Robotics and Automation, unveiling the transformative power of these technologies. So, embrace the wonders of Robotics and Automation and witness firsthand how intelligent machines are shaping the future of industries and societies. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Conclusion"},{"location":"articles/virtual_reality/","text":"Virtual Reality (VR) \uf0c1 What is VR? Virtual reality (VR) is a simulated environment that can be experienced through a headset or other device. [Image of Virtual reality headset] VR technology allows users to interact with the virtual environment in a realistic way, as if they were actually there. VR can be used for a variety of purposes, including entertainment, education, and training. How does VR work? VR headsets use a combination of sensors and displays to create the illusion of a virtual environment. The sensors track the user's head movements, and the displays update the virtual environment accordingly. This creates a sense of immersion, as the user feels like they are actually inside the virtual world. Types of VR There are two main types of VR: Head-mounted display (HMD): This is the most common type of VR. It uses a headset that fits over the user's head and displays the virtual environment in front of their eyes. [Image of Head-mounted display (HMD)] Non-HMD: This type of VR does not require a headset. Instead, it uses a computer monitor or projector to display the virtual environment. (adsbygoogle = window.adsbygoogle || []).push({}); Applications of VR VR is used in a variety of industries, including: Entertainment: VR is used to create immersive games and experiences. [Image of VR used in entertainment] Education: VR is used to simulate real-world environments and objects. This can be used for training, education, and research. [Image of VR used in education] Training: VR is used to simulate dangerous or expensive environments. This can be used to train soldiers, firefighters, and other professionals. [Image of VR used in training] Healthcare: VR is used to treat phobias, pain, and other conditions. It is also used to provide training and education to healthcare professionals. [Image of VR used in healthcare] Architecture: VR is used to create 3D models of buildings and other structures. This can be used for design, visualization, and planning. [Image of VR used in architecture] Manufacturing: VR is used to design and test products. It can also be used to train workers on how to operate machinery. [Image of VR used in manufacturing] Benefits of VR VR offers a number of benefits, including: Immersion: VR can create a sense of immersion that is not possible with traditional media. Interactivity: VR allows users to interact with the virtual environment in a realistic way. Customization: VR experiences can be customized to meet the needs of individual users. Learning: VR can be used to create immersive and interactive learning experiences. Training: VR can be used to create realistic and safe training environments. Entertainment: VR can be used to create immersive and interactive entertainment experiences. (adsbygoogle = window.adsbygoogle || []).push({}); Drawbacks of VR VR also has a number of drawbacks, including: Cost: VR headsets can be expensive. Motion sickness: Some people experience motion sickness when using VR. Eye strain: VR headsets can cause eye strain. Limited content: There is still a limited amount of VR content available. Acceptance: VR is still a relatively new technology, and it is not yet widely accepted by the general public. Future of VR The future of VR is bright. The technology is constantly evolving, and the cost of VR headsets is decreasing. As VR becomes more affordable and accessible, it is likely to be used in a wider range of applications. Overall, VR is a powerful technology with the potential to revolutionize many industries. It is still in its early stages of development, but it has the potential to change the way we interact with the world around us. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"What is Virtual Reality (VR) ?"},{"location":"articles/virtual_reality/#virtual-reality-vr","text":"What is VR? Virtual reality (VR) is a simulated environment that can be experienced through a headset or other device. [Image of Virtual reality headset] VR technology allows users to interact with the virtual environment in a realistic way, as if they were actually there. VR can be used for a variety of purposes, including entertainment, education, and training. How does VR work? VR headsets use a combination of sensors and displays to create the illusion of a virtual environment. The sensors track the user's head movements, and the displays update the virtual environment accordingly. This creates a sense of immersion, as the user feels like they are actually inside the virtual world. Types of VR There are two main types of VR: Head-mounted display (HMD): This is the most common type of VR. It uses a headset that fits over the user's head and displays the virtual environment in front of their eyes. [Image of Head-mounted display (HMD)] Non-HMD: This type of VR does not require a headset. Instead, it uses a computer monitor or projector to display the virtual environment. (adsbygoogle = window.adsbygoogle || []).push({}); Applications of VR VR is used in a variety of industries, including: Entertainment: VR is used to create immersive games and experiences. [Image of VR used in entertainment] Education: VR is used to simulate real-world environments and objects. This can be used for training, education, and research. [Image of VR used in education] Training: VR is used to simulate dangerous or expensive environments. This can be used to train soldiers, firefighters, and other professionals. [Image of VR used in training] Healthcare: VR is used to treat phobias, pain, and other conditions. It is also used to provide training and education to healthcare professionals. [Image of VR used in healthcare] Architecture: VR is used to create 3D models of buildings and other structures. This can be used for design, visualization, and planning. [Image of VR used in architecture] Manufacturing: VR is used to design and test products. It can also be used to train workers on how to operate machinery. [Image of VR used in manufacturing] Benefits of VR VR offers a number of benefits, including: Immersion: VR can create a sense of immersion that is not possible with traditional media. Interactivity: VR allows users to interact with the virtual environment in a realistic way. Customization: VR experiences can be customized to meet the needs of individual users. Learning: VR can be used to create immersive and interactive learning experiences. Training: VR can be used to create realistic and safe training environments. Entertainment: VR can be used to create immersive and interactive entertainment experiences. (adsbygoogle = window.adsbygoogle || []).push({}); Drawbacks of VR VR also has a number of drawbacks, including: Cost: VR headsets can be expensive. Motion sickness: Some people experience motion sickness when using VR. Eye strain: VR headsets can cause eye strain. Limited content: There is still a limited amount of VR content available. Acceptance: VR is still a relatively new technology, and it is not yet widely accepted by the general public. Future of VR The future of VR is bright. The technology is constantly evolving, and the cost of VR headsets is decreasing. As VR becomes more affordable and accessible, it is likely to be used in a wider range of applications. Overall, VR is a powerful technology with the potential to revolutionize many industries. It is still in its early stages of development, but it has the potential to change the way we interact with the world around us. (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ?","title":"Virtual Reality (VR)"},{"location":"nightwolf-cotribution/","text":"nightwolf-cotribution \uf0c1","title":"nightwolf-cotribution"},{"location":"nightwolf-cotribution/#nightwolf-cotribution","text":"","title":"nightwolf-cotribution"},{"location":"nightwolf-cotribution/TLS-Difference/","text":"TLS versions Difference \uf0c1 TLS 1.0 This was an upgrade from SSL 3.0 and the differences were not dramatic, but they are significant enough that SSL 3.0 and TLS 1.0 don't interoperate. Some of the major differences between SSL 3.0 and TLS 1.0 are: derivation functions are different MACs are different - SSL 3.0 uses a modification of an early HMAC while TLS 1.0 uses HMAC. The Finished messages are different TLS has more alerts TLS requires DSS/DH support TLS 1.1 is an update to TLS 1.0. The major changes are: The Implicit Initialization Vector (IV) is replaced with an explicit IV to protect against Cipher block chaining (CBC) attacks. Handling of padded errors is changed to use the bad_record_mac alert rather than the decryption_failed alert to protect against CBC attacks. IANA registries are defined for protocol parameters. Premature closes no longer cause a session to be non-resumable. TLS 1.2 Based on TLS 1.1, TLS 1.2 contains improved flexibility. The major differences include: The MD5/SHA-1 combination in the pseudorandom function (PRF) was replaced with cipher-suite-specified PRFs. The MD5/SHA-1 combination in the digitally-signed element was replaced with a single hash. Signed elements include a field explicitly specifying the hash algorithm used. There was substantial cleanup to the client's and server's ability to specify which hash and signature algorithms they will accept. Addition of support for authenticated encryption with additional data modes. TLS Extensions definition and AES Cipher Suites were merged in. Tighter checking of EncryptedPreMasterSecret version numbers. Many of the requirements were tightened. Verify_data length depends on the cipher suite. Description of Bleichenbacher/Dlima attack defenses cleaned up.","title":"TLS Version key differences"},{"location":"nightwolf-cotribution/TLS-Difference/#tls-versions-difference","text":"TLS 1.0 This was an upgrade from SSL 3.0 and the differences were not dramatic, but they are significant enough that SSL 3.0 and TLS 1.0 don't interoperate. Some of the major differences between SSL 3.0 and TLS 1.0 are: derivation functions are different MACs are different - SSL 3.0 uses a modification of an early HMAC while TLS 1.0 uses HMAC. The Finished messages are different TLS has more alerts TLS requires DSS/DH support TLS 1.1 is an update to TLS 1.0. The major changes are: The Implicit Initialization Vector (IV) is replaced with an explicit IV to protect against Cipher block chaining (CBC) attacks. Handling of padded errors is changed to use the bad_record_mac alert rather than the decryption_failed alert to protect against CBC attacks. IANA registries are defined for protocol parameters. Premature closes no longer cause a session to be non-resumable. TLS 1.2 Based on TLS 1.1, TLS 1.2 contains improved flexibility. The major differences include: The MD5/SHA-1 combination in the pseudorandom function (PRF) was replaced with cipher-suite-specified PRFs. The MD5/SHA-1 combination in the digitally-signed element was replaced with a single hash. Signed elements include a field explicitly specifying the hash algorithm used. There was substantial cleanup to the client's and server's ability to specify which hash and signature algorithms they will accept. Addition of support for authenticated encryption with additional data modes. TLS Extensions definition and AES Cipher Suites were merged in. Tighter checking of EncryptedPreMasterSecret version numbers. Many of the requirements were tightened. Verify_data length depends on the cipher suite. Description of Bleichenbacher/Dlima attack defenses cleaned up.","title":"TLS versions Difference"},{"location":"nightwolf-cotribution/ansible_interview_questions/","text":"Ansible Interview Questions \uf0c1 We have prepared a set of frequently asked Ansible questions to help Freshers and Experienced Admins in their preparations for Interview. You will find these questions very helpful in your Linux/Ansible/DevOps Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is Ansible architecture ? How a normal yaml file is different from Ansible playbook ? What is Ansible playbook, define all terms in playbook ? What is handlers and notifiers ? What are resgisters ? What are various modules you have used in Ansible ? How to used when condition and loops in Ansible ? Write a playbook to start a service, stop a service and check the health check of service. What are Ansible adhoc commands ? What are roles and tell where you will keep the diffrenet files in diffrenet folders ? What is Ansible vault and how you store the secret files ? How the SSH connectivity takes place between two server in Ansible ? What is different between Puppet and Ansible ? How can I set the PATH or any other environment variable for a task or entire play ? Setting environment variables can be done with the environment keyword. It can be used at the task or other levels in the play. How do I handle different machines needing different user accounts or ports to log in with ? Setting inventory variables in the inventory file is the easiest way. For instance, suppose these hosts have different usernames and ports: [webservers] test.nightwolf.in ansible_port=5000 ansible_user=nightwolf foo.nightwolf.in ansible_port=5001 ansible_user=bar How do I get ansible to reuse connections, enable Kerberized SSH, or have Ansible pay attention to my local SSH config file ? Switch your default connection type in the configuration file to ssh, or use -c ssh to use Native OpenSSH for connections instead of the python paramiko library. In Ansible 1.2.1 and later, ssh will be used by default if OpenSSH is new enough to support ControlPersist as an option. Paramiko is great for starting out, but the OpenSSH type offers many advanced options. You will want to run Ansible from a machine new enough to support ControlPersist, if you are using this connection type. You can still manage older clients. If you are using RHEL 6, CentOS 6, SLES 10 or SLES 11 the version of OpenSSH is still a bit old, so consider managing from a Fedora or openSUSE client even though you are managing older nodes, or just use paramiko. How do I get Ansible to notice a dead target in a timely manner ? You can add -o ServerAliveInterval=NumberOfSeconds in ssh_args from ansible.cfg. Without this option, SSH and therefore Ansible will wait until the TCP connection times out. Another solution is to add ServerAliveInterval into your global SSH configuration. A good value for ServerAliveInterval is up to you to decide; keep in mind that ServerAliveCountMax=3 is the SSH default so any value you set will be tripled before terminating the SSH session. How do I see a list of all of the ansible_ variables ? Ansible by default gathers \u201cfacts\u201d about the machines under management, and these facts can be accessed in playbooks and in templates. To see a list of all of the facts that are available about a machine, you can run the setup module as an ad hoc action: ansible -m setup hostname This will print out a dictionary of all of the facts that are available for that particular host. You might want to pipe the output to a pager.This does NOT include inventory variables or internal \u2018magic\u2019 variables. How do I see all the inventory variables defined for my host? By running the following command, you can see inventory variables for a host: ansible-inventory --list --yaml How do I see all the variables specific to my host ? To see all host specific variables, which might include facts and other sources: ansible -m debug -a \"var=hostvars['hostname']\" localhost Unless you are using a fact cache, you normally need to use a play that gathers facts first, for facts included in the task above. How do I loop over a list of hosts in a group, inside of a template ? How do I access a variable name programmatically ? An example may come up where we need to get the ipv4 address of an arbitrary interface, where the interface to be used may be supplied via a role parameter or other input. Variable names can be built by adding strings together using \u201c~\u201d, like so: {{ hostvars[inventory_hostname]['ansible_' ~ which_interface]['ipv4']['address'] }} The trick about going through hostvars is necessary because it\u2019s a dictionary of the entire namespace of variables. inventory_hostname is a magic variable that indicates the current host you are looping over in the host loop. In the example above, if your interface names have dashes, you must replace them with underscores: {{ hostvars[inventory_hostname]['ansible_' ~ which_interface | replace('_', '-') ]['ipv4']['address'] }} How do I access shell environment variables ? On controller machine : Access existing variables from controller use the env lookup plugin. For example, to access the value of the HOME environment variable on the management machine: --- # ... vars: local_home: \"{{ lookup('env','HOME') }}\" On target machines : Environment variables are available via facts in the ansible_env variable: {{ ansible_env.HOME }} How do I generate encrypted passwords for the user module ? Ansible ad hoc command is the easiest option: ansible all -i localhost, -m debug -a \"msg={{ 'mypassword' | password_hash('sha512', 'mysecretsalt') }}\" The mkpasswd utility that is available on most Linux systems is also a great option: mkpasswd --method=sha-512 How do I get the original ansible_host when I delegate a task ? As the documentation states, connection variables are taken from the delegate_to host so ansible_host is overwritten, but you can still access the original via hostvars: original_host: \"{{ hostvars[inventory_hostname]['ansible_host'] }}\" This works for all overridden connection variables, like ansible_user, ansible_port, and so on. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Ansible Interview Questions"},{"location":"nightwolf-cotribution/ansible_interview_questions/#ansible-interview-questions","text":"We have prepared a set of frequently asked Ansible questions to help Freshers and Experienced Admins in their preparations for Interview. You will find these questions very helpful in your Linux/Ansible/DevOps Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is Ansible architecture ? How a normal yaml file is different from Ansible playbook ? What is Ansible playbook, define all terms in playbook ? What is handlers and notifiers ? What are resgisters ? What are various modules you have used in Ansible ? How to used when condition and loops in Ansible ? Write a playbook to start a service, stop a service and check the health check of service. What are Ansible adhoc commands ? What are roles and tell where you will keep the diffrenet files in diffrenet folders ? What is Ansible vault and how you store the secret files ? How the SSH connectivity takes place between two server in Ansible ? What is different between Puppet and Ansible ? How can I set the PATH or any other environment variable for a task or entire play ? Setting environment variables can be done with the environment keyword. It can be used at the task or other levels in the play. How do I handle different machines needing different user accounts or ports to log in with ? Setting inventory variables in the inventory file is the easiest way. For instance, suppose these hosts have different usernames and ports: [webservers] test.nightwolf.in ansible_port=5000 ansible_user=nightwolf foo.nightwolf.in ansible_port=5001 ansible_user=bar How do I get ansible to reuse connections, enable Kerberized SSH, or have Ansible pay attention to my local SSH config file ? Switch your default connection type in the configuration file to ssh, or use -c ssh to use Native OpenSSH for connections instead of the python paramiko library. In Ansible 1.2.1 and later, ssh will be used by default if OpenSSH is new enough to support ControlPersist as an option. Paramiko is great for starting out, but the OpenSSH type offers many advanced options. You will want to run Ansible from a machine new enough to support ControlPersist, if you are using this connection type. You can still manage older clients. If you are using RHEL 6, CentOS 6, SLES 10 or SLES 11 the version of OpenSSH is still a bit old, so consider managing from a Fedora or openSUSE client even though you are managing older nodes, or just use paramiko. How do I get Ansible to notice a dead target in a timely manner ? You can add -o ServerAliveInterval=NumberOfSeconds in ssh_args from ansible.cfg. Without this option, SSH and therefore Ansible will wait until the TCP connection times out. Another solution is to add ServerAliveInterval into your global SSH configuration. A good value for ServerAliveInterval is up to you to decide; keep in mind that ServerAliveCountMax=3 is the SSH default so any value you set will be tripled before terminating the SSH session. How do I see a list of all of the ansible_ variables ? Ansible by default gathers \u201cfacts\u201d about the machines under management, and these facts can be accessed in playbooks and in templates. To see a list of all of the facts that are available about a machine, you can run the setup module as an ad hoc action: ansible -m setup hostname This will print out a dictionary of all of the facts that are available for that particular host. You might want to pipe the output to a pager.This does NOT include inventory variables or internal \u2018magic\u2019 variables. How do I see all the inventory variables defined for my host? By running the following command, you can see inventory variables for a host: ansible-inventory --list --yaml How do I see all the variables specific to my host ? To see all host specific variables, which might include facts and other sources: ansible -m debug -a \"var=hostvars['hostname']\" localhost Unless you are using a fact cache, you normally need to use a play that gathers facts first, for facts included in the task above. How do I loop over a list of hosts in a group, inside of a template ? How do I access a variable name programmatically ? An example may come up where we need to get the ipv4 address of an arbitrary interface, where the interface to be used may be supplied via a role parameter or other input. Variable names can be built by adding strings together using \u201c~\u201d, like so: {{ hostvars[inventory_hostname]['ansible_' ~ which_interface]['ipv4']['address'] }} The trick about going through hostvars is necessary because it\u2019s a dictionary of the entire namespace of variables. inventory_hostname is a magic variable that indicates the current host you are looping over in the host loop. In the example above, if your interface names have dashes, you must replace them with underscores: {{ hostvars[inventory_hostname]['ansible_' ~ which_interface | replace('_', '-') ]['ipv4']['address'] }} How do I access shell environment variables ? On controller machine : Access existing variables from controller use the env lookup plugin. For example, to access the value of the HOME environment variable on the management machine: --- # ... vars: local_home: \"{{ lookup('env','HOME') }}\" On target machines : Environment variables are available via facts in the ansible_env variable: {{ ansible_env.HOME }} How do I generate encrypted passwords for the user module ? Ansible ad hoc command is the easiest option: ansible all -i localhost, -m debug -a \"msg={{ 'mypassword' | password_hash('sha512', 'mysecretsalt') }}\" The mkpasswd utility that is available on most Linux systems is also a great option: mkpasswd --method=sha-512 How do I get the original ansible_host when I delegate a task ? As the documentation states, connection variables are taken from the delegate_to host so ansible_host is overwritten, but you can still access the original via hostvars: original_host: \"{{ hostvars[inventory_hostname]['ansible_host'] }}\" This works for all overridden connection variables, like ansible_user, ansible_port, and so on. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Ansible Interview Questions"},{"location":"nightwolf-cotribution/aws-2/","text":"AWS Certified SysOps Administrator - Questions and Answers - Cont. (2) \uf0c1 These are AWS interview questions for experienced professionals. You will find these questions very helpful in your AWS professional role interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 151 A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25 and a private subnet with CIDR 20.0.0.128/25. The user has launched one instance each in the private and public subnets. Which of the below mentioned options cannot be the correct IP address (private IP. assigned to an instance in the public or private subnet? A. 20.0.0.255 B. 20.0.0.132 C. 20.0.0.122 D. 20.0.0.55 Answer: A Explanation: When the user creates a subnet in VPC, he specifies the CIDR block for the subnet. In this case the user has created a VPC with the CIDR block 20.0.0.0/24, which supports 256 IP addresses (20.0.0.0 to 20.0.0.255.. The public subnet will have IP addresses between 20.0.0.0 - 20.0.0.127 and the private subnet will have IP addresses between 20.0.0.128 - 20.0.0.255. AWS reserves the first four IP addresses and the last IP address in each subnet\u2019s CIDR block. These are not available for the user to use. Thus, the instance cannot have an IP address of 20.0.0.255 QUESTION NO: 152 A user has launched an EBS backed EC2 instance. The user has rebooted the instance. Which of the below mentioned statements is not true with respect to the reboot action? A. The private and public address remains the same B. The Elastic IP remains associated with the instance C. The volume is preserved D. The instance runs on a new host computer Answer: D Explanation: A user can reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. However, it is recommended that the user use the Amazon EC2 to reboot the instance instead of running the operating system reboot command from the instance. The instance remains on the same host computer and maintains its public DNS name, private IP address, and any data on its instance store volumes. It typically takes a few minutes for the reboot to complete, but the time it takes to reboot depends on the instance configuration. QUESTION NO: 153 A user has setup a web application on EC2. The user is generating a log of the application performance at every second. There are multiple entries for each second. If the user wants to send that data to CloudWatch every minute, what should he do? A. The user should send only the data of the 60th second as CloudWatch will map the receive data timezone with the sent data timezone B. It is not possible to send the custom metric to CloudWatch every minute C. Give CloudWatch the Min, Max, Sum, and SampleCount of a number of every minute D. Calculate the average of one minute and send the data to CloudWatch Answer: C Explanation: Amazon CloudWatch aggregates statistics according to the period length that the user has specified while getting data from CloudWatch. The user can publish as many data points as he wants with the same or similartime stamps. CloudWatch aggregates them by the period length when the user calls get statistics about those data points. CloudWatch records the average (sum of all items divided by the number of items. of the values received for every 1-minute period, as well as the number of samples, maximum value, and minimum value for the same time period. CloudWatch will aggregate all the data which have time stamps within a one-minute period. QUESTION NO: 154 An AWS root account owner is trying to create a policy to access RDS. Which of the below mentioned statements is true with respect to the above information? A. Create a policy which allows the users to access RDS and apply it to the RDS instances B. The user cannot access the RDS database if he is not assigned the correct IAM policy C. The root account owner should create a policy for the IAM user and give him access to the RDS services D. The policy should be created for the user and provide access for RDS Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the account owner wants to create a policy for RDS, the owner has to create an IAM user and define the policy which entitles the IAM user with various RDS services such as Launch Instance, Manage security group, Manage parameter group etc. QUESTION NO: 155 A user is using a small MySQL RDS DB. The user is experiencing high latency due to the Multi AZ feature.Which of the below mentioned options may not help the user in this situation? A. Schedule the automated back up in non-working hours B. Use a large or higher size instance C. Use PIOPS D. Take a snapshot from standby Replica Answer: D Explanation: An RDS DB instance which has enabled Multi AZ deployments may experience increased write and commit latency compared to a Single AZ deployment, due to synchronous data replication. The user may also face changes in latency if deployment fails over to the standby replica. For production workloads, AWS recommends the user to use provisioned IOPS and DB instance classes (m1.large and larger. as they are optimized for provisioned IOPS to give a fast, and consistent performance. With Multi AZ feature, the user can not have option to take snapshot from replica. QUESTION NO: 156 A user is displaying the CPU utilization, and Network in and Network out CloudWatch metrics data of a single instance on the same graph. The graph uses one Y-axis for CPU utilization and Network in and another Y-axis for Network out. Since Network in is too high, the CPU utilization data is not visible clearly on graph to the user. How can the data be viewed better on the same graph? A. It is not possible to show multiple metrics with the different units on the same graph B. Add a third Y-axis with the console to show all the data in proportion C. Change the axis of Network by using the Switch command from the graph D. Change the units of CPU utilization so it can be shown in proportion with Network Answer: C Explanation: Amazon CloudWatch provides the functionality to graph the metric data generated either by the AWS services or the custom metric to make it easier for the user to analyse. It is possible to show the multiple metrics with different units on the same graph. If the graph is not plotted properly due to a difference in the unit data over two metrics, the user can change the Y-axis of one of the graph by selecting that graph and clicking on the Switch option. QUESTION NO: 157 A user is planning to use AWS services for his web application. If the user is trying to set up his own billing management system for AWS, how can he configure it? A. Set up programmatic billing access. Download and parse the bill as per the requirement B. It is not possible for the user to create his own billing management service with AWS C. Enable the AWS CloudWatch alarm which will provide APIs to download the alarm data D. Use AWS billing APIs to download the usage report of each service from the AWS billing console Answer: A Explanation: AWS provides an option to have programmatic access to billing. Programmatic Billing Access leverages the existing Amazon Simple Storage Service (Amazon S3. APIs. Thus, the user can build applications that reference his billing data from a CSV (comma-separated value. file stored in an Amazon S3 bucket. AWS will upload the bill to the bucket every few hours and the user can download the bill CSV from the bucket, parse itand create a billing system as per the requirement. QUESTION NO: 158 A user is planning to schedule a backup for an EBS volume. The user wants security of the snapshot data. How can the user achieve data encryption with a snapshot? A. Use encrypted EBS volumes so that the snapshot will be encrypted by AWS B. While creating a snapshot select the snapshot with encryption C. By default the snapshot is encrypted by AWS D. Enable server side encryption for the snapshot using S3 Answer: A Explanation: AWS EBS supports encryption of the volume. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. The data at rest, the I/O as well as all the snapshots of the encrypted EBS will also be encrypted. EBS encryption is based on the AES-256 cryptographic algorithm, which is the industry standard. QUESTION NO: 159 A user has created a public subnet with VPC and launched an EC2 instance within it. The user is trying to delete the subnet. What will happen in this scenario? A. It will delete the subnet and make the EC2 instance as a part of the default subnet B. It will not allow the user to delete the subnet until the instances are terminated C. It will delete the subnet as well as terminate the instances D. The subnet can never be deleted independently, but the user has to delete the VPC first Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When an instance is launched it will have a network interface attached with it. The user cannot delete the subnet until he terminates the instance and deletes the network interface. QUESTION NO: 160 A user has setup an EBS backed instance and attached 2 EBS volumes to it. The user has setup a CloudWatch alarm on each volume for the disk data. The user has stopped the EC2 instance and detached the EBS volumes. What will be the status of the alarms on the EBS volume? A. OK B. Insufficient Data C. Alarm D. The EBS cannot be detached until all the alarms are removed Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. Alarms invoke actions only for sustained state changes. There are three states of the alarm: OK, Alarm and Insufficient data. In this case since the EBS is detached and inactive the state will be Insufficient. QUESTION NO: 161 A user has launched an EC2 instance from an instance store backed AMI. The infrastructure team wants to create an AMI from the running instance. Which of the below mentioned credentials is not required while creating the AMI? A. AWS account ID B. X.509 certificate and private key C. AWS login ID to login to the console D. Access key and secret access key Answer: C Explanation: When the user has launched an EC2 instance from an instance store backed AMI and the admin team wants to create an AMI from it, the user needs to setup the AWS AMI or the API tools first. Once the tool is setup the user will need the following credentials: AWS account ID; AWS access and secret access key; X.509 certificate with private key. QUESTION NO: 162 A user has configured an SSL listener at ELB as well as on the back-end instances. Which of the below mentioned statements helps the user understand ELB traffic handling with respect to the SSL listener? A. It is not possible to have the SSL listener both at ELB and back-end instances B. ELB will modify headers to add requestor details C. ELB will intercept the request to add the cookie details if sticky session is enabled D. ELB will not modify the headers Answer: D Explanation: When the user has configured Transmission Control Protocol (TCP. or Secure Sockets Layer (SSL. for both front-end and back-end connections of the Elastic Load Balancer, the load balancer forwards the request to the back-end instances without modifying the request headers unless the proxy header is enabled. SSL does not support sticky sessions. If the user has enabled a proxy protocol it adds the source and destination IP to the header. QUESTION NO: 163 A user has created a Cloudformation stack. The stack creates AWS services, such as EC2 instances, ELB, AutoScaling, and RDS. While creating the stack it created EC2, ELB and AutoScaling but failed to create RDS. What will Cloudformation do in this scenario? A. Cloudformation can never throw an error after launching a few services since it verifies all the steps before launching. B. It will warn the user about the error and ask the user to manually create RDS C. Rollback all the changes and terminate all the created services D. It will wait for the user\u2019s input about the error and correct the mistake after the input Answer: C Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The AWS Cloudformation stack is a collection of AWS resources which are created and managed as a single unit when AWS CloudFormation instantiates a template. If any of the services fails to launch, Cloudformation will rollback all the changes and terminate or delete all the created services. QUESTION NO: 164 A user is trying to launch an EBS backed EC2 instance under free usage. The user wants to achieve encryption of the EBS volume. How can the user encrypt the data at rest? A. Use AWS EBS encryption to encrypt the data at rest B. The user cannot use EBS encryption and has to encrypt the data manually or using a third party tool C. The user has to select the encryption enabled flag while launching the EC2 instance D. Encryption of volume is not available as a part of the free usage tier Answer: B Explanation: AWS EBS supports encryption of the volume while creating new volumes. It supports encryption of the data at rest, the I/O as well as all the snapshots of the EBS volume. The EBS supports encryption for the selected instance type and the newer generation instances, such as m3, c3, cr1, r3, g2. It is not supported with a micro instance. QUESTION NO: 165 A user has created a VPC with public and private subnets using the VPC wizard. The user has not launched any instance manually and is trying to delete the VPC. What will happen in this scenario? A. It will not allow to delete the VPC as it has subnets with route tables B. It will not allow to delete the VPC since it has a running route instance C. It will terminate the VPC along with all the instances launched by the wizard D. It will not allow to delete the VPC since it has a running NAT instance Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance with an elastic IP. If the user is trying to delete the VPC it will not allow as the NAT instance is still running. QUESTION NO: 166 An organization is measuring the latency of an application every minute and storing data inside a file in the JSON format. The organization wants to send all latency data to AWS CloudWatch. How can the organization achieve this? A. The user has to parse the file before uploading data to CloudWatch B. It is not possible to upload the custom data to CloudWatch C. The user can supply the file as an input to the CloudWatch command D. The user can use the CloudWatch Import command to import data from the file to CloudWatch Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user has to always include the namespace as part of the request. If the user wants to upload the custom data from a file, he can supply file name along with the parameter -- metric-data to command put-metric-data. QUESTION NO: 167 A user has launched an EBS backed instance with EC2-Classic. The user stops and starts the instance. Which of the below mentioned statements is not true with respect to the stop/start action? A. The instance gets new private and public IP addresses B. The volume is preserved C. The Elastic IP remains associated with the instance D. The instance may run on a anew host computer Answer: C Explanation: A user can always stop/start an EBS backed EC2 instance. When the user stops the instance, it first enters the stopping state, and then the stopped state. AWS does not charge the running cost but charges only for the EBS storage cost. If the instance is running in EC2-Classic, it receives a new private IP address; as the Elastic IP address (EIP. associated with the instance is no longer associated with that instance. QUESTION NO: 168 A user has launched an RDS postgreSQL DB with AWS. The user did not specify the maintenance window during creation. The user has configured RDS to update the DB instance type from micro to large. If the user wants to have it during the maintenance window, what will AWS do? A. AWS will not allow to update the DB until the maintenance window is configured B. AWS will select the default maintenance window if the user has not provided it C. AWS will ask the user to specify the maintenance window during the update D. It is not possible to change the DB size from micro to large with RDS Answer: B Explanation: AWS RDS has a compulsory maintenance window which by default is 30 minutes. If the user does not specify the maintenance window during the creation of RDS then AWS will select a 30-minute maintenance window randomly from an 8-hour block of time per region. In this case, Amazon RDS assigns a 30-minute maintenance window on a randomly selected day of the week. QUESTION NO: 169 A user has created a subnet in VPC and launched an EC2 instance within it. The user has not selected the option to assign the IP address while launching the instance. The user has 3 elastic IPs and is trying to assign one of the Elastic IPs to the VPC instance from the console. The console does not show any instance in the IP assignment screen. What is a possible reason that the instance is unavailable in the assigned IP console? A. The IP address may be attached to one of the instances B. The IP address belongs to a different zone than the subnet zone C. The user has not created an internet gateway D. The IP addresses belong to EC2 Classic; so they cannot be assigned to VPC Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When the user is launching an instance he needs toselect an option which attaches a public IP to the instance. If the user has not selected the option to attach the public IP then it will only have a private IP when launched. If the user wants to connect to an instance from the internet he should create an elastic IP with VPC. If the elastic IP is a part of EC2 Classic it cannot be assigned to a VPC instance. QUESTION NO: 170 A user has launched multiple EC2 instances for the purpose of development and testing in the same region. The user wants to find the separate cost for the production and development instances. How can the user find the cost distribution? A. The user should download the activity report of the EC2 services as it has the instance ID wise data B. It is not possible to get the AWS cost usage data of single region instances separately C. The user should use Cost Distribution Metadata and AWS detailed billing D. The user should use Cost Allocation Tags and AWS billing reports Answer: D Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources (such as Amazon EC2 instances or Amazon S3 buckets., AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. The user can apply tags which represent business categories (such as cost centres, application names, or instance type \u2013 Production/Dev. to organize usage costs across multiple services. QUESTION NO: 171 A user has created a VPC with CIDR 20.0.0.0/16 using VPC Wizard. The user has created a public CIDR (20.0.0.0/24) and a VPN only subnet CIDR (20.0.1.0/24) along with the hardware VPN access to connect to the user\u2019s data centre. Which of the below mentioned components is not present when the VPC is setup with the wizard? A. Main route table attached with a VPN only subnet B. A NAT instance configured to allow the VPN subnet instances to connect with the internet C. Custom route table attached with a public subnet D. An internet gateway for a public subnet Answer: B Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will update the main route table used with the VPN-only subnet, create a custom route table and associate it with the public subnet. It also creates an internet gateway for the public subnet. The wizard does not create a NAT instance by default. The user can create it manually and attach it with a VPN only subnet. QUESTION NO: 172 A user has created a VPC with the public subnet. The user has created a security group for that VPC. Which of the below mentioned statements is true when a security group is created? A. It can connect to the AWS services, such as S3 and RDS by default B. It will have all the inbound traffic by default C. It will have all the outbound traffic by default D. It will by default allow traffic to the internet gateway Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level while ACLs work at the subnet level. When a user creates a security group with AWS VPC, by default it will allow all the outbound traffic but block all inbound traffic. QUESTION NO: 173 A user has setup an Auto Scaling group. The group has failed to launch a single instance for more than 24 hours. What will happen to Auto Scaling in this condition? A. Auto Scaling will keep trying to launch the instance for 72 hours B. Auto Scaling will suspend the scaling process C. Auto Scaling will start an instance in a separate region D. The Auto Scaling group will be terminated automatically Answer: B Explanation: If Auto Scaling is trying to launch an instance and if the launching of the instance fails continuously, it will suspend the processes for the Auto Scaling groups since it repeatedly failed to launch an instance. This is known as an administrative suspension. It commonly applies to the Auto Scaling group that has no running instances which is trying to launch instances for more than 24 hours, and has not succeeded in that to do so. QUESTION NO: 174 A user is planning to set up the Multi AZ feature of RDS. Which of the below mentioned conditions won't take advantage of the Multi AZ feature? A. Availability zone outage B. A manual failover of the DB instance using Reboot with failover option C. Region outage D. When the user changes the DB instance\u2019s server type Answer: C Explanation: Amazon RDS when enabled with Multi AZ will handle failovers automatically. Thus, the user can resume database operations as quickly as possible without administrative intervention. The primary DB instance switches over automatically to the standby replica if any of the following conditions occur: An Availability Zone outage The primary DB instance fails The DB instance's server type is changed The DB instance is undergoing software patching A manual failover of the DB instance was initiated using Reboot with failover QUESTION NO: 175 An organization has configured Auto Scaling with ELB. One of the instance health check returns the status as Impaired to Auto Scaling. What will Auto Scaling do in this scenario? A. Perform a health check until cool down before declaring that the instance has failed B. Terminate the instance and launch a new instance C. Notify the user using SNS for the failed state D. Notify ELB to stop sending traffic to the impaired instance Answer: B Explanation: The Auto Scaling group determines the health state of each instance periodically by checking the results of the Amazon EC2 instance status checks. If the instance status description shows any other state other than \u201crunning\u201d or the system status description shows impaired, Auto Scaling considers the instance to be unhealthy. Thus, it terminates the instance and launches a replacement. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 176 A user is using Cloudformation to launch an EC2 instance and then configure an application after the instance is launched. The user wants the stack creation of ELB and AutoScaling to wait until the EC2 instance is launched and configured properly. How can the user configure this? A. It is not possible that the stack creation will wait until one service is created and launched B. The user can use the HoldCondition resource to wait for the creation of the other dependent resources C. The user can use the DependentCondition resource to hold the creation of the other dependent resources D. The user can use the WaitCondition resource to hold the creation of the other 1034 dependent resources Answer: D Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. AWS CloudFormation provides a WaitCondition resource which acts as a barrier and blocks the creation of other resources until a completion signal is received from an external source, such as a user application or management system. QUESTION NO: 177 An organization has configured two single availability zones. The Auto Scaling groups are configured in separate zones. The user wants to merge the groups such that one group spans across multiple zones. How can the user configure this? A. Run the command as-join-auto-scaling-group to join the two groups B. Run the command as-update-auto-scaling-group to configure one group to span across zones and delete the other group C. Run the command as-copy-auto-scaling-group to join the two groups D. Run the command as-merge-auto-scaling-group to merge the groups Answer: B Explanation: If the user has configured two separate single availability zone Auto Scaling groups and wants to merge them then he should update one of the groups and delete the other one. While updating the first group it is recommended that the user should increase the size of the minimum, maximum and desired capacity as a summation of both the groups. QUESTION NO: 178 An AWS account wants to be part of the consolidated billing of his organization\u2019s payee account. How can the owner of that account achieve this? A. The payee account has to request AWS support to link the other accounts with his account B. The owner of the linked account should add the payee account to his master account list from the billing console C. The payee account will send a request to the linked account to be a part of consolidated billing D. The owner of the linked account requests the payee account to add his account to consolidated billing Answer: C Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. To add a particular account (linked. to the master (payee. account, the payee account has to request the linked account to join consolidated billing. Once the linked account accepts the request henceforth all charges incurred by the linked account will be paid by the payee account. QUESTION NO: 179 A sysadmin has created the below mentioned policy on an S3 bucket named cloudacademy. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy] }] A. It will make the cloudacademy bucket as well as all its objects as public B. It will allow everyone to view the ACL of the bucket C. It will give an error as no object is defined as part of the policy while the action defines the rule about the object D. It will make the cloudacademy bucket as public Answer: D Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. In the sample policy the action says \u201cS3:ListBucket\u201d for effect Allow on Resource arn:aws:s3:::cloudacademy. This will make the cloudacademy bucket public. \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy] }] QUESTION NO: 180 A user has launched two EBS backed EC2 instances in the US-East-1a region. The user wants to change the zone of one of the instances. How can the user change it? A. The zone can only be modified using the AWS CLI B. It is not possible to change the zone of an instance after it is launched C. Stop one of the instances and change the availability zone D. From the AWS EC2 console, select the Actions - > Change zones and specify the new zone Answer: B Explanation: With AWS EC2, when a user is launching an instance he can select the availability zone (AZ. at the time of launch. If the zone is not selected, AWS selects it on behalf of the user. Once the instance is launched, the user cannot change the zone of that instance unless he creates an AMI of that instance and launches a new instance from it. QUESTION NO: 181 An organization (account ID 123412341234. has configured the IAM policy to allow the user to modify his credentials. What will the below mentioned statement allow the user to perform? { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"iam:AddUserToGroup\", \"iam:RemoveUserFromGroup\", \"iam:GetGroup\" ], \"Resource\": \"arn:aws:iam:: 123412341234:group/TestingGroup\" }] A. The IAM policy will throw an error due to an invalid resource name B. The IAM policy will allow the user to subscribe to any IAM group C. Allow the IAM user to update the membership of the group called TestingGroup D. Allow the IAM user to delete the TestingGroup Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (account ID 123412341234. wants their users to manage their subscription to the groups, they should create a relevant policy for that. The below mentioned policy allows the respective IAM user to update the membership of the group called MarketingGroup. { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"iam:AddUserToGroup\", \"iam:RemoveUserFromGroup\", \"iam:GetGroup\" ], \"Resource\": \"arn:aws:iam:: 123412341234:group/ TestingGroup \" }] QUESTION NO: 182 A user has configured ELB with two EBS backed instances. The user has stopped the instances for 1 week to save costs. The user restarts the instances after 1 week. Which of the below mentioned statements will help the user to understand the ELB and instance registration better? A. There is no way to register the stopped instances with ELB B. The user cannot stop the instances if they are registered with ELB C. If the instances have the same Elastic IP assigned after reboot they will be registered with ELB D. The instances will automatically get registered with ELB Answer: C Explanation: Elastic Load Balancing registers the user\u2019s load balancer with his EC2 instance using the associated IP address. When the instances are stopped and started back they will have a different IP address. Thus, they will not get registered with ELB unless the user manually registers them. If the instances are assigned the same Elastic IP after reboot they will automatically get registered with ELB. QUESTION NO: 183 A user is trying to connect to a running EC2 instance using SSH. However, the user gets a Host key not found error. Which of the below mentioned options is a possible reason for rejection? A. The user has provided the wrong user name for the OS login B. The instance CPU is heavily loaded C. The security group is not configured properly D. The access key to connect to the instance is wrong Answer: A Explanation: If the user is trying to connect to a Linux EC2 instance and receives the Host Key not found error the probable reasons are: The private key pair is not right The user name to login is wrong QUESTION NO: 184 A user has hosted an application on EC2 instances. The EC2 instances are configured with ELB and Auto Scaling. The application server session time out is 2 hours. The user wants to configure connection draining to ensure that all in-flight requests are supported by ELB even though the instance is being deregistered. What time out period should the user specify for connection draining? A. 5 minutes B. 1 hour C. 30 minutes D. 2 hours Answer: B QUESTION NO: 185 A user is using the AWS EC2. The user wants to make so that when there is an issue in the EC2 server, such as instance status failed, it should start a new instance in the user\u2019s private cloud. Which AWS service helps to achieve this automation? A. AWS CloudWatch + Cloudformation B. AWS CloudWatch + AWS AutoScaling + AWS ELB C. AWS CloudWatch + AWS VPC D. AWS CloudWatch + AWS SNS Answer: D Explanation: Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can configure a web service (HTTP End point. in his data centre which receives data and launches an instance in the private cloud. The user should configure the CloudWatch alarm to send a notification to SNS when the \u201cStatusCheckFailed\u201d metric is true for the EC2 instance. The SNS topic can be configured to send a notification to the user\u2019s HTTP end point which launches an instance in the private cloud. QUESTION NO: 186 A sys admin has enabled logging on ELB. Which of the below mentioned fields will not be a part of the log file name? A. Load Balancer IP B. EC2 instance IP C. S3 bucket name D. Random string Answer: B Explanation: Elastic Load Balancing access logs capture detailed information for all the requests made to the load balancer. Elastic Load Balancing publishes a log file from each load balancer node at the interval that the user has specified. The load balancer can deliver multiple logs for the same period. Elastic Load Balancing creates log file names in the following format: \u201c{Bucket}/{Prefix}/ AWSLogs/{AWS AccountID}/elasticloadbalancing/{Region}/{Year}/{Month}/{Day}/{AWS Account ID}_elasticloadbalancing_{Region}_{Load Balancer Name}_{End Time}_{Load Balancer IP}_{Random String}.log\u201c QUESTION NO: 187 A user has created a queue named \u201cawsmodule\u201d with SQS. One of the consumers of queue is down for 3 days and then becomes available. Will that component receive message from queue? A. Yes, since SQS by default stores message for 4 days B. No, since SQS by default stores message for 1 day only C. No, since SQS sends message to consumers who are available that time D. Yes, since SQS will not delete message until it is delivered to all consumers Answer: A Explanation: SQS allows the user to move data between distributed components of applications so they can perform different tasks without losing messages or requiring each component to be always available. Queues retain messages for a set period of time. By default, a queue retains messages for four days. However, the user can configure a queue to retain messages for up to 14 days after the message has been sent. QUESTION NO: 188 An organization has setup multiple IAM users. The organization wants that each IAM user accesses the IAM console only within the organization and not from outside. How can it achieve this? A. Create an IAM policy with the security group and use that security group for AWS console login B. Create an IAM policy with a condition which denies access when the IP address range is not from the organization C. Configure the EC2 instance security group which allows traffic only from the organization\u2019s IP range D. Create an IAM policy with VPC and allow a secure gateway between the organization and AWS Console Answer: B Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user can add conditions as a part of the IAM policies. The condition can be set on AWS Tags, Time, and Client IP as well as on many other parameters. If the organization wants the user to access only from a specific IP range, they should set an IAM policy condition which denies access when the IP is not in a certain range. E.g. The sample policy given below denies all traffic when the IP is not in a certain range. \"Statement\": [{ \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\", \"Condition\": { \"NotIpAddress\": { \"aws:SourceIp\": [\"10.10.10.0/24\", \"20.20.30.0/24\"] } } }] QUESTION NO: 189 An organization has created one IAM user and applied the below mentioned policy to the user. What entitlements do the IAM users avail with this policy? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\" \"Action\": [ \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:Describe*\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"autoscaling:Describe*\", \"Resource\": \"*\" } ] } A. The policy will allow the user to perform all read only activities on the EC2 services B. The policy will allow the user to list all the EC2 resources except EBS C. The policy will allow the user to perform all read and write activities on the EC2 services D. The policy will allow the user to perform all read only activities on the EC2 services except load Balancing Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If an organization wants to setup read only access to EC2 for a particular user, they should mention the action in the IAM policy which entitles the user for Describe rights for EC2, CloudWatch, Auto Scaling and ELB. In the policy shown below, the user will have read only access for EC2 and EBS, CloudWatch and Auto Scaling. Since ELB is not mentioned as a part of the list, the user will not have access to ELB. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:Describe*\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"autoscaling:Describe*\", \"Resource\": \"*\" } ] } QUESTION NO: 190 A user has enabled session stickiness with ELB. The user does not want ELB to manage the cookie; instead he wants the application to manage the cookie. What will happen when the server instance, which is bound to a cookie, crashes? A. The response will have a cookie but stickiness will be deleted B. The session will not be sticky until a new cookie is inserted C. ELB will throw an error due to cookie unavailability D. The session will be sticky and ELB will route requests to another server as ELB keeps replicating the Cookie Answer: B Explanation: With Elastic Load Balancer, if the admin has enabled a sticky session with application controlled stickiness, the load balancer uses a special cookie generated by the application to associate the session with the original server which handles the request. ELB follows the lifetime of the application-generated cookie corresponding to the cookie name specified in the ELB policy configuration. The load balancer only inserts a new stickiness cookie if the application response includes a new application cookie. The load balancer stickiness cookie does not update with each request. If the application cookie is explicitly removed or expires, the session stops being sticky until a new application cookie is issued. QUESTION NO: 191 A user is observing the EC2 CPU utilization metric on CloudWatch. The user has observed some interesting patterns while filtering over the 1 week period for a particular hour. The user wants to zoom that data point to a more granular period. How can the user do that easily with CloudWatch? A. The user can zoom a particular period by selecting that period with the mouse and then releasing the mouse B. The user can zoom a particular period by double clicking on that period with the mouse C. The user can zoom a particular period by specifying the aggregation data for that period D. The user can zoom a particular period by specifying the period in the Time Range Answer: A QUESTION NO: 192 A user has created an Auto Scaling group with default configurations from CLI. The user wants to setup the CloudWatch alarm on the EC2 instances, which are launched by the Auto Scaling group. The user has setup an alarm to monitor the CPU utilization every minute. Which of the below mentioned statements is true? A. It will fetch the data at every minute but the four data points [corresponding to 4 minutes] will not have value since the EC2 basic monitoring metrics are collected every five minutes B. It will fetch the data at every minute as detailed monitoring on EC2 will be enabled by the default launch configuration of Auto Scaling C. The alarm creation will fail since the user has not enabled detailed monitoring on the EC2 instances D . The user has to first enable detailed monitoring on the EC2 instances to support alarm monitoring at every minute Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates an Auto Scaling launch config using CLI, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. Thus, by default detailed monitoring will be enabled for Auto Scaling as well as for all the instances launched by that Auto Scaling group. QUESTION NO: 193 A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is not true in this scenario? A. The VPC will create a routing instance and attach it with a public subnet B. The VPC will create two subnets 116789 C. The VPC will create one internet gateway and attach it to VPC D. The VPC will launch one NAT instance with an elastic IP Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance with an elastic IP. Wizard will also create two subnets with route tables. It will also create an internet gateway and attach it to the VPC. QUESTION NO: 194 A user has configured ELB with a TCP listener at ELB as well as on the back-end instances. The user wants to enable a proxy protocol to capture the source and destination IP information in the header. Which of the below mentioned statements helps the user understand a proxy protocol with TCP configuration? A. If the end user is requesting behind a proxy server then the user should not enable a proxy protocol on ELB B. ELB does not support a proxy protocol when it is listening on both the load balancer and the backend instances C. Whether the end user is requesting from a proxy server or directly, it does not make a difference for the proxy protocol D. If the end user is requesting behind the proxy then the user should add the \u201cisproxy\u201d flag to the ELB Configuration Answer: A Explanation: When the user has configured Transmission Control Protocol (TCP. or Secure Sockets Layer (SSL. for both front-end and back-end connections of the Elastic Load Balancer, the load balancer forwards the request to the back-end instances without modifying the request headers unless the proxy header is enabled. If the end user is requesting from a Proxy Protocol enabled proxy server, then the ELB admin should not enable the Proxy Protocol on the load balancer. If the Proxy Protocol is enabled on both the proxy server and the load balancer, the load balancer will add another header to the request which already has a header from the proxy server. This duplication may result in errors. QUESTION NO: 195 A user has launched 5 instances in EC2-CLASSIC and attached 5 elastic IPs to the five different instances in the US East region. The user is creating a VPC in the same region. The user wants to assign an elastic IP to the VPC instance. How can the user achieve this? A. The user has to request AWS to increase the number of elastic IPs associated with the account B. AWS allows 10 EC2 Classic IPs per region ; so it will allow to allocate new Elastic IPs to the same region C. The AWS will not allow to create a new elastic IP in VPC; it will throw an error D. The user can allocate a new IP address in VPC as it has a different limit than EC2 Answer: D Explanation: Section: (none) A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. A user can have 5 IP addresses per region with EC2 Classic. The user can have 5 separate IPs with VPC in the same region as it has a separate limit than EC2 Classic. QUESTION NO: 196 A user has created a subnet in VPC and launched an EC2 instance within it. The user has not selected the option to assign the IP address while launching the instance. Which of the below mentioned statements is true with respect to this scenario? A. The instance will always have a public DNS attached to the instance by default B. The user can directly attach an elastic IP to the instance C. The instance will never launch if the public IP is not assigned D. The user would need to create an internet gateway and then attach an elastic IP to the instance to connect from internet Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When the user is launching an instance he needs to select an option which attaches a public IP to the instance. If the user has not selected the option to attach the public IP then it will only have a private IP when launched. The user cannot connect to the instance from the internet. If the user wants an elastic IP to connect to the instance from the internet he should create an internet gateway and assign an elastic IP to instance. QUESTION NO: 197 An organization has applied the below mentioned policy on an IAM group which has selected the IAM users. What entitlements do the IAM users avail with this policy? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" } ] } A. The policy is not created correctly. It will throw an error for wrong resource name B. The policy is for the group. Thus, the IAM user cannot have any entitlement to this C. It allows full access to all AWS services for the IAM users who are a part of this group D. If this policy is applied to the EC2 resource, the users of the group will have full access to the EC2 Resources Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The IAM group allows the organization to specify permissions for a collection of users. With the below mentioned policy, it will allow the group full access (Admin. to all AWS services. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" } ] } QUESTION NO: 198 A user is configuring a CloudWatch alarm on RDS to receive a notification when the CPU utilization of RDS is higher than 50%. The user has setup an alarm when there is some iinactivity on RDS, such as RDS unavailability. How can the user configure this? A. Setup the notification when the CPU is more than 75% on RDS B. Setup the notification when the state is Insufficient Data C. Setup the notification when the CPU utilization is less than 10% D. It is not possible to setup the alarm on RDS Answer: B Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The alarm has three states: Alarm, OK and Insufficient data. The Alarm will change to Insufficient Data when any of the three situations arise: when the alarm has just started, when the metric is not available or when enough data is not available for the metric to determine the alarm state. If the user wants to find that RDS is not available, he can setup to receive the notification when the state is in Insufficient data. QUESTION NO: 199 George has shared an EC2 AMI created in the US East region from his AWS account with Stefano. George copies the same AMI to the US West region. Can Stefano access the copied AMI of George\u2019s account from the US West region? A. No, copy AMI does not copy the permission B. It is not possible to share the AMI with a specific account C. Yes, since copy AMI copies all private account sharing permissions D. Yes, since copy AMI copies all the permissions attached with the AMI Answer: A Explanation: Within EC2, when the user copies an AMI, the new AMI is fully independent of the source AMI; there is no link to the original (source. AMI. AWS does not copy launch the permissions, userdefined tags or the Amazon S3 bucket permissions from the source AMI to the new AMI. Thus, in this case by default Stefano will not have access to the AMI in the US West region. QUESTION NO: 200 A user has created a VPC with a subnet and a security group. The user has launched an instance in that subnet and attached a public IP. The user is still unable to connect to the instance. The internet gateway has also been created. What can be the reason for the error? A. The internet gateway is not configured with the route table B. The private IP is not present C. The outbound traffic on the security group is disabled D. The internet gateway is not configured with the security group Answer: A Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level. When a user launches an instance and wants to connect to an instance, he needs an internet gateway. The internet gateway should be configured with the route table to allow traffic from the internet. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 201 A user is trying to setup a security policy for ELB. The user wants ELB to meet the cipher supported by the client by configuring the server order preference in ELB security policy. Which of the below mentioned preconfigured policies supports this feature? A. ELBSecurity Policy-2014-01 B. ELBSecurity Policy-2011-08 C. ELBDefault Negotiation Policy D. ELBSample- OpenSSLDefault Cipher Policy Answer: A Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. If the load balancer is configured to support the Server Order Preference, then load balancer gets to select the first cipher in its list that matches any one of the ciphers in client's list. When the user verifies the preconfigured policies supported by ELB, the policy \u201cELBSecurity Policy-2014-01\u201d supports server order preference. QUESTION NO: 202 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling AlarmNotification (which notifies Auto Scaling for CloudWatch alarms. process for a while. What will Auto Scaling do during this period? A. AWS will not receive the alarms from CloudWatch B. AWS will receive the alarms but will not execute the Auto Scaling policy C. Auto Scaling will execute the policy but it will not launch the instances until the process is resumed D. It is not possible to suspend the AlarmNotification process Answer: B Explanation: Auto Scaling performs various processes, such as Launch, Terminate Alarm Notification etc. The user can also suspend individual process. The AlarmNotification process type accepts notifications from the Amazon CloudWatch alarms that are associated with the Auto Scaling group. If the user suspends this process type, Auto Scaling will not automatically execute the scaling policies that would be triggered by the alarms. QUESTION NO: 203 George has launched three EC2 instances inside the US-East-1a zone with his AWS account. Ray has launched two EC2 instances in the US-East-1a zone with his AWS account. Which of the below entioned statements will help George and Ray understand the availability zone (AZ. concept better? A. The instances of George and Ray will be running in the same data centre B. All the instances of George and Ray can communicate over a private IP with a minimal cost C. All the instances of George and Ray can communicate over a private IP without any cost D. The US-East-1a region of George and Ray can be different availability zones Answer: D Explanation: Each AWS region has multiple, isolated locations known as Availability Zones. To ensure that the AWS resources are distributed across the Availability Zones for a region, AWS independently maps the Availability Zones to identifiers for each account. In this case the Availability Zone US-East-1a where George\u2019s EC2 instances are running might not be the same location as the US-East-1a zone of Ray\u2019s EC2 instances. There is no way for the user to coordinate the Availability Zones between accounts. QUESTION NO: 204 A user had aggregated the CloudWatch metric data on the AMI ID. The user observed some abnormal behaviour of the CPU utilization metric while viewing the last 2 weeks of data. The user wants to share that data with his manager. How can the user achieve this easily with the AWS console? A. The user can use the copy URL functionality of CloudWatch to share the exact details B. The user can use the export data option from the CloudWatch console to export the current data point C. The user has to find the period and data and provide all the aggregation information to the manager D. The user can use the CloudWatch data copy functionality to copy the current data points Answer: A Explanation: Amazon CloudWatch provides the functionality to graph the metric data generated either by the AWS services or the custom metric to make it easier for the user to analyse. The console provides the option to save the URL or bookmark it so that it can be used in the future by typing the same URL. The Copy URL functionality is available under the console when the user selects any metric to view. QUESTION NO: 205 A user has setup a CloudWatch alarm on the EC2 instance for CPU utilization. The user has setup to receive a notification on email when the CPU utilization is higher than 60%. The user is running a virus scan on the same instance at a particular time. The user wants to avoid receiving an email at this time. What should the user do? A. Remove the alarm B. Disable the alarm for a while using CLI C. Modify the CPU utilization by removing the email alert D. Disable the alarm for a while using the console Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. When the user has setup an alarm and it is know that for some unavoidable event the status may change to Alarm, the user can disable the alarm using the DisableAlarmActions API or from the command line mon-disable-alarm-actions. QUESTION NO: 206 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned SSL protocols is not supported by the security policy? A. TLS 1.3 B. TLS 1.2 C. SSL 2.0 D. SSL 3.0 Answer: A Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and loadbalancer. Elastic Load Balancing supports the following versions of the SSL protocol: TLS 1.2 TLS 1.1 TLS 1.0 SSL 3.0 SSL 2.0 QUESTION NO: 207 A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group for the public subnet (WebSecGrp. and the private subnet(DBSecGrp). Which of the below mentioned entries is required in the private subnet database security group (DBSecGrp)? A. Allow Inbound on port 3306 for Source Web Server Security Group (WebSecGrp. B. Allow Inbound on port 3306 from source 20.0.0.0/16 C. Allow Outbound on port 3306 for Destination Web Server Security Group (WebSecGrp. D. Allow Outbound on port 80 for Destination NAT Instance IP Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the private subnet can receive inbound traffic from the public subnet on the DB port. Thus, configure port 3306 in Inbound with the source as the Web Server Security Group (WebSecGrp.. The user should configure ports 80 and 443 for Destination 0.0.0.0/0 as the route table directs traffic to the NAT instance from the private subnet. QUESTION NO: 208 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created public and VPN only subnets along with hardware VPN access to connect to the user\u2019s data centre. The user has not yet launched any instance as well as modified or deleted any setup. He wants to delete this VPC from the console. Will the console allow the user to delete the VPC? A. Yes, the console will delete all the setups and also delete the virtual private gateway B. No, the console will ask the user to manually detach the virtual private gateway first and then allow deleting the VPC C. Yes, the console will delete all the setups and detach the virtual private gateway D. No, since the NAT instance is running Answer: C Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. If the virtual private gateway is attached with VPC and the user deletes the VPC from the console it will first detach the gateway automatically and only then delete the VPC. QUESTION NO: 209 A user is trying to create a PIOPS EBS volume with 4000 IOPS and 100 GB size. AWS does not allow the user to create this volume. What is the possible root cause for this? A. The ratio between IOPS and the EBS volume is higher than 30 B. The maximum IOPS supported by EBS is 3000 C. The ratio between IOPS and the EBS volume is lower than 50 D. PIOPS is supported for EBS higher than 500 GB size Answer: A Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30; for example, a volume with 3000 IOPS must be at least 100 GB. QUESTION NO: 210 A user has setup a custom application which generates a number in decimals. The user wants to track that number and setup the alarm whenever the number is above a certain limit. The application is sending the data to CloudWatch at regular intervals for this purpose. Which of the below mentioned statements is not true with respect to the above scenario? A. The user can get the aggregate data of the numbers generated over a minute and send it to CloudWatch B. The user has to supply the timezone with each data point C. CloudWatch will not truncate the number until it has an exponent larger than 126 (i.e. (1 x 10^126) ). D. The user can create a file in the JSON format with the metric name and value and supply it to CloudWatch Answer: B QUESTION NO: 211 A user has launched an EC2 Windows instance from an instance store backed AMI. The user has also set the Instance initiated shutdown behavior to stop. What will happen when the user shuts down the OS? A. It will not allow the user to shutdown the OS when the shutdown behaviour is set to Stop B. It is not possible to set the termination behaviour to Stop for an Instance store backed AMI instance C. The instance will stay running but the OS will be shutdown D. The instance will be terminated Answer: B Explanation: When the EC2 instance is launched from an instance store backed AMI, it will not allow the user to configure the shutdown behaviour to \u201cStop\u201d. It gives a warning that the instance does not have the EBS root volume. QUESTION NO: 212 A user has enabled versioning on an S3 bucket. The user is using server side encryption for data at Rest. If the user is supplying his own keys for encryption (SSE-C., which of the below mentioned statements is true? A. The user should use the same encryption key for all versions of the same object B. It is possible to have different encryption keys for different versions of the same object C. AWS S3 does not allow the user to upload his own keys for server side encryption D. The SSE-C does not work when versioning is enabled Answer: B Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. If the bucket is versioningenabled, each object version uploaded by the user using the SSE-C feature can have its own encryption key. The user is responsible for tracking which encryption key was used for which object's version QUESTION NO: 213 A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 in this VPC. The user is trying to create another subnet with the same VPC for CIDR 20.0.0.1/24. What will happen in this scenario? A. The VPC will modify the first subnet CIDR automatically to allow the second subnet IP range B. It is not possible to create a subnet with the same CIDR as VPC C. The second subnet will be created D. It will throw a CIDR overlaps error Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. The user can create a subnet with the same size of VPC. However, he cannot create any other subnet since the CIDR of the second subnet will conflict with the first subnet. QUESTION NO: 214 A user has launched an RDS MySQL DB with the Multi AZ feature. The user has scheduled the scaling of instance storage during maintenance window. What is the correct order of events during maintenance window? Perform maintenance on standby Promote standby to primary Perform maintenance on original primary Promote original master back as primary A. 1, 2, 3, 4 B. 1, 2, 3 C. 2, 3, 1, 4 Answer: B Explanation: Running MySQL on the RDS DB instance as a Multi-AZ deployment can help the user reduce the impact of a maintenance event, as the Amazon will conduct maintenance by following the steps in the below mentioned order: Perform maintenance on standby Promote standby to primary Perform maintenance on original primary, which becomes the new standby. QUESTION NO: 215 A sys admin is using server side encryption with AWS S3. Which of the below mentioned statements helps the user understand the S3 encryption functionality? A. The server side encryption with the user supplied key works when versioning is enabled B. The user can use the AWS console, SDK and APIs to encrypt or decrypt the content for server side encryption with the user supplied key. C. The user must send an AES-128 encrypted key D. The user can upload his own encryption key to the S3 console Answer: A Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key. The encryption with the user supplied key (SSE-C. does not work with the AWS console. The S3 does not store the keys and the user has to send a key with each request. The SSE-C works when the user has enabled versioning. QUESTION NO: 216 A root account owner is trying to understand the S3 bucket ACL. Which of the below mentioned options cannot be used to grant ACL on the object using the authorized predefined group? A. Authenticated user group B. All users group C. Log Delivery Group D. Canonical user group Answer: D Explanation: An S3 bucket ACL grantee can be an AWS account or one of the predefined Amazon S3 groups. Amazon S3 has a set of predefined groups. When granting account access to a group, the user can specify one of the URLs of that group instead of a canonical user ID. AWS S3 has the following predefined groups: Authenticated Users group: It represents all AWS accounts. All Users group: Access permission to this group allows anyone to access the resource. Log Delivery group: WRITE permission on a bucket enables this group to write server access logs to the bucket. QUESTION NO: 217 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24. and VPN only subnets CIDR (20.0.1.0/24. along with the VPN gateway (vgw-12345. to connect to the user\u2019s data centre. The user\u2019s data centre has CIDR 172.28.0.0/12. The user has also setup a NAT instance (i-123456. to allow traffic to the internet from the VPN subnet. Which of the below mentioned options is not a valid entry for the main route table in this scenario? A. Destination: 20.0.1.0/24 and Target: i-12345 B. Destination: 0.0.0.0/0 and Target: i-12345 C. Destination: 172.28.0.0/12 and Target: vgw-12345 D. Destination: 20.0.0.0/16 and Target: local Answer: A Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. If the user has setup a NAT instance to route all the internet requests then all requests to the internet should be routed to it. All requests to the organization\u2019s DC will be routed to the VPN gateway. Here are the valid entries for the main route table in this scenario: Destination: 0.0.0.0/0 & Target: i-12345 (To route all internet traffic to the NAT Instance. Destination: 172.28.0.0/12 & Target: vgw-12345 (To route all the organization\u2019s data centre traffic to the VPN gateway. Destination: 20.0.0.0/16 & Target: local (To allow local routing in VPC. QUESTION NO: 218 A user has created a VPC with public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24 . The NAT instance ID is i-a12345. Which of the below mentioned entries are required in the main route table attached with the private subnet to allow instances to connect with the internet? A. Destination: 0.0.0.0/0 and Target: i-a12345 B. Destination: 20.0.0.0/0 and Target: 80 C. Destination: 20.0.0.0/0 and Target: i-a12345 D. Destination: 20.0.0.0/24 and Target: i-a12345 Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create two route tables and attach to the subnets. The main route table will have the entry \u201cDestination: 0.0.0.0/0 and Target: ia12345\u201d, which allows all the instances in the private subnet to connect to the internet using NAT. QUESTION NO: 219 A root account owner has given full access of his S3 bucket to one of the IAM users using the bucket ACL. When the IAM user logs in to the S3 console, which actions can he perform? A. He can just view the content of the bucket B. He can do all the operations on the bucket C. It is not possible to give access to an IAM user using ACL D. The IAM user can perform all operations on the bucket using only API/SDK Answer: C Explanation: Each AWS S3 bucket and object has an ACL (Access Control List. associated with it. An ACL is a list of grants identifying the grantee and the permission granted. The user can use ACLs to grant basic read/write permissions to other AWS accounts. ACLs use an Amazon S3\u2013specific XML schema. The user cannot grant permissions to other users (IAM users. in his account. QUESTION NO: 220 An organization has configured Auto Scaling with ELB. There is a memory issue in the application which is causing CPU utilization to go above 90%. The higher CPU usage triggers an event for Auto Scaling as per the scaling policy. If the user wants to find the root cause inside the application without triggering a scaling activity, how can he achieve this? A. Stop the scaling process until research is completed B. It is not possible to find the root cause from that instance without triggering scaling C. Delete Auto Scaling until research is completed D. Suspend the scaling process until research is completed Answer: D Explanation: Auto Scaling allows the user to suspend and then resume one or more of the Auto Scaling processes in the Auto Scaling group. This is very useful when the user wants to investigate a configuration problem or some other issue, such as a memory leak with the web application and then make changes to the application, without triggering the Auto Scaling process. QUESTION NO: 221 A sys admin is planning to subscribe to the RDS event notifications. For which of the below mentioned source categories the subscription cannot be configured? A. DB security group B. DB snapshot C. DB options group D. DB parameter group Answer: C Explanation: Amazon RDS uses the Amazon Simple Notification Service (SNS. to provide a notification when an Amazon RDS event occurs. These events can be configured for source categories, such as DB instance, DB security group, DB snapshot and DB parameter group. QUESTION NO: 222 A user has launched an EC2 instance. The instance got terminated as soon as it was launched. Which of the below mentioned options is not a possible reason for this? A. The user account has reached the maximum EC2 instance limit B. The snapshot is corrupt C. The AMI is missing. It is the required part D. The user account has reached the maximum volume limit Answer: A Explanation: When the user account has reached the maximum number of EC2 instances, it will not be allowed to launch an instance. AWS will throw an \u2018InstanceLimitExceeded\u2019 error. For all other reasons, such as \u201cAMI is missing part\u201d, \u201cCorrupt Snapshot\u201d or \u201dVolume limit has reached\u201d it will launch an EC2 instance and then terminate it. QUESTION NO: 223 A user is trying to understand the detailed CloudWatch monitoring concept. Which of the below mentioned services does not provide detailed monitoring with CloudWatch? A. AWS EMR B. AWS RDS C. AWS ELB D. AWS Route53 Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Services, such as RDS, EC2, Auto Scaling, ELB, and Route 53 can provide the monitoring data every minute. QUESTION NO: 224 A user is measuring the CPU utilization of a private data centre machine every minute. The machine provides the aggregate of data every hour, such as Sum of data\u201d, \u201cMin value\u201d, \u201cMax value, and \u201cNumber of Data points\u201d. The user wants to send these values to CloudWatch. How can the user achieve this? A. Send the data using the put-metric-data command with the aggregate-values parameter B. Send the data using the put-metric-data command with the average-values parameter C. Send the data using the put-metric-data command with the statistic-values parameter D. Send the data using the put-metric-data command with the aggregate \u2013data parameter Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish the data to CloudWatch as single data points or as an aggregated set of data points called a statistic set using the command put-metric-data. When sending the aggregate data, the user needs to send it with the parameter statistic-values: awscloudwatch put-metric-data --metric-name <Name> --namespace <Custom namespace> -- timestamp <UTC Format> --statistic-values Sum=XX,Minimum=YY,Maximum=AA,SampleCount=BB --unit Milliseconds QUESTION NO: 225 A user has enabled detailed CloudWatch monitoring with the AWS Simple Notification Service. Which of the below mentioned statements helps the user understand detailed monitoring better? A. SNS will send data every minute after configuration B. There is no need to enable since SNS provides data every minute C. AWS CloudWatch does not support monitoring for SNS D. SNS cannot provide data every minute Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. The AWS SNS service sends data every 5 minutes. Thus, it supports only the basic monitoring. The user cannot enable detailed monitoring with SNS. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 226 A user has setup a VPC with CIDR 20.0.0.0/16. The VPC has a private subnet (20.0.1.0/24) and a public subnet (20.0.0.0/240). The user\u2019s data centre has CIDR of 20.0.54.0/24 and 20.1.0.0/24 If the private subnet wants to communicate with the data centre, what will happen? A. It will allow traffic communication on both the CIDRs of the data centre B. It will not allow traffic with data centre on CIDR 20.1.0.0/24 but allows traffic communication on 20.0.54.0/24 C. It will not allow traffic communication on any of the data centre CIDRs D. It will allow traffic with data centre on CIDR 20.1.0.0/24 but does not allow on 20.0.54.0/24 Answer: D Explanation: VPC allows the user to set up a connection between his VPC and corporate or home network data centre. If the user has an IP address prefix in the VPC that overlaps with one of the networks' prefixes, any traffic to the network's prefix is dropped. In this case CIDR 20.0.54.0/24 falls in the VPC\u2019s CIDR range of 20.0.0.0/16. Thus, it will not allow traffic on that IP. In the case of 20.1.0.0/24, it does not fall in the VPC\u2019s CIDR range. Thus, traffic will be allowed on it. QUESTION NO: 227 A user wants to find the particular error that occurred on a certain date in the AWS MySQL RDS DB. Which of the below mentioned activities may help the user to get the data easily? A. It is not possible to get the log files for MySQL RDS B. Find all the transaction logs and query on those records C. Direct the logs to the DB table and then query that table D. Download the log file to DynamoDB and search for the record Answer: C Explanation: The user can view, download, and watch the database logs using the Amazon RDS console, the Command Line Interface (CLI. or the Amazon RDS API. For the MySQL RDS, the user can view the error log, slow query log, and general logs. The user can also view the MySQL logs easily by directing the logs to a database table in the main database and querying that table. QUESTION NO: 228 A user is trying to send custom metrics to CloudWatch using the PutMetricData APIs. Which of the below mentioned points should the user needs to take care while sending the data to CloudWatch? A. The size of a request is limited to 8KB for HTTP GET requests and 40KB for HTTP POST requests B. The size of a request is limited to 128KB for HTTP GET requests and 64KB for HTTP POST requests C. The size of a request is limited to 40KB for HTTP GET requests and 8KB for HTTP POST requests D. The size of a request is limited to 16KB for HTTP GET requests and 80KB for HTTP POST requests Answer: A Explanation: With AWS CloudWatch, the user can publish data points for a metric that share not only the same time stamp, but also the same namespace and dimensions. CloudWatch can accept multiple data points in the same PutMetricData call with the same time stamp. The only thing that the user needs to take care of is that the size of a PutMetricData request is limited to 8KB for HTTP GET requests and 40KB for HTTP POST requests. QUESTION NO: 229 An AWS account owner has setup multiple IAM users. One IAM user only has CloudWatch access. He has setup the alarm action which stops the EC2 instances when the CPU utilization is below the threshold limit. What will happen in this case? A. It is not possible to stop the instance using the CloudWatch alarm B. CloudWatch will stop the instance when the action is executed C. The user cannot set an alarm on EC2 since he does not have the permission D. The user can setup the action but it will not be executed if the user does not have EC2 rights Answer: D Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup an action which stops the instances when their CPU utilization is below a certain threshold for a certain period of time. The EC2 action can either terminate or stop the instance as part of the EC2 action. If the IAM user has read/write permissions for Amazon CloudWatch but not for Amazon EC2, he can still create an alarm. However, the stop or terminate actions will not be performed on the Amazon EC2 instance. QUESTION NO: 230 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling terminate process only for a while. What will happen to the availability zone rebalancing process (AZRebalance. during this period? A. Auto Scaling will not launch or terminate any instances B. Auto Scaling will allow the instances to grow more than the maximum size C. Auto Scaling will keep launching instances till the maximum instance size D. It is not possible to suspend the terminate process while keeping the launch active Answer: B Explanation: Auto Scaling performs various processes, such as Launch, Terminate, Availability Zone Rebalance (AZRebalance. etc. The AZRebalance process type seeks to maintain a balanced number of instances across Availability Zones within a region. If the user suspends the Terminate process, the AZRebalance process can cause the Auto Scaling group to grow up to ten percent larger than the maximum size. This is because Auto Scaling allows groups to temporarily grow larger than the maximum size during rebalancing activities. If Auto Scaling cannot terminate instances, the Auto Scaling group could remain up to ten percent larger than the maximum size until the user resumes the Terminate process type. QUESTION NO: 231 A user has created a mobile application which makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK and root account access/secret access key to connect to DynamoDB from mobile. Which of the below mentioned statements is true with respect to the best practice for security in this scenario? A. The user should create a separate IAM user for each mobile application and provide DynamoDB access with it. B. The user should create an IAM role with DynamoDB and EC2 access. Attach the role with EC2 and route all calls from the mobile through EC2. C. The application should use an IAM role with web identity federation which validates calls to DynamoDB with identity providers, such as Google, Amazon, and Facebook. D. Create an IAM Role with DynamoDB access and attach it with the mobile application Answer: C Explanation: With AWS IAM a user is creating an application which runs on an EC2 instance and makes requests to AWS, such as DynamoDB or S3 calls. Here it is recommended that the user should not create an IAM user and pass the user's credentials to the application or embed those credentials inside the application. If the user is creating an app that runs on a mobile phone and makes requests to AWS, the user should not create an IAMuser and distribute the user's access key with the app. Instead, he should use an identity provider, such as Login with Amazon, Facebook, or Google to authenticate the users, and then use that identity to get temporary security credentials. QUESTION NO: 232 A user is configuring the Multi AZ feature of an RDS DB. The user came to know that this RDS DB does not use the AWS technology, but uses server mirroring to achieve HA. Which DB is the user using right now? A. My SQL B. Oracle C. MS SQL D. PostgreSQL Answer: C Explanation: Amazon RDS provides high availability and failover support for DB instances using Multi AZ deployments. In a Multi AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Multi AZ deployments for Oracle, PostgreSQL, and MySQL DB instances use Amazon technology, while SQL Server (MS SQL. DB instances use SQL Server Mirroring. QUESTION NO: 233 A user is receiving a notification from the RDS DB whenever there is a change in the DB security group. The user does not want to receive these notifications for only a month. Thus, he does not want to delete the notification. How can the user configure this? A. Change the Disable button for notification to \u201cYes\u201d in the RDS console B. Set the send mail flag to false in the DB event notification console C. The only option is to delete the notification from the console D. Change the Enable button for notification to \u201cNo\u201d in the RDS console Answer: D Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. Event notifications are sent to the addresses that the user has provided while creating the subscription. The user can easily turn off the notification without deleting a subscription by setting the Enabled radio button to No in the Amazon RDS console or by setting the Enabled parameter to false using the CLI or Amazon RDS API. QUESTION NO: 234 A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 by mistake. The user is trying to create another subnet of CIDR 20.0.0.1/24. How can the user create the second subnet? A. There is no need to update the subnet as VPC automatically adjusts the CIDR of the first subnet based on the second subnet\u2019s CIDR B. The user can modify the first subnet CIDR from the console C. It is not possible to create a second subnet as one subnet with the same CIDR as the VPC has been created D. The user can modify the first subnet CIDR with AWS CLI Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside the subnet. The user can create a subnet with the same size of VPC. However, he cannot create any other subnet since the CIDR of the second subnet will conflict with the first subnet. The user cannot modify the CIDR of a subnet once it is created. Thus, in this case if required, the user has to delete the subnet and create new subnets. QUESTION NO: 235 A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group for the public subnet (WebSecGrp. and the private subnet (DBSecGrp.. Which of the below mentioned entries is required in the web server security group (WebSecGrp.? A. Configure Destination as DB Security group ID (DbSecGrp. for port 3306 Outbound B. 80 for Destination 0.0.0.0/0 Outbound C. Configure port 3306 for source 20.0.0.0/24 InBound D. Configure port 80 InBound for source 20.0.0.0/16 Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the public subnet can receive inbound traffic directly from the internet. Thus, the user should configure port 80 with source 0.0.0.0/0 in InBound. The user should configure that the instance in the public subnet can send traffic to the private subnet instances on the DB port. Thus, the user should configure the DB security group of the private subnet (DbSecGrp. as the destination for port 3306 in Outbound. QUESTION NO: 236 A user is trying to understand the detailed CloudWatch monitoring concept. Which of the below mentioned services provides detailed monitoring with CloudWatch without charging the user extra? A. AWS Auto Scaling B. AWS Route 53 C. AWS EMR D. AWS SNS Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Services, such as RDS, ELB, OpsWorks, and Route 53 can provide the monitoring data every minute without charging the user. QUESTION NO: 237 A user is trying to understand the CloudWatch metrics for the AWS services. It is required that the user should first understand the namespace for the AWS services. Which of the below mentioned is not a valid namespace for the AWS services? A. AWS/StorageGateway B. AWS/CloudTrail C. AWS/ElastiCache D. AWS/SWF Answer: B Explanation: Amazon CloudWatch is basically a metrics repository. The AWS product puts metrics into this repository, and the user can retrieve the data or statistics based on those metrics. To distinguish the data for each service, the CloudWatch metric has a namespace. Namespaces are containers for metrics. All AWS services that provide the Amazon CloudWatch data use a namespace string, beginning with \"AWS/\". All the services which are supported by CloudWatch will have some namespace. CloudWatch does not monitor CloudTrail. Thus, the namespace \u201cAWS/CloudTrail\u201d is incorrect. QUESTION NO: 238 A system admin is planning to encrypt all objects being uploaded to S3 from an application. The system admin does not want to implement his own encryption algorithm; instead he is planning to use server side encryption by supplying his own key (SSE-C.. Which parameter is not required while making a call for SSE-C? A. x-amz-server-side-encryption-customer-key-AES-256 B. x-amz-server-side-encryption-customer-key C. x-amz-server-side-encryption-customer-algorithm D. x-amz-server-side-encryption-customer-key-MD5 Answer: A Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. When the user is supplying his own encryption key, the user has to send the below mentioned parameters as a part of the API calls: x-amz-server-side-encryption-customer-algorithm: Specifies the encryption algorithm x-amzserver-side-encryption-customer-key: To provide the base64-encoded encryption key x-amzserver-side-encryption-customer-key-MD5: To provide the base64-encoded 128-bit MD5 digest of the encryption key QUESTION NO: 239 A user is using the AWS SQS to decouple the services. Which of the below mentioned operations is not supported by SQS? A. SendMessageBatch B. DeleteMessageBatch C. CreateQueue D. DeleteMessageQueue Answer: D Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. The user can perform the following set of operations using the Amazon SQS: CreateQueue, ListQueues, DeleteQueue, SendMessage, SendMessageBatch, ReceiveMessage, DeleteMessage, DeleteMessageBatch, ChangeMessageVisibility, ChangeMessageVisibilityBatch, SetQueueAttributes, GetQueueAttributes, GetQueueUrl, AddPermission and RemovePermission. Operations can be performed only by the AWS account owner or an AWS account that the account owner has delegated to. QUESTION NO: 240 A user has configured Auto Scaling with 3 instances. The user had created a new AMI after updating one of the instances. If the user wants to terminate two specific instances to ensure that Auto Scaling launches an instances with the new launch configuration, which command should he run? A. as-delete-instance-in-auto-scaling-group <Instance ID> --no-decrement-desired-capacity B. as-terminate-instance-in-auto-scaling-group <Instance ID> --update-desired-capacity C. as-terminate-instance-in-auto-scaling-group <Instance ID> --decrement-desired-capacity D. as-terminate-instance-in-auto-scaling-group <Instance ID> --no-decrement-desired-capacity Answer: D Explanation: The Auto Scaling command as-terminate-instance-in-auto-scaling-group <Instance ID> will terminate the specific instance ID. The user is required to specify the parameter as \u2013no-decrement-desiredcapacity to ensure that it launches a new instance from the launch config after terminating the instance. If the user specifies the parameter --decrement-desired-capacity then Auto Scaling will terminate the instance and decrease the desired capacity by 1. QUESTION NO: 241 A user has launched an EC2 instance from an instance store backed AMI. If the user restarts the instance, what will happen to the ephermal storage data? A. All the data will be erased but the ephermal storage will stay connected B. All data will be erased and the ephermal storage is released C. It is not possible to restart an instance launched from an instance store backed AMI D. The data is preserved Answer: D Explanation: A user can reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. However, it is recommended that the user use Amazon EC2 to reboot the instance instead of running the operating system reboot command from the instance. When an instance launched from an instance store backed AMI is rebooted all the ephermal storage data is still preserved. QUESTION NO: 242 A user has launched an EC2 instance. However, due to some reason the instance was terminated. If the user wants to find out the reason for termination, where can he find the details? A. It is not possible to find the details after the instance is terminated B. The user can get information from the AWS console, by checking the Instance description under the State transition reason label C. The user can get information from the AWS console, by checking the Instance description under the Instance Status Change reason label D. The user can get information from the AWS console, by checking the Instance description under the Instance Termination reason label Answer: D Explanation: An EC2 instance, once terminated, may be available in the AWS console for a while after termination. The user can find the details about the termination from the description tab under the label State transition reason. If the instance is still running, there will be no reason listed. If the user has explicitly stopped or terminated the instance, the reason will be \u201cUser initiated shutdown\u201d. QUESTION NO: 243 A user has created a VPC with CIDR 20.0.0.0/24. The user has used all the IPs of CIDR and wants to increase the size of the VPC. The user has two subnets: public (20.0.0.0/28. and private (20.0.1.0/28). How can the user change the size of the VPC? A. The user can delete all the instances of the subnet. Change the size of the subnets to 20.0.0.0/32 and 20.0.1.0/32, respectively. Then the user can increase the size of the VPC using CLI B. It is not possible to change the size of the VPC once it has been created C. The user can add a subnet with a higher range so that it will automatically increase the size of the VPC. D. The user can delete the subnets first and then modify the size of the VPC Answer: B Explanation: Once the user has created a VPC, he cannot change the CIDR of that VPC. The user has to terminate all the instances, delete the subnets and then delete the VPC. Create a new VPC with a higher size and launch instances with the newly created VPC and subnets. QUESTION NO: 244 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned security policies is supported by ELB? A. Dynamic Security Policy B. All the other options C. Predefined Security Policy D. Default Security Policy Answer: C Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. ELB supports two policies: Predefined Security Policy: which comes with predefined cipher and SSL protocols; Custom Security Policy: which allows the user to configure a policy. QUESTION NO: 245 A user has granted read/write permission of his S3 bucket using ACL. Which of the below mentioned options is a valid ID to grant permission to other AWS accounts (grantee. using ACL? A. IAM User ID B. S3 Secure ID C. Access ID D. Canonical user ID Answer: D Explanation: An S3 bucket ACL grantee can be an AWS account or one of the predefined Amazon S3 groups. The user can grant permission to an AWS account by the email address of that account or by the canonical user ID. If the user provides an email in the grant request, Amazon S3 finds the canonical user ID for that account and adds it to the ACL. The resulting ACL will always contain the canonical user ID for the AWS account, and not the AWS account's email address. QUESTION NO: 246 A user has configured an ELB to distribute the traffic among multiple instances. The user instances are facing some issues due to the back-end servers. Which of the below mentioned CloudWatch metrics helps the user understand the issue with the instances? A. HTTPCode_Backend_3XX B. HTTPCode_Backend_4XX C. HTTPCode_Backend_2XX D. HTTPCode_Backend_5XX Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. For ELB, CloudWatch provides various metrics including error code by ELB as well as by back-end servers (instances.. It gives data for the count of the number of HTTP response codes generated by the back-end instances. This metric does not include any response codes generated by the load balancer. These metrics are: The 2XX class status codes represents successful actions The 3XX class status code indicates that the user agent requires action The 4XX class status code represents client errors The 5XX class status code represents back-end server errors QUESTION NO: 247 A user has launched an EC2 instance store backed instance in the US-East-1a zone. The user created AMI #1 and copied it to the Europe region. After that, the user made a few updates to the application running in the US-East-1a zone. The user makes an AMI#2 after the changes. If the user launches a new instance in Europe from the AMI #1 copy, which of the below mentioned statements is true? A. The new instance will have the changes made after the AMI copy as AWS just copies the reference of the original AMI during the copying. Thus, the copied AMI will have all the updated data. B. The new instance will have the changes made after the AMI copy since AWS keeps updating the AMI. C. It is not possible to copy the instance store backed AMI from one region to another. D. The new instance in the EU region will not have the changes made after the AMI copy. Answer: D Explanation: Within EC2, when the user copies an AMI, the new AMI is fully independent of the source AMI; there is no link to the original (source. AMI. The user can modify the source AMI without affecting the new AMI and vice a versa. Therefore, in this case even if the source AMI is modified, the copied AMI of the EU region will not have the changes. Thus, after copy the user needs to copy the new source AMI to the destination region to get those changes. QUESTION NO: 248 A user runs the command \u201cdd if=/dev/zero of=/dev/xvdfbs=1M\u201d on a fresh blank EBS volume attached to a Linux instance. Which of the below mentioned activities is the user performing with the command given above? A. Creating a file system on the EBS volume B. Mounting the device to the instance C. Pre warming the EBS volume D. Formatting the EBS volume Answer: C Explanation: When the user creates a new EBS volume and is trying to access it for the first time it will encounter reduced IOPS due to wiping or initiating of the block storage. To avoid this as well as achieve the best performance it is required to pre warm the EBS volume. For a blank volume attached with a Linux OS, the \u201cdd\u201d command is used to write to all the blocks on the device. In the command \u201cdd if=/dev/zero of=/dev/xvdfbs=1M\u201d the parameter \u201cif =import file\u201d should be set to one of the Linux virtual devices, such as /dev/zero. The \u201cof=output file\u201d parameter should be set to the drive that the user wishes to warm. The \u201cbs\u201d parameter sets the block size of the write operation; for optimal performance, this should be set to 1 MB. QUESTION NO: 249 A user has created an Auto Scaling group using CLI. The user wants to enable CloudWatch detailed monitoring for that group. How can the user configure this? A. When the user sets an alarm on the Auto Scaling group, it automatically enables detail monitoring B. By default detailed monitoring is enabled for Auto Scaling C. Auto Scaling does not support detailed monitoring D. Enable detail monitoring from the AWS console Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates an Auto Scaling launch config as the first step for creating an Auto Scaling group, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. Thus, the user does not need to set this flag if he wants detailed monitoring. QUESTION NO: 250 A user has created a VPC with a public subnet. The user has terminated all the instances which are part of the subnet. Which of the below mentioned statements is true with respect to this scenario? A. The user cannot delete the VPC since the subnet is not deleted B. All network interface attached with the instances will be deleted C. When the user launches a new instance it cannot use the same subnet D. The subnet to which the instances were launched with will be deleted Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When an instance is launched it will have a network interface attached with it. The user cannot delete the subnet until he terminates the instance and deletes the network interface. When the user terminates the instance all the network interfaces attached with it are also deleted. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 251 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. The ELB security policy supports various ciphers. Which of the below mentioned options helps identify the matching cipher at the client side to the ELB cipher list when client is requesting ELB DNS over SSL? A. Cipher Protocol B. Client Configuration Preference C. Server Order Preference D. Load Balancer Preference Answer: C Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. When client is requesting ELB DNS over SSL and if the load balancer is configured to support the Server Order Preference, then the load balancer gets to select the first cipher in its list that matches any one of the ciphers in the client's list. Server Order Preference ensures that the load balancer determines which cipher is used for the SSL connection. QUESTION NO: 252 A user has created a VPC with public and private subnets. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.1.0/24 and the public subnet uses CIDR 20.0.0.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group of the NAT instance. Which of the below mentioned entries is not required for the NAT security group? A. For Inbound allow Source: 20.0.1.0/24 on port 80 B. For Outbound allow Destination: 0.0.0.0/0 on port 80 C. For Inbound allow Source: 20.0.0.0/24 on port 80 D. For Outbound allow Destination: 0.0.0.0/0 on port 443 Answer: C Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the private subnet can connect to the internet using the NAT instances. The user should first configure that NAT can receive traffic on ports 80 and 443 from the private subnet. Thus, allow ports 80 and 443 in Inbound for the private subnet 20.0.1.0/24. Now to route this traffic to the internet configure ports 80 and 443 in Outbound with destination 0.0.0.0/0. The NAT should not have an entry for the public subnet CIDR. QUESTION NO: 253 A user has created an application which will be hosted on EC2. The application makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK to connect with from the EC2 instance. Which of the below mentioned statements is true with respect to the best practice for security in this scenario? A. The user should attach an IAM role with DynamoDB access to the EC2 instance B. The user should create an IAM user with DynamoDB access and use its credentials within the application to connect with DynamoDB C. The user should create an IAM role, which has EC2 access so that it will allow deploying the application D. The user should create an IAM user with DynamoDB and EC2 access. Attach the user with the application so that it does not use the root account credentials Answer: A Explanation: With AWS IAM a user is creating an application which runs on an EC2 instance and makes requests to AWS, such as DynamoDB or S3 calls. Here it is recommended that the user should not create an IAM user and pass the user's credentials to the application or embed those credentials inside the application. Instead, the user should use roles for EC2 and give that role access to DynamoDB /S3. When the roles are attached to EC2, it will give temporary security credentials to the application hosted on that EC2, to connect with DynamoDB / S3. QUESTION NO: 254 An organization (Account ID 123412341234. has attached the below mentioned IAM policy to a user. What does this policy statement entitle the user to perform? { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*LoginProfile\", \"iam:*AccessKey*\", \"iam:*SigningCertificate*\" ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] }] } A. The policy allows the IAM user to modify all IAM user\u2019s credentials using the console, SDK, CLI or APIs B. The policy will give an invalid resource error C. The policy allows the IAM user to modify all credentials using only the console D. The policy allows the user to modify all IAM user\u2019s password, sign in certificates and access keys using only CLI, SDK or APIs Answer: D Explanation: WS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (Account ID 123412341234. wants some of their users to manage credentials (access keys, password, and sing in certificates. of all IAM users, they should set an applicable policy to that user or group of users. The below mentioned policy allows the IAM user to modify the credentials of all IAM user\u2019s using only CLI, SDK or APIs. The user cannot use the AWS console for this activity since he does not have list permission for the IAM users. { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\" \"Action\": [ \"iam:*LoginProfile\", \"iam:*AccessKey*\", \"iam:*SigningCertificate*\" ], \"Resource\": [\"arn:aws:iam::123412341234:user/${aws:username}\"] }] } QUESTION NO: 255 A sys admin is trying to understand the sticky session algorithm. Please select the correct sequence of steps, both when the cookie is present and when it is not, to help the admin understand the implementation of the sticky session: ELB inserts the cookie in the response ELB chooses the instance based on the load balancing algorithm Check the cookie in the service request The cookie is found in the request The cookie is not found in the request A. 3,1,4,2 [Cookie is not Present] & 3,1,5,2 [Cookie is Present] B. 3,4,1,2 [Cookie is not Present] & 3,5,1,2 [Cookie is Present] C. 3,5,2,1 [Cookie is not Present] & 3,4,2,1 [Cookie is Present] D. 3,2,5,4 [Cookie is not Present] & 3,2,4,5 [Cookie is Present] Answer: C Explanation: Generally AWS ELB routes each request to a zone with the minimum load. The Elastic Load Balancer provides a feature called sticky session which binds the user\u2019s session with a specific EC2 instance. The load balancer uses a special load-balancer-generated cookie to track the application instance for each request. When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the request is sent to the application instance specified in the cookie. If there is no cookie, the load balancer chooses an application instance based on the existing load balancing algorithm. A cookie is inserted into the response for binding subsequent requests from the same user to that application instance. QUESTION NO: 256 A user has a weighing plant. The user measures the weight of some goods every 5 minutes and sends data to AWS CloudWatch for monitoring and tracking. Which of the below mentioned parameters is mandatory for the user to include in the request list? A. Value B. Namespace C. Metric Name D. Timezone Answer: B Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish the data to CloudWatch as single data points or as an aggregated set of data points called a statistic set. The user has to always include the namespace as part of the request. The user can supply a file instead of the metric name. If the user does not supply the timezone, it accepts the current time. If the user is sending the data as a single data point it will have parameters, such as value. However, if the user is sending as an aggregate it will have parameters, such as statistic-values. QUESTION NO: 257 An organization has configured Auto Scaling for hosting their application. The system admin wants to understand the Auto Scaling health check process. If the instance is unhealthy, Auto Scaling launches an instance and terminates the unhealthy instance. What is the order execution? A. Auto Scaling launches a new instance first and then terminates the unhealthy instance B. Auto Scaling performs the launch and terminate processes in a random order C. Auto Scaling launches and terminates the instances simultaneously D. Auto Scaling terminates the instance first and then launches a new instance Answer: D Explanation: Auto Scaling keeps checking the health of the instances at regular intervals and marks the instance for replacement when it is unhealthy. The ReplaceUnhealthy process terminates instances which are marked as unhealthy and subsequently creates new instances to replace them. This process first terminates the instance and then launches a new instance. QUESTION NO: 258 A user is trying to connect to a running EC2 instance using SSH. However, the user gets an Unprotected Private Key File error. Which of the below mentioned options can be a possible reason for rejection? A. The private key file has the wrong file permission B. The ppk file used for SSH is read only C. The public key file has the wrong permission D. The user has provided the wrong user name for the OS login Answer: A Explanation: While doing SSH to an EC2 instance, if you get an Unprotected Private Key File error it means that the private key file's permissions on your computer are too open. Ideally the private key should have the Unix permission of 0400. To fix that, run the command: # chmod 0400 /path/to/private.key QUESTION NO: 259 A user has provisioned 2000 IOPS to the EBS volume. The application hosted on that EBS is experiencing less IOPS than provisioned. Which of the below mentioned options does not affect the IOPS of the volume? A. The application does not have enough IO for the volume B. The instance is EBS optimized C. The EC2 instance has 10 Gigabit Network connectivity D. The volume size is too large Answer: D Explanation: When the application does not experience the expected IOPS or throughput of the PIOPS EBS volume that was provisioned, the possible root cause could be that the EC2 bandwidth is the limiting factor and the instance might not be either EBS-optimized or might not have 10 Gigabit network connectivity. Another possible cause for not experiencing the expected IOPS could also be that the user is not driving enough I/O to the EBS volumes. The size of the volume may not affect IOPS. QUESTION NO: 260 A storage admin wants to encrypt all the objects stored in S3 using server side encryption. The user does not want to use the AES 256 encryption key provided by S3. How can the user achieve this? A. The admin should upload his secret key to the AWS console and let S3 decrypt the objects B. The admin should use CLI or API to upload the encryption key to the S3 bucket. When making a call to the S3 API mention the encryption key URL in each request C. S3 does not support client supplied encryption keys for server side encryption D. The admin should send the keys and encryption algorithm with each API call Answer: D Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API callto supply his own encryption key. Amazon S3 never stores the user\u2019s encryption key. The user has to supply it for each encryption or decryption call. QUESTION NO: 261 A user is trying to create a PIOPS EBS volume with 8 GB size and 200 IOPS. Will AWS create the volume? A. Yes, since the ratio between EBS and IOPS is less than 30 B. No, since the PIOPS and EBS size ratio is less than 30 C. No, the EBS size is less than 10 GB D. Yes, since PIOPS is higher than 100 Answer: C Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30; for example, a volume with 3000 IOPS must be at least 100 GB. QUESTION NO: 262 A user has scheduled the maintenance window of an RDS DB on Monday at 3 AM. Which of the below mentioned events may force to take the DB instance offline during the maintenance window? A. Enabling Read Replica B. Making the DB Multi AZ C. DB password change D. Security patching Answer: D Explanation: Amazon RDS performs maintenance on the DB instance during a user-definable maintenance window. The system may be offline or experience lower performance during that window. The only maintenance events that may require RDS to make the DB instance offline are: Scaling compute operations Software patching. Required software patching is automatically scheduled only for patches that are security and durability related. Such patching occurs infrequently (typically once every few months. and seldom requires more than a fraction of the maintenance window. QUESTION NO: 263 An organization has launched 5 instances: 2 for production and 3 for testing. The organization wants that one particular group of IAM users should only access the test instances and not the production ones. How can the organization set that as a part of the policy? A. Launch the test and production instances in separate regions and allow region wise access to the group B. Define the IAM policy which allows access based on the instance ID C. Create an IAM policy with a condition which allows access to only small instances D. Define the tags on the test and production servers and add a condition to the IAM policy which allows access to specific tags Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user can add conditions as a part of the IAM policies. The condition can be set on AWS Tags, Time, and Client IP as well as on various parameters. If the organization wants the user to access only specific instances he should define proper tags and add to the IAM policy condition. The sample policy is shown below. \"Statement\": [ { \"Action\": \"ec2:*\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/InstanceType\": \"Production\" } } } ] QUESTION NO: 264 A user has configured Auto Scaling with the minimum capacity as 2 and the desired capacity as 2. The user is trying to terminate one of the existing instance with the command: as-terminate-instance-in-auto-scaling-group<Instance ID> --decrement-desired-capacity What will Auto Scaling do in this scenario? A. Terminates the instance and does not launch a new instance B. Terminates the instance and updates the desired capacity to 1 C. Terminates the instance and updates the desired capacity and minimum size to 1 D. Throws an error Answer: D Explanation: The Auto Scaling command as-terminate-instance-in-auto-scaling-group <Instance ID> will terminate the specific instance ID. The user is required to specify the parameter as --decrement-desiredcapacity. Then Auto Scaling will terminate the instance and decrease the desired capacity by 1. In this case since the minimum size is 2, Auto Scaling will not allow the desired capacity to go below 2. Thus, it will throw an error. QUESTION NO: 265 A user is collecting 1000 records per second. The user wants to send the data to CloudWatch using the custom namespace. Which of the below mentioned options is recommended for this activity? A. Aggregate the data with statistics, such as Min, max, Average, Sum and Sample data and send the data to CloudWatch B. Send all the data values to CloudWatch in a single command by separating them with a comma. CloudWatch will parse automatically C. Create one csv file of all the data and send a single file to CloudWatch D. It is not possible to send all the data in one call. Thus, it should be sent one by one. CloudWatch will aggregate the data automatically Answer: A Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish data to CloudWatch as single data points or as an aggregated set of data points called a statistic set using the command put-metric-data. It is recommended that when the user is having multiple data points per minute, he should aggregate the data so that it will minimize the number of calls to put-metric-data. In this case it will be single call to CloudWatch instead of 1000 calls if the data is aggregated. QUESTION NO: 266 A user is trying to create an EBS volume with the highest PIOPS supported by EBS. What is the minimum size of EBS required to have the maximum IOPS? A. 124 B. 150 C. 134 D. 128 Answer: C Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30. QUESTION NO: 267 An organization is trying to create various IAM users. Which of the below mentioned options is not a valid IAM username? A. John.cloud B. john@cloud C. John=cloud D. john#cloud Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Whenever the organization is creating an IAM user, there should be a unique ID for each user. The names of users, groups, roles, instance profiles must be alphanumeric, including the following common characters: plus (+., equal (=., comma (,., period (.., at (@., and dash (-.. QUESTION NO: 268 A user is having data generated randomly based on a certain event. The user wants to upload that data to CloudWatch. It may happen that event may not have data generated for some period due to andomness. Which of the below mentioned options is a recommended option for this case? A. For the period when there is no data, the user should not send the data at all B. For the period when there is no data the user should send a blank value C. For the period when there is no data the user should send the value as 0 D. The user must upload the data to CloudWatch as having no data for some period will cause an error at CloudWatch monitoring Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. When the user data is more random and not generated at regular intervals, there can be a period which has no associated data. The user can either publish the zero (0. Value for that period or not publish the data at all. It is recommended that the user should publish zero instead of no value to monitor the health of the application. This is helpful in an alarm as well as in the generation of the sample data count. QUESTION NO: 269 A user is sending the data to CloudWatch using the CloudWatch API. The user is sending data 90 minutes in the future. What will CloudWatch do in this case? A. CloudWatch will accept the data B. It is not possible to send data of the future C. It is not possible to send the data manually to CloudWatch D. The user cannot send data for more than 60 minutes in the future Answer: A Explanation: With Amazon CloudWatch, each metric data point must be marked with a time stamp. The user can send the data using CLI but the time has to be in the UTC format. If the user does not provide the time, CloudWatch will take the data received time in the UTC timezone. The time stamp sent by the user can be up to two weeks in the past and up to two hours into the future. QUESTION NO: 270 A user wants to upload a complete folder to AWS S3 using the S3 Management console. How can the user perform this activity? A. Just drag and drop the folder using the flash tool provided by S3 B. Use the Enable Enhanced Folder option from the S3 console while uploading objects C. The user cannot upload the whole folder in one go with the S3 management console D. Use the Enable Enhanced Uploader option from the S3 console while uploading objects Answer: D Explanation: AWS S3 provides a console to upload objects to a bucket. The user can use the file upload screen to upload the whole folder in one go by clicking on the Enable Enhanced Uploader option. When the user uploads afolder, Amazon S3 uploads all the files and subfolders from the specified folder to the user\u2019s bucket. It then assigns a key value that is a combination of the uploaded file name and the folder name. QUESTION NO: 271 Which of the below mentioned AWS RDS logs cannot be viewed from the console for MySQL? A. Error Log B. Slow Query Log C. Transaction Log D. General Log Answer: C Explanation: The user can view, download, and watch the database logs using the Amazon RDS console, the Command Line Interface (CLI., or the Amazon RDS API. For the MySQL RDS, the user can view the error log, slow querylog, and general logs. RDS does not support viewing the transaction logs. QUESTION NO: 272 A user has launched an EBS backed EC2 instance in the US-East-1a region. The user stopped the instance and started it back after 20 days. AWS throws up an \u2018InsufficientInstanceCapacity\u2019 error. What can be the possible reason for this? A. AWS does not have sufficient capacity in that availability zone B. AWS zone mapping is changed for that user account C. There is some issue with the host capacity on which the instance is launched D. The user account has reached the maximum EC2 instance limit Answer: A Explanation: When the user gets an \u2018InsufficientInstanceCapacity\u2019 error while launching or starting an EC2 instance, it means that AWS does not currently have enough available capacity to service the user request. If the user is requesting a large number of instances, there might not be enough server capacity to host them. The user can either try again later, by specifying a smaller number of instances or changing the availability zone if launching a fresh instance. QUESTION NO: 273 A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is true in this scenario? A. The AWS VPC will automatically create a NAT instance with the micro size B. VPC bounds the main route table with a private subnet and a custom route table with a public subnet C. The user has to manually create a NAT instance D. VPC bounds the main route table with a public subnet and a custom route table with a private subnet Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance of a smaller or higher size, respectively. The VPC has an implied router and the VPC wizard updates the main route table used with the private subnet, creates a custom route table and associates it with the public subnet. QUESTION NO: 274 The CFO of a company wants to allow one of his employees to view only the AWS usage report page. Which of the below mentioned IAM policy statements allows the user to have access to the AWS usage report page? A. \"Effect\": \"Allow\", \"Action\": [\u201cDescribe\u201d], \"Resource\": \"Billing\" B. \"Effect\": \"Allow\", \"Action\": [\"AccountUsage], \"Resource\": \"*\" C. \"Effect\": \"Allow\", \"Action\": [\"aws-portal:ViewUsage\"], \"Resource\": \"*\" D. \"Effect\": \"Allow\", \"Action\": [\"aws-portal: ViewBilling\"], \"Resource\": \"*\" Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the CFO wants to allow only AWS usage report page access, the policy for that IAM user will be as given below: { \"Version\": \"2012-10-17\", \"Statement\": [ 168 { \"Effect\": \"Allow\", \"Action\": [ \"aws-portal:ViewUsage\" ], \"Resource\": \"*\" } ] } QUESTION NO: 275 An organization has created 10 IAM users. The organization wants each of the IAM users to have access to a separate DyanmoDB table. All the users are added to the same group and the organization wants to setup a group level policy for this. How can the organization achieve this? A. Define the group policy and add a condition which allows the access based on the IAM name B. Create a DynamoDB table with the same name as the IAM user name and define the policy rule which grants access based on the DynamoDB ARN using a variable C. Create a separate DynamoDB database for each user and configure a policy in the group based on the DB variable. D. It is not possible to have a group level policy which allows different IAM users to different DynamoDB Tables. Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. AWS DynamoDB has only tables and the organization cannot makeseparate databases. The organization should create a table with the same name as the IAM user name and use the ARN of DynamoDB as part of the group policy. The sample policy is shown below: { \"Version\": \"2012-10-17\", \"Statement\": [{ 169 \"Effect\": \"Allow\", \"Action\": [\"dynamodb:*\"], \"Resource\": \"arn:aws:dynamodb:region:account-number-without-hyphens:table/ ${aws:username}\" } ] } (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 276 A user has configured an HTTPS listener on an ELB. The user has not configured any security policy which can help to negotiate SSL between the client and ELB. What will ELB do in this scenario? A. By default ELB will select the first version of the security policy B. By default ELB will select the latest version of the policy C. ELB creation will fail without a security policy D. It is not required to have a security policy since SSL is already installed Answer: B Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. If the user has created an HTTPS/SSL listener without associating any security policy, Elastic Load Balancing will, bydefault, associate the latest version of the ELBSecurityPolicyYYYY-MM with the load balancer. QUESTION NO: 277 A user is creating a Cloudformation stack. Which of the below mentioned limitations does not hold true for Cloudformation? A. One account by default is limited to 100 templates B. The user can use 60 parameters and 60 outputs in a single template C. The template, parameter, output, and resource description fields are limited to 4096 characters D. One account by default is limited to 20 stacks Answer: A Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The limitations given below apply to the Cloudformation template and stack. There are no limits to the number of templates but each AWS CloudFormation account is limited to a maximum of 20 stacks by default. The Template, Parameter, Output, and Resource description fields are limited to 4096 characters. The user can include up to 60 parameters and 60 outputs in a template. QUESTION NO: 278 A user has two EC2 instances running in two separate regions. The user is running an internal memory management tool, which captures the data and sends it to CloudWatch in US East, using a CLI with the same namespace and metric. Which of the below mentioned options is true with respect to the above statement? A. The setup will not work as CloudWatch cannot receive data across regions B. CloudWatch will receive and aggregate the data based on the namespace and metric C. CloudWatch will give an error since the data will conflict due to two sources D. CloudWatch will take the data of the server, which sends the data first Answer: B Explanation: Amazon CloudWatch does not differentiate the source of a metric when receiving custom data. If the user is publishing a metric with the same namespace and dimensions from different sources, CloudWatch will treat them as a single metric. If the data is coming with the same timezone within a minute, CloudWatch will aggregate the data. It treats these as a single metric, allowing the user to get the statistics, such as minimum, maximum, average, and the sum of all across all servers. QUESTION NO: 279 An organization has created a Queue named \u201cmodularqueue\u201d with SQS. The organization is not performing any operations such as SendMessage, ReceiveMessage, DeleteMessage, GetQueueAttributes, SetQueueAttributes, AddPermission, and RemovePermission on the queue. What can happen in this scenario? A. AWS SQS sends notification after 15 days for inactivity on queue B. AWS SQS can delete queue after 30 days without notification C. AWS SQS marks queue inactive after 30 days D. AWS SQS notifies the user after 2 weeks and deletes the queue after 3 weeks. Answer: B Explanation: Amazon SQS can delete a queue without notification if one of the following actions hasn't been performed on it for 30 consecutive days: SendMessage, ReceiveMessage, DeleteMessage, GetQueueAttributes, SetQueueAttributes, AddPermission, and RemovePermission. QUESTION NO: 280 An organization has setup Auto Scaling with ELB. Due to some manual error, one of the instances got rebooted. Thus, it failed the Auto Scaling health check. Auto Scaling has marked it for replacement. How can the system admin ensure that the instance does not get terminated? A. Update the Auto Scaling group to ignore the instance reboot event B. It is not possible to change the status once it is marked for replacement C. Manually add that instance to the Auto Scaling group after reboot to avoid replacement D. Change the health of the instance to healthy using the Auto Scaling commands Answer: D Explanation: After an instance has been marked unhealthy by Auto Scaling, as a result of an Amazon EC2 or ELB health check, it is almost immediately scheduled for replacement as it will never automatically recover its health. If the user knows that the instance is healthy then he can manually call the SetInstanceHealth action (or the as-setinstance- health command from CLI. to set the instance's health status back to healthy. Auto Scaling will throw an error if the instance is already terminating or else it will mark it healthy. QUESTION NO: 281 A system admin wants to add more zones to the existing ELB. The system admin wants to perform this activity from CLI. Which of the below mentioned command helps the system admin to add new zones to the existing ELB? A. elb-enable-zones-for-lb B. elb-add-zones-for-lb C. It is not possible to add more zones to the existing ELB D. elb-configure-zones-for-lb Answer: A Explanation: The user has created an Elastic Load Balancer with the availability zone and wants to add more zones to the existing ELB. The user can do so in two ways: From the console or CLI, add new zones to ELB; QUESTION NO: 282 An organization is planning to create a user with IAM. They are trying to understand the limitations of IAM so that they can plan accordingly. Which of the below mentioned statements is not true with respect to the limitations of IAM? A. One IAM user can be a part of a maximum of 5 groups B. The organization can create 100 groups per AWS account C. One AWS account can have a maximum of 5000 IAM users D. One AWS account can have 250 roles Answer: A Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The default maximums for each of the IAM entities is given below: Groups per AWS account: 100 Users per AWS account: 5000 Roles per AWS account: 250 Number of groups per user: 10 (that is, one user can be part of these many groups. QUESTION NO: 283 A user is planning to scale up an application by 8 AM and scale down by 7 PM daily using Auto Scaling. What should the user do in this case? A. Setup the scaling policy to scale up and down based on the CloudWatch alarms B. The user should increase the desired capacity at 8 AM and decrease it by 7 PM manually C. The user should setup a batch process which launches the EC2 instance at a specific time D. Setup scheduled actions to scale up or down at a specific time Answer: A Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. To configure the Auto Scaling group to scale based on a schedule, the user needs to create scheduled actions. A scheduled action tells Auto Scaling to perform a scaling action at a certain time in the future. QUESTION NO: 284 A user has created a VPC with two subnets: one public and one private. The user is planning to run the patch update for the instances in the private subnet. How can the instances in the private subnet connect to theinternet? A. Use the internet gateway with a private IP B. Allow outbound traffic in the security group for port 80 to allow internet updates C. The private subnet can never connect to the internet D. Use NAT with an elastic IP Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created two subnets (one private and one public., he would need a Network Address Translation (NAT. instance with the elastic IP address. This enables the instances in the private subnet to send requests to the internet (for example, to perform software updates.. QUESTION NO: 285 A user has configured an EC2 instance in the US-East-1a zone. The user has enabled detailed monitoring of the instance. The user is trying to get the data from CloudWatch using a CLI. Which of the below mentioned CloudWatch endpoint URLs should the user use? A. monitoring.us-east-1.amazonaws.com B. monitoring.us-east-1-a.amazonaws.com C. monitoring.us-east-1a.amazonaws.com D. cloudwatch.us-east-1a.amazonaws.com Answer: A Explanation: The CloudWatch resources are always region specific and they will have the end point as region specific. If the user is trying to access the metric in the US-East-1 region, the endpoint URL will be: monitoring.us-east- 1.amazonaws.com QUESTION NO: 286 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling AddToLoadBalancer (which adds instances to the load balancer. process for a while. What will happen to the instances launched during the suspension period? A. The instances will not be registered with ELB and the user has to manually register when the process is resumed B. The instances will be registered with ELB only once the process has resumed C. Auto Scaling will not launch the instance during this period due to process suspension D. It is not possible to suspend only the AddToLoadBalancer process Answer: A Explanation: Auto Scaling performs various processes, such as Launch, Terminate, add to Load Balancer etc. The user can also suspend the individual process. The AddToLoadBalancer process type adds instances to the load balancer when the instances are launched. If this process is suspended, Auto Scaling will launch the instances but will not add them to the load balancer. When the user resumes this process, Auto Scaling will resume adding new instances launched after resumption to the load balancer. However, it will not add running instances that were launched while the process was suspended; those instances must be added manually. QUESTION NO: 287 A sys admin has enabled a log on ELB. Which of the below mentioned activities are not captured by the log? A. Response processing time B. Front end processing time C. Backend processing time D. Request processing time Answer: B Explanation: Elastic Load Balancing access logs capture detailed information for all the requests made to the load balancer. Each request will have details, such as client IP, request path, ELB IP, time, and latencies. The time will have information, such as Request Processing time, Backend Processing time and Response Processing time. QUESTION NO: 288 A user has moved an object to Glacier using the life cycle rules. The user requests to restore the archive after 6 months. When the restore request is completed the user accesses that archive. Which of the below mentioned statements is not true in this condition? A. The archive will be available as an object for the duration specified by the user during the restoration request B. The restored object\u2019s storage class will be RRS C. The user can modify the restoration period only by issuing a new restore request with the updated period D. The user needs to pay storage for both RRS (restored) and Glacier (Archive) Rates. Answer: B Explanation: AWS Glacier is an archival service offered by AWS. AWS S3 provides lifecycle rules to archive and restore objects from S3 to Glacier. Once the object is archived their storage class will change to Glacier. If the user sends a request for restore, the storage class will still be Glacier for the restored object. The user will be paying for both the archived copy as well as for the restored object. The object is available only for the duration specified in the restore request and if the user wants to modify that period, he has to raise another restore request with the updated duration. QUESTION NO: 289 A user is running a batch process on EBS backed EC2 instances. The batch process starts a few instances to process hadoop Map reduce jobs which can run between 50 \u2013 600 minutes or sometimes for more time. The user wants to configure that the instance gets terminated only when the process is completed. How can the user configure this with CloudWatch? A. Setup the CloudWatch action to terminate the instance when the CPU utilization is less than 5% B. Setup the CloudWatch with Auto Scaling to terminate all the instances C. Setup a job which terminates all instances after 600 minutes D. It is not possible to terminate instances automatically Answer: D Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup an action which terminates the instances when their CPU utilization is below a certain threshold for a certain period of time. The EC2 action can either terminate or stop the instance as part of the EC2 action. QUESTION NO: 290 A user has enabled versioning on an S3 bucket. The user is using server side encryption for data at rest. If the user is supplying his own keys for encryption (SSE-C., what is recommended to the user for the purpose of security? A. The user should not use his own security key as it is not secure B. Configure S3 to rotate the user\u2019s encryption key at regular intervals C. Configure S3 to store the user\u2019s keys securely with SSL D. Keep rotating the encryption key manually at the client side Answer: D Explanation: AWS S3 supports client side or server side encryption to encrypt all data at Rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. Since S3 does not store the encryption keys in SSE-C, it is recommended that the user should manage keys securely and keep rotating them regularly at the client side version. QUESTION NO: 291 A user runs the command \u201cdd if=/dev/xvdf of=/dev/null bs=1M\u201d on an EBS volume created from a snapshot and attached to a Linux instance. Which of the below mentioned activities is the user performing with the step given above? A. Pre warming the EBS volume B. Initiating the device to mount on the EBS volume C. Formatting the volume D. Copying the data from a snapshot to the device Answer: A Explanation: When the user creates an EBS volume and is trying to access it for the first time it will encounter reduced IOPS due to wiping or initiating of the block storage. To avoid this as well as achieve the best performance it is required to pre warm the EBS volume. For a volume created from a snapshot and attached with a Linux OS, the \u201cdd\u201d command pre warms the existing data on EBS and any restored snapshots of volumes that have been previously fully pre warmed. This command maintains incremental snapshots; however, because this operation is read-only, it does not pre warm unused space that has never been written to on the original volume. In the command \u201cdd if=/dev/xvdf of=/dev/null bs=1M\u201d , the parameter \u201cif=input file\u201d should be set to the drive that the user wishes to warm. The \u201cof=output file\u201d parameter should be set to the Linux null virtual device, /dev/null. The \u201cbs\u201d parameter sets the block size of the read operation; for optimal performance, this should be set to 1 MB. QUESTION NO: 292 A user has launched an EC2 Windows instance from an instance store backed AMI. The user wants to convert the AMI to an EBS backed AMI. How can the user convert it? A. Attach an EBS volume to the instance and unbundle all the AMI bundled data inside the EBS B. A Windows based instance store backed AMI cannot be converted to an EBS backed AMI C. It is not possible to convert an instance store backed AMI to an EBS backed AMI D. Attach an EBS volume and use the copy command to copy all the ephermal content to the EBS Volume Answer: B Explanation: Generally when a user has launched an EC2 instance from an instance store backed AMI, it can be converted to an EBS backed AMI provided the user has attached the EBS volume to the instance and unbundles the AMI data to it. However, if the instance is a Windows instance, AWS does not allow this. In this case, since the instance is a Windows instance, the user cannot convert it to an EBS backed AMI. QUESTION NO: 293 A user has created a VPC with public and private subnets using the VPC Wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24. Which of the below mentioned entries are required in the main route table to allow the instances in VPC to communicate with each other? A. Destination : 20.0.0.0/24 and Target : VPC B. Destination : 20.0.0.0/16 and Target : ALL C. Destination : 20.0.0.0/0 and Target : ALL D. Destination : 20.0.0.0/16 and Target : Local Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create two route tables and attach to the subnets. The main route table will have the entry \u201cDestination: 20.0.0.0/24 and Target: Local\u201d, which allows all instances in the VPC to communicate with each other. QUESTION NO: 294 A sysadmin has created the below mentioned policy on an S3 bucket named cloudacademy. The bucket has both AWS.jpg and index.html objects. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg] }] A. It will make all the objects as well as the bucket public B. It will throw an error for the wrong action and does not allow to save the policy C. It will make the AWS.jpg object as public D. It will make the AWS.jpg as well as the cloudacademy bucket as public Answer: B Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. In the below policy the action says \u201cS3:ListBucket\u201d for effect Allow and when there is no bucket name mentioned as a part of the resource, it will throw an error and not save the policy. \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg] }] QUESTION NO: 295 A user has launched an EC2 instance and deployed a production application in it. The user wants to prohibit any mistakes from the production team to avoid accidental termination. How can the user achieve this? A. The user can the set DisableApiTermination attribute to avoid accidental termination B. It is not possible to avoid accidental termination C. The user can set the Deletion termination flag to avoid accidental termination D. The user can set the InstanceInitiatedShutdownBehavior flag to avoid accidental termination Answer: A Explanation: It is always possible that someone can terminate an EC2 instance using the Amazon EC2 console, command line interface or API by mistake. If the admin wants to prevent the instance from being accidentally terminated, he can enable termination protection for that instance. The DisableApiTermination attribute controls whether the instance can be terminated using the console, CLI or API. By default, termination protection is disabled for an EC2 instance. When it is set it will not allow the user to terminate the instance from CLI, API or the console. QUESTION NO: 296 A user has created a launch configuration for Auto Scaling where CloudWatch detailed monitoring is disabled. The user wants to now enable detailed monitoring. How can the user achieve this? A. Update the Launch config with CLI to set InstanceMonitoringDisabled = false B. The user should change the Auto Scaling group from the AWS console to enable detailed monitoring C. Update the Launch config with CLI to set InstanceMonitoring.Enabled = true D. Create a new Launch Config with detail monitoring enabled and update the Auto Scaling group Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates the AutoScaling launch config as the first step for creating an Auto Scaling group, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. When the user has created a launch configuration with InstanceMonitoring.Enabled = false it will involve multiple steps to enable detail monitoring. The steps are: Create a new Launch config with detailed monitoring enabled Update the Auto Scaling group with a new launch config Enable detail monitoring on each EC2 instance QUESTION NO: 297 A user is trying to pre-warm a blank EBS volume attached to a Linux instance. Which of the below mentioned steps should be performed by the user? A. There is no need to pre-warm an EBS volume B. Contact AWS support to pre-warm C. Unmount the volume before pre-warming D. Format the device Answer: C Explanation: When the user creates a new EBS volume or restores a volume from the snapshot, the backend storage blocks are immediately allocated to the user EBS. However, the first time when the user is trying to access a block of the storage, it is recommended to either be wiped from the new volumes or instantiated from the snapshot (for restored volumes. before the user can access the block. This preliminary action takes time and can cause a 5 to 50 percent loss of IOPS for the volume when the block is accessed for the first time. To avoid this it is required to pre warm the volume. Prewarming an EBS volume on a Linux instance requires that the user should unmount the blank device first and then write all the blocks on the device using a command, such as \u201cdd\u201d. QUESTION NO: 298 A user has launched an EC2 instance from an instance store backed AMI. The user has attached an additional instance store volume to the instance. The user wants to create an AMI from the running instance. Will the AMI have the additional instance store volume data? A. Yes, the block device mapping will have information about the additional instance store volume B. No, since the instance store backed AMI can have only the root volume bundled C. It is not possible to attach an additional instance store volume to the existing instance store backed AMI instance D. No, since this is ephermal storage it will not be a part of the AMI Answer: A Explanation: When the user has launched an EC2 instance from an instance store backed AMI and added an instance store volume to the instance in addition to the root device volume, the block device mapping for the new AMI contains the information for these volumes as well. In addition, the block device mappings for the instances those are launched from the new AMI will automatically contain information for these volumes. QUESTION NO: 299 A user has created an EBS volume of 10 GB and attached it to a running instance. The user is trying to access EBS for first time. Which of the below mentioned options is the correct statement with respect to a first time EBS access? A. The volume will show a size of 8 GB B. The volume will show a loss of the IOPS performance the first time C. The volume will be blank D. If the EBS is mounted it will ask the user to create a file system Answer: B Explanation: A user can create an EBS volume either from a snapshot or as a blank volume. If the volume is from a snapshot it will not be blank. The volume shows the right size only as long as it is mounted. This shows that the file system is created. When the user is accessing the volume the AWS EBS will wipe out the block storage or instantiate from the snapshot. Thus, the volume will show a loss of IOPS. It is recommended that the user should pre warm the EBS before use to achieve better IO. QUESTION NO: 300 A user has enabled termination protection on an EC2 instance. The user has also set Instance initiated shutdown behaviour to terminate. When the user shuts down the instance from the OS, what will happen? A. The OS will shutdown but the instance will not be terminated due to protection B. It will terminate the instance C. It will not allow the user to shutdown the instance from the OS D. It is not possible to set the termination protection when an Instance initiated shutdown is set to Terminate Answer: B Explanation: It is always possible that someone can terminate an EC2 instance using the Amazon EC2 console, command line interface or API by mistake. If the admin wants to prevent the instance from being accidentally terminated, he can enable termination protection for that instance. The user can also setup shutdown behaviour for an EBS backed instance to guide the instance on what should be done when he initiates shutdown from the OS using Instance initiated shutdown behaviour. If the instance initiated behaviour is set to terminate and the user shuts off the OS even though termination protection is enabled, it will still terminate the instance. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 301 A user has deployed an application on an EBS backed EC2 instance. For a better performance of application, it requires dedicated EC2 to EBS traffic. How can the user achieve this? A. Launch the EC2 instance as EBS dedicated with PIOPS EBS B. Launch the EC2 instance as EBS enhanced with PIOPS EBS C. Launch the EC2 instance as EBS dedicated with PIOPS EBS D. Launch the EC2 instance as EBS optimized with PIOPS EBS Answer: D Explanation: Any application which has performance sensitive workloads and requires minimal variability with dedicated EC2 to EBS traffic should use provisioned IOPS EBS volumes, which are attached to an EBS-optimized EC2 instance or it should use an instance with 10 Gigabit network connectivity. Launching an instance that is EBSoptimized provides the user with a dedicated connection between the EC2 instance and the EBS volume. QUESTION NO: 302 A user has launched a Windows based EC2 instance. However, the instance has some issues and the user wants to check the log. When the user checks the Instance console output from the AWS console, what will it display? A. All the event logs since instance boot B. The last 10 system event log error C. The Windows instance does not support the console output D. The last three system events\u2019 log errors Answer: D Explanation: The AWS EC2 console provides a useful tool called Console output for problem diagnosis. It is useful to find out any kernel issues, termination reasons or service configuration issues. For a Windows instance it lists the last three system event log errors. For Linux it displays the exact console output. Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"AWS Certified SysOps Administrator Questions- 2nd"},{"location":"nightwolf-cotribution/aws-2/#aws-certified-sysops-administrator-questions-and-answers-cont-2","text":"These are AWS interview questions for experienced professionals. You will find these questions very helpful in your AWS professional role interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 151 A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25 and a private subnet with CIDR 20.0.0.128/25. The user has launched one instance each in the private and public subnets. Which of the below mentioned options cannot be the correct IP address (private IP. assigned to an instance in the public or private subnet? A. 20.0.0.255 B. 20.0.0.132 C. 20.0.0.122 D. 20.0.0.55 Answer: A Explanation: When the user creates a subnet in VPC, he specifies the CIDR block for the subnet. In this case the user has created a VPC with the CIDR block 20.0.0.0/24, which supports 256 IP addresses (20.0.0.0 to 20.0.0.255.. The public subnet will have IP addresses between 20.0.0.0 - 20.0.0.127 and the private subnet will have IP addresses between 20.0.0.128 - 20.0.0.255. AWS reserves the first four IP addresses and the last IP address in each subnet\u2019s CIDR block. These are not available for the user to use. Thus, the instance cannot have an IP address of 20.0.0.255 QUESTION NO: 152 A user has launched an EBS backed EC2 instance. The user has rebooted the instance. Which of the below mentioned statements is not true with respect to the reboot action? A. The private and public address remains the same B. The Elastic IP remains associated with the instance C. The volume is preserved D. The instance runs on a new host computer Answer: D Explanation: A user can reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. However, it is recommended that the user use the Amazon EC2 to reboot the instance instead of running the operating system reboot command from the instance. The instance remains on the same host computer and maintains its public DNS name, private IP address, and any data on its instance store volumes. It typically takes a few minutes for the reboot to complete, but the time it takes to reboot depends on the instance configuration. QUESTION NO: 153 A user has setup a web application on EC2. The user is generating a log of the application performance at every second. There are multiple entries for each second. If the user wants to send that data to CloudWatch every minute, what should he do? A. The user should send only the data of the 60th second as CloudWatch will map the receive data timezone with the sent data timezone B. It is not possible to send the custom metric to CloudWatch every minute C. Give CloudWatch the Min, Max, Sum, and SampleCount of a number of every minute D. Calculate the average of one minute and send the data to CloudWatch Answer: C Explanation: Amazon CloudWatch aggregates statistics according to the period length that the user has specified while getting data from CloudWatch. The user can publish as many data points as he wants with the same or similartime stamps. CloudWatch aggregates them by the period length when the user calls get statistics about those data points. CloudWatch records the average (sum of all items divided by the number of items. of the values received for every 1-minute period, as well as the number of samples, maximum value, and minimum value for the same time period. CloudWatch will aggregate all the data which have time stamps within a one-minute period. QUESTION NO: 154 An AWS root account owner is trying to create a policy to access RDS. Which of the below mentioned statements is true with respect to the above information? A. Create a policy which allows the users to access RDS and apply it to the RDS instances B. The user cannot access the RDS database if he is not assigned the correct IAM policy C. The root account owner should create a policy for the IAM user and give him access to the RDS services D. The policy should be created for the user and provide access for RDS Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the account owner wants to create a policy for RDS, the owner has to create an IAM user and define the policy which entitles the IAM user with various RDS services such as Launch Instance, Manage security group, Manage parameter group etc. QUESTION NO: 155 A user is using a small MySQL RDS DB. The user is experiencing high latency due to the Multi AZ feature.Which of the below mentioned options may not help the user in this situation? A. Schedule the automated back up in non-working hours B. Use a large or higher size instance C. Use PIOPS D. Take a snapshot from standby Replica Answer: D Explanation: An RDS DB instance which has enabled Multi AZ deployments may experience increased write and commit latency compared to a Single AZ deployment, due to synchronous data replication. The user may also face changes in latency if deployment fails over to the standby replica. For production workloads, AWS recommends the user to use provisioned IOPS and DB instance classes (m1.large and larger. as they are optimized for provisioned IOPS to give a fast, and consistent performance. With Multi AZ feature, the user can not have option to take snapshot from replica. QUESTION NO: 156 A user is displaying the CPU utilization, and Network in and Network out CloudWatch metrics data of a single instance on the same graph. The graph uses one Y-axis for CPU utilization and Network in and another Y-axis for Network out. Since Network in is too high, the CPU utilization data is not visible clearly on graph to the user. How can the data be viewed better on the same graph? A. It is not possible to show multiple metrics with the different units on the same graph B. Add a third Y-axis with the console to show all the data in proportion C. Change the axis of Network by using the Switch command from the graph D. Change the units of CPU utilization so it can be shown in proportion with Network Answer: C Explanation: Amazon CloudWatch provides the functionality to graph the metric data generated either by the AWS services or the custom metric to make it easier for the user to analyse. It is possible to show the multiple metrics with different units on the same graph. If the graph is not plotted properly due to a difference in the unit data over two metrics, the user can change the Y-axis of one of the graph by selecting that graph and clicking on the Switch option. QUESTION NO: 157 A user is planning to use AWS services for his web application. If the user is trying to set up his own billing management system for AWS, how can he configure it? A. Set up programmatic billing access. Download and parse the bill as per the requirement B. It is not possible for the user to create his own billing management service with AWS C. Enable the AWS CloudWatch alarm which will provide APIs to download the alarm data D. Use AWS billing APIs to download the usage report of each service from the AWS billing console Answer: A Explanation: AWS provides an option to have programmatic access to billing. Programmatic Billing Access leverages the existing Amazon Simple Storage Service (Amazon S3. APIs. Thus, the user can build applications that reference his billing data from a CSV (comma-separated value. file stored in an Amazon S3 bucket. AWS will upload the bill to the bucket every few hours and the user can download the bill CSV from the bucket, parse itand create a billing system as per the requirement. QUESTION NO: 158 A user is planning to schedule a backup for an EBS volume. The user wants security of the snapshot data. How can the user achieve data encryption with a snapshot? A. Use encrypted EBS volumes so that the snapshot will be encrypted by AWS B. While creating a snapshot select the snapshot with encryption C. By default the snapshot is encrypted by AWS D. Enable server side encryption for the snapshot using S3 Answer: A Explanation: AWS EBS supports encryption of the volume. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. The data at rest, the I/O as well as all the snapshots of the encrypted EBS will also be encrypted. EBS encryption is based on the AES-256 cryptographic algorithm, which is the industry standard. QUESTION NO: 159 A user has created a public subnet with VPC and launched an EC2 instance within it. The user is trying to delete the subnet. What will happen in this scenario? A. It will delete the subnet and make the EC2 instance as a part of the default subnet B. It will not allow the user to delete the subnet until the instances are terminated C. It will delete the subnet as well as terminate the instances D. The subnet can never be deleted independently, but the user has to delete the VPC first Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When an instance is launched it will have a network interface attached with it. The user cannot delete the subnet until he terminates the instance and deletes the network interface. QUESTION NO: 160 A user has setup an EBS backed instance and attached 2 EBS volumes to it. The user has setup a CloudWatch alarm on each volume for the disk data. The user has stopped the EC2 instance and detached the EBS volumes. What will be the status of the alarms on the EBS volume? A. OK B. Insufficient Data C. Alarm D. The EBS cannot be detached until all the alarms are removed Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. Alarms invoke actions only for sustained state changes. There are three states of the alarm: OK, Alarm and Insufficient data. In this case since the EBS is detached and inactive the state will be Insufficient. QUESTION NO: 161 A user has launched an EC2 instance from an instance store backed AMI. The infrastructure team wants to create an AMI from the running instance. Which of the below mentioned credentials is not required while creating the AMI? A. AWS account ID B. X.509 certificate and private key C. AWS login ID to login to the console D. Access key and secret access key Answer: C Explanation: When the user has launched an EC2 instance from an instance store backed AMI and the admin team wants to create an AMI from it, the user needs to setup the AWS AMI or the API tools first. Once the tool is setup the user will need the following credentials: AWS account ID; AWS access and secret access key; X.509 certificate with private key. QUESTION NO: 162 A user has configured an SSL listener at ELB as well as on the back-end instances. Which of the below mentioned statements helps the user understand ELB traffic handling with respect to the SSL listener? A. It is not possible to have the SSL listener both at ELB and back-end instances B. ELB will modify headers to add requestor details C. ELB will intercept the request to add the cookie details if sticky session is enabled D. ELB will not modify the headers Answer: D Explanation: When the user has configured Transmission Control Protocol (TCP. or Secure Sockets Layer (SSL. for both front-end and back-end connections of the Elastic Load Balancer, the load balancer forwards the request to the back-end instances without modifying the request headers unless the proxy header is enabled. SSL does not support sticky sessions. If the user has enabled a proxy protocol it adds the source and destination IP to the header. QUESTION NO: 163 A user has created a Cloudformation stack. The stack creates AWS services, such as EC2 instances, ELB, AutoScaling, and RDS. While creating the stack it created EC2, ELB and AutoScaling but failed to create RDS. What will Cloudformation do in this scenario? A. Cloudformation can never throw an error after launching a few services since it verifies all the steps before launching. B. It will warn the user about the error and ask the user to manually create RDS C. Rollback all the changes and terminate all the created services D. It will wait for the user\u2019s input about the error and correct the mistake after the input Answer: C Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The AWS Cloudformation stack is a collection of AWS resources which are created and managed as a single unit when AWS CloudFormation instantiates a template. If any of the services fails to launch, Cloudformation will rollback all the changes and terminate or delete all the created services. QUESTION NO: 164 A user is trying to launch an EBS backed EC2 instance under free usage. The user wants to achieve encryption of the EBS volume. How can the user encrypt the data at rest? A. Use AWS EBS encryption to encrypt the data at rest B. The user cannot use EBS encryption and has to encrypt the data manually or using a third party tool C. The user has to select the encryption enabled flag while launching the EC2 instance D. Encryption of volume is not available as a part of the free usage tier Answer: B Explanation: AWS EBS supports encryption of the volume while creating new volumes. It supports encryption of the data at rest, the I/O as well as all the snapshots of the EBS volume. The EBS supports encryption for the selected instance type and the newer generation instances, such as m3, c3, cr1, r3, g2. It is not supported with a micro instance. QUESTION NO: 165 A user has created a VPC with public and private subnets using the VPC wizard. The user has not launched any instance manually and is trying to delete the VPC. What will happen in this scenario? A. It will not allow to delete the VPC as it has subnets with route tables B. It will not allow to delete the VPC since it has a running route instance C. It will terminate the VPC along with all the instances launched by the wizard D. It will not allow to delete the VPC since it has a running NAT instance Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance with an elastic IP. If the user is trying to delete the VPC it will not allow as the NAT instance is still running. QUESTION NO: 166 An organization is measuring the latency of an application every minute and storing data inside a file in the JSON format. The organization wants to send all latency data to AWS CloudWatch. How can the organization achieve this? A. The user has to parse the file before uploading data to CloudWatch B. It is not possible to upload the custom data to CloudWatch C. The user can supply the file as an input to the CloudWatch command D. The user can use the CloudWatch Import command to import data from the file to CloudWatch Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user has to always include the namespace as part of the request. If the user wants to upload the custom data from a file, he can supply file name along with the parameter -- metric-data to command put-metric-data. QUESTION NO: 167 A user has launched an EBS backed instance with EC2-Classic. The user stops and starts the instance. Which of the below mentioned statements is not true with respect to the stop/start action? A. The instance gets new private and public IP addresses B. The volume is preserved C. The Elastic IP remains associated with the instance D. The instance may run on a anew host computer Answer: C Explanation: A user can always stop/start an EBS backed EC2 instance. When the user stops the instance, it first enters the stopping state, and then the stopped state. AWS does not charge the running cost but charges only for the EBS storage cost. If the instance is running in EC2-Classic, it receives a new private IP address; as the Elastic IP address (EIP. associated with the instance is no longer associated with that instance. QUESTION NO: 168 A user has launched an RDS postgreSQL DB with AWS. The user did not specify the maintenance window during creation. The user has configured RDS to update the DB instance type from micro to large. If the user wants to have it during the maintenance window, what will AWS do? A. AWS will not allow to update the DB until the maintenance window is configured B. AWS will select the default maintenance window if the user has not provided it C. AWS will ask the user to specify the maintenance window during the update D. It is not possible to change the DB size from micro to large with RDS Answer: B Explanation: AWS RDS has a compulsory maintenance window which by default is 30 minutes. If the user does not specify the maintenance window during the creation of RDS then AWS will select a 30-minute maintenance window randomly from an 8-hour block of time per region. In this case, Amazon RDS assigns a 30-minute maintenance window on a randomly selected day of the week. QUESTION NO: 169 A user has created a subnet in VPC and launched an EC2 instance within it. The user has not selected the option to assign the IP address while launching the instance. The user has 3 elastic IPs and is trying to assign one of the Elastic IPs to the VPC instance from the console. The console does not show any instance in the IP assignment screen. What is a possible reason that the instance is unavailable in the assigned IP console? A. The IP address may be attached to one of the instances B. The IP address belongs to a different zone than the subnet zone C. The user has not created an internet gateway D. The IP addresses belong to EC2 Classic; so they cannot be assigned to VPC Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When the user is launching an instance he needs toselect an option which attaches a public IP to the instance. If the user has not selected the option to attach the public IP then it will only have a private IP when launched. If the user wants to connect to an instance from the internet he should create an elastic IP with VPC. If the elastic IP is a part of EC2 Classic it cannot be assigned to a VPC instance. QUESTION NO: 170 A user has launched multiple EC2 instances for the purpose of development and testing in the same region. The user wants to find the separate cost for the production and development instances. How can the user find the cost distribution? A. The user should download the activity report of the EC2 services as it has the instance ID wise data B. It is not possible to get the AWS cost usage data of single region instances separately C. The user should use Cost Distribution Metadata and AWS detailed billing D. The user should use Cost Allocation Tags and AWS billing reports Answer: D Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources (such as Amazon EC2 instances or Amazon S3 buckets., AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. The user can apply tags which represent business categories (such as cost centres, application names, or instance type \u2013 Production/Dev. to organize usage costs across multiple services. QUESTION NO: 171 A user has created a VPC with CIDR 20.0.0.0/16 using VPC Wizard. The user has created a public CIDR (20.0.0.0/24) and a VPN only subnet CIDR (20.0.1.0/24) along with the hardware VPN access to connect to the user\u2019s data centre. Which of the below mentioned components is not present when the VPC is setup with the wizard? A. Main route table attached with a VPN only subnet B. A NAT instance configured to allow the VPN subnet instances to connect with the internet C. Custom route table attached with a public subnet D. An internet gateway for a public subnet Answer: B Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will update the main route table used with the VPN-only subnet, create a custom route table and associate it with the public subnet. It also creates an internet gateway for the public subnet. The wizard does not create a NAT instance by default. The user can create it manually and attach it with a VPN only subnet. QUESTION NO: 172 A user has created a VPC with the public subnet. The user has created a security group for that VPC. Which of the below mentioned statements is true when a security group is created? A. It can connect to the AWS services, such as S3 and RDS by default B. It will have all the inbound traffic by default C. It will have all the outbound traffic by default D. It will by default allow traffic to the internet gateway Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level while ACLs work at the subnet level. When a user creates a security group with AWS VPC, by default it will allow all the outbound traffic but block all inbound traffic. QUESTION NO: 173 A user has setup an Auto Scaling group. The group has failed to launch a single instance for more than 24 hours. What will happen to Auto Scaling in this condition? A. Auto Scaling will keep trying to launch the instance for 72 hours B. Auto Scaling will suspend the scaling process C. Auto Scaling will start an instance in a separate region D. The Auto Scaling group will be terminated automatically Answer: B Explanation: If Auto Scaling is trying to launch an instance and if the launching of the instance fails continuously, it will suspend the processes for the Auto Scaling groups since it repeatedly failed to launch an instance. This is known as an administrative suspension. It commonly applies to the Auto Scaling group that has no running instances which is trying to launch instances for more than 24 hours, and has not succeeded in that to do so. QUESTION NO: 174 A user is planning to set up the Multi AZ feature of RDS. Which of the below mentioned conditions won't take advantage of the Multi AZ feature? A. Availability zone outage B. A manual failover of the DB instance using Reboot with failover option C. Region outage D. When the user changes the DB instance\u2019s server type Answer: C Explanation: Amazon RDS when enabled with Multi AZ will handle failovers automatically. Thus, the user can resume database operations as quickly as possible without administrative intervention. The primary DB instance switches over automatically to the standby replica if any of the following conditions occur: An Availability Zone outage The primary DB instance fails The DB instance's server type is changed The DB instance is undergoing software patching A manual failover of the DB instance was initiated using Reboot with failover QUESTION NO: 175 An organization has configured Auto Scaling with ELB. One of the instance health check returns the status as Impaired to Auto Scaling. What will Auto Scaling do in this scenario? A. Perform a health check until cool down before declaring that the instance has failed B. Terminate the instance and launch a new instance C. Notify the user using SNS for the failed state D. Notify ELB to stop sending traffic to the impaired instance Answer: B Explanation: The Auto Scaling group determines the health state of each instance periodically by checking the results of the Amazon EC2 instance status checks. If the instance status description shows any other state other than \u201crunning\u201d or the system status description shows impaired, Auto Scaling considers the instance to be unhealthy. Thus, it terminates the instance and launches a replacement. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 176 A user is using Cloudformation to launch an EC2 instance and then configure an application after the instance is launched. The user wants the stack creation of ELB and AutoScaling to wait until the EC2 instance is launched and configured properly. How can the user configure this? A. It is not possible that the stack creation will wait until one service is created and launched B. The user can use the HoldCondition resource to wait for the creation of the other dependent resources C. The user can use the DependentCondition resource to hold the creation of the other dependent resources D. The user can use the WaitCondition resource to hold the creation of the other 1034 dependent resources Answer: D Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. AWS CloudFormation provides a WaitCondition resource which acts as a barrier and blocks the creation of other resources until a completion signal is received from an external source, such as a user application or management system. QUESTION NO: 177 An organization has configured two single availability zones. The Auto Scaling groups are configured in separate zones. The user wants to merge the groups such that one group spans across multiple zones. How can the user configure this? A. Run the command as-join-auto-scaling-group to join the two groups B. Run the command as-update-auto-scaling-group to configure one group to span across zones and delete the other group C. Run the command as-copy-auto-scaling-group to join the two groups D. Run the command as-merge-auto-scaling-group to merge the groups Answer: B Explanation: If the user has configured two separate single availability zone Auto Scaling groups and wants to merge them then he should update one of the groups and delete the other one. While updating the first group it is recommended that the user should increase the size of the minimum, maximum and desired capacity as a summation of both the groups. QUESTION NO: 178 An AWS account wants to be part of the consolidated billing of his organization\u2019s payee account. How can the owner of that account achieve this? A. The payee account has to request AWS support to link the other accounts with his account B. The owner of the linked account should add the payee account to his master account list from the billing console C. The payee account will send a request to the linked account to be a part of consolidated billing D. The owner of the linked account requests the payee account to add his account to consolidated billing Answer: C Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. To add a particular account (linked. to the master (payee. account, the payee account has to request the linked account to join consolidated billing. Once the linked account accepts the request henceforth all charges incurred by the linked account will be paid by the payee account. QUESTION NO: 179 A sysadmin has created the below mentioned policy on an S3 bucket named cloudacademy. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy] }] A. It will make the cloudacademy bucket as well as all its objects as public B. It will allow everyone to view the ACL of the bucket C. It will give an error as no object is defined as part of the policy while the action defines the rule about the object D. It will make the cloudacademy bucket as public Answer: D Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. In the sample policy the action says \u201cS3:ListBucket\u201d for effect Allow on Resource arn:aws:s3:::cloudacademy. This will make the cloudacademy bucket public. \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy] }] QUESTION NO: 180 A user has launched two EBS backed EC2 instances in the US-East-1a region. The user wants to change the zone of one of the instances. How can the user change it? A. The zone can only be modified using the AWS CLI B. It is not possible to change the zone of an instance after it is launched C. Stop one of the instances and change the availability zone D. From the AWS EC2 console, select the Actions - > Change zones and specify the new zone Answer: B Explanation: With AWS EC2, when a user is launching an instance he can select the availability zone (AZ. at the time of launch. If the zone is not selected, AWS selects it on behalf of the user. Once the instance is launched, the user cannot change the zone of that instance unless he creates an AMI of that instance and launches a new instance from it. QUESTION NO: 181 An organization (account ID 123412341234. has configured the IAM policy to allow the user to modify his credentials. What will the below mentioned statement allow the user to perform? { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"iam:AddUserToGroup\", \"iam:RemoveUserFromGroup\", \"iam:GetGroup\" ], \"Resource\": \"arn:aws:iam:: 123412341234:group/TestingGroup\" }] A. The IAM policy will throw an error due to an invalid resource name B. The IAM policy will allow the user to subscribe to any IAM group C. Allow the IAM user to update the membership of the group called TestingGroup D. Allow the IAM user to delete the TestingGroup Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (account ID 123412341234. wants their users to manage their subscription to the groups, they should create a relevant policy for that. The below mentioned policy allows the respective IAM user to update the membership of the group called MarketingGroup. { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"iam:AddUserToGroup\", \"iam:RemoveUserFromGroup\", \"iam:GetGroup\" ], \"Resource\": \"arn:aws:iam:: 123412341234:group/ TestingGroup \" }] QUESTION NO: 182 A user has configured ELB with two EBS backed instances. The user has stopped the instances for 1 week to save costs. The user restarts the instances after 1 week. Which of the below mentioned statements will help the user to understand the ELB and instance registration better? A. There is no way to register the stopped instances with ELB B. The user cannot stop the instances if they are registered with ELB C. If the instances have the same Elastic IP assigned after reboot they will be registered with ELB D. The instances will automatically get registered with ELB Answer: C Explanation: Elastic Load Balancing registers the user\u2019s load balancer with his EC2 instance using the associated IP address. When the instances are stopped and started back they will have a different IP address. Thus, they will not get registered with ELB unless the user manually registers them. If the instances are assigned the same Elastic IP after reboot they will automatically get registered with ELB. QUESTION NO: 183 A user is trying to connect to a running EC2 instance using SSH. However, the user gets a Host key not found error. Which of the below mentioned options is a possible reason for rejection? A. The user has provided the wrong user name for the OS login B. The instance CPU is heavily loaded C. The security group is not configured properly D. The access key to connect to the instance is wrong Answer: A Explanation: If the user is trying to connect to a Linux EC2 instance and receives the Host Key not found error the probable reasons are: The private key pair is not right The user name to login is wrong QUESTION NO: 184 A user has hosted an application on EC2 instances. The EC2 instances are configured with ELB and Auto Scaling. The application server session time out is 2 hours. The user wants to configure connection draining to ensure that all in-flight requests are supported by ELB even though the instance is being deregistered. What time out period should the user specify for connection draining? A. 5 minutes B. 1 hour C. 30 minutes D. 2 hours Answer: B QUESTION NO: 185 A user is using the AWS EC2. The user wants to make so that when there is an issue in the EC2 server, such as instance status failed, it should start a new instance in the user\u2019s private cloud. Which AWS service helps to achieve this automation? A. AWS CloudWatch + Cloudformation B. AWS CloudWatch + AWS AutoScaling + AWS ELB C. AWS CloudWatch + AWS VPC D. AWS CloudWatch + AWS SNS Answer: D Explanation: Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can configure a web service (HTTP End point. in his data centre which receives data and launches an instance in the private cloud. The user should configure the CloudWatch alarm to send a notification to SNS when the \u201cStatusCheckFailed\u201d metric is true for the EC2 instance. The SNS topic can be configured to send a notification to the user\u2019s HTTP end point which launches an instance in the private cloud. QUESTION NO: 186 A sys admin has enabled logging on ELB. Which of the below mentioned fields will not be a part of the log file name? A. Load Balancer IP B. EC2 instance IP C. S3 bucket name D. Random string Answer: B Explanation: Elastic Load Balancing access logs capture detailed information for all the requests made to the load balancer. Elastic Load Balancing publishes a log file from each load balancer node at the interval that the user has specified. The load balancer can deliver multiple logs for the same period. Elastic Load Balancing creates log file names in the following format: \u201c{Bucket}/{Prefix}/ AWSLogs/{AWS AccountID}/elasticloadbalancing/{Region}/{Year}/{Month}/{Day}/{AWS Account ID}_elasticloadbalancing_{Region}_{Load Balancer Name}_{End Time}_{Load Balancer IP}_{Random String}.log\u201c QUESTION NO: 187 A user has created a queue named \u201cawsmodule\u201d with SQS. One of the consumers of queue is down for 3 days and then becomes available. Will that component receive message from queue? A. Yes, since SQS by default stores message for 4 days B. No, since SQS by default stores message for 1 day only C. No, since SQS sends message to consumers who are available that time D. Yes, since SQS will not delete message until it is delivered to all consumers Answer: A Explanation: SQS allows the user to move data between distributed components of applications so they can perform different tasks without losing messages or requiring each component to be always available. Queues retain messages for a set period of time. By default, a queue retains messages for four days. However, the user can configure a queue to retain messages for up to 14 days after the message has been sent. QUESTION NO: 188 An organization has setup multiple IAM users. The organization wants that each IAM user accesses the IAM console only within the organization and not from outside. How can it achieve this? A. Create an IAM policy with the security group and use that security group for AWS console login B. Create an IAM policy with a condition which denies access when the IP address range is not from the organization C. Configure the EC2 instance security group which allows traffic only from the organization\u2019s IP range D. Create an IAM policy with VPC and allow a secure gateway between the organization and AWS Console Answer: B Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user can add conditions as a part of the IAM policies. The condition can be set on AWS Tags, Time, and Client IP as well as on many other parameters. If the organization wants the user to access only from a specific IP range, they should set an IAM policy condition which denies access when the IP is not in a certain range. E.g. The sample policy given below denies all traffic when the IP is not in a certain range. \"Statement\": [{ \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\", \"Condition\": { \"NotIpAddress\": { \"aws:SourceIp\": [\"10.10.10.0/24\", \"20.20.30.0/24\"] } } }] QUESTION NO: 189 An organization has created one IAM user and applied the below mentioned policy to the user. What entitlements do the IAM users avail with this policy? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\" \"Action\": [ \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:Describe*\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"autoscaling:Describe*\", \"Resource\": \"*\" } ] } A. The policy will allow the user to perform all read only activities on the EC2 services B. The policy will allow the user to list all the EC2 resources except EBS C. The policy will allow the user to perform all read and write activities on the EC2 services D. The policy will allow the user to perform all read only activities on the EC2 services except load Balancing Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If an organization wants to setup read only access to EC2 for a particular user, they should mention the action in the IAM policy which entitles the user for Describe rights for EC2, CloudWatch, Auto Scaling and ELB. In the policy shown below, the user will have read only access for EC2 and EBS, CloudWatch and Auto Scaling. Since ELB is not mentioned as a part of the list, the user will not have access to ELB. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:Describe*\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"autoscaling:Describe*\", \"Resource\": \"*\" } ] } QUESTION NO: 190 A user has enabled session stickiness with ELB. The user does not want ELB to manage the cookie; instead he wants the application to manage the cookie. What will happen when the server instance, which is bound to a cookie, crashes? A. The response will have a cookie but stickiness will be deleted B. The session will not be sticky until a new cookie is inserted C. ELB will throw an error due to cookie unavailability D. The session will be sticky and ELB will route requests to another server as ELB keeps replicating the Cookie Answer: B Explanation: With Elastic Load Balancer, if the admin has enabled a sticky session with application controlled stickiness, the load balancer uses a special cookie generated by the application to associate the session with the original server which handles the request. ELB follows the lifetime of the application-generated cookie corresponding to the cookie name specified in the ELB policy configuration. The load balancer only inserts a new stickiness cookie if the application response includes a new application cookie. The load balancer stickiness cookie does not update with each request. If the application cookie is explicitly removed or expires, the session stops being sticky until a new application cookie is issued. QUESTION NO: 191 A user is observing the EC2 CPU utilization metric on CloudWatch. The user has observed some interesting patterns while filtering over the 1 week period for a particular hour. The user wants to zoom that data point to a more granular period. How can the user do that easily with CloudWatch? A. The user can zoom a particular period by selecting that period with the mouse and then releasing the mouse B. The user can zoom a particular period by double clicking on that period with the mouse C. The user can zoom a particular period by specifying the aggregation data for that period D. The user can zoom a particular period by specifying the period in the Time Range Answer: A QUESTION NO: 192 A user has created an Auto Scaling group with default configurations from CLI. The user wants to setup the CloudWatch alarm on the EC2 instances, which are launched by the Auto Scaling group. The user has setup an alarm to monitor the CPU utilization every minute. Which of the below mentioned statements is true? A. It will fetch the data at every minute but the four data points [corresponding to 4 minutes] will not have value since the EC2 basic monitoring metrics are collected every five minutes B. It will fetch the data at every minute as detailed monitoring on EC2 will be enabled by the default launch configuration of Auto Scaling C. The alarm creation will fail since the user has not enabled detailed monitoring on the EC2 instances D . The user has to first enable detailed monitoring on the EC2 instances to support alarm monitoring at every minute Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates an Auto Scaling launch config using CLI, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. Thus, by default detailed monitoring will be enabled for Auto Scaling as well as for all the instances launched by that Auto Scaling group. QUESTION NO: 193 A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is not true in this scenario? A. The VPC will create a routing instance and attach it with a public subnet B. The VPC will create two subnets 116789 C. The VPC will create one internet gateway and attach it to VPC D. The VPC will launch one NAT instance with an elastic IP Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance with an elastic IP. Wizard will also create two subnets with route tables. It will also create an internet gateway and attach it to the VPC. QUESTION NO: 194 A user has configured ELB with a TCP listener at ELB as well as on the back-end instances. The user wants to enable a proxy protocol to capture the source and destination IP information in the header. Which of the below mentioned statements helps the user understand a proxy protocol with TCP configuration? A. If the end user is requesting behind a proxy server then the user should not enable a proxy protocol on ELB B. ELB does not support a proxy protocol when it is listening on both the load balancer and the backend instances C. Whether the end user is requesting from a proxy server or directly, it does not make a difference for the proxy protocol D. If the end user is requesting behind the proxy then the user should add the \u201cisproxy\u201d flag to the ELB Configuration Answer: A Explanation: When the user has configured Transmission Control Protocol (TCP. or Secure Sockets Layer (SSL. for both front-end and back-end connections of the Elastic Load Balancer, the load balancer forwards the request to the back-end instances without modifying the request headers unless the proxy header is enabled. If the end user is requesting from a Proxy Protocol enabled proxy server, then the ELB admin should not enable the Proxy Protocol on the load balancer. If the Proxy Protocol is enabled on both the proxy server and the load balancer, the load balancer will add another header to the request which already has a header from the proxy server. This duplication may result in errors. QUESTION NO: 195 A user has launched 5 instances in EC2-CLASSIC and attached 5 elastic IPs to the five different instances in the US East region. The user is creating a VPC in the same region. The user wants to assign an elastic IP to the VPC instance. How can the user achieve this? A. The user has to request AWS to increase the number of elastic IPs associated with the account B. AWS allows 10 EC2 Classic IPs per region ; so it will allow to allocate new Elastic IPs to the same region C. The AWS will not allow to create a new elastic IP in VPC; it will throw an error D. The user can allocate a new IP address in VPC as it has a different limit than EC2 Answer: D Explanation: Section: (none) A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. A user can have 5 IP addresses per region with EC2 Classic. The user can have 5 separate IPs with VPC in the same region as it has a separate limit than EC2 Classic. QUESTION NO: 196 A user has created a subnet in VPC and launched an EC2 instance within it. The user has not selected the option to assign the IP address while launching the instance. Which of the below mentioned statements is true with respect to this scenario? A. The instance will always have a public DNS attached to the instance by default B. The user can directly attach an elastic IP to the instance C. The instance will never launch if the public IP is not assigned D. The user would need to create an internet gateway and then attach an elastic IP to the instance to connect from internet Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When the user is launching an instance he needs to select an option which attaches a public IP to the instance. If the user has not selected the option to attach the public IP then it will only have a private IP when launched. The user cannot connect to the instance from the internet. If the user wants an elastic IP to connect to the instance from the internet he should create an internet gateway and assign an elastic IP to instance. QUESTION NO: 197 An organization has applied the below mentioned policy on an IAM group which has selected the IAM users. What entitlements do the IAM users avail with this policy? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" } ] } A. The policy is not created correctly. It will throw an error for wrong resource name B. The policy is for the group. Thus, the IAM user cannot have any entitlement to this C. It allows full access to all AWS services for the IAM users who are a part of this group D. If this policy is applied to the EC2 resource, the users of the group will have full access to the EC2 Resources Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The IAM group allows the organization to specify permissions for a collection of users. With the below mentioned policy, it will allow the group full access (Admin. to all AWS services. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" } ] } QUESTION NO: 198 A user is configuring a CloudWatch alarm on RDS to receive a notification when the CPU utilization of RDS is higher than 50%. The user has setup an alarm when there is some iinactivity on RDS, such as RDS unavailability. How can the user configure this? A. Setup the notification when the CPU is more than 75% on RDS B. Setup the notification when the state is Insufficient Data C. Setup the notification when the CPU utilization is less than 10% D. It is not possible to setup the alarm on RDS Answer: B Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The alarm has three states: Alarm, OK and Insufficient data. The Alarm will change to Insufficient Data when any of the three situations arise: when the alarm has just started, when the metric is not available or when enough data is not available for the metric to determine the alarm state. If the user wants to find that RDS is not available, he can setup to receive the notification when the state is in Insufficient data. QUESTION NO: 199 George has shared an EC2 AMI created in the US East region from his AWS account with Stefano. George copies the same AMI to the US West region. Can Stefano access the copied AMI of George\u2019s account from the US West region? A. No, copy AMI does not copy the permission B. It is not possible to share the AMI with a specific account C. Yes, since copy AMI copies all private account sharing permissions D. Yes, since copy AMI copies all the permissions attached with the AMI Answer: A Explanation: Within EC2, when the user copies an AMI, the new AMI is fully independent of the source AMI; there is no link to the original (source. AMI. AWS does not copy launch the permissions, userdefined tags or the Amazon S3 bucket permissions from the source AMI to the new AMI. Thus, in this case by default Stefano will not have access to the AMI in the US West region. QUESTION NO: 200 A user has created a VPC with a subnet and a security group. The user has launched an instance in that subnet and attached a public IP. The user is still unable to connect to the instance. The internet gateway has also been created. What can be the reason for the error? A. The internet gateway is not configured with the route table B. The private IP is not present C. The outbound traffic on the security group is disabled D. The internet gateway is not configured with the security group Answer: A Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level. When a user launches an instance and wants to connect to an instance, he needs an internet gateway. The internet gateway should be configured with the route table to allow traffic from the internet. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 201 A user is trying to setup a security policy for ELB. The user wants ELB to meet the cipher supported by the client by configuring the server order preference in ELB security policy. Which of the below mentioned preconfigured policies supports this feature? A. ELBSecurity Policy-2014-01 B. ELBSecurity Policy-2011-08 C. ELBDefault Negotiation Policy D. ELBSample- OpenSSLDefault Cipher Policy Answer: A Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. If the load balancer is configured to support the Server Order Preference, then load balancer gets to select the first cipher in its list that matches any one of the ciphers in client's list. When the user verifies the preconfigured policies supported by ELB, the policy \u201cELBSecurity Policy-2014-01\u201d supports server order preference. QUESTION NO: 202 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling AlarmNotification (which notifies Auto Scaling for CloudWatch alarms. process for a while. What will Auto Scaling do during this period? A. AWS will not receive the alarms from CloudWatch B. AWS will receive the alarms but will not execute the Auto Scaling policy C. Auto Scaling will execute the policy but it will not launch the instances until the process is resumed D. It is not possible to suspend the AlarmNotification process Answer: B Explanation: Auto Scaling performs various processes, such as Launch, Terminate Alarm Notification etc. The user can also suspend individual process. The AlarmNotification process type accepts notifications from the Amazon CloudWatch alarms that are associated with the Auto Scaling group. If the user suspends this process type, Auto Scaling will not automatically execute the scaling policies that would be triggered by the alarms. QUESTION NO: 203 George has launched three EC2 instances inside the US-East-1a zone with his AWS account. Ray has launched two EC2 instances in the US-East-1a zone with his AWS account. Which of the below entioned statements will help George and Ray understand the availability zone (AZ. concept better? A. The instances of George and Ray will be running in the same data centre B. All the instances of George and Ray can communicate over a private IP with a minimal cost C. All the instances of George and Ray can communicate over a private IP without any cost D. The US-East-1a region of George and Ray can be different availability zones Answer: D Explanation: Each AWS region has multiple, isolated locations known as Availability Zones. To ensure that the AWS resources are distributed across the Availability Zones for a region, AWS independently maps the Availability Zones to identifiers for each account. In this case the Availability Zone US-East-1a where George\u2019s EC2 instances are running might not be the same location as the US-East-1a zone of Ray\u2019s EC2 instances. There is no way for the user to coordinate the Availability Zones between accounts. QUESTION NO: 204 A user had aggregated the CloudWatch metric data on the AMI ID. The user observed some abnormal behaviour of the CPU utilization metric while viewing the last 2 weeks of data. The user wants to share that data with his manager. How can the user achieve this easily with the AWS console? A. The user can use the copy URL functionality of CloudWatch to share the exact details B. The user can use the export data option from the CloudWatch console to export the current data point C. The user has to find the period and data and provide all the aggregation information to the manager D. The user can use the CloudWatch data copy functionality to copy the current data points Answer: A Explanation: Amazon CloudWatch provides the functionality to graph the metric data generated either by the AWS services or the custom metric to make it easier for the user to analyse. The console provides the option to save the URL or bookmark it so that it can be used in the future by typing the same URL. The Copy URL functionality is available under the console when the user selects any metric to view. QUESTION NO: 205 A user has setup a CloudWatch alarm on the EC2 instance for CPU utilization. The user has setup to receive a notification on email when the CPU utilization is higher than 60%. The user is running a virus scan on the same instance at a particular time. The user wants to avoid receiving an email at this time. What should the user do? A. Remove the alarm B. Disable the alarm for a while using CLI C. Modify the CPU utilization by removing the email alert D. Disable the alarm for a while using the console Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. When the user has setup an alarm and it is know that for some unavoidable event the status may change to Alarm, the user can disable the alarm using the DisableAlarmActions API or from the command line mon-disable-alarm-actions. QUESTION NO: 206 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned SSL protocols is not supported by the security policy? A. TLS 1.3 B. TLS 1.2 C. SSL 2.0 D. SSL 3.0 Answer: A Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and loadbalancer. Elastic Load Balancing supports the following versions of the SSL protocol: TLS 1.2 TLS 1.1 TLS 1.0 SSL 3.0 SSL 2.0 QUESTION NO: 207 A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group for the public subnet (WebSecGrp. and the private subnet(DBSecGrp). Which of the below mentioned entries is required in the private subnet database security group (DBSecGrp)? A. Allow Inbound on port 3306 for Source Web Server Security Group (WebSecGrp. B. Allow Inbound on port 3306 from source 20.0.0.0/16 C. Allow Outbound on port 3306 for Destination Web Server Security Group (WebSecGrp. D. Allow Outbound on port 80 for Destination NAT Instance IP Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the private subnet can receive inbound traffic from the public subnet on the DB port. Thus, configure port 3306 in Inbound with the source as the Web Server Security Group (WebSecGrp.. The user should configure ports 80 and 443 for Destination 0.0.0.0/0 as the route table directs traffic to the NAT instance from the private subnet. QUESTION NO: 208 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created public and VPN only subnets along with hardware VPN access to connect to the user\u2019s data centre. The user has not yet launched any instance as well as modified or deleted any setup. He wants to delete this VPC from the console. Will the console allow the user to delete the VPC? A. Yes, the console will delete all the setups and also delete the virtual private gateway B. No, the console will ask the user to manually detach the virtual private gateway first and then allow deleting the VPC C. Yes, the console will delete all the setups and detach the virtual private gateway D. No, since the NAT instance is running Answer: C Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. If the virtual private gateway is attached with VPC and the user deletes the VPC from the console it will first detach the gateway automatically and only then delete the VPC. QUESTION NO: 209 A user is trying to create a PIOPS EBS volume with 4000 IOPS and 100 GB size. AWS does not allow the user to create this volume. What is the possible root cause for this? A. The ratio between IOPS and the EBS volume is higher than 30 B. The maximum IOPS supported by EBS is 3000 C. The ratio between IOPS and the EBS volume is lower than 50 D. PIOPS is supported for EBS higher than 500 GB size Answer: A Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30; for example, a volume with 3000 IOPS must be at least 100 GB. QUESTION NO: 210 A user has setup a custom application which generates a number in decimals. The user wants to track that number and setup the alarm whenever the number is above a certain limit. The application is sending the data to CloudWatch at regular intervals for this purpose. Which of the below mentioned statements is not true with respect to the above scenario? A. The user can get the aggregate data of the numbers generated over a minute and send it to CloudWatch B. The user has to supply the timezone with each data point C. CloudWatch will not truncate the number until it has an exponent larger than 126 (i.e. (1 x 10^126) ). D. The user can create a file in the JSON format with the metric name and value and supply it to CloudWatch Answer: B QUESTION NO: 211 A user has launched an EC2 Windows instance from an instance store backed AMI. The user has also set the Instance initiated shutdown behavior to stop. What will happen when the user shuts down the OS? A. It will not allow the user to shutdown the OS when the shutdown behaviour is set to Stop B. It is not possible to set the termination behaviour to Stop for an Instance store backed AMI instance C. The instance will stay running but the OS will be shutdown D. The instance will be terminated Answer: B Explanation: When the EC2 instance is launched from an instance store backed AMI, it will not allow the user to configure the shutdown behaviour to \u201cStop\u201d. It gives a warning that the instance does not have the EBS root volume. QUESTION NO: 212 A user has enabled versioning on an S3 bucket. The user is using server side encryption for data at Rest. If the user is supplying his own keys for encryption (SSE-C., which of the below mentioned statements is true? A. The user should use the same encryption key for all versions of the same object B. It is possible to have different encryption keys for different versions of the same object C. AWS S3 does not allow the user to upload his own keys for server side encryption D. The SSE-C does not work when versioning is enabled Answer: B Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. If the bucket is versioningenabled, each object version uploaded by the user using the SSE-C feature can have its own encryption key. The user is responsible for tracking which encryption key was used for which object's version QUESTION NO: 213 A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 in this VPC. The user is trying to create another subnet with the same VPC for CIDR 20.0.0.1/24. What will happen in this scenario? A. The VPC will modify the first subnet CIDR automatically to allow the second subnet IP range B. It is not possible to create a subnet with the same CIDR as VPC C. The second subnet will be created D. It will throw a CIDR overlaps error Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. The user can create a subnet with the same size of VPC. However, he cannot create any other subnet since the CIDR of the second subnet will conflict with the first subnet. QUESTION NO: 214 A user has launched an RDS MySQL DB with the Multi AZ feature. The user has scheduled the scaling of instance storage during maintenance window. What is the correct order of events during maintenance window? Perform maintenance on standby Promote standby to primary Perform maintenance on original primary Promote original master back as primary A. 1, 2, 3, 4 B. 1, 2, 3 C. 2, 3, 1, 4 Answer: B Explanation: Running MySQL on the RDS DB instance as a Multi-AZ deployment can help the user reduce the impact of a maintenance event, as the Amazon will conduct maintenance by following the steps in the below mentioned order: Perform maintenance on standby Promote standby to primary Perform maintenance on original primary, which becomes the new standby. QUESTION NO: 215 A sys admin is using server side encryption with AWS S3. Which of the below mentioned statements helps the user understand the S3 encryption functionality? A. The server side encryption with the user supplied key works when versioning is enabled B. The user can use the AWS console, SDK and APIs to encrypt or decrypt the content for server side encryption with the user supplied key. C. The user must send an AES-128 encrypted key D. The user can upload his own encryption key to the S3 console Answer: A Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key. The encryption with the user supplied key (SSE-C. does not work with the AWS console. The S3 does not store the keys and the user has to send a key with each request. The SSE-C works when the user has enabled versioning. QUESTION NO: 216 A root account owner is trying to understand the S3 bucket ACL. Which of the below mentioned options cannot be used to grant ACL on the object using the authorized predefined group? A. Authenticated user group B. All users group C. Log Delivery Group D. Canonical user group Answer: D Explanation: An S3 bucket ACL grantee can be an AWS account or one of the predefined Amazon S3 groups. Amazon S3 has a set of predefined groups. When granting account access to a group, the user can specify one of the URLs of that group instead of a canonical user ID. AWS S3 has the following predefined groups: Authenticated Users group: It represents all AWS accounts. All Users group: Access permission to this group allows anyone to access the resource. Log Delivery group: WRITE permission on a bucket enables this group to write server access logs to the bucket. QUESTION NO: 217 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24. and VPN only subnets CIDR (20.0.1.0/24. along with the VPN gateway (vgw-12345. to connect to the user\u2019s data centre. The user\u2019s data centre has CIDR 172.28.0.0/12. The user has also setup a NAT instance (i-123456. to allow traffic to the internet from the VPN subnet. Which of the below mentioned options is not a valid entry for the main route table in this scenario? A. Destination: 20.0.1.0/24 and Target: i-12345 B. Destination: 0.0.0.0/0 and Target: i-12345 C. Destination: 172.28.0.0/12 and Target: vgw-12345 D. Destination: 20.0.0.0/16 and Target: local Answer: A Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. If the user has setup a NAT instance to route all the internet requests then all requests to the internet should be routed to it. All requests to the organization\u2019s DC will be routed to the VPN gateway. Here are the valid entries for the main route table in this scenario: Destination: 0.0.0.0/0 & Target: i-12345 (To route all internet traffic to the NAT Instance. Destination: 172.28.0.0/12 & Target: vgw-12345 (To route all the organization\u2019s data centre traffic to the VPN gateway. Destination: 20.0.0.0/16 & Target: local (To allow local routing in VPC. QUESTION NO: 218 A user has created a VPC with public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24 . The NAT instance ID is i-a12345. Which of the below mentioned entries are required in the main route table attached with the private subnet to allow instances to connect with the internet? A. Destination: 0.0.0.0/0 and Target: i-a12345 B. Destination: 20.0.0.0/0 and Target: 80 C. Destination: 20.0.0.0/0 and Target: i-a12345 D. Destination: 20.0.0.0/24 and Target: i-a12345 Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create two route tables and attach to the subnets. The main route table will have the entry \u201cDestination: 0.0.0.0/0 and Target: ia12345\u201d, which allows all the instances in the private subnet to connect to the internet using NAT. QUESTION NO: 219 A root account owner has given full access of his S3 bucket to one of the IAM users using the bucket ACL. When the IAM user logs in to the S3 console, which actions can he perform? A. He can just view the content of the bucket B. He can do all the operations on the bucket C. It is not possible to give access to an IAM user using ACL D. The IAM user can perform all operations on the bucket using only API/SDK Answer: C Explanation: Each AWS S3 bucket and object has an ACL (Access Control List. associated with it. An ACL is a list of grants identifying the grantee and the permission granted. The user can use ACLs to grant basic read/write permissions to other AWS accounts. ACLs use an Amazon S3\u2013specific XML schema. The user cannot grant permissions to other users (IAM users. in his account. QUESTION NO: 220 An organization has configured Auto Scaling with ELB. There is a memory issue in the application which is causing CPU utilization to go above 90%. The higher CPU usage triggers an event for Auto Scaling as per the scaling policy. If the user wants to find the root cause inside the application without triggering a scaling activity, how can he achieve this? A. Stop the scaling process until research is completed B. It is not possible to find the root cause from that instance without triggering scaling C. Delete Auto Scaling until research is completed D. Suspend the scaling process until research is completed Answer: D Explanation: Auto Scaling allows the user to suspend and then resume one or more of the Auto Scaling processes in the Auto Scaling group. This is very useful when the user wants to investigate a configuration problem or some other issue, such as a memory leak with the web application and then make changes to the application, without triggering the Auto Scaling process. QUESTION NO: 221 A sys admin is planning to subscribe to the RDS event notifications. For which of the below mentioned source categories the subscription cannot be configured? A. DB security group B. DB snapshot C. DB options group D. DB parameter group Answer: C Explanation: Amazon RDS uses the Amazon Simple Notification Service (SNS. to provide a notification when an Amazon RDS event occurs. These events can be configured for source categories, such as DB instance, DB security group, DB snapshot and DB parameter group. QUESTION NO: 222 A user has launched an EC2 instance. The instance got terminated as soon as it was launched. Which of the below mentioned options is not a possible reason for this? A. The user account has reached the maximum EC2 instance limit B. The snapshot is corrupt C. The AMI is missing. It is the required part D. The user account has reached the maximum volume limit Answer: A Explanation: When the user account has reached the maximum number of EC2 instances, it will not be allowed to launch an instance. AWS will throw an \u2018InstanceLimitExceeded\u2019 error. For all other reasons, such as \u201cAMI is missing part\u201d, \u201cCorrupt Snapshot\u201d or \u201dVolume limit has reached\u201d it will launch an EC2 instance and then terminate it. QUESTION NO: 223 A user is trying to understand the detailed CloudWatch monitoring concept. Which of the below mentioned services does not provide detailed monitoring with CloudWatch? A. AWS EMR B. AWS RDS C. AWS ELB D. AWS Route53 Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Services, such as RDS, EC2, Auto Scaling, ELB, and Route 53 can provide the monitoring data every minute. QUESTION NO: 224 A user is measuring the CPU utilization of a private data centre machine every minute. The machine provides the aggregate of data every hour, such as Sum of data\u201d, \u201cMin value\u201d, \u201cMax value, and \u201cNumber of Data points\u201d. The user wants to send these values to CloudWatch. How can the user achieve this? A. Send the data using the put-metric-data command with the aggregate-values parameter B. Send the data using the put-metric-data command with the average-values parameter C. Send the data using the put-metric-data command with the statistic-values parameter D. Send the data using the put-metric-data command with the aggregate \u2013data parameter Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish the data to CloudWatch as single data points or as an aggregated set of data points called a statistic set using the command put-metric-data. When sending the aggregate data, the user needs to send it with the parameter statistic-values: awscloudwatch put-metric-data --metric-name <Name> --namespace <Custom namespace> -- timestamp <UTC Format> --statistic-values Sum=XX,Minimum=YY,Maximum=AA,SampleCount=BB --unit Milliseconds QUESTION NO: 225 A user has enabled detailed CloudWatch monitoring with the AWS Simple Notification Service. Which of the below mentioned statements helps the user understand detailed monitoring better? A. SNS will send data every minute after configuration B. There is no need to enable since SNS provides data every minute C. AWS CloudWatch does not support monitoring for SNS D. SNS cannot provide data every minute Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. The AWS SNS service sends data every 5 minutes. Thus, it supports only the basic monitoring. The user cannot enable detailed monitoring with SNS. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 226 A user has setup a VPC with CIDR 20.0.0.0/16. The VPC has a private subnet (20.0.1.0/24) and a public subnet (20.0.0.0/240). The user\u2019s data centre has CIDR of 20.0.54.0/24 and 20.1.0.0/24 If the private subnet wants to communicate with the data centre, what will happen? A. It will allow traffic communication on both the CIDRs of the data centre B. It will not allow traffic with data centre on CIDR 20.1.0.0/24 but allows traffic communication on 20.0.54.0/24 C. It will not allow traffic communication on any of the data centre CIDRs D. It will allow traffic with data centre on CIDR 20.1.0.0/24 but does not allow on 20.0.54.0/24 Answer: D Explanation: VPC allows the user to set up a connection between his VPC and corporate or home network data centre. If the user has an IP address prefix in the VPC that overlaps with one of the networks' prefixes, any traffic to the network's prefix is dropped. In this case CIDR 20.0.54.0/24 falls in the VPC\u2019s CIDR range of 20.0.0.0/16. Thus, it will not allow traffic on that IP. In the case of 20.1.0.0/24, it does not fall in the VPC\u2019s CIDR range. Thus, traffic will be allowed on it. QUESTION NO: 227 A user wants to find the particular error that occurred on a certain date in the AWS MySQL RDS DB. Which of the below mentioned activities may help the user to get the data easily? A. It is not possible to get the log files for MySQL RDS B. Find all the transaction logs and query on those records C. Direct the logs to the DB table and then query that table D. Download the log file to DynamoDB and search for the record Answer: C Explanation: The user can view, download, and watch the database logs using the Amazon RDS console, the Command Line Interface (CLI. or the Amazon RDS API. For the MySQL RDS, the user can view the error log, slow query log, and general logs. The user can also view the MySQL logs easily by directing the logs to a database table in the main database and querying that table. QUESTION NO: 228 A user is trying to send custom metrics to CloudWatch using the PutMetricData APIs. Which of the below mentioned points should the user needs to take care while sending the data to CloudWatch? A. The size of a request is limited to 8KB for HTTP GET requests and 40KB for HTTP POST requests B. The size of a request is limited to 128KB for HTTP GET requests and 64KB for HTTP POST requests C. The size of a request is limited to 40KB for HTTP GET requests and 8KB for HTTP POST requests D. The size of a request is limited to 16KB for HTTP GET requests and 80KB for HTTP POST requests Answer: A Explanation: With AWS CloudWatch, the user can publish data points for a metric that share not only the same time stamp, but also the same namespace and dimensions. CloudWatch can accept multiple data points in the same PutMetricData call with the same time stamp. The only thing that the user needs to take care of is that the size of a PutMetricData request is limited to 8KB for HTTP GET requests and 40KB for HTTP POST requests. QUESTION NO: 229 An AWS account owner has setup multiple IAM users. One IAM user only has CloudWatch access. He has setup the alarm action which stops the EC2 instances when the CPU utilization is below the threshold limit. What will happen in this case? A. It is not possible to stop the instance using the CloudWatch alarm B. CloudWatch will stop the instance when the action is executed C. The user cannot set an alarm on EC2 since he does not have the permission D. The user can setup the action but it will not be executed if the user does not have EC2 rights Answer: D Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup an action which stops the instances when their CPU utilization is below a certain threshold for a certain period of time. The EC2 action can either terminate or stop the instance as part of the EC2 action. If the IAM user has read/write permissions for Amazon CloudWatch but not for Amazon EC2, he can still create an alarm. However, the stop or terminate actions will not be performed on the Amazon EC2 instance. QUESTION NO: 230 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling terminate process only for a while. What will happen to the availability zone rebalancing process (AZRebalance. during this period? A. Auto Scaling will not launch or terminate any instances B. Auto Scaling will allow the instances to grow more than the maximum size C. Auto Scaling will keep launching instances till the maximum instance size D. It is not possible to suspend the terminate process while keeping the launch active Answer: B Explanation: Auto Scaling performs various processes, such as Launch, Terminate, Availability Zone Rebalance (AZRebalance. etc. The AZRebalance process type seeks to maintain a balanced number of instances across Availability Zones within a region. If the user suspends the Terminate process, the AZRebalance process can cause the Auto Scaling group to grow up to ten percent larger than the maximum size. This is because Auto Scaling allows groups to temporarily grow larger than the maximum size during rebalancing activities. If Auto Scaling cannot terminate instances, the Auto Scaling group could remain up to ten percent larger than the maximum size until the user resumes the Terminate process type. QUESTION NO: 231 A user has created a mobile application which makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK and root account access/secret access key to connect to DynamoDB from mobile. Which of the below mentioned statements is true with respect to the best practice for security in this scenario? A. The user should create a separate IAM user for each mobile application and provide DynamoDB access with it. B. The user should create an IAM role with DynamoDB and EC2 access. Attach the role with EC2 and route all calls from the mobile through EC2. C. The application should use an IAM role with web identity federation which validates calls to DynamoDB with identity providers, such as Google, Amazon, and Facebook. D. Create an IAM Role with DynamoDB access and attach it with the mobile application Answer: C Explanation: With AWS IAM a user is creating an application which runs on an EC2 instance and makes requests to AWS, such as DynamoDB or S3 calls. Here it is recommended that the user should not create an IAM user and pass the user's credentials to the application or embed those credentials inside the application. If the user is creating an app that runs on a mobile phone and makes requests to AWS, the user should not create an IAMuser and distribute the user's access key with the app. Instead, he should use an identity provider, such as Login with Amazon, Facebook, or Google to authenticate the users, and then use that identity to get temporary security credentials. QUESTION NO: 232 A user is configuring the Multi AZ feature of an RDS DB. The user came to know that this RDS DB does not use the AWS technology, but uses server mirroring to achieve HA. Which DB is the user using right now? A. My SQL B. Oracle C. MS SQL D. PostgreSQL Answer: C Explanation: Amazon RDS provides high availability and failover support for DB instances using Multi AZ deployments. In a Multi AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Multi AZ deployments for Oracle, PostgreSQL, and MySQL DB instances use Amazon technology, while SQL Server (MS SQL. DB instances use SQL Server Mirroring. QUESTION NO: 233 A user is receiving a notification from the RDS DB whenever there is a change in the DB security group. The user does not want to receive these notifications for only a month. Thus, he does not want to delete the notification. How can the user configure this? A. Change the Disable button for notification to \u201cYes\u201d in the RDS console B. Set the send mail flag to false in the DB event notification console C. The only option is to delete the notification from the console D. Change the Enable button for notification to \u201cNo\u201d in the RDS console Answer: D Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. Event notifications are sent to the addresses that the user has provided while creating the subscription. The user can easily turn off the notification without deleting a subscription by setting the Enabled radio button to No in the Amazon RDS console or by setting the Enabled parameter to false using the CLI or Amazon RDS API. QUESTION NO: 234 A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 by mistake. The user is trying to create another subnet of CIDR 20.0.0.1/24. How can the user create the second subnet? A. There is no need to update the subnet as VPC automatically adjusts the CIDR of the first subnet based on the second subnet\u2019s CIDR B. The user can modify the first subnet CIDR from the console C. It is not possible to create a second subnet as one subnet with the same CIDR as the VPC has been created D. The user can modify the first subnet CIDR with AWS CLI Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside the subnet. The user can create a subnet with the same size of VPC. However, he cannot create any other subnet since the CIDR of the second subnet will conflict with the first subnet. The user cannot modify the CIDR of a subnet once it is created. Thus, in this case if required, the user has to delete the subnet and create new subnets. QUESTION NO: 235 A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group for the public subnet (WebSecGrp. and the private subnet (DBSecGrp.. Which of the below mentioned entries is required in the web server security group (WebSecGrp.? A. Configure Destination as DB Security group ID (DbSecGrp. for port 3306 Outbound B. 80 for Destination 0.0.0.0/0 Outbound C. Configure port 3306 for source 20.0.0.0/24 InBound D. Configure port 80 InBound for source 20.0.0.0/16 Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the public subnet can receive inbound traffic directly from the internet. Thus, the user should configure port 80 with source 0.0.0.0/0 in InBound. The user should configure that the instance in the public subnet can send traffic to the private subnet instances on the DB port. Thus, the user should configure the DB security group of the private subnet (DbSecGrp. as the destination for port 3306 in Outbound. QUESTION NO: 236 A user is trying to understand the detailed CloudWatch monitoring concept. Which of the below mentioned services provides detailed monitoring with CloudWatch without charging the user extra? A. AWS Auto Scaling B. AWS Route 53 C. AWS EMR D. AWS SNS Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Services, such as RDS, ELB, OpsWorks, and Route 53 can provide the monitoring data every minute without charging the user. QUESTION NO: 237 A user is trying to understand the CloudWatch metrics for the AWS services. It is required that the user should first understand the namespace for the AWS services. Which of the below mentioned is not a valid namespace for the AWS services? A. AWS/StorageGateway B. AWS/CloudTrail C. AWS/ElastiCache D. AWS/SWF Answer: B Explanation: Amazon CloudWatch is basically a metrics repository. The AWS product puts metrics into this repository, and the user can retrieve the data or statistics based on those metrics. To distinguish the data for each service, the CloudWatch metric has a namespace. Namespaces are containers for metrics. All AWS services that provide the Amazon CloudWatch data use a namespace string, beginning with \"AWS/\". All the services which are supported by CloudWatch will have some namespace. CloudWatch does not monitor CloudTrail. Thus, the namespace \u201cAWS/CloudTrail\u201d is incorrect. QUESTION NO: 238 A system admin is planning to encrypt all objects being uploaded to S3 from an application. The system admin does not want to implement his own encryption algorithm; instead he is planning to use server side encryption by supplying his own key (SSE-C.. Which parameter is not required while making a call for SSE-C? A. x-amz-server-side-encryption-customer-key-AES-256 B. x-amz-server-side-encryption-customer-key C. x-amz-server-side-encryption-customer-algorithm D. x-amz-server-side-encryption-customer-key-MD5 Answer: A Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. When the user is supplying his own encryption key, the user has to send the below mentioned parameters as a part of the API calls: x-amz-server-side-encryption-customer-algorithm: Specifies the encryption algorithm x-amzserver-side-encryption-customer-key: To provide the base64-encoded encryption key x-amzserver-side-encryption-customer-key-MD5: To provide the base64-encoded 128-bit MD5 digest of the encryption key QUESTION NO: 239 A user is using the AWS SQS to decouple the services. Which of the below mentioned operations is not supported by SQS? A. SendMessageBatch B. DeleteMessageBatch C. CreateQueue D. DeleteMessageQueue Answer: D Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. The user can perform the following set of operations using the Amazon SQS: CreateQueue, ListQueues, DeleteQueue, SendMessage, SendMessageBatch, ReceiveMessage, DeleteMessage, DeleteMessageBatch, ChangeMessageVisibility, ChangeMessageVisibilityBatch, SetQueueAttributes, GetQueueAttributes, GetQueueUrl, AddPermission and RemovePermission. Operations can be performed only by the AWS account owner or an AWS account that the account owner has delegated to. QUESTION NO: 240 A user has configured Auto Scaling with 3 instances. The user had created a new AMI after updating one of the instances. If the user wants to terminate two specific instances to ensure that Auto Scaling launches an instances with the new launch configuration, which command should he run? A. as-delete-instance-in-auto-scaling-group <Instance ID> --no-decrement-desired-capacity B. as-terminate-instance-in-auto-scaling-group <Instance ID> --update-desired-capacity C. as-terminate-instance-in-auto-scaling-group <Instance ID> --decrement-desired-capacity D. as-terminate-instance-in-auto-scaling-group <Instance ID> --no-decrement-desired-capacity Answer: D Explanation: The Auto Scaling command as-terminate-instance-in-auto-scaling-group <Instance ID> will terminate the specific instance ID. The user is required to specify the parameter as \u2013no-decrement-desiredcapacity to ensure that it launches a new instance from the launch config after terminating the instance. If the user specifies the parameter --decrement-desired-capacity then Auto Scaling will terminate the instance and decrease the desired capacity by 1. QUESTION NO: 241 A user has launched an EC2 instance from an instance store backed AMI. If the user restarts the instance, what will happen to the ephermal storage data? A. All the data will be erased but the ephermal storage will stay connected B. All data will be erased and the ephermal storage is released C. It is not possible to restart an instance launched from an instance store backed AMI D. The data is preserved Answer: D Explanation: A user can reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. However, it is recommended that the user use Amazon EC2 to reboot the instance instead of running the operating system reboot command from the instance. When an instance launched from an instance store backed AMI is rebooted all the ephermal storage data is still preserved. QUESTION NO: 242 A user has launched an EC2 instance. However, due to some reason the instance was terminated. If the user wants to find out the reason for termination, where can he find the details? A. It is not possible to find the details after the instance is terminated B. The user can get information from the AWS console, by checking the Instance description under the State transition reason label C. The user can get information from the AWS console, by checking the Instance description under the Instance Status Change reason label D. The user can get information from the AWS console, by checking the Instance description under the Instance Termination reason label Answer: D Explanation: An EC2 instance, once terminated, may be available in the AWS console for a while after termination. The user can find the details about the termination from the description tab under the label State transition reason. If the instance is still running, there will be no reason listed. If the user has explicitly stopped or terminated the instance, the reason will be \u201cUser initiated shutdown\u201d. QUESTION NO: 243 A user has created a VPC with CIDR 20.0.0.0/24. The user has used all the IPs of CIDR and wants to increase the size of the VPC. The user has two subnets: public (20.0.0.0/28. and private (20.0.1.0/28). How can the user change the size of the VPC? A. The user can delete all the instances of the subnet. Change the size of the subnets to 20.0.0.0/32 and 20.0.1.0/32, respectively. Then the user can increase the size of the VPC using CLI B. It is not possible to change the size of the VPC once it has been created C. The user can add a subnet with a higher range so that it will automatically increase the size of the VPC. D. The user can delete the subnets first and then modify the size of the VPC Answer: B Explanation: Once the user has created a VPC, he cannot change the CIDR of that VPC. The user has to terminate all the instances, delete the subnets and then delete the VPC. Create a new VPC with a higher size and launch instances with the newly created VPC and subnets. QUESTION NO: 244 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned security policies is supported by ELB? A. Dynamic Security Policy B. All the other options C. Predefined Security Policy D. Default Security Policy Answer: C Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. ELB supports two policies: Predefined Security Policy: which comes with predefined cipher and SSL protocols; Custom Security Policy: which allows the user to configure a policy. QUESTION NO: 245 A user has granted read/write permission of his S3 bucket using ACL. Which of the below mentioned options is a valid ID to grant permission to other AWS accounts (grantee. using ACL? A. IAM User ID B. S3 Secure ID C. Access ID D. Canonical user ID Answer: D Explanation: An S3 bucket ACL grantee can be an AWS account or one of the predefined Amazon S3 groups. The user can grant permission to an AWS account by the email address of that account or by the canonical user ID. If the user provides an email in the grant request, Amazon S3 finds the canonical user ID for that account and adds it to the ACL. The resulting ACL will always contain the canonical user ID for the AWS account, and not the AWS account's email address. QUESTION NO: 246 A user has configured an ELB to distribute the traffic among multiple instances. The user instances are facing some issues due to the back-end servers. Which of the below mentioned CloudWatch metrics helps the user understand the issue with the instances? A. HTTPCode_Backend_3XX B. HTTPCode_Backend_4XX C. HTTPCode_Backend_2XX D. HTTPCode_Backend_5XX Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. For ELB, CloudWatch provides various metrics including error code by ELB as well as by back-end servers (instances.. It gives data for the count of the number of HTTP response codes generated by the back-end instances. This metric does not include any response codes generated by the load balancer. These metrics are: The 2XX class status codes represents successful actions The 3XX class status code indicates that the user agent requires action The 4XX class status code represents client errors The 5XX class status code represents back-end server errors QUESTION NO: 247 A user has launched an EC2 instance store backed instance in the US-East-1a zone. The user created AMI #1 and copied it to the Europe region. After that, the user made a few updates to the application running in the US-East-1a zone. The user makes an AMI#2 after the changes. If the user launches a new instance in Europe from the AMI #1 copy, which of the below mentioned statements is true? A. The new instance will have the changes made after the AMI copy as AWS just copies the reference of the original AMI during the copying. Thus, the copied AMI will have all the updated data. B. The new instance will have the changes made after the AMI copy since AWS keeps updating the AMI. C. It is not possible to copy the instance store backed AMI from one region to another. D. The new instance in the EU region will not have the changes made after the AMI copy. Answer: D Explanation: Within EC2, when the user copies an AMI, the new AMI is fully independent of the source AMI; there is no link to the original (source. AMI. The user can modify the source AMI without affecting the new AMI and vice a versa. Therefore, in this case even if the source AMI is modified, the copied AMI of the EU region will not have the changes. Thus, after copy the user needs to copy the new source AMI to the destination region to get those changes. QUESTION NO: 248 A user runs the command \u201cdd if=/dev/zero of=/dev/xvdfbs=1M\u201d on a fresh blank EBS volume attached to a Linux instance. Which of the below mentioned activities is the user performing with the command given above? A. Creating a file system on the EBS volume B. Mounting the device to the instance C. Pre warming the EBS volume D. Formatting the EBS volume Answer: C Explanation: When the user creates a new EBS volume and is trying to access it for the first time it will encounter reduced IOPS due to wiping or initiating of the block storage. To avoid this as well as achieve the best performance it is required to pre warm the EBS volume. For a blank volume attached with a Linux OS, the \u201cdd\u201d command is used to write to all the blocks on the device. In the command \u201cdd if=/dev/zero of=/dev/xvdfbs=1M\u201d the parameter \u201cif =import file\u201d should be set to one of the Linux virtual devices, such as /dev/zero. The \u201cof=output file\u201d parameter should be set to the drive that the user wishes to warm. The \u201cbs\u201d parameter sets the block size of the write operation; for optimal performance, this should be set to 1 MB. QUESTION NO: 249 A user has created an Auto Scaling group using CLI. The user wants to enable CloudWatch detailed monitoring for that group. How can the user configure this? A. When the user sets an alarm on the Auto Scaling group, it automatically enables detail monitoring B. By default detailed monitoring is enabled for Auto Scaling C. Auto Scaling does not support detailed monitoring D. Enable detail monitoring from the AWS console Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates an Auto Scaling launch config as the first step for creating an Auto Scaling group, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. Thus, the user does not need to set this flag if he wants detailed monitoring. QUESTION NO: 250 A user has created a VPC with a public subnet. The user has terminated all the instances which are part of the subnet. Which of the below mentioned statements is true with respect to this scenario? A. The user cannot delete the VPC since the subnet is not deleted B. All network interface attached with the instances will be deleted C. When the user launches a new instance it cannot use the same subnet D. The subnet to which the instances were launched with will be deleted Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When an instance is launched it will have a network interface attached with it. The user cannot delete the subnet until he terminates the instance and deletes the network interface. When the user terminates the instance all the network interfaces attached with it are also deleted. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 251 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. The ELB security policy supports various ciphers. Which of the below mentioned options helps identify the matching cipher at the client side to the ELB cipher list when client is requesting ELB DNS over SSL? A. Cipher Protocol B. Client Configuration Preference C. Server Order Preference D. Load Balancer Preference Answer: C Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. When client is requesting ELB DNS over SSL and if the load balancer is configured to support the Server Order Preference, then the load balancer gets to select the first cipher in its list that matches any one of the ciphers in the client's list. Server Order Preference ensures that the load balancer determines which cipher is used for the SSL connection. QUESTION NO: 252 A user has created a VPC with public and private subnets. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.1.0/24 and the public subnet uses CIDR 20.0.0.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group of the NAT instance. Which of the below mentioned entries is not required for the NAT security group? A. For Inbound allow Source: 20.0.1.0/24 on port 80 B. For Outbound allow Destination: 0.0.0.0/0 on port 80 C. For Inbound allow Source: 20.0.0.0/24 on port 80 D. For Outbound allow Destination: 0.0.0.0/0 on port 443 Answer: C Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the private subnet can connect to the internet using the NAT instances. The user should first configure that NAT can receive traffic on ports 80 and 443 from the private subnet. Thus, allow ports 80 and 443 in Inbound for the private subnet 20.0.1.0/24. Now to route this traffic to the internet configure ports 80 and 443 in Outbound with destination 0.0.0.0/0. The NAT should not have an entry for the public subnet CIDR. QUESTION NO: 253 A user has created an application which will be hosted on EC2. The application makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK to connect with from the EC2 instance. Which of the below mentioned statements is true with respect to the best practice for security in this scenario? A. The user should attach an IAM role with DynamoDB access to the EC2 instance B. The user should create an IAM user with DynamoDB access and use its credentials within the application to connect with DynamoDB C. The user should create an IAM role, which has EC2 access so that it will allow deploying the application D. The user should create an IAM user with DynamoDB and EC2 access. Attach the user with the application so that it does not use the root account credentials Answer: A Explanation: With AWS IAM a user is creating an application which runs on an EC2 instance and makes requests to AWS, such as DynamoDB or S3 calls. Here it is recommended that the user should not create an IAM user and pass the user's credentials to the application or embed those credentials inside the application. Instead, the user should use roles for EC2 and give that role access to DynamoDB /S3. When the roles are attached to EC2, it will give temporary security credentials to the application hosted on that EC2, to connect with DynamoDB / S3. QUESTION NO: 254 An organization (Account ID 123412341234. has attached the below mentioned IAM policy to a user. What does this policy statement entitle the user to perform? { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*LoginProfile\", \"iam:*AccessKey*\", \"iam:*SigningCertificate*\" ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] }] } A. The policy allows the IAM user to modify all IAM user\u2019s credentials using the console, SDK, CLI or APIs B. The policy will give an invalid resource error C. The policy allows the IAM user to modify all credentials using only the console D. The policy allows the user to modify all IAM user\u2019s password, sign in certificates and access keys using only CLI, SDK or APIs Answer: D Explanation: WS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (Account ID 123412341234. wants some of their users to manage credentials (access keys, password, and sing in certificates. of all IAM users, they should set an applicable policy to that user or group of users. The below mentioned policy allows the IAM user to modify the credentials of all IAM user\u2019s using only CLI, SDK or APIs. The user cannot use the AWS console for this activity since he does not have list permission for the IAM users. { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\" \"Action\": [ \"iam:*LoginProfile\", \"iam:*AccessKey*\", \"iam:*SigningCertificate*\" ], \"Resource\": [\"arn:aws:iam::123412341234:user/${aws:username}\"] }] } QUESTION NO: 255 A sys admin is trying to understand the sticky session algorithm. Please select the correct sequence of steps, both when the cookie is present and when it is not, to help the admin understand the implementation of the sticky session: ELB inserts the cookie in the response ELB chooses the instance based on the load balancing algorithm Check the cookie in the service request The cookie is found in the request The cookie is not found in the request A. 3,1,4,2 [Cookie is not Present] & 3,1,5,2 [Cookie is Present] B. 3,4,1,2 [Cookie is not Present] & 3,5,1,2 [Cookie is Present] C. 3,5,2,1 [Cookie is not Present] & 3,4,2,1 [Cookie is Present] D. 3,2,5,4 [Cookie is not Present] & 3,2,4,5 [Cookie is Present] Answer: C Explanation: Generally AWS ELB routes each request to a zone with the minimum load. The Elastic Load Balancer provides a feature called sticky session which binds the user\u2019s session with a specific EC2 instance. The load balancer uses a special load-balancer-generated cookie to track the application instance for each request. When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the request is sent to the application instance specified in the cookie. If there is no cookie, the load balancer chooses an application instance based on the existing load balancing algorithm. A cookie is inserted into the response for binding subsequent requests from the same user to that application instance. QUESTION NO: 256 A user has a weighing plant. The user measures the weight of some goods every 5 minutes and sends data to AWS CloudWatch for monitoring and tracking. Which of the below mentioned parameters is mandatory for the user to include in the request list? A. Value B. Namespace C. Metric Name D. Timezone Answer: B Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish the data to CloudWatch as single data points or as an aggregated set of data points called a statistic set. The user has to always include the namespace as part of the request. The user can supply a file instead of the metric name. If the user does not supply the timezone, it accepts the current time. If the user is sending the data as a single data point it will have parameters, such as value. However, if the user is sending as an aggregate it will have parameters, such as statistic-values. QUESTION NO: 257 An organization has configured Auto Scaling for hosting their application. The system admin wants to understand the Auto Scaling health check process. If the instance is unhealthy, Auto Scaling launches an instance and terminates the unhealthy instance. What is the order execution? A. Auto Scaling launches a new instance first and then terminates the unhealthy instance B. Auto Scaling performs the launch and terminate processes in a random order C. Auto Scaling launches and terminates the instances simultaneously D. Auto Scaling terminates the instance first and then launches a new instance Answer: D Explanation: Auto Scaling keeps checking the health of the instances at regular intervals and marks the instance for replacement when it is unhealthy. The ReplaceUnhealthy process terminates instances which are marked as unhealthy and subsequently creates new instances to replace them. This process first terminates the instance and then launches a new instance. QUESTION NO: 258 A user is trying to connect to a running EC2 instance using SSH. However, the user gets an Unprotected Private Key File error. Which of the below mentioned options can be a possible reason for rejection? A. The private key file has the wrong file permission B. The ppk file used for SSH is read only C. The public key file has the wrong permission D. The user has provided the wrong user name for the OS login Answer: A Explanation: While doing SSH to an EC2 instance, if you get an Unprotected Private Key File error it means that the private key file's permissions on your computer are too open. Ideally the private key should have the Unix permission of 0400. To fix that, run the command: # chmod 0400 /path/to/private.key QUESTION NO: 259 A user has provisioned 2000 IOPS to the EBS volume. The application hosted on that EBS is experiencing less IOPS than provisioned. Which of the below mentioned options does not affect the IOPS of the volume? A. The application does not have enough IO for the volume B. The instance is EBS optimized C. The EC2 instance has 10 Gigabit Network connectivity D. The volume size is too large Answer: D Explanation: When the application does not experience the expected IOPS or throughput of the PIOPS EBS volume that was provisioned, the possible root cause could be that the EC2 bandwidth is the limiting factor and the instance might not be either EBS-optimized or might not have 10 Gigabit network connectivity. Another possible cause for not experiencing the expected IOPS could also be that the user is not driving enough I/O to the EBS volumes. The size of the volume may not affect IOPS. QUESTION NO: 260 A storage admin wants to encrypt all the objects stored in S3 using server side encryption. The user does not want to use the AES 256 encryption key provided by S3. How can the user achieve this? A. The admin should upload his secret key to the AWS console and let S3 decrypt the objects B. The admin should use CLI or API to upload the encryption key to the S3 bucket. When making a call to the S3 API mention the encryption key URL in each request C. S3 does not support client supplied encryption keys for server side encryption D. The admin should send the keys and encryption algorithm with each API call Answer: D Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API callto supply his own encryption key. Amazon S3 never stores the user\u2019s encryption key. The user has to supply it for each encryption or decryption call. QUESTION NO: 261 A user is trying to create a PIOPS EBS volume with 8 GB size and 200 IOPS. Will AWS create the volume? A. Yes, since the ratio between EBS and IOPS is less than 30 B. No, since the PIOPS and EBS size ratio is less than 30 C. No, the EBS size is less than 10 GB D. Yes, since PIOPS is higher than 100 Answer: C Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30; for example, a volume with 3000 IOPS must be at least 100 GB. QUESTION NO: 262 A user has scheduled the maintenance window of an RDS DB on Monday at 3 AM. Which of the below mentioned events may force to take the DB instance offline during the maintenance window? A. Enabling Read Replica B. Making the DB Multi AZ C. DB password change D. Security patching Answer: D Explanation: Amazon RDS performs maintenance on the DB instance during a user-definable maintenance window. The system may be offline or experience lower performance during that window. The only maintenance events that may require RDS to make the DB instance offline are: Scaling compute operations Software patching. Required software patching is automatically scheduled only for patches that are security and durability related. Such patching occurs infrequently (typically once every few months. and seldom requires more than a fraction of the maintenance window. QUESTION NO: 263 An organization has launched 5 instances: 2 for production and 3 for testing. The organization wants that one particular group of IAM users should only access the test instances and not the production ones. How can the organization set that as a part of the policy? A. Launch the test and production instances in separate regions and allow region wise access to the group B. Define the IAM policy which allows access based on the instance ID C. Create an IAM policy with a condition which allows access to only small instances D. Define the tags on the test and production servers and add a condition to the IAM policy which allows access to specific tags Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user can add conditions as a part of the IAM policies. The condition can be set on AWS Tags, Time, and Client IP as well as on various parameters. If the organization wants the user to access only specific instances he should define proper tags and add to the IAM policy condition. The sample policy is shown below. \"Statement\": [ { \"Action\": \"ec2:*\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/InstanceType\": \"Production\" } } } ] QUESTION NO: 264 A user has configured Auto Scaling with the minimum capacity as 2 and the desired capacity as 2. The user is trying to terminate one of the existing instance with the command: as-terminate-instance-in-auto-scaling-group<Instance ID> --decrement-desired-capacity What will Auto Scaling do in this scenario? A. Terminates the instance and does not launch a new instance B. Terminates the instance and updates the desired capacity to 1 C. Terminates the instance and updates the desired capacity and minimum size to 1 D. Throws an error Answer: D Explanation: The Auto Scaling command as-terminate-instance-in-auto-scaling-group <Instance ID> will terminate the specific instance ID. The user is required to specify the parameter as --decrement-desiredcapacity. Then Auto Scaling will terminate the instance and decrease the desired capacity by 1. In this case since the minimum size is 2, Auto Scaling will not allow the desired capacity to go below 2. Thus, it will throw an error. QUESTION NO: 265 A user is collecting 1000 records per second. The user wants to send the data to CloudWatch using the custom namespace. Which of the below mentioned options is recommended for this activity? A. Aggregate the data with statistics, such as Min, max, Average, Sum and Sample data and send the data to CloudWatch B. Send all the data values to CloudWatch in a single command by separating them with a comma. CloudWatch will parse automatically C. Create one csv file of all the data and send a single file to CloudWatch D. It is not possible to send all the data in one call. Thus, it should be sent one by one. CloudWatch will aggregate the data automatically Answer: A Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish data to CloudWatch as single data points or as an aggregated set of data points called a statistic set using the command put-metric-data. It is recommended that when the user is having multiple data points per minute, he should aggregate the data so that it will minimize the number of calls to put-metric-data. In this case it will be single call to CloudWatch instead of 1000 calls if the data is aggregated. QUESTION NO: 266 A user is trying to create an EBS volume with the highest PIOPS supported by EBS. What is the minimum size of EBS required to have the maximum IOPS? A. 124 B. 150 C. 134 D. 128 Answer: C Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30. QUESTION NO: 267 An organization is trying to create various IAM users. Which of the below mentioned options is not a valid IAM username? A. John.cloud B. john@cloud C. John=cloud D. john#cloud Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Whenever the organization is creating an IAM user, there should be a unique ID for each user. The names of users, groups, roles, instance profiles must be alphanumeric, including the following common characters: plus (+., equal (=., comma (,., period (.., at (@., and dash (-.. QUESTION NO: 268 A user is having data generated randomly based on a certain event. The user wants to upload that data to CloudWatch. It may happen that event may not have data generated for some period due to andomness. Which of the below mentioned options is a recommended option for this case? A. For the period when there is no data, the user should not send the data at all B. For the period when there is no data the user should send a blank value C. For the period when there is no data the user should send the value as 0 D. The user must upload the data to CloudWatch as having no data for some period will cause an error at CloudWatch monitoring Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. When the user data is more random and not generated at regular intervals, there can be a period which has no associated data. The user can either publish the zero (0. Value for that period or not publish the data at all. It is recommended that the user should publish zero instead of no value to monitor the health of the application. This is helpful in an alarm as well as in the generation of the sample data count. QUESTION NO: 269 A user is sending the data to CloudWatch using the CloudWatch API. The user is sending data 90 minutes in the future. What will CloudWatch do in this case? A. CloudWatch will accept the data B. It is not possible to send data of the future C. It is not possible to send the data manually to CloudWatch D. The user cannot send data for more than 60 minutes in the future Answer: A Explanation: With Amazon CloudWatch, each metric data point must be marked with a time stamp. The user can send the data using CLI but the time has to be in the UTC format. If the user does not provide the time, CloudWatch will take the data received time in the UTC timezone. The time stamp sent by the user can be up to two weeks in the past and up to two hours into the future. QUESTION NO: 270 A user wants to upload a complete folder to AWS S3 using the S3 Management console. How can the user perform this activity? A. Just drag and drop the folder using the flash tool provided by S3 B. Use the Enable Enhanced Folder option from the S3 console while uploading objects C. The user cannot upload the whole folder in one go with the S3 management console D. Use the Enable Enhanced Uploader option from the S3 console while uploading objects Answer: D Explanation: AWS S3 provides a console to upload objects to a bucket. The user can use the file upload screen to upload the whole folder in one go by clicking on the Enable Enhanced Uploader option. When the user uploads afolder, Amazon S3 uploads all the files and subfolders from the specified folder to the user\u2019s bucket. It then assigns a key value that is a combination of the uploaded file name and the folder name. QUESTION NO: 271 Which of the below mentioned AWS RDS logs cannot be viewed from the console for MySQL? A. Error Log B. Slow Query Log C. Transaction Log D. General Log Answer: C Explanation: The user can view, download, and watch the database logs using the Amazon RDS console, the Command Line Interface (CLI., or the Amazon RDS API. For the MySQL RDS, the user can view the error log, slow querylog, and general logs. RDS does not support viewing the transaction logs. QUESTION NO: 272 A user has launched an EBS backed EC2 instance in the US-East-1a region. The user stopped the instance and started it back after 20 days. AWS throws up an \u2018InsufficientInstanceCapacity\u2019 error. What can be the possible reason for this? A. AWS does not have sufficient capacity in that availability zone B. AWS zone mapping is changed for that user account C. There is some issue with the host capacity on which the instance is launched D. The user account has reached the maximum EC2 instance limit Answer: A Explanation: When the user gets an \u2018InsufficientInstanceCapacity\u2019 error while launching or starting an EC2 instance, it means that AWS does not currently have enough available capacity to service the user request. If the user is requesting a large number of instances, there might not be enough server capacity to host them. The user can either try again later, by specifying a smaller number of instances or changing the availability zone if launching a fresh instance. QUESTION NO: 273 A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is true in this scenario? A. The AWS VPC will automatically create a NAT instance with the micro size B. VPC bounds the main route table with a private subnet and a custom route table with a public subnet C. The user has to manually create a NAT instance D. VPC bounds the main route table with a public subnet and a custom route table with a private subnet Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance of a smaller or higher size, respectively. The VPC has an implied router and the VPC wizard updates the main route table used with the private subnet, creates a custom route table and associates it with the public subnet. QUESTION NO: 274 The CFO of a company wants to allow one of his employees to view only the AWS usage report page. Which of the below mentioned IAM policy statements allows the user to have access to the AWS usage report page? A. \"Effect\": \"Allow\", \"Action\": [\u201cDescribe\u201d], \"Resource\": \"Billing\" B. \"Effect\": \"Allow\", \"Action\": [\"AccountUsage], \"Resource\": \"*\" C. \"Effect\": \"Allow\", \"Action\": [\"aws-portal:ViewUsage\"], \"Resource\": \"*\" D. \"Effect\": \"Allow\", \"Action\": [\"aws-portal: ViewBilling\"], \"Resource\": \"*\" Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the CFO wants to allow only AWS usage report page access, the policy for that IAM user will be as given below: { \"Version\": \"2012-10-17\", \"Statement\": [ 168 { \"Effect\": \"Allow\", \"Action\": [ \"aws-portal:ViewUsage\" ], \"Resource\": \"*\" } ] } QUESTION NO: 275 An organization has created 10 IAM users. The organization wants each of the IAM users to have access to a separate DyanmoDB table. All the users are added to the same group and the organization wants to setup a group level policy for this. How can the organization achieve this? A. Define the group policy and add a condition which allows the access based on the IAM name B. Create a DynamoDB table with the same name as the IAM user name and define the policy rule which grants access based on the DynamoDB ARN using a variable C. Create a separate DynamoDB database for each user and configure a policy in the group based on the DB variable. D. It is not possible to have a group level policy which allows different IAM users to different DynamoDB Tables. Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. AWS DynamoDB has only tables and the organization cannot makeseparate databases. The organization should create a table with the same name as the IAM user name and use the ARN of DynamoDB as part of the group policy. The sample policy is shown below: { \"Version\": \"2012-10-17\", \"Statement\": [{ 169 \"Effect\": \"Allow\", \"Action\": [\"dynamodb:*\"], \"Resource\": \"arn:aws:dynamodb:region:account-number-without-hyphens:table/ ${aws:username}\" } ] } (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 276 A user has configured an HTTPS listener on an ELB. The user has not configured any security policy which can help to negotiate SSL between the client and ELB. What will ELB do in this scenario? A. By default ELB will select the first version of the security policy B. By default ELB will select the latest version of the policy C. ELB creation will fail without a security policy D. It is not required to have a security policy since SSL is already installed Answer: B Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. If the user has created an HTTPS/SSL listener without associating any security policy, Elastic Load Balancing will, bydefault, associate the latest version of the ELBSecurityPolicyYYYY-MM with the load balancer. QUESTION NO: 277 A user is creating a Cloudformation stack. Which of the below mentioned limitations does not hold true for Cloudformation? A. One account by default is limited to 100 templates B. The user can use 60 parameters and 60 outputs in a single template C. The template, parameter, output, and resource description fields are limited to 4096 characters D. One account by default is limited to 20 stacks Answer: A Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The limitations given below apply to the Cloudformation template and stack. There are no limits to the number of templates but each AWS CloudFormation account is limited to a maximum of 20 stacks by default. The Template, Parameter, Output, and Resource description fields are limited to 4096 characters. The user can include up to 60 parameters and 60 outputs in a template. QUESTION NO: 278 A user has two EC2 instances running in two separate regions. The user is running an internal memory management tool, which captures the data and sends it to CloudWatch in US East, using a CLI with the same namespace and metric. Which of the below mentioned options is true with respect to the above statement? A. The setup will not work as CloudWatch cannot receive data across regions B. CloudWatch will receive and aggregate the data based on the namespace and metric C. CloudWatch will give an error since the data will conflict due to two sources D. CloudWatch will take the data of the server, which sends the data first Answer: B Explanation: Amazon CloudWatch does not differentiate the source of a metric when receiving custom data. If the user is publishing a metric with the same namespace and dimensions from different sources, CloudWatch will treat them as a single metric. If the data is coming with the same timezone within a minute, CloudWatch will aggregate the data. It treats these as a single metric, allowing the user to get the statistics, such as minimum, maximum, average, and the sum of all across all servers. QUESTION NO: 279 An organization has created a Queue named \u201cmodularqueue\u201d with SQS. The organization is not performing any operations such as SendMessage, ReceiveMessage, DeleteMessage, GetQueueAttributes, SetQueueAttributes, AddPermission, and RemovePermission on the queue. What can happen in this scenario? A. AWS SQS sends notification after 15 days for inactivity on queue B. AWS SQS can delete queue after 30 days without notification C. AWS SQS marks queue inactive after 30 days D. AWS SQS notifies the user after 2 weeks and deletes the queue after 3 weeks. Answer: B Explanation: Amazon SQS can delete a queue without notification if one of the following actions hasn't been performed on it for 30 consecutive days: SendMessage, ReceiveMessage, DeleteMessage, GetQueueAttributes, SetQueueAttributes, AddPermission, and RemovePermission. QUESTION NO: 280 An organization has setup Auto Scaling with ELB. Due to some manual error, one of the instances got rebooted. Thus, it failed the Auto Scaling health check. Auto Scaling has marked it for replacement. How can the system admin ensure that the instance does not get terminated? A. Update the Auto Scaling group to ignore the instance reboot event B. It is not possible to change the status once it is marked for replacement C. Manually add that instance to the Auto Scaling group after reboot to avoid replacement D. Change the health of the instance to healthy using the Auto Scaling commands Answer: D Explanation: After an instance has been marked unhealthy by Auto Scaling, as a result of an Amazon EC2 or ELB health check, it is almost immediately scheduled for replacement as it will never automatically recover its health. If the user knows that the instance is healthy then he can manually call the SetInstanceHealth action (or the as-setinstance- health command from CLI. to set the instance's health status back to healthy. Auto Scaling will throw an error if the instance is already terminating or else it will mark it healthy. QUESTION NO: 281 A system admin wants to add more zones to the existing ELB. The system admin wants to perform this activity from CLI. Which of the below mentioned command helps the system admin to add new zones to the existing ELB? A. elb-enable-zones-for-lb B. elb-add-zones-for-lb C. It is not possible to add more zones to the existing ELB D. elb-configure-zones-for-lb Answer: A Explanation: The user has created an Elastic Load Balancer with the availability zone and wants to add more zones to the existing ELB. The user can do so in two ways: From the console or CLI, add new zones to ELB; QUESTION NO: 282 An organization is planning to create a user with IAM. They are trying to understand the limitations of IAM so that they can plan accordingly. Which of the below mentioned statements is not true with respect to the limitations of IAM? A. One IAM user can be a part of a maximum of 5 groups B. The organization can create 100 groups per AWS account C. One AWS account can have a maximum of 5000 IAM users D. One AWS account can have 250 roles Answer: A Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The default maximums for each of the IAM entities is given below: Groups per AWS account: 100 Users per AWS account: 5000 Roles per AWS account: 250 Number of groups per user: 10 (that is, one user can be part of these many groups. QUESTION NO: 283 A user is planning to scale up an application by 8 AM and scale down by 7 PM daily using Auto Scaling. What should the user do in this case? A. Setup the scaling policy to scale up and down based on the CloudWatch alarms B. The user should increase the desired capacity at 8 AM and decrease it by 7 PM manually C. The user should setup a batch process which launches the EC2 instance at a specific time D. Setup scheduled actions to scale up or down at a specific time Answer: A Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. To configure the Auto Scaling group to scale based on a schedule, the user needs to create scheduled actions. A scheduled action tells Auto Scaling to perform a scaling action at a certain time in the future. QUESTION NO: 284 A user has created a VPC with two subnets: one public and one private. The user is planning to run the patch update for the instances in the private subnet. How can the instances in the private subnet connect to theinternet? A. Use the internet gateway with a private IP B. Allow outbound traffic in the security group for port 80 to allow internet updates C. The private subnet can never connect to the internet D. Use NAT with an elastic IP Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created two subnets (one private and one public., he would need a Network Address Translation (NAT. instance with the elastic IP address. This enables the instances in the private subnet to send requests to the internet (for example, to perform software updates.. QUESTION NO: 285 A user has configured an EC2 instance in the US-East-1a zone. The user has enabled detailed monitoring of the instance. The user is trying to get the data from CloudWatch using a CLI. Which of the below mentioned CloudWatch endpoint URLs should the user use? A. monitoring.us-east-1.amazonaws.com B. monitoring.us-east-1-a.amazonaws.com C. monitoring.us-east-1a.amazonaws.com D. cloudwatch.us-east-1a.amazonaws.com Answer: A Explanation: The CloudWatch resources are always region specific and they will have the end point as region specific. If the user is trying to access the metric in the US-East-1 region, the endpoint URL will be: monitoring.us-east- 1.amazonaws.com QUESTION NO: 286 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling AddToLoadBalancer (which adds instances to the load balancer. process for a while. What will happen to the instances launched during the suspension period? A. The instances will not be registered with ELB and the user has to manually register when the process is resumed B. The instances will be registered with ELB only once the process has resumed C. Auto Scaling will not launch the instance during this period due to process suspension D. It is not possible to suspend only the AddToLoadBalancer process Answer: A Explanation: Auto Scaling performs various processes, such as Launch, Terminate, add to Load Balancer etc. The user can also suspend the individual process. The AddToLoadBalancer process type adds instances to the load balancer when the instances are launched. If this process is suspended, Auto Scaling will launch the instances but will not add them to the load balancer. When the user resumes this process, Auto Scaling will resume adding new instances launched after resumption to the load balancer. However, it will not add running instances that were launched while the process was suspended; those instances must be added manually. QUESTION NO: 287 A sys admin has enabled a log on ELB. Which of the below mentioned activities are not captured by the log? A. Response processing time B. Front end processing time C. Backend processing time D. Request processing time Answer: B Explanation: Elastic Load Balancing access logs capture detailed information for all the requests made to the load balancer. Each request will have details, such as client IP, request path, ELB IP, time, and latencies. The time will have information, such as Request Processing time, Backend Processing time and Response Processing time. QUESTION NO: 288 A user has moved an object to Glacier using the life cycle rules. The user requests to restore the archive after 6 months. When the restore request is completed the user accesses that archive. Which of the below mentioned statements is not true in this condition? A. The archive will be available as an object for the duration specified by the user during the restoration request B. The restored object\u2019s storage class will be RRS C. The user can modify the restoration period only by issuing a new restore request with the updated period D. The user needs to pay storage for both RRS (restored) and Glacier (Archive) Rates. Answer: B Explanation: AWS Glacier is an archival service offered by AWS. AWS S3 provides lifecycle rules to archive and restore objects from S3 to Glacier. Once the object is archived their storage class will change to Glacier. If the user sends a request for restore, the storage class will still be Glacier for the restored object. The user will be paying for both the archived copy as well as for the restored object. The object is available only for the duration specified in the restore request and if the user wants to modify that period, he has to raise another restore request with the updated duration. QUESTION NO: 289 A user is running a batch process on EBS backed EC2 instances. The batch process starts a few instances to process hadoop Map reduce jobs which can run between 50 \u2013 600 minutes or sometimes for more time. The user wants to configure that the instance gets terminated only when the process is completed. How can the user configure this with CloudWatch? A. Setup the CloudWatch action to terminate the instance when the CPU utilization is less than 5% B. Setup the CloudWatch with Auto Scaling to terminate all the instances C. Setup a job which terminates all instances after 600 minutes D. It is not possible to terminate instances automatically Answer: D Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup an action which terminates the instances when their CPU utilization is below a certain threshold for a certain period of time. The EC2 action can either terminate or stop the instance as part of the EC2 action. QUESTION NO: 290 A user has enabled versioning on an S3 bucket. The user is using server side encryption for data at rest. If the user is supplying his own keys for encryption (SSE-C., what is recommended to the user for the purpose of security? A. The user should not use his own security key as it is not secure B. Configure S3 to rotate the user\u2019s encryption key at regular intervals C. Configure S3 to store the user\u2019s keys securely with SSL D. Keep rotating the encryption key manually at the client side Answer: D Explanation: AWS S3 supports client side or server side encryption to encrypt all data at Rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. Since S3 does not store the encryption keys in SSE-C, it is recommended that the user should manage keys securely and keep rotating them regularly at the client side version. QUESTION NO: 291 A user runs the command \u201cdd if=/dev/xvdf of=/dev/null bs=1M\u201d on an EBS volume created from a snapshot and attached to a Linux instance. Which of the below mentioned activities is the user performing with the step given above? A. Pre warming the EBS volume B. Initiating the device to mount on the EBS volume C. Formatting the volume D. Copying the data from a snapshot to the device Answer: A Explanation: When the user creates an EBS volume and is trying to access it for the first time it will encounter reduced IOPS due to wiping or initiating of the block storage. To avoid this as well as achieve the best performance it is required to pre warm the EBS volume. For a volume created from a snapshot and attached with a Linux OS, the \u201cdd\u201d command pre warms the existing data on EBS and any restored snapshots of volumes that have been previously fully pre warmed. This command maintains incremental snapshots; however, because this operation is read-only, it does not pre warm unused space that has never been written to on the original volume. In the command \u201cdd if=/dev/xvdf of=/dev/null bs=1M\u201d , the parameter \u201cif=input file\u201d should be set to the drive that the user wishes to warm. The \u201cof=output file\u201d parameter should be set to the Linux null virtual device, /dev/null. The \u201cbs\u201d parameter sets the block size of the read operation; for optimal performance, this should be set to 1 MB. QUESTION NO: 292 A user has launched an EC2 Windows instance from an instance store backed AMI. The user wants to convert the AMI to an EBS backed AMI. How can the user convert it? A. Attach an EBS volume to the instance and unbundle all the AMI bundled data inside the EBS B. A Windows based instance store backed AMI cannot be converted to an EBS backed AMI C. It is not possible to convert an instance store backed AMI to an EBS backed AMI D. Attach an EBS volume and use the copy command to copy all the ephermal content to the EBS Volume Answer: B Explanation: Generally when a user has launched an EC2 instance from an instance store backed AMI, it can be converted to an EBS backed AMI provided the user has attached the EBS volume to the instance and unbundles the AMI data to it. However, if the instance is a Windows instance, AWS does not allow this. In this case, since the instance is a Windows instance, the user cannot convert it to an EBS backed AMI. QUESTION NO: 293 A user has created a VPC with public and private subnets using the VPC Wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24. Which of the below mentioned entries are required in the main route table to allow the instances in VPC to communicate with each other? A. Destination : 20.0.0.0/24 and Target : VPC B. Destination : 20.0.0.0/16 and Target : ALL C. Destination : 20.0.0.0/0 and Target : ALL D. Destination : 20.0.0.0/16 and Target : Local Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create two route tables and attach to the subnets. The main route table will have the entry \u201cDestination: 20.0.0.0/24 and Target: Local\u201d, which allows all instances in the VPC to communicate with each other. QUESTION NO: 294 A sysadmin has created the below mentioned policy on an S3 bucket named cloudacademy. The bucket has both AWS.jpg and index.html objects. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg] }] A. It will make all the objects as well as the bucket public B. It will throw an error for the wrong action and does not allow to save the policy C. It will make the AWS.jpg object as public D. It will make the AWS.jpg as well as the cloudacademy bucket as public Answer: B Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. In the below policy the action says \u201cS3:ListBucket\u201d for effect Allow and when there is no bucket name mentioned as a part of the resource, it will throw an error and not save the policy. \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg] }] QUESTION NO: 295 A user has launched an EC2 instance and deployed a production application in it. The user wants to prohibit any mistakes from the production team to avoid accidental termination. How can the user achieve this? A. The user can the set DisableApiTermination attribute to avoid accidental termination B. It is not possible to avoid accidental termination C. The user can set the Deletion termination flag to avoid accidental termination D. The user can set the InstanceInitiatedShutdownBehavior flag to avoid accidental termination Answer: A Explanation: It is always possible that someone can terminate an EC2 instance using the Amazon EC2 console, command line interface or API by mistake. If the admin wants to prevent the instance from being accidentally terminated, he can enable termination protection for that instance. The DisableApiTermination attribute controls whether the instance can be terminated using the console, CLI or API. By default, termination protection is disabled for an EC2 instance. When it is set it will not allow the user to terminate the instance from CLI, API or the console. QUESTION NO: 296 A user has created a launch configuration for Auto Scaling where CloudWatch detailed monitoring is disabled. The user wants to now enable detailed monitoring. How can the user achieve this? A. Update the Launch config with CLI to set InstanceMonitoringDisabled = false B. The user should change the Auto Scaling group from the AWS console to enable detailed monitoring C. Update the Launch config with CLI to set InstanceMonitoring.Enabled = true D. Create a new Launch Config with detail monitoring enabled and update the Auto Scaling group Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates the AutoScaling launch config as the first step for creating an Auto Scaling group, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. When the user has created a launch configuration with InstanceMonitoring.Enabled = false it will involve multiple steps to enable detail monitoring. The steps are: Create a new Launch config with detailed monitoring enabled Update the Auto Scaling group with a new launch config Enable detail monitoring on each EC2 instance QUESTION NO: 297 A user is trying to pre-warm a blank EBS volume attached to a Linux instance. Which of the below mentioned steps should be performed by the user? A. There is no need to pre-warm an EBS volume B. Contact AWS support to pre-warm C. Unmount the volume before pre-warming D. Format the device Answer: C Explanation: When the user creates a new EBS volume or restores a volume from the snapshot, the backend storage blocks are immediately allocated to the user EBS. However, the first time when the user is trying to access a block of the storage, it is recommended to either be wiped from the new volumes or instantiated from the snapshot (for restored volumes. before the user can access the block. This preliminary action takes time and can cause a 5 to 50 percent loss of IOPS for the volume when the block is accessed for the first time. To avoid this it is required to pre warm the volume. Prewarming an EBS volume on a Linux instance requires that the user should unmount the blank device first and then write all the blocks on the device using a command, such as \u201cdd\u201d. QUESTION NO: 298 A user has launched an EC2 instance from an instance store backed AMI. The user has attached an additional instance store volume to the instance. The user wants to create an AMI from the running instance. Will the AMI have the additional instance store volume data? A. Yes, the block device mapping will have information about the additional instance store volume B. No, since the instance store backed AMI can have only the root volume bundled C. It is not possible to attach an additional instance store volume to the existing instance store backed AMI instance D. No, since this is ephermal storage it will not be a part of the AMI Answer: A Explanation: When the user has launched an EC2 instance from an instance store backed AMI and added an instance store volume to the instance in addition to the root device volume, the block device mapping for the new AMI contains the information for these volumes as well. In addition, the block device mappings for the instances those are launched from the new AMI will automatically contain information for these volumes. QUESTION NO: 299 A user has created an EBS volume of 10 GB and attached it to a running instance. The user is trying to access EBS for first time. Which of the below mentioned options is the correct statement with respect to a first time EBS access? A. The volume will show a size of 8 GB B. The volume will show a loss of the IOPS performance the first time C. The volume will be blank D. If the EBS is mounted it will ask the user to create a file system Answer: B Explanation: A user can create an EBS volume either from a snapshot or as a blank volume. If the volume is from a snapshot it will not be blank. The volume shows the right size only as long as it is mounted. This shows that the file system is created. When the user is accessing the volume the AWS EBS will wipe out the block storage or instantiate from the snapshot. Thus, the volume will show a loss of IOPS. It is recommended that the user should pre warm the EBS before use to achieve better IO. QUESTION NO: 300 A user has enabled termination protection on an EC2 instance. The user has also set Instance initiated shutdown behaviour to terminate. When the user shuts down the instance from the OS, what will happen? A. The OS will shutdown but the instance will not be terminated due to protection B. It will terminate the instance C. It will not allow the user to shutdown the instance from the OS D. It is not possible to set the termination protection when an Instance initiated shutdown is set to Terminate Answer: B Explanation: It is always possible that someone can terminate an EC2 instance using the Amazon EC2 console, command line interface or API by mistake. If the admin wants to prevent the instance from being accidentally terminated, he can enable termination protection for that instance. The user can also setup shutdown behaviour for an EBS backed instance to guide the instance on what should be done when he initiates shutdown from the OS using Instance initiated shutdown behaviour. If the instance initiated behaviour is set to terminate and the user shuts off the OS even though termination protection is enabled, it will still terminate the instance. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 301 A user has deployed an application on an EBS backed EC2 instance. For a better performance of application, it requires dedicated EC2 to EBS traffic. How can the user achieve this? A. Launch the EC2 instance as EBS dedicated with PIOPS EBS B. Launch the EC2 instance as EBS enhanced with PIOPS EBS C. Launch the EC2 instance as EBS dedicated with PIOPS EBS D. Launch the EC2 instance as EBS optimized with PIOPS EBS Answer: D Explanation: Any application which has performance sensitive workloads and requires minimal variability with dedicated EC2 to EBS traffic should use provisioned IOPS EBS volumes, which are attached to an EBS-optimized EC2 instance or it should use an instance with 10 Gigabit network connectivity. Launching an instance that is EBSoptimized provides the user with a dedicated connection between the EC2 instance and the EBS volume. QUESTION NO: 302 A user has launched a Windows based EC2 instance. However, the instance has some issues and the user wants to check the log. When the user checks the Instance console output from the AWS console, what will it display? A. All the event logs since instance boot B. The last 10 system event log error C. The Windows instance does not support the console output D. The last three system events\u2019 log errors Answer: D Explanation: The AWS EC2 console provides a useful tool called Console output for problem diagnosis. It is useful to find out any kernel issues, termination reasons or service configuration issues. For a Windows instance it lists the last three system event log errors. For Linux it displays the exact console output.","title":"AWS Certified SysOps Administrator - Questions and Answers - Cont. (2)"},{"location":"nightwolf-cotribution/aws-3/","text":"AWS Interview Questions & Answers \uf0c1 These are AWS interview questions for experienced professionals. You will find these questions very helpful in your AWS professional role interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); List the components required to build Amazon VPC? Subnet, Internet Gateway, NAT Gateway, HW VPN Connection, Virtual Private Gateway, Customer Gateway, Router, Peering Connection, VPC Endpoint for S3, Egressonly Internet Gateway. How do you safeguard your EC2 instances running in a VPC? Security Groups can be used to protect your EC2 instances in a VPC. We can configure both INBOUND and OUTBOUND traffic in a Security Group which enables secured access to your EC2 instances. Security Group automatically denies any unauthorized access to your EC2 instances. In a VPC how many EC2 instances can you use? Initially you are limited to launch 20 EC2 Instances at one time. Maximum VPC size is 65,536 instances. Can you establish a peering connection to a VPC in a different REGION? Not possible. Peering Connection are available only between VPC in the same region. Can you connect your VPC with a VPC owned by another AWS account? Yes, Possible. Provided the owner of other VPCs accepts your connection. What are all the different connectivity options available for your VPC? Internet Gateway, Virtual Private Gateway, NAT, EndPoints, Peering Connections. Can a EC2 instance inside your VPC connect with the EC2 instance belonging to other VPCs? Yes, Possible. Provided an Internet Gateway is configured in such a way that traffic bounded for EC2 instances running in other VPCs. How can you monitor network traffic in your VPC? It is possible using Amazon VPC Flow-Logs feature. Difference between Security Groups and ACLs in a VPC? A Security Group defines which traffic is allowed TO or FROM EC2 instance. Whereas ACL, controls at the SUBNET level, scrutinize the traffic TO or FROM a Subnet. Hon an EC2 instance in a VPC establish the connection with the internet? Using either a Public IP or an Elastic IP. Different types of Cloud Computing as per services? PAAS (Platform As A Service), IAAS (Infrastructure As A Service), SAAS (Software As A Service) What is Auto Scaling? Automatically adding duplicate instances to application farm during heavy business hours. Scale-IN and Scale-OUT are two different types of Scaling. Scale-IN: Reducing the instances. Scale-OUT: Increasing the instances by duplicating. What is AMI? AMI is defined as Amazon Machine Image. Basically it\u2019s a template comprising software configuration part. For example, Operating System, DB Server, Application Server, etc. Difference between Stopping and Terminating the Instances? When you STOP an instance it is a normal shutdown. The corresponding EBS volume attached to that instance remains attached and you can restart the instance later. When you TERMINATE an instance it gets deleted and you cannot restart that instance again later. And any EBS volume attached with that instance also deleted. When you launch a standby Relational Database Service instance will it be available in the same Available Zone? Not advisable. Because the purpose of having standby RDS instance is to avoid an infrastructure failure. So you have to keep your standby RDS service in a different Availability Zone, which may have different infrastructure. Difference between Amazon RDS, DynamoDB and Redshift? RDS is meant for structured data only. DynamoDB is meant for unstructured data which is a NoSQL service. Redshift is a data warehouse product used for data analysis. What are Lifecycle Hooks? Lifecycle Hooks are used in Auto Scaling. Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. Each Auto Scaling group can have multiple lifecycle hooks. What is S3? S3 stands for Simple Storage Service, with a simple web service interface to store and retrieve any amount of data from anywhere on the web. What is AWS Lambada? Lambda is an event-driven platform. It is a compute service that runs code in response to events and automatically manages the compute resources required by that code. In S3 how many buckets can be created? By default 100 buckets can be created in a region. What is CloudFront? Amazon CloudFront is a content delivery network operated by Amazon Web Services. It is a service that speeds up transfer of your static and dynamic web content such as HTML files, IMAGE files., etc., CloudFront delivers your particulars thru worldwide data centers named Edge Locations. Brief about S3 service in AWS? S3, a Simple Storage Service from Amazon. You can move your files TO and FROM S3. Its like a FTP storage. You can keep your SNAPSHOTS in S3. You can also ENCRYPT your sensitive data in S3. Explain Regions and Available Zones in EC2? Amazon has hosted EC2 in various locations around the world. These locations are called REGIONS. For example in Asia, Mumbai is one region and Singapore is another region. Each region is composed of isolated locations which are known as AVAILABLE ZONES. Region is independent. But the Available Zones are linked thru low-latency links. What are the two types of Load Balancer? Classic LB and Application LB. ALB is the Content Based Routing. Can a AMI be shared? Yes. A developer can create an AMI and share it with other developers for their use. A shared AMI is packed with the components you need and you can customize the same as per your needs. As you are not an owner of a shared AMI there is a risk always involved. What is a Hypervisor? A Hypervisor is a kind of software that enables Virtualization. It combines physical hardware resources into a platform which is delivered virtually to one or more users. XEN is the Hypervisor for EC2. Key Pair and its uses? You use Key Pair to login to your Instance in a secured way. You can create a key pair using EC2 console. When your instances are spread across regions you need to create key pair in each region. What is the feature of ClassicLink? ClassicLink allows instances in EC2 classic platform to communicate with instances in VPC using Private IP address. EC2 classic platform instances cannot not be linked to more than one VPC at a time. Can you edit a Route Table in VPC? Yes. You can always modify route rules to specify which subnets are routed to the Internet gateway, the virtual private gateway, or other instances. How many Elastic IPs can you create? 5 VPC Elastic IP addresses per AWS account per region. Can you ping the router or default gateway that connects your subnets? NO, you cannot. It is not supported. However you can ping EC2 instances within a VPC, provided your firewall, Security Groups and network ACLs allows such traffic. How will you monitor the network traffic in a VPC? Using Amazon VPC Flow Logs feature. Can you make a VPC available in multiple Available Zones? Yes. How do you ensure an EC2 instance is launched in a particular Available Zone? After selecting your AMI Template and Instance Type, in the third step while configuring the instance you must select the SUBNET in which you wish to launch your instance. It will be launched in the AZ associated with that SUBNET. For Internet Gateways do you find any Bandwidth constraints? NO. Normally an IG is HORIZONTALLY SCALLED, Redundant and Highly Available. It is not having nay Bandwidth constraints usually. What is the significance of a Default VPC? When you launch your instances in a Default VPC in a Region, you would be getting the benefit of advanced Network Functionalities. You can also make use of Security Groups, multiple IP addresses, and multiple Network interfaces. Can you make use of default EBS Snapshots? You can use, provided if it is located in the same region where your VPC is presented. What will happen when you delete a PEERING CONNECTION in your side? The PEERING CONNECTION available in the other side would also get terminated. There will no more traffic flow. Can you establish a Peering connection to a VPC in a different region? NO. Its possible between VPCs in the same region. Can you connect your VPC with a VPC created by another AWS account? Yes. Only when that owner accepts your peering connection request. When you delete your DB instance what will happen to your backups and DB snapshots? When a DB instance is deleted, RDS retains the user-created DB snapshot along with all other manually created DB snapshots. Also automated backups are deleted and only manually created DB Snapshots are retained. What is the significance of an Elastic IP? The Public IP is associated with the instance until it is stopped or terminated Only. A Public IP is not static. Every time your instance is stopped or terminated the associated Public IP gets vanished and a new Public IP gets assigned with that instance. To over come this issue a public IP can be replaced by an Elastic IP address, which stays with the instance as long as the user doesn\u2019t manually detach it. Similarly when if you are hosting multiple websites on your EC2 server, in that case you may require more than one Elastic IP address. How will you use S3 with your EC2 instances? Websites hosted on your EC2 instances can load their static contents directly from S3. It provides highly scalable, reliable, fast, inexpensive data storage infrastructure. Is this possible to connect your company datacenter to Amazon Cloud? Yes, you can very well do this by establishing a VPN connection between your company\u2019s network and Amazon VPC. Can you change the Private IP of an EC2 instance while it is running or stopped? A Private IP is STATIC. And it is attached with an instance throughout is lifetime and cannot be changed. What is the use of Subnets? When a network has more number of HOSTS, managing these hosts can be tedious under a single large network. Therefore we divide this large network into easily manageable sub-networks (subnets) so that managing hosts under each subnet becomes easier. What is the use of Route Table? Route Table is used to route the network pockets. Generally one route table would be available in each subnet. Route table can have any no. of records or information, hence attaching multiple subnets to a route table is also possible. Can you use the Standby DB instance for read and write along with your Primary DB instance? Standby server cannot be used in parallel with primary server unless your Primary instance goes down. What is the use of Connection Draining? Connection Draining is a service under Elastic Load Balancing. It keeps monitoring the healthiness of the instances. If any instance fails Connection Draining pulls all the traffic from that particular failed instance and re-route the traffic to other healthy instances. What is the role of AWS CloudTrail? CloudTrail is designed for logging and tracking API calls. Also used to audit all S3 bucket accesses. What is the use of Amazon Transfer Acceleration Service? ATA service speeds up your data transfer with the use of optimized network paths. Also, speed up your CDN up to 300% compared to normal data transfer speed. What is the name of AWS CEO or Chief ? Jeff Bezos or Lisa Su or Denise Morrison ? Jeff Bezos EC2 officially launch in \u2026.. 2002 or 2006 or 2008 ? 2006 S3 Launched officially lunched in \u2026.. 2002 or 2006 or 2008 ? 2006 You cannot store unlimited data in Amazon Web Services. True or False ? False Rapid provisioning allows you to very quickly spin up a new virtual machine with minimal effort. True or false ? True A hybrid setup is one in which part of your resources are AWS and the rest are with another cloud provider. True or False ? False As an added layer of security for AWS management, which of the following should be you do ? Create multiple Admin accounts Generate a new security key each time you log in Create IAM users Ans: Create IAM users Is AMI template ? True EC2 Instances are Virtual Server in AWS. True or False ? True What does \u201celastic\u201d refer to in Elastic Compute Cloud(EC2)? Select all that apply. Increasing and decreasing capacity as needed Monitoring services on multiple devices Operating on Mac, Windows and Linux Paying only for running virtual machines Stretching applications across virtual machines Ans: 1. Increasing and decreasing capacity as needed AND 4. Paying only for running virtual machines You can upload a custom configuration virtual image and sell it on the AWS Marketplace. True or false ? True EC2 Machine types define which of the following ? AWS Region Core Count User Location Ans: Core Count Which is default instance type ? On-demand RI Spot instance Ans: On-demand What is Elastic Computing ? Data will be replicate to different AZs You can spin up and spin down VMs Automatically VMs will be add and remove Ans: You can spin up and spin down VMs In cloud computing, elasticity is defined as \"the degree to which a system is able to adapt to workload changes by provisioning and de-provisioning resources in an autonomic manner, such that at each pointi in time the available resources match the current demand as closely as possible\" Can We launch multiple instances with same AMI ? True PEM file is one time physical password ? True Windows user required PPK file to connect Linux instance hosted on AWS. True or False ? True You can purchase time on EC2 directly from other users and specify the price you want to pay. True or false ? True Which of the following might prevent your EC2 instance from appearing in the list of instances? EC2 is not selected Correct region is not selected AWS marketplace is not selected Ans: Correct region is not selected Which of the following main reason to terminate an unused EC2 instance ? Security Concerns Additional fees Data Loss Ans: Additional fees Which AWS service exists only to redundantly cache data and images ? AWS Availability Zones AWS Edge Locations AWS Regions Ans: AWS Edge Locations Regions, AZs and Edge Locations all terms are same\u2026 True or False ? Ans: False AWS every service is available at every regions\u2026. True or False ? Ans: False Premium support is Available in AWS for Developer, Business & Enterprise level ? Ans: True Can you add new Debit/Credit card in your AWS Account ? Ans: True Can you increase micro to large of instance ? Ans: True On-demand instances is based on a bid mechanism. True or False ? Ans: False RI can be sold on the AWS marketplace? Ans: True What are different options in instances ? Which instance is best on Production? - On-demand - RI - Depends on Application or Website Ans: Depends on Application or Website Which is most expensive options in instance ? Ans: On-demand Amazon S3 is internet accessible storage via HTTP /HTTPS. True or False ? Ans: True Amazon S3 is not a object level of storage. True or False? Ans: False Amazon S3 is storage for the Internet. True or False? Ans: True Temporary storage access speed is not guaranteed. True or False? Ans: True There is 99.99% SLA(Service Level Agreement) for temporary storage. True or False? Ans: False Ephemeral storage is block-level storage ? Ans: True Single object size is up to 5 TB in Amazon S3. True or False ? Ans: True You can create unlimited bucket size in Amazon S3. True or False ? Ans: True By default, Instance-Backed and EBS-Backed root volumes delete all data. However, when using EBS-Backed storage, you can configure it to save the data on the root volume. True or false ? Ans: True You can switch from an Instance-Backed to an EBS-Backed root volume at any time. True or False ? Ans: False When using an EBS-Backed machine, you can override the terminate option and save the root volume. True or False ? Ans: True Which of the following is a service of AWS Simple Storage Service(S3)? Select all that apply. Database Indexing File searching Secure Hosting Storage Scaling Ans: 3. Secure Hosting & 4. Storage Scaling What\u2019s the difference between instance store and EBS? Issue: I\u2019m not sure whether to store the data associated with my Amazon EC2 instance in instance store or in an attached Amazon Elastic Block Store (Amazon EBS) volume. Which option is best for me? Resolution: Some Amazon EC2 instance types come with a form of directly attached, block-device storage known as the instance store. The instance store is ideal for temporary storage, because the data stored in instance store volumes is not persistent through instance stops, terminations, or hardware failures. You can find more detailed information about the instance store at Amazon EC2 Instance Store. For data you want to retain longer-term, or if you need to encrypt the data, we recommend using EBS volumes instead. EBS volumes preserve their data through instance stops and terminations, can be easily backed up with EBS snapshots, can be removed from instances and reattached to another, and support full-volume encryption. For more detailed information about EBS volumes, see Features of Amazon EBS. BS can be attached to any running instance that is in the same Availability Zone ? Ans: True EBS is internet accessible. True or False ? Ans: False EBS has persistent file system for EC2. True or False ? Ans: True EBS supports incremental snapshots. True or False ? Ans: True Amazon Glacier enables customers to offload the administrative burdens of operating and scaling storage to AWS. True or False ? Ans: True Amazon Glacier is a great storage choice when low storage cost is paramount. True or False ? Ans: True Data is rarely retrieved, and retrieval latency of several hours is acceptable in Glacier. True or False ? Ans: True Glacier is basically for data archival. True or False ? Ans: True It is very cheap storage. True or False ? Ans: True Glacier has very, very slow retrieval times. True or False ? Ans: True By Default, Instance-Backed and EBS-Backed root volumes delete all data. However, when using EBS-Backed storage, you can configure it to save the data on the root volume. True or False ? Ans: True You can switch from an Instance-Backed to an EBS-Backed root volume at any time. True or False ? Ans: False When using an EBS-Backed machine, you can override the terminate option and save the root volume. True or False ? Ans: True VPC is Private, Isolated, Virtual Network. True or False ? Ans: True VPC would be logically isolated network in AWS cloud. True or False ? Ans: True VPC is also give control of network architecture. True or False ? Ans: True VPC is also going to enhanced security. True or False ? Ans: True VPC has ability to interwork with other organizations. True or False ? Ans: True VPC does not enable hybrid cloud(site-to-site VPN). True or False ? Ans: False Route Table is a set of Rules tells the direction of network. True or False ? Ans: True Security Group is a subnet level of security. True or False ? Ans: False NACLs(Network Access Lists) is a resource level of security. True or False ? Ans: False Any default stack is available in Cloud Formation ? Ans: You can not create default stack but you can choose the type of stack to create e.g : A sample stack A Linux-based chef 12 stack A Windows-based Chef 12.2 stack A Linux-based Chef 11.10 stack What is the difference between Stack and Template in Cloud Formation ? Ans: Stack : Cloud-based applications usually require a group of related resources\u2014 application servers, database servers, and so on\u2014that must be created and managed collectively. This collection of instances is called a stack. We can create multiple server for same stack ? Ans: you can select one \u201cinstance type\u201d e.g: t2.micro at a time but you can set more then one \u201cWebserver Capacity\u201d which is \u201cThe initial number of Webserver instances\u201c means automatically same kind of instances will launch. Can you explain the term SQS is pull based, not pushed base. Ans: It means that you have to actively poll the queue in order to receive a messages. The messages are pushed into the queue by the producers but pulled out of the queue by the consumers.You have to call the Receive Message action from the consumer in order to get the messages, they are not pushed to you automatically when they arrive. Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"AWS Interview Questions- 1st"},{"location":"nightwolf-cotribution/aws-3/#aws-interview-questions-answers","text":"These are AWS interview questions for experienced professionals. You will find these questions very helpful in your AWS professional role interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); List the components required to build Amazon VPC? Subnet, Internet Gateway, NAT Gateway, HW VPN Connection, Virtual Private Gateway, Customer Gateway, Router, Peering Connection, VPC Endpoint for S3, Egressonly Internet Gateway. How do you safeguard your EC2 instances running in a VPC? Security Groups can be used to protect your EC2 instances in a VPC. We can configure both INBOUND and OUTBOUND traffic in a Security Group which enables secured access to your EC2 instances. Security Group automatically denies any unauthorized access to your EC2 instances. In a VPC how many EC2 instances can you use? Initially you are limited to launch 20 EC2 Instances at one time. Maximum VPC size is 65,536 instances. Can you establish a peering connection to a VPC in a different REGION? Not possible. Peering Connection are available only between VPC in the same region. Can you connect your VPC with a VPC owned by another AWS account? Yes, Possible. Provided the owner of other VPCs accepts your connection. What are all the different connectivity options available for your VPC? Internet Gateway, Virtual Private Gateway, NAT, EndPoints, Peering Connections. Can a EC2 instance inside your VPC connect with the EC2 instance belonging to other VPCs? Yes, Possible. Provided an Internet Gateway is configured in such a way that traffic bounded for EC2 instances running in other VPCs. How can you monitor network traffic in your VPC? It is possible using Amazon VPC Flow-Logs feature. Difference between Security Groups and ACLs in a VPC? A Security Group defines which traffic is allowed TO or FROM EC2 instance. Whereas ACL, controls at the SUBNET level, scrutinize the traffic TO or FROM a Subnet. Hon an EC2 instance in a VPC establish the connection with the internet? Using either a Public IP or an Elastic IP. Different types of Cloud Computing as per services? PAAS (Platform As A Service), IAAS (Infrastructure As A Service), SAAS (Software As A Service) What is Auto Scaling? Automatically adding duplicate instances to application farm during heavy business hours. Scale-IN and Scale-OUT are two different types of Scaling. Scale-IN: Reducing the instances. Scale-OUT: Increasing the instances by duplicating. What is AMI? AMI is defined as Amazon Machine Image. Basically it\u2019s a template comprising software configuration part. For example, Operating System, DB Server, Application Server, etc. Difference between Stopping and Terminating the Instances? When you STOP an instance it is a normal shutdown. The corresponding EBS volume attached to that instance remains attached and you can restart the instance later. When you TERMINATE an instance it gets deleted and you cannot restart that instance again later. And any EBS volume attached with that instance also deleted. When you launch a standby Relational Database Service instance will it be available in the same Available Zone? Not advisable. Because the purpose of having standby RDS instance is to avoid an infrastructure failure. So you have to keep your standby RDS service in a different Availability Zone, which may have different infrastructure. Difference between Amazon RDS, DynamoDB and Redshift? RDS is meant for structured data only. DynamoDB is meant for unstructured data which is a NoSQL service. Redshift is a data warehouse product used for data analysis. What are Lifecycle Hooks? Lifecycle Hooks are used in Auto Scaling. Lifecycle hooks enable you to perform custom actions by pausing instances as an Auto Scaling group launches or terminates them. Each Auto Scaling group can have multiple lifecycle hooks. What is S3? S3 stands for Simple Storage Service, with a simple web service interface to store and retrieve any amount of data from anywhere on the web. What is AWS Lambada? Lambda is an event-driven platform. It is a compute service that runs code in response to events and automatically manages the compute resources required by that code. In S3 how many buckets can be created? By default 100 buckets can be created in a region. What is CloudFront? Amazon CloudFront is a content delivery network operated by Amazon Web Services. It is a service that speeds up transfer of your static and dynamic web content such as HTML files, IMAGE files., etc., CloudFront delivers your particulars thru worldwide data centers named Edge Locations. Brief about S3 service in AWS? S3, a Simple Storage Service from Amazon. You can move your files TO and FROM S3. Its like a FTP storage. You can keep your SNAPSHOTS in S3. You can also ENCRYPT your sensitive data in S3. Explain Regions and Available Zones in EC2? Amazon has hosted EC2 in various locations around the world. These locations are called REGIONS. For example in Asia, Mumbai is one region and Singapore is another region. Each region is composed of isolated locations which are known as AVAILABLE ZONES. Region is independent. But the Available Zones are linked thru low-latency links. What are the two types of Load Balancer? Classic LB and Application LB. ALB is the Content Based Routing. Can a AMI be shared? Yes. A developer can create an AMI and share it with other developers for their use. A shared AMI is packed with the components you need and you can customize the same as per your needs. As you are not an owner of a shared AMI there is a risk always involved. What is a Hypervisor? A Hypervisor is a kind of software that enables Virtualization. It combines physical hardware resources into a platform which is delivered virtually to one or more users. XEN is the Hypervisor for EC2. Key Pair and its uses? You use Key Pair to login to your Instance in a secured way. You can create a key pair using EC2 console. When your instances are spread across regions you need to create key pair in each region. What is the feature of ClassicLink? ClassicLink allows instances in EC2 classic platform to communicate with instances in VPC using Private IP address. EC2 classic platform instances cannot not be linked to more than one VPC at a time. Can you edit a Route Table in VPC? Yes. You can always modify route rules to specify which subnets are routed to the Internet gateway, the virtual private gateway, or other instances. How many Elastic IPs can you create? 5 VPC Elastic IP addresses per AWS account per region. Can you ping the router or default gateway that connects your subnets? NO, you cannot. It is not supported. However you can ping EC2 instances within a VPC, provided your firewall, Security Groups and network ACLs allows such traffic. How will you monitor the network traffic in a VPC? Using Amazon VPC Flow Logs feature. Can you make a VPC available in multiple Available Zones? Yes. How do you ensure an EC2 instance is launched in a particular Available Zone? After selecting your AMI Template and Instance Type, in the third step while configuring the instance you must select the SUBNET in which you wish to launch your instance. It will be launched in the AZ associated with that SUBNET. For Internet Gateways do you find any Bandwidth constraints? NO. Normally an IG is HORIZONTALLY SCALLED, Redundant and Highly Available. It is not having nay Bandwidth constraints usually. What is the significance of a Default VPC? When you launch your instances in a Default VPC in a Region, you would be getting the benefit of advanced Network Functionalities. You can also make use of Security Groups, multiple IP addresses, and multiple Network interfaces. Can you make use of default EBS Snapshots? You can use, provided if it is located in the same region where your VPC is presented. What will happen when you delete a PEERING CONNECTION in your side? The PEERING CONNECTION available in the other side would also get terminated. There will no more traffic flow. Can you establish a Peering connection to a VPC in a different region? NO. Its possible between VPCs in the same region. Can you connect your VPC with a VPC created by another AWS account? Yes. Only when that owner accepts your peering connection request. When you delete your DB instance what will happen to your backups and DB snapshots? When a DB instance is deleted, RDS retains the user-created DB snapshot along with all other manually created DB snapshots. Also automated backups are deleted and only manually created DB Snapshots are retained. What is the significance of an Elastic IP? The Public IP is associated with the instance until it is stopped or terminated Only. A Public IP is not static. Every time your instance is stopped or terminated the associated Public IP gets vanished and a new Public IP gets assigned with that instance. To over come this issue a public IP can be replaced by an Elastic IP address, which stays with the instance as long as the user doesn\u2019t manually detach it. Similarly when if you are hosting multiple websites on your EC2 server, in that case you may require more than one Elastic IP address. How will you use S3 with your EC2 instances? Websites hosted on your EC2 instances can load their static contents directly from S3. It provides highly scalable, reliable, fast, inexpensive data storage infrastructure. Is this possible to connect your company datacenter to Amazon Cloud? Yes, you can very well do this by establishing a VPN connection between your company\u2019s network and Amazon VPC. Can you change the Private IP of an EC2 instance while it is running or stopped? A Private IP is STATIC. And it is attached with an instance throughout is lifetime and cannot be changed. What is the use of Subnets? When a network has more number of HOSTS, managing these hosts can be tedious under a single large network. Therefore we divide this large network into easily manageable sub-networks (subnets) so that managing hosts under each subnet becomes easier. What is the use of Route Table? Route Table is used to route the network pockets. Generally one route table would be available in each subnet. Route table can have any no. of records or information, hence attaching multiple subnets to a route table is also possible. Can you use the Standby DB instance for read and write along with your Primary DB instance? Standby server cannot be used in parallel with primary server unless your Primary instance goes down. What is the use of Connection Draining? Connection Draining is a service under Elastic Load Balancing. It keeps monitoring the healthiness of the instances. If any instance fails Connection Draining pulls all the traffic from that particular failed instance and re-route the traffic to other healthy instances. What is the role of AWS CloudTrail? CloudTrail is designed for logging and tracking API calls. Also used to audit all S3 bucket accesses. What is the use of Amazon Transfer Acceleration Service? ATA service speeds up your data transfer with the use of optimized network paths. Also, speed up your CDN up to 300% compared to normal data transfer speed. What is the name of AWS CEO or Chief ? Jeff Bezos or Lisa Su or Denise Morrison ? Jeff Bezos EC2 officially launch in \u2026.. 2002 or 2006 or 2008 ? 2006 S3 Launched officially lunched in \u2026.. 2002 or 2006 or 2008 ? 2006 You cannot store unlimited data in Amazon Web Services. True or False ? False Rapid provisioning allows you to very quickly spin up a new virtual machine with minimal effort. True or false ? True A hybrid setup is one in which part of your resources are AWS and the rest are with another cloud provider. True or False ? False As an added layer of security for AWS management, which of the following should be you do ? Create multiple Admin accounts Generate a new security key each time you log in Create IAM users Ans: Create IAM users Is AMI template ? True EC2 Instances are Virtual Server in AWS. True or False ? True What does \u201celastic\u201d refer to in Elastic Compute Cloud(EC2)? Select all that apply. Increasing and decreasing capacity as needed Monitoring services on multiple devices Operating on Mac, Windows and Linux Paying only for running virtual machines Stretching applications across virtual machines Ans: 1. Increasing and decreasing capacity as needed AND 4. Paying only for running virtual machines You can upload a custom configuration virtual image and sell it on the AWS Marketplace. True or false ? True EC2 Machine types define which of the following ? AWS Region Core Count User Location Ans: Core Count Which is default instance type ? On-demand RI Spot instance Ans: On-demand What is Elastic Computing ? Data will be replicate to different AZs You can spin up and spin down VMs Automatically VMs will be add and remove Ans: You can spin up and spin down VMs In cloud computing, elasticity is defined as \"the degree to which a system is able to adapt to workload changes by provisioning and de-provisioning resources in an autonomic manner, such that at each pointi in time the available resources match the current demand as closely as possible\" Can We launch multiple instances with same AMI ? True PEM file is one time physical password ? True Windows user required PPK file to connect Linux instance hosted on AWS. True or False ? True You can purchase time on EC2 directly from other users and specify the price you want to pay. True or false ? True Which of the following might prevent your EC2 instance from appearing in the list of instances? EC2 is not selected Correct region is not selected AWS marketplace is not selected Ans: Correct region is not selected Which of the following main reason to terminate an unused EC2 instance ? Security Concerns Additional fees Data Loss Ans: Additional fees Which AWS service exists only to redundantly cache data and images ? AWS Availability Zones AWS Edge Locations AWS Regions Ans: AWS Edge Locations Regions, AZs and Edge Locations all terms are same\u2026 True or False ? Ans: False AWS every service is available at every regions\u2026. True or False ? Ans: False Premium support is Available in AWS for Developer, Business & Enterprise level ? Ans: True Can you add new Debit/Credit card in your AWS Account ? Ans: True Can you increase micro to large of instance ? Ans: True On-demand instances is based on a bid mechanism. True or False ? Ans: False RI can be sold on the AWS marketplace? Ans: True What are different options in instances ? Which instance is best on Production? - On-demand - RI - Depends on Application or Website Ans: Depends on Application or Website Which is most expensive options in instance ? Ans: On-demand Amazon S3 is internet accessible storage via HTTP /HTTPS. True or False ? Ans: True Amazon S3 is not a object level of storage. True or False? Ans: False Amazon S3 is storage for the Internet. True or False? Ans: True Temporary storage access speed is not guaranteed. True or False? Ans: True There is 99.99% SLA(Service Level Agreement) for temporary storage. True or False? Ans: False Ephemeral storage is block-level storage ? Ans: True Single object size is up to 5 TB in Amazon S3. True or False ? Ans: True You can create unlimited bucket size in Amazon S3. True or False ? Ans: True By default, Instance-Backed and EBS-Backed root volumes delete all data. However, when using EBS-Backed storage, you can configure it to save the data on the root volume. True or false ? Ans: True You can switch from an Instance-Backed to an EBS-Backed root volume at any time. True or False ? Ans: False When using an EBS-Backed machine, you can override the terminate option and save the root volume. True or False ? Ans: True Which of the following is a service of AWS Simple Storage Service(S3)? Select all that apply. Database Indexing File searching Secure Hosting Storage Scaling Ans: 3. Secure Hosting & 4. Storage Scaling What\u2019s the difference between instance store and EBS? Issue: I\u2019m not sure whether to store the data associated with my Amazon EC2 instance in instance store or in an attached Amazon Elastic Block Store (Amazon EBS) volume. Which option is best for me? Resolution: Some Amazon EC2 instance types come with a form of directly attached, block-device storage known as the instance store. The instance store is ideal for temporary storage, because the data stored in instance store volumes is not persistent through instance stops, terminations, or hardware failures. You can find more detailed information about the instance store at Amazon EC2 Instance Store. For data you want to retain longer-term, or if you need to encrypt the data, we recommend using EBS volumes instead. EBS volumes preserve their data through instance stops and terminations, can be easily backed up with EBS snapshots, can be removed from instances and reattached to another, and support full-volume encryption. For more detailed information about EBS volumes, see Features of Amazon EBS. BS can be attached to any running instance that is in the same Availability Zone ? Ans: True EBS is internet accessible. True or False ? Ans: False EBS has persistent file system for EC2. True or False ? Ans: True EBS supports incremental snapshots. True or False ? Ans: True Amazon Glacier enables customers to offload the administrative burdens of operating and scaling storage to AWS. True or False ? Ans: True Amazon Glacier is a great storage choice when low storage cost is paramount. True or False ? Ans: True Data is rarely retrieved, and retrieval latency of several hours is acceptable in Glacier. True or False ? Ans: True Glacier is basically for data archival. True or False ? Ans: True It is very cheap storage. True or False ? Ans: True Glacier has very, very slow retrieval times. True or False ? Ans: True By Default, Instance-Backed and EBS-Backed root volumes delete all data. However, when using EBS-Backed storage, you can configure it to save the data on the root volume. True or False ? Ans: True You can switch from an Instance-Backed to an EBS-Backed root volume at any time. True or False ? Ans: False When using an EBS-Backed machine, you can override the terminate option and save the root volume. True or False ? Ans: True VPC is Private, Isolated, Virtual Network. True or False ? Ans: True VPC would be logically isolated network in AWS cloud. True or False ? Ans: True VPC is also give control of network architecture. True or False ? Ans: True VPC is also going to enhanced security. True or False ? Ans: True VPC has ability to interwork with other organizations. True or False ? Ans: True VPC does not enable hybrid cloud(site-to-site VPN). True or False ? Ans: False Route Table is a set of Rules tells the direction of network. True or False ? Ans: True Security Group is a subnet level of security. True or False ? Ans: False NACLs(Network Access Lists) is a resource level of security. True or False ? Ans: False Any default stack is available in Cloud Formation ? Ans: You can not create default stack but you can choose the type of stack to create e.g : A sample stack A Linux-based chef 12 stack A Windows-based Chef 12.2 stack A Linux-based Chef 11.10 stack What is the difference between Stack and Template in Cloud Formation ? Ans: Stack : Cloud-based applications usually require a group of related resources\u2014 application servers, database servers, and so on\u2014that must be created and managed collectively. This collection of instances is called a stack. We can create multiple server for same stack ? Ans: you can select one \u201cinstance type\u201d e.g: t2.micro at a time but you can set more then one \u201cWebserver Capacity\u201d which is \u201cThe initial number of Webserver instances\u201c means automatically same kind of instances will launch. Can you explain the term SQS is pull based, not pushed base. Ans: It means that you have to actively poll the queue in order to receive a messages. The messages are pushed into the queue by the producers but pulled out of the queue by the consumers.You have to call the Receive Message action from the consumer in order to get the messages, they are not pushed to you automatically when they arrive.","title":"AWS Interview Questions &amp; Answers"},{"location":"nightwolf-cotribution/aws-4/","text":"AWS Interview Questions & Answers - 4 \uf0c1 These are AWS interview questions for experienced professionals. You will find these questions very helpful in your AWS professional role interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. How many Elastic IP address can be associated with a single account? Ans: 5 What is the name to the additional network interfaces that can be created and attached to any Amazon EC2 instance in your VPC? Ans: Elastic Network Interface You have configured ELB with three instances connected to that. If your instances are unhealthy or terminated, the traffic should be automatically replaced to another instance, what type of service can be used to achieve this requirement? Ans: Fault Tolerance After configuring ELB, you need to ensure that the user requests are always attached to a single instance. What setting can you use? Ans: Sticky session Which of the following metrics cannot have a cloud watch alarm? EC2 instance status check failed EC2 CPU utilization RRS lost object Auto scaling group CPU utilization Ans: RRS lost object Which of the below mentioned service is provided by Cloud watch? Monitor estimated AWS usage Monitor EC2 log files Monitor S3 storage Monitor AWS calls using Cloud trail Ans: Monitor estimated AWS usage A user has Launched an EC2 instance which of the below mentioned statements is not true respect to instance addressing? The private IP addresses are not reachable from the internet The user can communicate using the private IP across regions The private IP address and pubic IP address for an instance are directly mapped to each other using NAT The private IP address for the instance is assigned using DHCP Ans: The user can communicate using the private IP across regions Which of the following service provides the edge \u2013 storage or content delivery system that caches data at different locations? Amazon RDS Simple DB Amazon Cloud Front Amazon associates web services Ans: Amazon Cloud Front A user is launching an instance under the free usage tier from the AMI with a snapshot size of 50 GB. How can the user launch the instance under the free usage tier? Launch a micro instance Launch a micro instance, but in the EBS configuration modify the size of EBS to 50 GB. Launch a micro instance, but do not store the data of more than 30 GB on the EBS storage. It is not possible to have this instance under the free usage tier Ans: It is not possible to have this instance under the free usage tier What are the possible connection issues you can face while connecting to your instance? Connection timed out Server refused our key No supported authentication methods available All of the above Ans: All of the above You are enabled sticky session with ELB. What does it do with your instance? Routes all the requests to a single DNS Binds the user session with a specific instance Binds the user IP with a specific session Provides a single ELB DNS for each IP address Ans: Binds the user session with a specific instance Q137. Which is a main email platform that provides an easy, cost effective way for you to send compliance and receive a response using your own email address and domains? SES SNS SQS SAS Ans: SES Q138. Which type of load balancer makes routing decisions at either the transport layer or the application layer and supports either EC2 or VPC. Application Load Balancer Classic Load Balancer Primary Load Balancer Secondary Load Balancer Ans: Classic Load Balancer Q139. AWS Cloud Front has been configured to handle the customer requests to the web server launched in Linux machine. How many requests per second can Amazon Cloud Front handle? 1000 100 10000 There is no such limit Ans: There is no such limit Q140. You are going to launched one instance with security group. While configuring security group, what are the things you have to select? Protocol and type Port Source All of the above Ans: Source Q141. Which is virtual network interface that you can attach to an instance in a VPC? Elastic IP AWS Elastic Interface Elastic Network Interface AWS Network ACL Ans: Elastic Network Interface Q142. You have launched a Linux instance in AWS EC2. While configuring security group, you have selected SSH, HTTP, HTTPS protocol. Why do we need to select SSH? To verity that there is a rule that allows traffic from your computer to port 22 To verify that there is a rule that allows traffic from EC2 Instance to your computer Allows web traffic from instance to your computer Allows web traffic from your computer to EC2 instance Ans: To verify that there is a rule that allows traffic from EC2 Instance to your computer Q143. You need to quickly set up an email service because a client needs to start using it in the next hour. Amazon service seems to be the logical choice but there are several options available to set it up. Which of the following options to set up AWS service would best meet the needs of the client? Amazon SES console AWS Cloud Formation SMTP interface AWS Elastic Beanstalk Ans: Amazon SES console Q144.You have chosen a windows instance with Classic and you want to make some change to the security group. How will these changes be effective? Security group rules cannot be changed Changes are automatically applied to windows instances Changes will be effective after rebooting the instance in that security group Changes will be effective after 24-hours Ans: Changes are automatically applied to windows instances Q145. Load Balancer and DNS service comes under which type of cloud service? IAAS-Network IAAS-Computational IAAS-Storage None of the above Ans: IAAS-Storage Q146. You have an EC2 instance that has an unencrypted volume. You want to create another encrypted volume from this unencrypted volume. Which of the following steps can achieve this? Just simply create a copy of the unencrypted volume, you will have the option to encrypt the volume. Create a snapshot of the unencrypted volume and then while creating a volume from the snapshot you can encrypt it Create a snapshot of the unencrypted volume (applying encryption parameters), copy the snapshot and create a volume from the copied snapshot This is not possible, once a volume is unencrypted, there is no way to create an encrypted volume from this Ans: Create a snapshot of the unencrypted volume (applying encryption parameters), copy the snapshot and create a volume from the copied snapshot Q147. Where does the user specify the maximum number of instances with the auto scaling commands? Auto scaling Launch Config Auto scaling group Auto scaling policy Auto scaling size Ans: Auto scaling Launch Config Q148. A user is identify that a huge data download is occurring on his instance he has already set the auto scaling policy to increase the instance count when the network Input Output increase beyond a threshold limits how can the user ensure that this temporary event does not result in scaling The network I/O are not affecting during data download The policy cannot be set on the network I/O There is no way the can stop scaling as it already configured Suspend scaling Ans: Suspend scaling Q149. Which are the types of AMI provided by AWS? EBS Backed Instance Store backed None its volume type and not AMI types Both A and B Ans: Both A and B AWS Interview Questions and Answers for Freshers Q150. What is the significance of forming Subnets? A. Because, not enough hosts B. To manage small number of hosts C. To utilize the Volume available across different subnets D. Smartly utilize network that have large number of hosts The answer is: D Q2: If you want to launch your instance on a single-tenancy platform, which option you would select against Instance Tenancy Attribute parameter? A. One to one B. Sole Owner C. Dedicated D. Reserved The answer is: C Q151. Which AWS Service you would use to transfer objects from your data center, when you are using Amazon CloudFront? A. AWS CloudWatch B. AWS SNS Service C. AWS SMS Service D. AWS Direct Connect The answer is: D Q152 _ _ _ is a fully managed Data Warehouse service from AWS? A. Amazon Redshift B. Amazon Neptune C. Amazon Aurora D. Amazon DynamoDB The answer is: A Q153. Which of the following statements are applicable to AWS Elastic File System(EFS)? A. EFS provides simple, scalable file storage for use with Amazon EC2 B. EFS with MS-Windows based EC2 instances is not supported C. EFS supports the Network File System version 4 protocol D. All of the above The answer is: D Q154. What is the role of Connection Draining? A. Helps to launch an EC2 instance B. Automatically terminates instances which are not in use C. Establishes connection between EC2 and RDS instances D. Auto Scaling wait for outstanding requests to complete before terminating instances when CD is enabled The answer is: D Q155. What is the use of Lambda? A. Lambda is used for running server-less applications B. It is a testing tool from AWS C. It is a database service from AWS D. It is an Anti Virus software from AWS The answer is: A Q156. What is Application Load Balancing? A. It is a feature of Elastic Load Balancing B. Use to distribute traffic to different Target Groups C. It is a service generating Elastic IPs for AWS customers D. It is a kind of Firewall The answers are: A and B Q157. What are the uses of Elastic Beanstalk? A. Quickly deploy and manage applications in the AWS Cloud B. Supports Java, .NET, Node.js, PHP, Python applications C. It is an Application Server from AWS D. Use to deploy only Java-Beans applications The answers are: A and B Q158. Can you connect your company\u2019s datacenter to the Amazon Cloud network? A. Not possible B. You can connect thru a Dedicated N/W line C. By establishing a Virtual Private Network (VPN) between your datacenter and VPC D. Connect with a hotline The answer is: C Q159. You have commissioned PRIVATE servers in your premises. You also distributed some of your workloads with the PUBLIC cloud. What type of architecture is this? A. Virtual Private Cloud B. Community Cloud C. Public Cloud D. Hybrid Cloud The answer is: D Q160. DynamoDB _ _ _ . Which one of the following is true regarding DynamoDB? A. Manages Notification Service B. Stores Metadata C. Manages Queue Service D. None of the above The answer is: B Q161: What are the significances of AWS CloudTrail? A. Takes care of Message Queuing Service B. It enables governance, compliance, operational auditing and risk auditing of your AWS account. C. Used as a database service D. It provides an event history of your AWS account activities The answers are: B and D Q162. Which one is a global Content Delivery Network service that securely delivers data, videos, applications, and APIs to your viewers with low latency and high transfer speeds? A. Amazon CloudWatch B. Amazon CloudFront C. Amazon CloudTrail D. Amazon VPC The answer is: B Q163. Is AWS offering Reserved Instances facility for Multiple-Subnet deployments? A. Yes, available for all kind of instances B. No, available only for Dedicated Tenancy C. Offering only for LINUX based instances D. None of the above The answer is: A Q164. Select the correct statement from the below: A. You can have multiple ACLs for a subnet B. Security Group is not necessary for an EC2 instance C. You can attach multiple Zones/Subnets to a Route Table D. You can create S3 bucket using AWS AMI templates The answer is: C Q165. Name the AWS DB Service which is Server-Less and NoSQL DB which delivers consistent single-digit millisecond latency at any scale? A. Amazon Redshift B. Amazon Neptune C. Amazon Aurora D. Amazon DynamoDB The answer is: D Q166. Is this advisable to keep your Standby-Database instance in the same zone where your primary instance is running? A. Yes, you can keep B. Possible only for MySQL instance C. No, not recommended for any kind of DB instance D. Recommended only for MS-SQL instance The answer is: C Q167. Can objects in S3 be delivered by Amazon CloudFront? A. Yes, you can place any objects in S3 which CloudFront quickly delivers B. CloudFront delivers only movie type objects C. No, S3 cannot be integrated with CloudFront D. Amazon VPC will deliver the objects The answer is: A Q168. What you should do if you want to launch an EC2 instance with a pre-allocated private IP address? A. Launch it in a Subnet Group B. Launch the instance from a Private AMI C. Assign EIP address to that instance D. Launch that instance in AWS VPC cloud The answer is: D Q169. Can you edit a Security Group (SG) rules when it is used by multiple EC2 instances? Will new rules apply to all previously running EC2 instances? A. No, you cannot edit a SG when used by a EC2 instance B. Yes, you can edit. Immediately apply to all instances. C. You can edit only the Outbound rules D. Only Outbound rules apply to all EC2 instances The answer is: B Q170. Which of the following statements are true with Route 53? A. Amazon Route 53 is a scalable and highly available Domain Name System (DNS) B. Amazon Route 53 is fully compliant with IPv6 as well C. Will automatically configure DNS settings for your domains D. Route 53 provides low latency database service The answers are: A,B and C Q171. What is a Virtual Private Cloud (VPC)? A. VPC enables you to launch AWS resources into a virtual network B. VPC is a virtual network dedicated to your AWS account C. VPC is used to create domain name for your organization D. VPC can also be connected to your own office data center The answers are: A,B and D Q172. What is an Elastic IP? A. There is no such IP. Only public & private IPs are valid. B. Used in Elastic Load Balancing C. An Elastic IP address is a static IPv4 address D. An Elastic IP address is for use in a specific region only The answers are: C and D Q173. _ _ _ is a fully managed in-memory data store service offered by Amazon Web Services (AWS)? A. Amazon Neptune B. Amazon Redshift C. Amazon ElastiCache D. Amazon Aurora The answer is: C Q174. In AWS which service is used to create Domain Name for their customers? A. Amazon CloudWatch B. Amazon Route53 C. Amazon CloudDomain D. Amazon VPC The answer is: B Q175. Which one is a valid statement regarding EBS-Volumes? A. You can attach maximum of 5 volumes to an instance B. You can attach multiple instances to one volume C. You can attach multiple volumes to a single EC2 instance D. You cannot attach a additional volume to an instance The answer is: C Q176. Which one is a valid statement regarding EBS-Snapshots? A. You can access Snapshots thru S3 APIs B. You can store your Snapshots in a S3 BUCKET C. Snapshots are available only thru EC2 instances D. You can access your Snapshots thru VPC APIs The answer is: C Q178. Which one is the valid scenario? A. Creating PEERING connection to a VPC in a Different Region B. Creating PEERING connection between VPCs in Same Region C. Attaching VOLUME in one subnet/zone with EC2 instance in another subnet/zone D. Keeping your primary db and secondary db in the same zone The answer is: B Q179. How do you connect a VPC to your Office Datacenter? A. By keeping AWS VPC and Office Datacenter in same IP range B. Establishing VPN connection between VPC and Datacenter C. Establishing a dedicated hotlink between VPC and Datacenter D. You cannot connect VPC and your Datacenter The answer is: B Q180. Choose the valid scenarios regarding VPC? A. You can delete the Default VPC available in your region B. VPC can span across multiple Availability Zones C. Trying to launch an instance without having VPC in a region D. Launching an instance onto a VPC created by you The answers are: A,B and D Q181. How the EC2 instances inside a VPC directly access the internet? A. With the help of instance\u2019s Public IP B. By attaching a Elastic IP to that instance C. Internet Gateway enables the access to the internet D. With the help of Route Table The answer is: C Q182. Which one is the highly secured design? A. Keeping both EC2 and Database instances in a public subnet B. Keep EC2 in public subnet and Database in private subnet C. Keep EC2 in public subnet and Database in a S3 bucket D. Defining ANYWHERE in the DB security group INBOUND rule The answer is: B Q183. Keeping your instance in a public subnet and database in a private subnet. What type of cloud deployment model is this? A. Community Cloud B. Private Cloud C. Public Cloud D. Hybrid Cloud The answer is: D Q184. Which service distribute the contents from Edge Locations to the end users to reduce the latency? A. Amazon CloudWatch B. Amazon CloudTrail C. Amazon CloudFront D. Amazon PushData The answer is: C Q185. I am a cloud web service used for hosting your application. Who am I? A. AWS Route 53 B. AWS VPC C. AWS S3 D. AWS EC2 The answer is: D Q186. You can add _ _ to your Auto Scaling group so that you can perform custom actions when instances launch or terminate. A. CloudWatch B. CloudTrail C. Load Balancer D. Lifecycle Hooks The answer is: D Q187. What is Auto Scaling? A. Accelerating VPC Speed B. Creating/Terminating duplicate instances using Scale IN/OUT C. Automating backup/restore service D. None of the above The answer is: B Q188. You want complex querying capabilities but don\u2019t want data warehouse. Which database service you would choose? A. Amazon DynamoDB B. Amazon Redshift C. Amazon RDS D. Amazon ElastiCache The answer is: C Q189. What is an Availability Zone? A. A Container where all your S3 buckets are stored B. Denotes an Entire Region C. A location inside a Region which is protected from failures D. Collection of Regions The answer is: C Q190. The cloud infrastructure is shared by several organizations and supports specific group that has shared concerns. Government departments, universities, central banks etc. often find this type of cloud useful. What kind of cloud deployment model is this? A. Private Cloud B. Hybrid Cloud C. Community Cloud D. Public Cloud The answer is: C Q191. How many Buckets you can create in S3? A. 150 B. 250 C. 500 D. 100 The answer is: D Q192. What is the maximum size of a S3 Bucket? A. 3 Terabytes B. 10 Terabytes C. 5 Terabytes D. 7 Terabytes The answer is: C Q193. Which service of Amazon AWS is used to host a static website? A. Amazon Simple Storage Service(S3) B. Amazon CloudFront C. Amazon Route53 D. Amazon CloudWatch The answer is: A Q194. Which of the following is not a Part of Security groups? A. List of Protocols B. List of Users C. Ports D. IP Address The answer is: B Q195. A data transport solution that accelerates moving terabytes to petabytes of data into and out of AWS using storage devices designed to be secure for physical transport. Name this solution. A. Amazon EFS B. Amazon S3 C. Amazon Glacier D. Amazon Snowball The answer is: D Q196. What type of IP address do you use for your CGW (Customer Gateway) address? A. You will use PRIVATE IP address of your NAT device B. You will use PUBLIC IP address of your NAT device C. You will use ELASTIC IP address of your NAT device D. You will use VPN The answer is: B Q197. How many subnets you can have per VPC? A. 100 B. 300 C. 250 D. 200 The answer is: D Q198. I have a REST API interface and uses secure HMAC-SHA1 authentication keys. I am also a data storage system. Who am I? A. SS3 B. Elastic Block Store C. S3 D. Snapshots The answer is: C Q199. I am a structured data store. I support indexing and data queries to both EC2 and S3. Who am I? A. DynamoDB B. SimpleDB C. MySQL D. Aurora The answer is: B Q200. How many Elastic IP address can be associated with a single account? A) 4 B) 10 C) 5 D) None the above Q201. After configuring ELB, you need to ensure that the user requests are always attached to a single instance. What setting can you use? A) Session cookie B) Cross one load balancing C) Connection drainage D) Sticky session Q202. Which of the following metrics cannot have a cloud watch alarm? A) EC2 instance status check failed B) EC2 CPU utilization C) RRS lost object D) Auto scaling group CPU utilization Q203. Which of the below mentioned service is provided by Cloud watch? A) Monitor estimated AWS usage B) Monitor EC2 log files C) Monitor S3 storage D) Monitor AWS calls using Cloud trail Q204. Which of the following service provides the edge \u2013 storage or content delivery system that caches data at different locations? A) Amazon RDS B) Simple DB C) Amazon Cloud Front D) Amazon associates web services Q205. What are the possible connection issues you can face while connecting to your instance? A) Connection timed out B) Server refused our key C) No supported authentication methods available D) All of the above Q206. You are enabled sticky session with ELB. What does it do with your instance? A) Routes all the requests to a single DNS B) Binds the user session with a specific instance C) Binds the user IP with a specific session D) Provides a single ELB DNS for each IP address Q207. Which is an email platform that provides an easy, cost effective way for you to send and receive email using your own email address and domains? A) SES B) SNS C) SQS D) SAS Q208. AWS Cloud Front has been configured to handle the customer requests to the web server launched in Linux machine. How many requests per second can Amazon Cloud Front handle? A) 1000 B) 100 C) 10000 D) There is no such limit Q209. Which is virtual network interface that you can attach to an instance in a VPC? A) Elastic IP B) AWS Elastic Interface C) Elastic Network Interface D) AWS Network ACL Q210. You have launched an instance in EC2-Classic and you want to make some change to the security group rule. How will these changes be effective? A) Security group rules cannot be changed B) Changes are automatically applied to all instances that are associated with the security group C) Changes will be effective after rebooting the instance in that security group D) Changes will be effective after 24-hours Q211. Load Balancer and DNS service comes under which type of cloud service? A) IAAS-Network B) IAAS-Computational C) IAAS-Storage D) None of the above Q212. You have an EC2 instance that has an unencrypted volume. You want to create another encrypted volume from this unencrypted volume. Which of the following steps can achieve this? A) Just simply create a copy of the unencrypted volume, you will have the option to encrypt the volume. B) Create a snapshot of the unencrypted volume and then while creating a volume from the snapshot you can encrypt it C) Create a snapshot of the unencrypted volume (applying encryption parameters), copy the snapshot and create a volume from the copied snapshot D) This is not possible, once a volume is unencrypted, there is no way to create an encrypted volume from this Q213. Where does the user specify the maximum number of instances with the auto scaling commands? A) Auto scaling Launch Config B) Auto scaling group C) Auto scaling policy D) Auto scaling size Q214. A user is aware that a huge download is occurring on his instance he has already set the auto scaling policy to increase the instance count when the network I/O increase beyond a certain limits how can the user ensure that this temporary event does not result in scaling A) The network I/O are not affecting during data download B) The policy cannot be set on the network I/O C) There is no way the can stop scaling as it already configured D) Suspend scaling Q.215.Which are the types of AMI provided by AWS? A) EBS Backed B) Instance Store backed C) None its volume type and not AMI types D) Both A and B Q 216. Name some cloud service providers for public & private cloud ? Public: Amazon web services, Microsoft Azure, Google Cloud, Oracle Cloud, Alibaba Cloud. Private: Redhat-Openstack, Rackspace, VMware, IBM Private Cloud. Q 217. What are all the different Instance categories based on pricing and explain them briefly ? On-demand Instances: On-demand instances are the virtual servers that are provisioned by AWS EC2 service at an hourly price basis. Reserved Instances: Instances which are reserved for a time, 1 year or 3 years , is called reserved Instances. Hourly prices are reduced significantly compared to on-demand Instances with reservation. Spot Instances: Spot Instances are the special instance category where you request the unused resources of EC2 from the datacenter for steep discounts. Spot prices are fixed by AWS EC2 and you need to bid the spot price more than the pricing of AWS EC2. Q 218. I have some private servers on my premises, also I have distributed some of my workload on the public cloud, what is this architecture called ? Hybrid Cloud Q 219. What is the difference between S3 and Glacier storage ? S3 is a simple storage service, which is used to store and retrieve data. We can store any amount of data and any type of data. Data that we are storing here are referred as objects. Whereas the Glacier storage is an archival store which is used to store infrequently accessed data or cold data. Major use case of glacier is data archiving and backup. Q 220. Name some Database engines available natively in RDS services ? MYSQL MSSQL server Oracle DB Postgres DB Amazon AURORA Maria DB Q 221. How can you automate resource provisioning in AWS ? We can use the native service tool called AWS Cloud Formation for automation. It is also a good option to consider the third-party tools like Ansible, Chef, Puppet etc. to automate the services. Q 222. What is autoscaling & mentions some of its benefits ? Autoscaling is a service that automatically scales EC2 instance capacity out and in based on the criteria\u2019s that we are going to set. Autoscaling benefits its use for dynamic workloads like web spikes, retail shop flash sales, ticket booking system on the vacations etc., Q 223. What is the difference between S3 availability & durability ? Availability and durability are closely related to each other, but they are not the same. Availability refers to the uptime of the service i.e.., S3 storage system\u2019s uptime and can able to deliver the requests and data. Durability on the other hand, refers to the data that is stored should not suffer from degradation and corruption. Q 224. Mention some important features of S3 buckets ? Static web hosting Versioning Encryption Object lifecycle management Unlimited storage Q 225. What are all the measures that you take to protect the data in S3 ? lists and pre-signed Encrypt the data using Server-Side Encryption or Client-Side Encryption. Enable MFA delete to protect data against accidental deletion. Usage of access control URL\u2019s. Q 226. What is Elastic IP address ? Elastic IP address(EIP) is a static, internet routable address that is managed by the AWS platform. Each Elastic IP address are assigned to the Instances from a Pool of IP address in each region. Charges are applied once you allocate the EIP address no matter whether you associate the IP to an Instance or not. When you release the allocated IP Address, EIP will to returned to the pool. Q 227. You have a webserver running on an Amazon EC2 instances that is approaching 100% CPU utilization. Which option will reduce load on the Amazon EC2 instance and describe why ? We should create an Elastic load balancer with Autoscaling , and associate it with the EC2 instances. Layer 7 or Application layer Load balancers are used for this use case. ELB should be used because ELB can balance the incoming load across the EC2 resources. Q 228.what is CloudWatch and mention what can we do with it ? CloudWatch is native service used to monitor our resources and applications in the AWS cloud. CloudWatch does this by collecting information in the form of logs, metrics and events from the resources that we provisioned in the AWS environment. We can define alarms, troubleshoot issues using logs to optimize our infrastructure using CloudWatch. Q 229.How will you classify the cloud, based on the services ? We can classify the cloud computing platform into three types based on the services. Infrastructure As A Service. Platform As A Service. Software As A Service. Q 230.Name the messaging service available in AWS and point out a use case of it ? Simple Notification Services is a complete messaging service to deliver the messages end to end. It is shortly referred to as SNS. A real time use case would be a banking system where SNS will be sending a real time message (Email, SMS etc.,) to the end users who debits his account by withdrawing some amount of money. Q 231.Your company wants to use AWS for their newly designed analytics platform. They have got around 20 TB of data In the on-premises. They want to construct an analytics platform in AWS with this 20 TB of data for analysis. Once analysis is done they want to archive this data for best backup and recovery. What are the services that best matches this use case and say why ? Redshift would be the proper analytics platform which AWS provides. For data storage S3 is the ideal option and once data analytics is done, data must get moved to glacier for backup & Archival system. To do this data migration from s3 to glacier wee need to setup a lifecycle management policy in S3 to get moved to glacier. Q232.Your Relational database engine in AWS got crashes often when the traffic to your RDS instance is high. The Replica of the RDS instance is not promoted as master instance. What would you do to handle this situation ?? Under these circumstances, we need to choose a bigger RDS instance type for handling the huge amount of traffic. Creation of manual or automated snapshots is a must to recover from the disaster cases. Q 233.There is a production DB server running in a EC2 Linux instance which has a ext4 formatted EBS volumes/disks attached. The database is about to run out of storage space. How can you address this problem ? First, we need to increase the EBS volumes level to a consistent amount in the AWS management console. Next step we should use resize2fs command to use the provisioned space in the Operating system level because an increase in the EBS volumes doesn\u2019t guarantee the increase in the OS level. For this to happen we should consider increasing the provisioned space in the operating system level. Q 234.A company wants to migrate the on-premises servers to the AWS cloud platform. The company wants to estimate the cost of the machines that is going to get provisioned in the cloud. How would you proceed to determine the cost ? Perform a mapping of the on-premises server\u2019s cores and RAM to the nearest machine types in the AWS Cloud. Then use the online AWS pricing calculator to estimate the cost of the machines in the AWS Cloud. Q 235. A XYZ company is using AWS services for the past one month for its production servers. They have established a VPN connectivity from onpremises to AWS with a single IPSEC tunnel. During peak production hours, servers are not reachable in the AWS Cloud due to network problem. How would you mitigate this problem with minimal cost ? Considering the cost factor, we should first consider increasing the number if IPSEC tunnels that are used for the secure connectivity to AWS. If the problem persists even after increasing the tunnels, consider the other options for better a network. Q236: What is the Cloud Computing? Practice of using a network of the remote servers, hosted on the Internet to store, manage, and process data, Rather more than a local server or a personal computer is called Cloud Computing. Companies offering the computing services are called \u201ccloud providers\u201d and typically charge for cloud. Computing services based on the usage, similar to how you are billed for water or electricity at home. E.g.: AWS, AZURE, IBM BLUEMIX, GOOGLE CLOUD This cloud model is composed of the five essential characteristics, three service models and four deployment models. The primary reasons for the moving to the cloud are: \u2013 \uf0b7 It will never run out of the capacity, since it is a virtually infinite. \uf0b7 You can access your cloud-based on applications from anywhere, you just need a device which can Connect to the Internet. Q237:What is merits of the Cloud Computing? \uf0b7 Totally free from Maintenance i.e., You do not have to maintain or administer any infrastructurefor the same. \uf0b7 Lower Computing Cost. \uf0b7 Improved Performance. \uf0b7 Reduced Software Cost. \uf0b7 Instant Software Updates. \uf0b7 Unlimited Storage Capacity i.e., It will never run out of the capacity, since it is virtually infinite. \uf0b7 Increased Data Reliability. \uf0b7 Device Independence and the \u201calways on! Anywhere and any of place\u201d i.e., You can access your Cloud \u2013 based on applications from anywhere, you just need a device which can connect to the Internet.Cloud Computing is the fastest growing part of the network-based computing. It provides to tremendous.Benefits to customers of the all sizes: simple users, developers, enterprises and all types of organizations. Q238:What are the Cloud Computing? \uf0b7 Lower TCO. \uf0b7 Reliability, Scalability & Sustainability. \uf0b7 Secure Store Management. \uf0b7 Low Capital Expenditure. \uf0b7 Frees from Internal Resources. \uf0b7 Utility Based. \uf0b7 Easy & Agile Deployment. \uf0b7 Device & Location Independent. \uf0b7 24 * 7 Support. \uf0b7 Pay As You Use. Q239:What are the top 10 advantages of Cloud Computing? \uf0b7 Pay as you Go Model. \uf0b7 Increased Mobility. \uf0b7 Less or No CAPEX. \uf0b7 High Availability. \uf0b7 Easy to Manage. \uf0b7 High Productivity. \uf0b7 Environment Friendly. \uf0b7 Less Deployment Time. \uf0b7 Dynamic Scaling. \uf0b7 Shared Resources. Q240:What are the different layers (Service Models) of cloud computing? Cloud computing consists of the 3 layers in the hierarchy and these are as follows: 1. Infrastructure as a Service (IAAS) provides cloud infrastructure in terms of the hardware like memory, processor speed etc. 2. Platform as a Service (PAAS) provides cloud applications platform for the developers. 3. Software as a Service (SAAS) provides cloud applications which is used by the user directly without Installing anything on the system. Q241:How do disable Password-Based Logins for the Root in Amazon EC2 Instance? Using a fixed for the root password for a public AMI is a security risk that can be quickly become known. Even Relying on users to change the password after to the first login opens a small window of the opportunity for potential abuses. Following are the steps to disable password-based on remote logins for the root users. 1.Open the /etc/ssh/sshd config file with an text editor and locate to the following line: PermitRootLogin yes. \uf0c1 2.Change to the line to: PermitRoot Login without-password. Q242:How can I take an Snapshot of a RAID Array? Problem \u2013 Take an snapshot excludes data held in the cache by the applications and the OS. This tends not to matter on a single volume, however using a multiple volumes in the RAID Array, this can be a problem due to inter dependencies of arrays. Q243:What is the difference between Volume and Snapshot in the Amazon Web Services? In Amazon Web Services, a Volume is durables, block level storage can device that can be attached to a singles EC2 instance. In plain words it is like an hard disk on which we can be write or read from.A Snapshot is created by copying the data of volume to the another location at a specific time. We can even replicate samen of Snapshot to multiple availability zones. So, Snapshot is the single point in time view of a volume. We can create an Snapshot only when we have a Volumes. Also, from a Snapshot we can create an Volumes. In AWS, we have to pay for the storage that is used by Volume as well as the one used by a Snapshots. Q244:What happens if my application to stops responding to requests in beanstalk? AWS Beanstalk applications have an system in place for avoiding to failures in the underlying infrastructures. Q245:How to update AMI tools at the Boot Time? AWS is recommends that your AMIs downloads and upgrade to the Amazon EC2 AMI creation tools during the startup. This ensures that a new AMIs based on your shared AMIs have to the latest AMI tools. Q246:How to update AMI tools at the Boot Time on linux? Update to Amazon EC2 AMI tools \uf0c1 echo \u201d + Updating EC2 AMI tools\u201d yum update -y aws-amitools-ec2 echo \u201d + Updated EC2 AMI tools\u201d Q247:How does AWS Lambda to handle failure during event processing? In AWS Lambda we can run a function of synchronous or asynchronous modes. In synchronous mode, if AWS Lambda function is fails, then it will just give on the exception to the calling application. In asynchronous modes, if AWS Lambda function is fails then it will retry to the same function at least 3 times. If AWS Lambda is running in response to an event in the Amazon DynamoDB or Amazon Kinesis, then event will be retried till that Lambda function succeeds or the data expires. In DynamoDB or Kinesis, AWS maintains datas for at least 24 hours. Q248:What are the Storage of classes of Amazon? \uf0b7 Amazon S3 \uf0b7 Scalable Storage in Cloud \uf0b7 Amazon EBS \uf0b7 Block Storage for EC2 \uf0b7 AWS Elastic File System \uf0b7 Managed File Storage for EC2 \uf0b7 Amazon Glacier \uf0b7 Low-cost Achieve Storage in the \uf0b7 cloud \uf0b7 AWS Storage Gateway \uf0b7 Hybrid Storage Integration \uf0b7 Amazon Snowball \uf0b7 Petabyte-Scale Data Transport \uf0b7 AWS Snowball Edge \uf0b7 Petabyte-scale Data to Transport with \uf0b7 On-Demand Compute \uf0b7 AWS Snowmobile \uf0b7 Exabyte-scale Data to Transport Q249:How do Encryption is done in S3? \uf0b7 In Transit: SSL/TLS \uf0b7 At Rest \uf0b7 Server-Side in Encryption \uf0b7 S3 Managed Keys \u2013 SSE-S3 \uf0b7 AWS Key Management Service, Managed of Keys \u2013 SSE-KMS \uf0b7 6.Server-Side Encryption with Customer Provided Keys \u2013 SSE-C \uf0b7 Client-Side Encryptions Q250:How will do upload a file greater than 100 megabytes in Amazon S3? Amazon S3 supports of storing objects or files up to 5 terabytes. To upload an file greater than 100 megabytes, we have to use of Multipart upload utility from AWS. By using Multipart upload we can upload an large file in multiple parts. Each part will be independently to be uploaded. It doesn\u2019t matter in what order to each part is uploaded. It even to supports uploading these parts of parallel to decrease overall time. Once of all the parts are uploaded, this utility makes a these as one single objects or file from which the parts were do created. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"AWS Interview Questions- 2nd"},{"location":"nightwolf-cotribution/aws-4/#aws-interview-questions-answers-4","text":"These are AWS interview questions for experienced professionals. You will find these questions very helpful in your AWS professional role interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. How many Elastic IP address can be associated with a single account? Ans: 5 What is the name to the additional network interfaces that can be created and attached to any Amazon EC2 instance in your VPC? Ans: Elastic Network Interface You have configured ELB with three instances connected to that. If your instances are unhealthy or terminated, the traffic should be automatically replaced to another instance, what type of service can be used to achieve this requirement? Ans: Fault Tolerance After configuring ELB, you need to ensure that the user requests are always attached to a single instance. What setting can you use? Ans: Sticky session Which of the following metrics cannot have a cloud watch alarm? EC2 instance status check failed EC2 CPU utilization RRS lost object Auto scaling group CPU utilization Ans: RRS lost object Which of the below mentioned service is provided by Cloud watch? Monitor estimated AWS usage Monitor EC2 log files Monitor S3 storage Monitor AWS calls using Cloud trail Ans: Monitor estimated AWS usage A user has Launched an EC2 instance which of the below mentioned statements is not true respect to instance addressing? The private IP addresses are not reachable from the internet The user can communicate using the private IP across regions The private IP address and pubic IP address for an instance are directly mapped to each other using NAT The private IP address for the instance is assigned using DHCP Ans: The user can communicate using the private IP across regions Which of the following service provides the edge \u2013 storage or content delivery system that caches data at different locations? Amazon RDS Simple DB Amazon Cloud Front Amazon associates web services Ans: Amazon Cloud Front A user is launching an instance under the free usage tier from the AMI with a snapshot size of 50 GB. How can the user launch the instance under the free usage tier? Launch a micro instance Launch a micro instance, but in the EBS configuration modify the size of EBS to 50 GB. Launch a micro instance, but do not store the data of more than 30 GB on the EBS storage. It is not possible to have this instance under the free usage tier Ans: It is not possible to have this instance under the free usage tier What are the possible connection issues you can face while connecting to your instance? Connection timed out Server refused our key No supported authentication methods available All of the above Ans: All of the above You are enabled sticky session with ELB. What does it do with your instance? Routes all the requests to a single DNS Binds the user session with a specific instance Binds the user IP with a specific session Provides a single ELB DNS for each IP address Ans: Binds the user session with a specific instance Q137. Which is a main email platform that provides an easy, cost effective way for you to send compliance and receive a response using your own email address and domains? SES SNS SQS SAS Ans: SES Q138. Which type of load balancer makes routing decisions at either the transport layer or the application layer and supports either EC2 or VPC. Application Load Balancer Classic Load Balancer Primary Load Balancer Secondary Load Balancer Ans: Classic Load Balancer Q139. AWS Cloud Front has been configured to handle the customer requests to the web server launched in Linux machine. How many requests per second can Amazon Cloud Front handle? 1000 100 10000 There is no such limit Ans: There is no such limit Q140. You are going to launched one instance with security group. While configuring security group, what are the things you have to select? Protocol and type Port Source All of the above Ans: Source Q141. Which is virtual network interface that you can attach to an instance in a VPC? Elastic IP AWS Elastic Interface Elastic Network Interface AWS Network ACL Ans: Elastic Network Interface Q142. You have launched a Linux instance in AWS EC2. While configuring security group, you have selected SSH, HTTP, HTTPS protocol. Why do we need to select SSH? To verity that there is a rule that allows traffic from your computer to port 22 To verify that there is a rule that allows traffic from EC2 Instance to your computer Allows web traffic from instance to your computer Allows web traffic from your computer to EC2 instance Ans: To verify that there is a rule that allows traffic from EC2 Instance to your computer Q143. You need to quickly set up an email service because a client needs to start using it in the next hour. Amazon service seems to be the logical choice but there are several options available to set it up. Which of the following options to set up AWS service would best meet the needs of the client? Amazon SES console AWS Cloud Formation SMTP interface AWS Elastic Beanstalk Ans: Amazon SES console Q144.You have chosen a windows instance with Classic and you want to make some change to the security group. How will these changes be effective? Security group rules cannot be changed Changes are automatically applied to windows instances Changes will be effective after rebooting the instance in that security group Changes will be effective after 24-hours Ans: Changes are automatically applied to windows instances Q145. Load Balancer and DNS service comes under which type of cloud service? IAAS-Network IAAS-Computational IAAS-Storage None of the above Ans: IAAS-Storage Q146. You have an EC2 instance that has an unencrypted volume. You want to create another encrypted volume from this unencrypted volume. Which of the following steps can achieve this? Just simply create a copy of the unencrypted volume, you will have the option to encrypt the volume. Create a snapshot of the unencrypted volume and then while creating a volume from the snapshot you can encrypt it Create a snapshot of the unencrypted volume (applying encryption parameters), copy the snapshot and create a volume from the copied snapshot This is not possible, once a volume is unencrypted, there is no way to create an encrypted volume from this Ans: Create a snapshot of the unencrypted volume (applying encryption parameters), copy the snapshot and create a volume from the copied snapshot Q147. Where does the user specify the maximum number of instances with the auto scaling commands? Auto scaling Launch Config Auto scaling group Auto scaling policy Auto scaling size Ans: Auto scaling Launch Config Q148. A user is identify that a huge data download is occurring on his instance he has already set the auto scaling policy to increase the instance count when the network Input Output increase beyond a threshold limits how can the user ensure that this temporary event does not result in scaling The network I/O are not affecting during data download The policy cannot be set on the network I/O There is no way the can stop scaling as it already configured Suspend scaling Ans: Suspend scaling Q149. Which are the types of AMI provided by AWS? EBS Backed Instance Store backed None its volume type and not AMI types Both A and B Ans: Both A and B AWS Interview Questions and Answers for Freshers Q150. What is the significance of forming Subnets? A. Because, not enough hosts B. To manage small number of hosts C. To utilize the Volume available across different subnets D. Smartly utilize network that have large number of hosts The answer is: D Q2: If you want to launch your instance on a single-tenancy platform, which option you would select against Instance Tenancy Attribute parameter? A. One to one B. Sole Owner C. Dedicated D. Reserved The answer is: C Q151. Which AWS Service you would use to transfer objects from your data center, when you are using Amazon CloudFront? A. AWS CloudWatch B. AWS SNS Service C. AWS SMS Service D. AWS Direct Connect The answer is: D Q152 _ _ _ is a fully managed Data Warehouse service from AWS? A. Amazon Redshift B. Amazon Neptune C. Amazon Aurora D. Amazon DynamoDB The answer is: A Q153. Which of the following statements are applicable to AWS Elastic File System(EFS)? A. EFS provides simple, scalable file storage for use with Amazon EC2 B. EFS with MS-Windows based EC2 instances is not supported C. EFS supports the Network File System version 4 protocol D. All of the above The answer is: D Q154. What is the role of Connection Draining? A. Helps to launch an EC2 instance B. Automatically terminates instances which are not in use C. Establishes connection between EC2 and RDS instances D. Auto Scaling wait for outstanding requests to complete before terminating instances when CD is enabled The answer is: D Q155. What is the use of Lambda? A. Lambda is used for running server-less applications B. It is a testing tool from AWS C. It is a database service from AWS D. It is an Anti Virus software from AWS The answer is: A Q156. What is Application Load Balancing? A. It is a feature of Elastic Load Balancing B. Use to distribute traffic to different Target Groups C. It is a service generating Elastic IPs for AWS customers D. It is a kind of Firewall The answers are: A and B Q157. What are the uses of Elastic Beanstalk? A. Quickly deploy and manage applications in the AWS Cloud B. Supports Java, .NET, Node.js, PHP, Python applications C. It is an Application Server from AWS D. Use to deploy only Java-Beans applications The answers are: A and B Q158. Can you connect your company\u2019s datacenter to the Amazon Cloud network? A. Not possible B. You can connect thru a Dedicated N/W line C. By establishing a Virtual Private Network (VPN) between your datacenter and VPC D. Connect with a hotline The answer is: C Q159. You have commissioned PRIVATE servers in your premises. You also distributed some of your workloads with the PUBLIC cloud. What type of architecture is this? A. Virtual Private Cloud B. Community Cloud C. Public Cloud D. Hybrid Cloud The answer is: D Q160. DynamoDB _ _ _ . Which one of the following is true regarding DynamoDB? A. Manages Notification Service B. Stores Metadata C. Manages Queue Service D. None of the above The answer is: B Q161: What are the significances of AWS CloudTrail? A. Takes care of Message Queuing Service B. It enables governance, compliance, operational auditing and risk auditing of your AWS account. C. Used as a database service D. It provides an event history of your AWS account activities The answers are: B and D Q162. Which one is a global Content Delivery Network service that securely delivers data, videos, applications, and APIs to your viewers with low latency and high transfer speeds? A. Amazon CloudWatch B. Amazon CloudFront C. Amazon CloudTrail D. Amazon VPC The answer is: B Q163. Is AWS offering Reserved Instances facility for Multiple-Subnet deployments? A. Yes, available for all kind of instances B. No, available only for Dedicated Tenancy C. Offering only for LINUX based instances D. None of the above The answer is: A Q164. Select the correct statement from the below: A. You can have multiple ACLs for a subnet B. Security Group is not necessary for an EC2 instance C. You can attach multiple Zones/Subnets to a Route Table D. You can create S3 bucket using AWS AMI templates The answer is: C Q165. Name the AWS DB Service which is Server-Less and NoSQL DB which delivers consistent single-digit millisecond latency at any scale? A. Amazon Redshift B. Amazon Neptune C. Amazon Aurora D. Amazon DynamoDB The answer is: D Q166. Is this advisable to keep your Standby-Database instance in the same zone where your primary instance is running? A. Yes, you can keep B. Possible only for MySQL instance C. No, not recommended for any kind of DB instance D. Recommended only for MS-SQL instance The answer is: C Q167. Can objects in S3 be delivered by Amazon CloudFront? A. Yes, you can place any objects in S3 which CloudFront quickly delivers B. CloudFront delivers only movie type objects C. No, S3 cannot be integrated with CloudFront D. Amazon VPC will deliver the objects The answer is: A Q168. What you should do if you want to launch an EC2 instance with a pre-allocated private IP address? A. Launch it in a Subnet Group B. Launch the instance from a Private AMI C. Assign EIP address to that instance D. Launch that instance in AWS VPC cloud The answer is: D Q169. Can you edit a Security Group (SG) rules when it is used by multiple EC2 instances? Will new rules apply to all previously running EC2 instances? A. No, you cannot edit a SG when used by a EC2 instance B. Yes, you can edit. Immediately apply to all instances. C. You can edit only the Outbound rules D. Only Outbound rules apply to all EC2 instances The answer is: B Q170. Which of the following statements are true with Route 53? A. Amazon Route 53 is a scalable and highly available Domain Name System (DNS) B. Amazon Route 53 is fully compliant with IPv6 as well C. Will automatically configure DNS settings for your domains D. Route 53 provides low latency database service The answers are: A,B and C Q171. What is a Virtual Private Cloud (VPC)? A. VPC enables you to launch AWS resources into a virtual network B. VPC is a virtual network dedicated to your AWS account C. VPC is used to create domain name for your organization D. VPC can also be connected to your own office data center The answers are: A,B and D Q172. What is an Elastic IP? A. There is no such IP. Only public & private IPs are valid. B. Used in Elastic Load Balancing C. An Elastic IP address is a static IPv4 address D. An Elastic IP address is for use in a specific region only The answers are: C and D Q173. _ _ _ is a fully managed in-memory data store service offered by Amazon Web Services (AWS)? A. Amazon Neptune B. Amazon Redshift C. Amazon ElastiCache D. Amazon Aurora The answer is: C Q174. In AWS which service is used to create Domain Name for their customers? A. Amazon CloudWatch B. Amazon Route53 C. Amazon CloudDomain D. Amazon VPC The answer is: B Q175. Which one is a valid statement regarding EBS-Volumes? A. You can attach maximum of 5 volumes to an instance B. You can attach multiple instances to one volume C. You can attach multiple volumes to a single EC2 instance D. You cannot attach a additional volume to an instance The answer is: C Q176. Which one is a valid statement regarding EBS-Snapshots? A. You can access Snapshots thru S3 APIs B. You can store your Snapshots in a S3 BUCKET C. Snapshots are available only thru EC2 instances D. You can access your Snapshots thru VPC APIs The answer is: C Q178. Which one is the valid scenario? A. Creating PEERING connection to a VPC in a Different Region B. Creating PEERING connection between VPCs in Same Region C. Attaching VOLUME in one subnet/zone with EC2 instance in another subnet/zone D. Keeping your primary db and secondary db in the same zone The answer is: B Q179. How do you connect a VPC to your Office Datacenter? A. By keeping AWS VPC and Office Datacenter in same IP range B. Establishing VPN connection between VPC and Datacenter C. Establishing a dedicated hotlink between VPC and Datacenter D. You cannot connect VPC and your Datacenter The answer is: B Q180. Choose the valid scenarios regarding VPC? A. You can delete the Default VPC available in your region B. VPC can span across multiple Availability Zones C. Trying to launch an instance without having VPC in a region D. Launching an instance onto a VPC created by you The answers are: A,B and D Q181. How the EC2 instances inside a VPC directly access the internet? A. With the help of instance\u2019s Public IP B. By attaching a Elastic IP to that instance C. Internet Gateway enables the access to the internet D. With the help of Route Table The answer is: C Q182. Which one is the highly secured design? A. Keeping both EC2 and Database instances in a public subnet B. Keep EC2 in public subnet and Database in private subnet C. Keep EC2 in public subnet and Database in a S3 bucket D. Defining ANYWHERE in the DB security group INBOUND rule The answer is: B Q183. Keeping your instance in a public subnet and database in a private subnet. What type of cloud deployment model is this? A. Community Cloud B. Private Cloud C. Public Cloud D. Hybrid Cloud The answer is: D Q184. Which service distribute the contents from Edge Locations to the end users to reduce the latency? A. Amazon CloudWatch B. Amazon CloudTrail C. Amazon CloudFront D. Amazon PushData The answer is: C Q185. I am a cloud web service used for hosting your application. Who am I? A. AWS Route 53 B. AWS VPC C. AWS S3 D. AWS EC2 The answer is: D Q186. You can add _ _ to your Auto Scaling group so that you can perform custom actions when instances launch or terminate. A. CloudWatch B. CloudTrail C. Load Balancer D. Lifecycle Hooks The answer is: D Q187. What is Auto Scaling? A. Accelerating VPC Speed B. Creating/Terminating duplicate instances using Scale IN/OUT C. Automating backup/restore service D. None of the above The answer is: B Q188. You want complex querying capabilities but don\u2019t want data warehouse. Which database service you would choose? A. Amazon DynamoDB B. Amazon Redshift C. Amazon RDS D. Amazon ElastiCache The answer is: C Q189. What is an Availability Zone? A. A Container where all your S3 buckets are stored B. Denotes an Entire Region C. A location inside a Region which is protected from failures D. Collection of Regions The answer is: C Q190. The cloud infrastructure is shared by several organizations and supports specific group that has shared concerns. Government departments, universities, central banks etc. often find this type of cloud useful. What kind of cloud deployment model is this? A. Private Cloud B. Hybrid Cloud C. Community Cloud D. Public Cloud The answer is: C Q191. How many Buckets you can create in S3? A. 150 B. 250 C. 500 D. 100 The answer is: D Q192. What is the maximum size of a S3 Bucket? A. 3 Terabytes B. 10 Terabytes C. 5 Terabytes D. 7 Terabytes The answer is: C Q193. Which service of Amazon AWS is used to host a static website? A. Amazon Simple Storage Service(S3) B. Amazon CloudFront C. Amazon Route53 D. Amazon CloudWatch The answer is: A Q194. Which of the following is not a Part of Security groups? A. List of Protocols B. List of Users C. Ports D. IP Address The answer is: B Q195. A data transport solution that accelerates moving terabytes to petabytes of data into and out of AWS using storage devices designed to be secure for physical transport. Name this solution. A. Amazon EFS B. Amazon S3 C. Amazon Glacier D. Amazon Snowball The answer is: D Q196. What type of IP address do you use for your CGW (Customer Gateway) address? A. You will use PRIVATE IP address of your NAT device B. You will use PUBLIC IP address of your NAT device C. You will use ELASTIC IP address of your NAT device D. You will use VPN The answer is: B Q197. How many subnets you can have per VPC? A. 100 B. 300 C. 250 D. 200 The answer is: D Q198. I have a REST API interface and uses secure HMAC-SHA1 authentication keys. I am also a data storage system. Who am I? A. SS3 B. Elastic Block Store C. S3 D. Snapshots The answer is: C Q199. I am a structured data store. I support indexing and data queries to both EC2 and S3. Who am I? A. DynamoDB B. SimpleDB C. MySQL D. Aurora The answer is: B Q200. How many Elastic IP address can be associated with a single account? A) 4 B) 10 C) 5 D) None the above Q201. After configuring ELB, you need to ensure that the user requests are always attached to a single instance. What setting can you use? A) Session cookie B) Cross one load balancing C) Connection drainage D) Sticky session Q202. Which of the following metrics cannot have a cloud watch alarm? A) EC2 instance status check failed B) EC2 CPU utilization C) RRS lost object D) Auto scaling group CPU utilization Q203. Which of the below mentioned service is provided by Cloud watch? A) Monitor estimated AWS usage B) Monitor EC2 log files C) Monitor S3 storage D) Monitor AWS calls using Cloud trail Q204. Which of the following service provides the edge \u2013 storage or content delivery system that caches data at different locations? A) Amazon RDS B) Simple DB C) Amazon Cloud Front D) Amazon associates web services Q205. What are the possible connection issues you can face while connecting to your instance? A) Connection timed out B) Server refused our key C) No supported authentication methods available D) All of the above Q206. You are enabled sticky session with ELB. What does it do with your instance? A) Routes all the requests to a single DNS B) Binds the user session with a specific instance C) Binds the user IP with a specific session D) Provides a single ELB DNS for each IP address Q207. Which is an email platform that provides an easy, cost effective way for you to send and receive email using your own email address and domains? A) SES B) SNS C) SQS D) SAS Q208. AWS Cloud Front has been configured to handle the customer requests to the web server launched in Linux machine. How many requests per second can Amazon Cloud Front handle? A) 1000 B) 100 C) 10000 D) There is no such limit Q209. Which is virtual network interface that you can attach to an instance in a VPC? A) Elastic IP B) AWS Elastic Interface C) Elastic Network Interface D) AWS Network ACL Q210. You have launched an instance in EC2-Classic and you want to make some change to the security group rule. How will these changes be effective? A) Security group rules cannot be changed B) Changes are automatically applied to all instances that are associated with the security group C) Changes will be effective after rebooting the instance in that security group D) Changes will be effective after 24-hours Q211. Load Balancer and DNS service comes under which type of cloud service? A) IAAS-Network B) IAAS-Computational C) IAAS-Storage D) None of the above Q212. You have an EC2 instance that has an unencrypted volume. You want to create another encrypted volume from this unencrypted volume. Which of the following steps can achieve this? A) Just simply create a copy of the unencrypted volume, you will have the option to encrypt the volume. B) Create a snapshot of the unencrypted volume and then while creating a volume from the snapshot you can encrypt it C) Create a snapshot of the unencrypted volume (applying encryption parameters), copy the snapshot and create a volume from the copied snapshot D) This is not possible, once a volume is unencrypted, there is no way to create an encrypted volume from this Q213. Where does the user specify the maximum number of instances with the auto scaling commands? A) Auto scaling Launch Config B) Auto scaling group C) Auto scaling policy D) Auto scaling size Q214. A user is aware that a huge download is occurring on his instance he has already set the auto scaling policy to increase the instance count when the network I/O increase beyond a certain limits how can the user ensure that this temporary event does not result in scaling A) The network I/O are not affecting during data download B) The policy cannot be set on the network I/O C) There is no way the can stop scaling as it already configured D) Suspend scaling Q.215.Which are the types of AMI provided by AWS? A) EBS Backed B) Instance Store backed C) None its volume type and not AMI types D) Both A and B Q 216. Name some cloud service providers for public & private cloud ? Public: Amazon web services, Microsoft Azure, Google Cloud, Oracle Cloud, Alibaba Cloud. Private: Redhat-Openstack, Rackspace, VMware, IBM Private Cloud. Q 217. What are all the different Instance categories based on pricing and explain them briefly ? On-demand Instances: On-demand instances are the virtual servers that are provisioned by AWS EC2 service at an hourly price basis. Reserved Instances: Instances which are reserved for a time, 1 year or 3 years , is called reserved Instances. Hourly prices are reduced significantly compared to on-demand Instances with reservation. Spot Instances: Spot Instances are the special instance category where you request the unused resources of EC2 from the datacenter for steep discounts. Spot prices are fixed by AWS EC2 and you need to bid the spot price more than the pricing of AWS EC2. Q 218. I have some private servers on my premises, also I have distributed some of my workload on the public cloud, what is this architecture called ? Hybrid Cloud Q 219. What is the difference between S3 and Glacier storage ? S3 is a simple storage service, which is used to store and retrieve data. We can store any amount of data and any type of data. Data that we are storing here are referred as objects. Whereas the Glacier storage is an archival store which is used to store infrequently accessed data or cold data. Major use case of glacier is data archiving and backup. Q 220. Name some Database engines available natively in RDS services ? MYSQL MSSQL server Oracle DB Postgres DB Amazon AURORA Maria DB Q 221. How can you automate resource provisioning in AWS ? We can use the native service tool called AWS Cloud Formation for automation. It is also a good option to consider the third-party tools like Ansible, Chef, Puppet etc. to automate the services. Q 222. What is autoscaling & mentions some of its benefits ? Autoscaling is a service that automatically scales EC2 instance capacity out and in based on the criteria\u2019s that we are going to set. Autoscaling benefits its use for dynamic workloads like web spikes, retail shop flash sales, ticket booking system on the vacations etc., Q 223. What is the difference between S3 availability & durability ? Availability and durability are closely related to each other, but they are not the same. Availability refers to the uptime of the service i.e.., S3 storage system\u2019s uptime and can able to deliver the requests and data. Durability on the other hand, refers to the data that is stored should not suffer from degradation and corruption. Q 224. Mention some important features of S3 buckets ? Static web hosting Versioning Encryption Object lifecycle management Unlimited storage Q 225. What are all the measures that you take to protect the data in S3 ? lists and pre-signed Encrypt the data using Server-Side Encryption or Client-Side Encryption. Enable MFA delete to protect data against accidental deletion. Usage of access control URL\u2019s. Q 226. What is Elastic IP address ? Elastic IP address(EIP) is a static, internet routable address that is managed by the AWS platform. Each Elastic IP address are assigned to the Instances from a Pool of IP address in each region. Charges are applied once you allocate the EIP address no matter whether you associate the IP to an Instance or not. When you release the allocated IP Address, EIP will to returned to the pool. Q 227. You have a webserver running on an Amazon EC2 instances that is approaching 100% CPU utilization. Which option will reduce load on the Amazon EC2 instance and describe why ? We should create an Elastic load balancer with Autoscaling , and associate it with the EC2 instances. Layer 7 or Application layer Load balancers are used for this use case. ELB should be used because ELB can balance the incoming load across the EC2 resources. Q 228.what is CloudWatch and mention what can we do with it ? CloudWatch is native service used to monitor our resources and applications in the AWS cloud. CloudWatch does this by collecting information in the form of logs, metrics and events from the resources that we provisioned in the AWS environment. We can define alarms, troubleshoot issues using logs to optimize our infrastructure using CloudWatch. Q 229.How will you classify the cloud, based on the services ? We can classify the cloud computing platform into three types based on the services. Infrastructure As A Service. Platform As A Service. Software As A Service. Q 230.Name the messaging service available in AWS and point out a use case of it ? Simple Notification Services is a complete messaging service to deliver the messages end to end. It is shortly referred to as SNS. A real time use case would be a banking system where SNS will be sending a real time message (Email, SMS etc.,) to the end users who debits his account by withdrawing some amount of money. Q 231.Your company wants to use AWS for their newly designed analytics platform. They have got around 20 TB of data In the on-premises. They want to construct an analytics platform in AWS with this 20 TB of data for analysis. Once analysis is done they want to archive this data for best backup and recovery. What are the services that best matches this use case and say why ? Redshift would be the proper analytics platform which AWS provides. For data storage S3 is the ideal option and once data analytics is done, data must get moved to glacier for backup & Archival system. To do this data migration from s3 to glacier wee need to setup a lifecycle management policy in S3 to get moved to glacier. Q232.Your Relational database engine in AWS got crashes often when the traffic to your RDS instance is high. The Replica of the RDS instance is not promoted as master instance. What would you do to handle this situation ?? Under these circumstances, we need to choose a bigger RDS instance type for handling the huge amount of traffic. Creation of manual or automated snapshots is a must to recover from the disaster cases. Q 233.There is a production DB server running in a EC2 Linux instance which has a ext4 formatted EBS volumes/disks attached. The database is about to run out of storage space. How can you address this problem ? First, we need to increase the EBS volumes level to a consistent amount in the AWS management console. Next step we should use resize2fs command to use the provisioned space in the Operating system level because an increase in the EBS volumes doesn\u2019t guarantee the increase in the OS level. For this to happen we should consider increasing the provisioned space in the operating system level. Q 234.A company wants to migrate the on-premises servers to the AWS cloud platform. The company wants to estimate the cost of the machines that is going to get provisioned in the cloud. How would you proceed to determine the cost ? Perform a mapping of the on-premises server\u2019s cores and RAM to the nearest machine types in the AWS Cloud. Then use the online AWS pricing calculator to estimate the cost of the machines in the AWS Cloud. Q 235. A XYZ company is using AWS services for the past one month for its production servers. They have established a VPN connectivity from onpremises to AWS with a single IPSEC tunnel. During peak production hours, servers are not reachable in the AWS Cloud due to network problem. How would you mitigate this problem with minimal cost ? Considering the cost factor, we should first consider increasing the number if IPSEC tunnels that are used for the secure connectivity to AWS. If the problem persists even after increasing the tunnels, consider the other options for better a network. Q236: What is the Cloud Computing? Practice of using a network of the remote servers, hosted on the Internet to store, manage, and process data, Rather more than a local server or a personal computer is called Cloud Computing. Companies offering the computing services are called \u201ccloud providers\u201d and typically charge for cloud. Computing services based on the usage, similar to how you are billed for water or electricity at home. E.g.: AWS, AZURE, IBM BLUEMIX, GOOGLE CLOUD This cloud model is composed of the five essential characteristics, three service models and four deployment models. The primary reasons for the moving to the cloud are: \u2013 \uf0b7 It will never run out of the capacity, since it is a virtually infinite. \uf0b7 You can access your cloud-based on applications from anywhere, you just need a device which can Connect to the Internet. Q237:What is merits of the Cloud Computing? \uf0b7 Totally free from Maintenance i.e., You do not have to maintain or administer any infrastructurefor the same. \uf0b7 Lower Computing Cost. \uf0b7 Improved Performance. \uf0b7 Reduced Software Cost. \uf0b7 Instant Software Updates. \uf0b7 Unlimited Storage Capacity i.e., It will never run out of the capacity, since it is virtually infinite. \uf0b7 Increased Data Reliability. \uf0b7 Device Independence and the \u201calways on! Anywhere and any of place\u201d i.e., You can access your Cloud \u2013 based on applications from anywhere, you just need a device which can connect to the Internet.Cloud Computing is the fastest growing part of the network-based computing. It provides to tremendous.Benefits to customers of the all sizes: simple users, developers, enterprises and all types of organizations. Q238:What are the Cloud Computing? \uf0b7 Lower TCO. \uf0b7 Reliability, Scalability & Sustainability. \uf0b7 Secure Store Management. \uf0b7 Low Capital Expenditure. \uf0b7 Frees from Internal Resources. \uf0b7 Utility Based. \uf0b7 Easy & Agile Deployment. \uf0b7 Device & Location Independent. \uf0b7 24 * 7 Support. \uf0b7 Pay As You Use. Q239:What are the top 10 advantages of Cloud Computing? \uf0b7 Pay as you Go Model. \uf0b7 Increased Mobility. \uf0b7 Less or No CAPEX. \uf0b7 High Availability. \uf0b7 Easy to Manage. \uf0b7 High Productivity. \uf0b7 Environment Friendly. \uf0b7 Less Deployment Time. \uf0b7 Dynamic Scaling. \uf0b7 Shared Resources. Q240:What are the different layers (Service Models) of cloud computing? Cloud computing consists of the 3 layers in the hierarchy and these are as follows: 1. Infrastructure as a Service (IAAS) provides cloud infrastructure in terms of the hardware like memory, processor speed etc. 2. Platform as a Service (PAAS) provides cloud applications platform for the developers. 3. Software as a Service (SAAS) provides cloud applications which is used by the user directly without Installing anything on the system. Q241:How do disable Password-Based Logins for the Root in Amazon EC2 Instance? Using a fixed for the root password for a public AMI is a security risk that can be quickly become known. Even Relying on users to change the password after to the first login opens a small window of the opportunity for potential abuses. Following are the steps to disable password-based on remote logins for the root users. 1.Open the /etc/ssh/sshd config file with an text editor and locate to the following line:","title":"AWS Interview Questions &amp; Answers - 4"},{"location":"nightwolf-cotribution/aws-4/#permitrootlogin-yes","text":"2.Change to the line to: PermitRoot Login without-password. Q242:How can I take an Snapshot of a RAID Array? Problem \u2013 Take an snapshot excludes data held in the cache by the applications and the OS. This tends not to matter on a single volume, however using a multiple volumes in the RAID Array, this can be a problem due to inter dependencies of arrays. Q243:What is the difference between Volume and Snapshot in the Amazon Web Services? In Amazon Web Services, a Volume is durables, block level storage can device that can be attached to a singles EC2 instance. In plain words it is like an hard disk on which we can be write or read from.A Snapshot is created by copying the data of volume to the another location at a specific time. We can even replicate samen of Snapshot to multiple availability zones. So, Snapshot is the single point in time view of a volume. We can create an Snapshot only when we have a Volumes. Also, from a Snapshot we can create an Volumes. In AWS, we have to pay for the storage that is used by Volume as well as the one used by a Snapshots. Q244:What happens if my application to stops responding to requests in beanstalk? AWS Beanstalk applications have an system in place for avoiding to failures in the underlying infrastructures. Q245:How to update AMI tools at the Boot Time? AWS is recommends that your AMIs downloads and upgrade to the Amazon EC2 AMI creation tools during the startup. This ensures that a new AMIs based on your shared AMIs have to the latest AMI tools. Q246:How to update AMI tools at the Boot Time on linux?","title":"PermitRootLogin yes."},{"location":"nightwolf-cotribution/aws-4/#update-to-amazon-ec2-ami-tools","text":"echo \u201d + Updating EC2 AMI tools\u201d yum update -y aws-amitools-ec2 echo \u201d + Updated EC2 AMI tools\u201d Q247:How does AWS Lambda to handle failure during event processing? In AWS Lambda we can run a function of synchronous or asynchronous modes. In synchronous mode, if AWS Lambda function is fails, then it will just give on the exception to the calling application. In asynchronous modes, if AWS Lambda function is fails then it will retry to the same function at least 3 times. If AWS Lambda is running in response to an event in the Amazon DynamoDB or Amazon Kinesis, then event will be retried till that Lambda function succeeds or the data expires. In DynamoDB or Kinesis, AWS maintains datas for at least 24 hours. Q248:What are the Storage of classes of Amazon? \uf0b7 Amazon S3 \uf0b7 Scalable Storage in Cloud \uf0b7 Amazon EBS \uf0b7 Block Storage for EC2 \uf0b7 AWS Elastic File System \uf0b7 Managed File Storage for EC2 \uf0b7 Amazon Glacier \uf0b7 Low-cost Achieve Storage in the \uf0b7 cloud \uf0b7 AWS Storage Gateway \uf0b7 Hybrid Storage Integration \uf0b7 Amazon Snowball \uf0b7 Petabyte-Scale Data Transport \uf0b7 AWS Snowball Edge \uf0b7 Petabyte-scale Data to Transport with \uf0b7 On-Demand Compute \uf0b7 AWS Snowmobile \uf0b7 Exabyte-scale Data to Transport Q249:How do Encryption is done in S3? \uf0b7 In Transit: SSL/TLS \uf0b7 At Rest \uf0b7 Server-Side in Encryption \uf0b7 S3 Managed Keys \u2013 SSE-S3 \uf0b7 AWS Key Management Service, Managed of Keys \u2013 SSE-KMS \uf0b7 6.Server-Side Encryption with Customer Provided Keys \u2013 SSE-C \uf0b7 Client-Side Encryptions Q250:How will do upload a file greater than 100 megabytes in Amazon S3? Amazon S3 supports of storing objects or files up to 5 terabytes. To upload an file greater than 100 megabytes, we have to use of Multipart upload utility from AWS. By using Multipart upload we can upload an large file in multiple parts. Each part will be independently to be uploaded. It doesn\u2019t matter in what order to each part is uploaded. It even to supports uploading these parts of parallel to decrease overall time. Once of all the parts are uploaded, this utility makes a these as one single objects or file from which the parts were do created. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Update to Amazon EC2 AMI tools"},{"location":"nightwolf-cotribution/aws/","text":"AWS Certified SysOps Administrator - Questions and Answers \uf0c1 These are AWS interview questions for experienced professionals. You will find these questions very helpful in your AWS professional role interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 1 You are currently hosting multiple applications in a VPC and have logged numerous port scans coming in from a specific IP address block. Your security team has requested that all access from the offending IP address block be denied tor the next 24 hours. Which of the following is the best method to quickly and temporarily deny access from the specified IP address block? A. Create an AD policy to modify Windows Firewall settings on all hosts in the VPC to deny access from the IP address block B. Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP address block C. Add a rule to all of the VPC 5 Security Groups to deny access from the IP address block D. Modify the Windows Firewall settings on all Amazon Machine Images (AMIs) that your organization uses in that VPC to deny access from the IP address block Answer: C Reference: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html QUESTION NO: 2 When preparing for a compliance assessment of your system built inside of AWS. what are three best practices for you to prepare for anaudit? Choose any 3 answers. A. Gather evidence of your IT operational controls B. Request and obtain applicable third-party audited AWS compliance reports and certifications C. Request and obtain a compliance and security tour of an AWS data center for a pre-assessment security review D. Request and obtain approval from AWS to perform relevant network scans and indepth penetration tests of your system and endpoints E. Schedule meetings with AWS third-party auditors to provide evidence of AWS compliance that maps to your control objectives Answer: B,D,E QUESTION NO: 3 You have started a new job and are reviewing your company infrastructure on AWS You notice one web application where they have an Elastic Load Balancer (&B) in front of web instances in an Auto Scaling Group. When you check the metrics for the ELB in CloudWatch you see four healthy instances In Availability Zone (AZ) A and zero in AZ B There are zero unhealthy instances. What do you need to fix to balance the instances across AZs? A. Set the ELB to only be attached to another AZ B. Make sure Auto Scaling is configured to launch in both AZs C. Make sure your AMI is available in both AZs D. Make sure the maximum size of the Auto Scaling Group is greater than 4 Answer: B QUESTION NO: 4 You have been asked to leverage Amazon VPC BC2 and SOS to implement an application that submits and receives millions of messages per second to a message queue. You want to ensure your application has sufficient bandwidth between your EC2 instances and SQS Which option will provide (he most scalable solution for bandwidth between the application and SOS? A. Ensure the application instances are properly configured with an Elastic Load Balancer. B. Ensure the application instances are launched in private subnets with the EBS-optimized option enabled. C. Ensure the application instances are launched in public subnets with the associate-publicIP-address=true option enabled D. Launch application instances in private subnets with an Auto Scaling group and Auto Scaling triggers configured to watch the SOS queue size Answer: C Reference: http://www.cardinalpath.com/autoscaling-your-website-with-amazon-web-services-part-2/ QUESTION NO: 5 You have identified network throughput as a bottleneck on your ml small EC2 instance when uploading data into Amazon S3 In the same region. How do you remedy this situation? A. Add an additional ENI B. Change to a larger Instance C. Use DirectConnect between EC2 and S3 D. Use EBS PIOPS on the local volume Answer: B Reference: https://media.amazonwebservices.com/AWS_Amazon_EMR_Best_Practices.pdf QUESTION NO: 6 When attached to an Amazon VPC which two components provide connectivity with external networks? Choose 2 answers A. Elastic IPS (EIP) B. NAT Gateway (NAT) C. Internet Gateway {IGW) D. Virtual Private Gateway (VGW) Answer: C,D QUESTION NO: 7 Your application currently leverages AWS Auto Scaling to grow and shrink as load Increases/decreases and has been performing well. Your marketing team expects a steady ramp up in traffic to follow an upcoming campaign that will result in a 20x growth in traffic over 4 weeks. Your forecast for the approximate number of Amazon EC2 instances necessary to meet the peak demand is 175. What should you do to avoid potential service disruptions during the ramp up in traffic? 456789 A. Ensure that you have pre-allocated 175 Elastic IP addresses so that each server will be able to obtain one as it launches B. Check the service limits in Trusted Advisor and adjust as necessary so the forecasted count remains within limits. C. Change your Auto Scaling configuration to set a desired capacity of 175 prior to the launch of the marketing campaign. D. Pre-warm your Elastic Load Balancer to match the requests per second anticipated during peak demand prior to the marketing campaign. Answer: D QUESTION NO: 8 You have an Auto Scaling group associated with an Elastic Load Balancer (ELB). You have noticed that instances launched via the Auto Scaling group are being marked unhealthy due to an ELB health check, but these unhealthy instances are not being terminated. What do you need to do to ensure trial instances marked unhealthy by the ELB will be terminated and replaced? A. Change the thresholds set on the Auto Scaling group health check B. Add an Elastic Load Balancing health check to your Auto Scaling group C. Increase the value for the Health check interval set on the Elastic Load Balancer D. Change the health check set on the Elastic Load Balancer to use TCP rather than HTTP checks Answer: B Reference: http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/as-add-elb-healthcheck.html Add an Elastic Load Balancing Health Check to your Auto Scaling GroupBy default, an Auto Scaling group periodically reviews the results of EC2 instance status to determine the health state of each instance. However, if you have associated your Auto Scaling group with an Elastic Load Balancing load balancer, you can choose to use the Elastic Load Balancing health check. In this case, Auto Scaling determines the health status of your instances by checking the results of both the EC2 instance status check and the Elastic Load Balancing instance health check. For information about EC2 instance status checks, see Monitor Instances With Status Checks in the Amazon EC2 User Guide for Linux Instances. For information about Elastic Load Balancing health checks, see Health Check in the Elastic Load Balancing Developer Guide. This topic shows you how to add an Elastic Load Balancing health check to your Auto Scaling group, assuming that you have created a load balancer and have registered the load balancer with your Auto Scaling group. If you have not registered the load balancer with your Auto Scaling group, see Set Up a Scaled and Load-Balanced Application. Auto Scaling marks an instance unhealthy if the calls to the Amazon EC2 action DescribeInstanceStatus return any state other than running, the system status shows impaired, or the calls to Elastic Load Balancing action DescribeInstanceHealth returns OutOfService in the instance state field. If there are multiple load balancers associated with your Auto Scaling group, Auto Scaling checks the health state of your EC2 instances by making health check calls to each load balancer. For each call, if the Elastic Load Balancing action returns any state other than InService, the instance is marked as unhealthy. After Auto Scaling marks an instance as unhealthy, it remains in that state, even if subsequent calls from other load balancers return an InService state for the same instance. QUESTION NO: 9 Which two AWS services provide out-of-the-box user configurable automatic backup-as-a-service and backup rotation options? Choose any 2 answers. A. Amazon S3 B. Amazon RDS C. Amazon EBS D. Amazon Red shift Answer: C,D QUESTION NO: 10 An organization has configured a VPC with an Internet Gateway (IGW). pairs of public and private subnets (each with one subnet per Availability Zone), and an Elastic Load Balancer (ELB) configured to use the public subnets. The application web tier leverages the ELB, Auto Scaling and a mum-AZ RDS database instance. The Organization would like to eliminate any potential single points ft failure in this design. What step should you take to achieve this organization's objective? A. Nothing, there are no single points of failure in this architecture. B. Create and attach a second IGW to provide redundant internet connectivity. C. Create and configure a second Elastic Load Balancer to provide a redundant load balancer. D. Create a second multi-AZ RDS instance in another Availability Zone and configurereplication to provide a redundant database. Answer: C QUESTION NO: 11 Which of the following are characteristics of Amazon VPC subnets? Choose any 2 answers. A. Each subnet maps to a single Availability Zone B. A CIDR block mask of /25 is the smallest range supported C. Instances in a private subnet can communicate with the internet only if they have an Elastic IP. D. By default, all subnets can route between each other, whether they are private or public E. V Each subnet spans at least 2 Availability zones to provide a high-availability environment. Answer: C,E QUESTION NO: 12 You are creating an Auto Scaling group whose Instances need to insert a custom metric into CloudWatch. Which method would be the best way to authenticate your CloudWatch PUT request? A. Create an IAM role with the Put MetricData permission and modify the Auto Scaling launch configuration to launch instances in that role. B. Create an IAM user with the PutMetricData permission and modify the Auto Scaling launch configuration to inject the userscredentials into the instance User Data. C. Modify the appropriate Cloud Watch metric policies to allow the Put MetricData permission to instances from the Auto Scaling group. D. Create an IAM user with the PutMetricData permission and put the credentials in a private repository and have applications on the server pull the credentials as needed. Answer: B QUESTION NO: 13 When an EC2 instance that is backed by an S3-based AMI Is terminated, what happens to the data on me root volume? A. Data is automatically saved as an E8S volume. B. Data is automatically saved as an ESS snapshot. C. Data is automatically deleted. D. Data is unavailable until the instance is restarted. Answer: D Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html QUESTION NO: 14 You have a web application leveraging an Elastic Load Balancer (ELB) In front of the web servers deployed using an Auto Scaling Group Your database is running on Relational Database Service (RDS) The application serves out technical articles and responses to them in general there are more views of an article than there are responses to the article. On occasion, an article on the site becomes extremely popular resulting in significant traffic Increases that causes the site to go down. What could you do to help alleviate the pressure on the infrastructure while maintaining availability during these events? Choose 3 answers A. Leverage CloudFront for the delivery of the articles. B. Add RDS read-replicas for the read traffic going to your relational database C. Leverage ElastiCache for caching the most frequently used data. D. Use SOS to queue up the requests for the technical posts and deliver them out of the queue. E. Use Route53 health checks to fail over to an S3 bucket for an error page. Answer: A,C,E QUESTION NO: 15 The majority of your Infrastructure is on premises and you have a small footprint on AWS Your company has decided to roll out a new application that is heavily dependent on low latency connectivity to LOAP for authentication Your security policy requires minimal changes to the company's existing application user management processes. What option would you implement to successfully launch this application1? A. Create a second, independent LOAP server in AWS for your application to use for authentication B. Establish a VPN connection so your applications can authenticate against your existing on-premises LDAP servers C. Establish a VPN connection between your data center and AWS create a LDAP replica on AWS and configure your application to use the LDAP replica for authentication D. Create a second LDAP domain on AWS establish a VPN connection to establish a trust relationship between your new and existing domains and use the new domain for authentication Answer: D Reference: http://msdn.microsoft.com/en-us/library/azure/jj156090.aspx QUESTION NO: 16 You need to design a VPC for a web-application consisting of an Elastic Load Balancer (ELB). a fleet of web/application servers, and an RDS database The entire Infrastructure must be distributed over 2 availability zones. Which VPC configuration works while assuring the database is not available from the Internet? A. One public subnet for ELB one public subnet for the web-servers, and one private subnet for the database B. One public subnet for ELB two private subnets for the web-servers, two private subnets for RDS C. Two public subnets for ELB two private subnets for the web-servers and two private subnets for RDS D. Two public subnets for ELB two public subnets for the web-servers, and two public subnets for RDS Answer: A QUESTION NO: 17 An application that you are managing has EC2 instances & Dynamo OB tables deployed to several AWS Regions In order to monitor the performance of the application globally, you would like to see two graphs 1) Avg CPU Utilization across all EC2 instances and 2) Number of Throttled Requests for all DynamoDB tables. How can you accomplish this? A. Tag your resources with the application name, and select the tag name as the dimension in the Cloudwatch Management console to view the respective graphs B. Use the Cloud Watch CLI tools to pull the respective metrics from each regional endpoint Aggregate the data offline & store it for graphing in CloudWatch. C. Add SNMP traps to each instance and DynamoDB table Leverage a central monitoring server to capture data from each instance and table Put the aggregate data into Cloud Watch for graphing. D. Add a CloudWatch agent to each instance and attach one to each DynamoDB table. When configuring the agent set the appropriate application name & view the graphs in CloudWatch. Answer: C QUESTION NO: 18 When assessing an organization s use of AWS API access credentials which of the following three credentials should be evaluated? Choose 3 answers A. Key pairs B. Console passwords 10 C. Access keys D. Signing certificates E. Security Group memberships Answer: A,C,D Reference: http://media.amazonwebservices.com/AWS_Operational_Checklists.pdf QUESTION NO: 19 You have a Linux EC2 web server instance running inside a VPC The instance is In a public subnet and has an EIP associated with it so you can connect to It over the Internet via HTTP or SSH The instance was also fully accessible when you last logged in via SSH. and was also serving web requests on port 80. Now you are not able to SSH into the host nor does it respond to web requests on port 80 that were working fine last time you checked You have double-checked that all networking configuration parameters (security groups route tables. IGW'EIP. NACLs etc) are properly configured {and you haven\u2019t made any changes to those anyway since you were last able to reach the Instance). You look at the EC2 console and notice that system status check shows \"impaired.\" Which should be your next step in troubleshooting and attempting to get the instance back to a healthy state so that you can log in again? A. Stop and start the instance so that it will be able to be redeployed on a healthy host system that most likely will fix the \"impaired\" system status B. Reboot your instance so that the operating system will have a chance to boot in a clean healthy state that most likely will fix the 'impaired\" system status C. Add another dynamic private IP address to me instance and try to connect via mat new path, since the networking stack of the OS may be locked up causing the \u201cimpaired\u201d system status. D. Add another Elastic Network Interface to the instance and try to connect via that new path since the networking stack of the OS may be locked up causing the \"impaired\" system status E. un-map and then re-map the EIP to the instance, since the IGWVNAT gateway may not be working properly, causing the \"impaired\" system status Answer: B QUESTION NO: 20 What is a placement group? A. A collection of Auto Scaling groups in the same Region B. Feature that enables EC2 instances to interact with each other via nigh bandwidth, low latency connections C. A collection of Elastic Load Balancers in the same Region or Availability Zone D. A collection of authorized Cloud Front edge locations for a distribution Answer: C Reference: http://aws.amazon.com/ec2/faqs/ QUESTION NO: 21 Your entire AWS infrastructure lives inside of one Amazon VPC You have an Infrastructure monitoring application running on an Amazon instance in Availability Zone (AZ) A of the region, and another application instance running in AZ B. The monitoring application needs to make use of ICMP ping to confirm network reachability of the instance hosting the application. Can you configure the security groups for these instances to only allow the ICMP ping to pass from the monitoringinstance to the application instance and nothing else\" If so how? A. No Two instances in two different AZ's can't talk directly to each other via ICMP ping as that protocol is not allowed across subnet (iebroadcast) boundaries B. Yes Both the monitoring instance and the application instance have to be a part of the same security group, and that security group needs to allow inbound ICMP C. Yes, The security group for the monitoring instance needs to allow outbound ICMP and the application instance's security group needs to allow Inbound ICMP D. Yes, Both the monitoring instance's security group and the application instance's security group need to allow both inbound and outbound ICMP ping packets since ICMP is not a connection-oriented protocol Answer: D QUESTION NO: 22 You have two Elastic Compute Cloud (EC2) instances inside a Virtual Private Cloud (VPC) in the same Availability Zone (AZ) but in different subnets.One instance is running a database and the other instance an application that will interface with the database. You want to confirm that they can talk to each other for your application to work properly. Which two things do we need to confirm in the VPC settings so that these EC2 instances can communicate inside the VPC? Choose 2 answers A. A network ACL that allows communication between the two subnets. B. Both instances are the same instance class and using the same Key-pair. C. That the default route is set to a NAT instance or internet Gateway (IGW) for them to communicate. D. Security groups are set to allow the application host to talk to the database on the right port/protocol. Answer: A,C QUESTION NO: 23 Which services allow the customer to retain full administrative privileges of the underlying EC2 instances? Choose 2 answers A. Amazon Elastic Map Reduce B. Elastic Load Balancing C. AWS Elastic Beanstalk D. I\" Amazon Elasticache E. Amazon Relational Database service Answer: B,C QUESTION NO: 24 You have a web-style application with a stateless but CPU and memory-intensive web tier running on a cc2 8xlarge EC2 instance inside of a VPC The instance when under load is having 134 problems returning requests within the SLA as defined by your business The application maintains its state in a DynamoDB table, but the data tier is properly provisioned and responses are consistently fast. How can you best resolve the issue of the application responses not meeting your SLA? A. Add another cc2 8xlarge application instance, and put both behind an Elastic Load Balancer B. Move the cc2 8xlarge to the same Availability Zone as the DynamoDB table C. Cache the database responses in ElastiCache for more rapid access D. Move the database from DynamoDB to RDS MySQL in scale-out read-replica configuration Answer: B Reference: http://aws.amazon.com/elasticmapreduce/faqs/ QUESTION NO: 25 You are managing a legacy application Inside VPC with hard coded IP addresses in its configuration. Which two mechanisms will allow the application to failover to new instances without the need for reconfiguration? Choose 2 answers A. Create an ELB to reroute traffic to a failover instance B. Create a secondary ENI that can be moved to a failover instance C. Use Route53 health checks to fail traffic over to a failover instance D. Assign a secondary private IP address to the primary ENIO that can De moved to a failover instance Answer: A,D (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 26 You are designing a system that has a Bastion host. This component needs to be highly available without human intervention. Which of the following approaches would you select? A. Run the bastion on two instances one in each AZ B. Run the bastion on an active Instance in one AZ and have an AMI ready to boot up in the event of failure C. Configure the bastion instance in an Auto Scaling group Specify the Auto Scaling group to include multiple AZs but have a min-size of 1 and max-size of 1 D. Configure an ELB in front of the bastion instance Answer: C QUESTION NO: 27 Which of the following statements about this S3 bucket policy is true? 15 A. Denies the server with the IP address 192 168 100 0 full access to the \"mybucket\" bucket B. Denies the server with the IP address 192 168 100 188 full access to the \"mybucket\" bucket C. Grants all the servers within the 192 168 100 0/24 subnet full access to the \"mybucket\" bucket D. Grants all the servers within the 192 168 100 188/32 subnet full access to the \"mybucket\" bucket Answer: C QUESTION NO: 28 Which of the following requires a custom CloudWatch metric to monitor? A. Data transfer of an EC2 instance B. Disk usage activity of an EC2 instance C. Memory Utilization of an EC2 instance D. CPU Utilization of an EC2mstance Answer: B Reference: http://aws.amazon.com/cloudwatch/ QUESTION NO: 29 You run a web application where web servers on EC2 Instances are In an Auto Scaling group Monitoring over the last 6 months shows that 6 web servers are necessary to handle the minimum load During the day up to 12 servers are needed Five to six days per year, the number of web servers required might go up to 15. What would you recommend to minimize costs while being able to provide hill availability? A. 6 Reserved instances (heavy utilization). 6 Reserved instances {medium utilization), rest covered by On-Demand instances B. 6 Reserved instances (heavy utilization). 6 On-Demand instances, rest covered by Spot Instances C. 6 Reserved instances (heavy utilization) 6 Spot instances, rest covered by OnDemand instances D. 6 Reserved instances (heavy utilization) 6 Reserved instances (medium utilization) rest covered by Spot instances 1678 Answer: C QUESTION NO: 30 You have been asked to propose a multi-region deployment of a web-facing application where a controlled portion of your traffic is being processed by an alternate region. Which configuration would achieve that goal? A. Route53 record sets with weighted routing policy B. Route53 record sets with latency based routing policy C. Auto Scaling with scheduled scaling actions set D. Elastic Load Balancing with health checks enabled Answer: D Reference: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/TerminologyandKeyConcepts.html QUESTION NO: 31 You have set up Individual AWS accounts for each project. You have been asked to make sure your AWS Infrastructure costs do not exceed the budget set per project for each month. Which of the following approaches can help ensure that you do not exceed the budget each month? A. Consolidate your accounts so you have a single bill for all accounts and projects B. Set up auto scaling with CloudWatch alarms using SNS to notify you when you are running too many Instances in a given account C. Set up CloudWatch billing alerts for all AWS resources used by each project, with a notification occurring when the amount for each resource tagged to a particular project matches the budget allocated to the project. D. Set up CloudWatch billing alerts for all AWS resources used by each account, with email notifications when it hits 50%. 80% and 90% of its budgeted monthly spend Answer: C QUESTION NO: 32 When creation of an EBS snapshot Is initiated but not completed the EBS volume? A. Cannot De detached or attached to an EC2 instance until me snapshot completes B. Can be used in read-only mode while me snapshot is in progress C. Can be used while me snapshot Is in progress D. Cannot be used until the snapshot completes Answer: C Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html QUESTION NO: 33 You are using ElastiCache Memcached to store session state and cache database queries in your infrastructure You notice in Cloud Watch that Evictions and GetMisses are Doth very high. What two actions could you take to rectify this? Choose 2 answers A. Increase the number of nodes in your cluster B. Tweak the max-item-size parameter C. Shrink the number of nodes in your cluster D. Increase the size of the nodes in the duster Answer: B,D QUESTION NO: 34 You are running a database on an EC2 instance, with the data stored on Elastic Block Store (EBS) for persistence At times throughout the day, you are seeing large variance in the response times of the database queries Looking into the instance with the isolate command you see a lot of wait time on the disk volume that the database's data is stored on. What two ways can you improve the performance of the database's storage while maintaining the current persistence of the data? Choose 2 answers A. Move to an SSD backed instance B. Move the database to an EBS-Optimized Instance C. T Use Provisioned IOPs EBS D. Use the ephemeral storage on an m2 4xiarge Instance Instead Answer: A,B QUESTION NO: 35 Your EC2-Based Multi-tier application includes a monitoring instance that periodically makes application -level read only requests of various application components and if any of those fail more than three times 30 seconds calls CloudWatch lo fire an alarm, and the alarm notifies your operations team by email and SMS of a possible application health problem. However, you also need to watch the watcher -the monitoring instance itself - and be notified if it becomes unhealthy. Which of the following Is a simple way to achieve that goal? A. Run another monitoring instance that pings the monitoring instance and fires a could watch alarm mat notifies your operations teamshould the primary monitoring instance become unhealthy. B. Set a Cloud Watch alarm based on EC2 system and instance status checks and have the alarm notify your operations team of anydetected problem with the monitoring instance. C. Set a Cloud Watch alarm based on the CPU utilization of the monitoring instance and nave the alarm notify your operations team if C r the CPU usage exceeds 50% few more than one minute: then have your monitoring application go into a CPU-bound loop should itDetect any application problems. D. Have the monitoring instances post messages to an SOS queue and then dequeue those messages on another instance should D c- the queue cease to have new messages, the second instance should first terminate the original monitoring instance start anotherbackup monitoring instance and assume (he role of the previous monitoring instance and beginning adding messages to the SOSqueue. 19 Answer: D QUESTION NO: 36 You have decided to change the Instance type for instances running In your application tier that are using Auto Scaling. In which area below would you change the instance type definition? A. Auto Scaling launch configuration B. Auto Scaling group C. Auto Scaling policy D. Auto Scaling tags Answer: B Reference: http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/WhatIsAutoScaling.html QUESTION NO: 37 You are attempting to connect to an instance in Amazon VPC without success You have already verified that the VPC has an Internet Gateway (IGW) the instance has an associated Elastic IP (EIP) and correct security group rules are in place. Which VPC component should you evaluate next? A. The configuration of a MAT instance B. The configuration of the Routing Table C. The configuration of the internet Gateway (IGW) D. The configuration of SRC/DST checking Answer: C Reference: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/UserScenariosForVPC.html QUESTION NO: 38 You are tasked with the migration of a highly trafficked Node JS application to AWS In order to comply with organizational standards Chef recipes must be used to configure the application servers that host this application and to support application lifecycle events. Which deployment option meets these requirements while minimizing administrative burden? A. Create a new stack within Opsworks add the appropriate layers to the stack and deploy the application B. Create a new application within Elastic Beanstalk and deploy this application to a new environment C. Launch a Mode JS server from a community AMI and manually deploy the application to the launched EC2 instance D. Launch and configure Chef Server on an EC2 instance and leverage the AWS CLI to launch application servers and configure those instances using Chef. Answer: B Reference: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deployment.html QUESTION NO: 39 You have been asked to automate many routine systems administrator backup and recovery activities Your current plan is to leverage AWS-managed solutions as much as possible and automate the rest with the AWS CU and scripts. Which task would be best accomplished with a script? A. Creating daily EBS snapshots with a monthly rotation of snapshots B. Creating daily ROS snapshots with a monthly rotation of snapshots C. Automatically detect and stop unused or underutilized EC2 instances D. Automatically add Auto Scaled EC2 instances to an Amazon Elastic Load Balancer Answer: C QUESTION NO: 40 Your organization's security policy requires that all privileged users either use frequently rotated passwords or one-time access credentials in addition to username/password. Which two of the following options would allow an organization to enforce this policy for AWS users? Choose 2 answers A. Configure multi-factor authentication for privileged 1AM users B. Create 1AM users for privileged accounts C. Implement identity federation between your organization's Identity provider leveraging the 1AM Security Token Service D. Enable the 1AM single-use password policy option for privileged users Answer: C,D QUESTION NO: 41 What are characteristics of Amazon S3? Choose 2 answers A. Objects are directly accessible via a URL B. S3 should be used to host a relational database C. S3 allows you to store objects or virtually unlimited size D. S3 allows you to store virtually unlimited amounts of data E. S3 offers Provisioned IOPS Answer: B,C QUESTION NO: 42 You receive a frantic call from a new DBA who accidentally dropped a table containing all your customers. Which Amazon RDS feature will allow you to reliably restore your database to within 5 minutes of when the mistake was made? A. Multi-AZ RDS B. RDS snapshots C. RDS read replicas D. RDS automated backup Answer: B Reference: http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.BackingUpAndRestoringAmazonRDSInstances.html QUESTION NO: 43 A media company produces new video files on-premises every day with a total size of around 100GBS after compression All files have a size of 1 -2 GB and need to be uploaded to Amazon S3 every night in a fixed time window between 3am and 5am Current upload takes almost 3 hours, although less than half of the available bandwidth is used. What step(s) would ensure that the file uploads are able to complete in the allotted time window? A. Increase your network bandwidth to provide faster throughput to S3 B. Upload the files in parallel to S3 C. Pack all files into a single archive, upload it to S3, then extract the files in AWS D. Use AWS Import/Export to transfer the video files Answer: D Reference: http://aws.amazon.com/importexport/faqs/ QUESTION NO: 44 You are running a web-application on AWS consisting of the following components an Elastic Load Balancer (ELB) an Auto-Scaling Group of EC2 instances running Linux/PHP/Apache, and Relational DataBase Service (RDS) MySQL. Which security measures fall into AWS's responsibility? A. Protect the EC2 instances against unsolicited access by enforcing the principle of least-privilege access B. Protect against IP spoofing or packet sniffing C. Assure all communication between EC2 instances and ELB is encrypted D. Install latest security patches on ELB. RDS and EC2 instances Answer: B QUESTION NO: 45 You use S3 to store critical data for your company Several users within your group currently have lull permissions to your S3 buckets You need to come up with a solution mat does not impact your users and also protect against the accidental deletion of objects. Which two options will address this issue? Choose 2 answers A. Enable versioning on your S3 Buckets B. Configure your S3 Buckets with MFA delete C. Create a Bucket policy and only allow read only permissions to all users at the bucket level D. Enable object life cycle policies and configure the data older than 3 months to be archived in Glacier Answer: B,C QUESTION NO: 46 An organization's security policy requires multiple copies of all critical data to be replicated across at least a primary and backup data center. The organization has decided to store some critical data on Amazon S3. 245 Which option should you implement to ensure this requirement is met? A. Use the S3 copy API to replicate data between two S3 buckets in different regions B. You do not need to implement anything since S3 data is automatically replicated between regions C. Use the S3 copy API to replicate data between two S3 buckets in different facilities within an AWS Region D. You do not need to implement anything since S3 data is automatically replicated between multiple facilities within an AWS Region Answer: C QUESTION NO: 47 You are tasked with setting up a cluster of EC2 Instances for a NoSOL database The database requires random read 10 disk performance up to a 100.000 IOPS at 4KB block side per node Which of the following EC2 instances will perform the best for this workload? A. A High-Memory Quadruple Extra Large (m2 4xlarge) with EBS-Optimized set to true and a PIOPs EBS volume B. A Cluster Compute Eight Extra Large (cc2 8xlarge) using instance storage C. High I/O Quadruple Extra Large (hil 4xiarge) using instance storage D. A Cluster GPU Quadruple Extra Large (cg1 4xlarge) using four separate 4000 PIOPS EBS volumes in a RAID 0 configuration Answer: B Reference: http://aws.amazon.com/ec2/instance-types/ QUESTION NO: 48 When an EC2 EBS-backed (EBS root) instance is stopped, what happens to the data on any ephemeral store volumes? A. Data will be deleted and win no longer be accessible B. Data Is automatically saved in an EBS volume. C. Data Is automatically saved as an E8S snapshot D. Data is unavailable until the instance is restarted Answer: D QUESTION NO: 49 Your team Is excited about theuse of AWS because now they have access to programmable Infrastructure\" You have been asked to manage your AWS infrastructure In a manner similar to the way you might manage application code You want to be able to deploy exact copies of different versions of your infrastructure, stage changes into different environments, revert back to previous versions, and identify what versions are running at any particular time (development test QA. production). Which approach addresses this requirement? A. Use cost allocation reports and AWS Opsworks to deploy and manage your infrastructure. B. Use AWS CloudWatch metrics and alerts along with resource tagging to deploy and manage your infrastructure. C. Use AWS Beanstalk and a version control system like GIT to deploy and manage your infrastructure. D. Use AWS CloudFormation and a version control system like GIT to deploy and manage your infrastructure. Answer: B Reference: http://aws.amazon.com/opsworks/faqs/ QUESTION NO: 50 You have a server with a 5O0GB Amazon EBS data volume. The volume is 80% full. You need to back up the volume at regular intervals and be able to re-create the volume in a new Availability Zone in the shortest time possible. All applications using the volume can be paused for a period of a few minutes with no discernible user impact. Which of the following backup methods will best fulfill your requirements? A. Take periodic snapshots of the EBS volume 2678930 B. Use a third party Incremental backup application to back up to Amazon Glacier C. Periodically back up all data to a single compressed archive and archive to Amazon S3 using a parallelized multi-part upload D. Create another EBS volume in the second Availability Zone attach it to the Amazon EC2 instance, and use a disk manager to mirror me two disks Answer: D Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 51 Your company Is moving towards tracking web page users with a small tracking Image loaded on each page Currently you are serving this image out of US-East, but are starting to get concerned about the time It takes to load the image for users on the west coast. What are the two best ways to speed up serving this image? Choose 2 answers A. Use Route 53's Latency Based Routing and serve the image out of US-West-2 as well as US-East-1 B. Serve the image out through CloudFront C. Serve the image out of S3 so that it isn't being served oft of your web application tier D. Use EBS PIOPs to serve the image faster out of your EC2 instances Answer: A,B QUESTION NO: 52 If you want to launch Amazon Elastic Compute Cloud (EC2) Instances and assign each Instance a predetermined private IP address you should: A. Assign a group or sequential Elastic IP address to the instances B. Launch the instances in a Placement Group C. Launch the instances in the Amazon virtual Private Cloud (VPC). D. Use standard EC2 instances since each instance gets a private Domain Name Service (DNS) already E. Launch the Instance from a private Amazon Machine image (Mil) Answer: C Reference: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-ip-addressing.html QUESTION NO: 53 A customer has a web application that uses cookie Based sessions to track logged in users It Is deployed on AWS using ELB and Auto Scaling The customer observes that when load increases. Auto Scaling launches new Instances but the load on the easting Instances does not decrease, causing all existing users to have a sluggish experience. Which two answer choices independently describe a behavior that could be the cause of the sluggish user experience? Choose 2 answers A. ELB's normal behavior sends requests from the same user to the same backend instance B. ELB's behavior when sticky sessions are enabled causes ELB to send requests in the same session to the same backend instance C. A faulty browser is not honoring the TTL of the ELB DNS name. D. The web application uses long polling such as comet or websockets. Thereby keeping a connection open to a web server tor a long time E. The web application uses long polling such as comet or websockets. Thereby keeping a connection open to a web server for a long time. Answer: B,D QUESTION NO: 54 What would happen to an RDS (Relational Database Service) multi-Availability Zone deployment of the primary OB instance fails? A. The IP of the primary DB instance is switched to the standby OB instance B. The RDS (Relational Database Service) DB instance reboots C. A new DB instance is created in the standby availability zone D. The canonical name record (CNAME) is changed from primary to standby Answer: B QUESTION NO: 55 How can the domain's zone apex for example \"myzoneapexdomain com\" be pointed towards an Elastic Load Balancer? A. By using an AAAA record B. By using an A record C. By using an Amazon Route 53 CNAME record D. By using an Amazon Route 53 Alias record Answer: C Reference: http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosingalias-non-alias.html Topic 2, Volume B QUESTION NO: 56 An organization has created 5 IAM users. The organization wants to give them the same login ID but different passwords. How can the organization achieve this? A. The organization should create a separate login ID but give the IAM users the same alias so that each one can login with their alias B. The organization should create each user in a separate region so that they have their own URL to login C. It is not possible to have the same login ID for multiple IAM users of the same account D. The organization should create various groups and add each user with the same login ID to different groups. The user can login with their own group ID Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Whenever the organization is creating an IAM user, there should be a unique ID for each user. It is not possible to have the same login ID for multiple users. The names of users,groups, roles, instance profiles must be alphanumeric, including the following common characters: plus(+), equal(=), comma(,), period(.), at(@), and dash (-) QUESTION NO: 57 A user is planning to evaluate AWS for their internal use. The user does not want to incur any charge on his account during the evaluation. Which of the below mentioned AWS services would incur a charge if used? A. AWS S3 with 1 GB of storage B. AWS micro instance running 24 hours daily C. AWS ELB running 24 hours a day D. AWS PIOPS volume of 10 GB size Answer: D Explanation: AWS is introducing a free usage tier for one year to help the new AWS customers get started in Cloud. The free tier can be used for anything that the user wants to run in the Cloud. AWS offers a handful of AWS services as a part of this which includes 750 hours of free micro instances and 750 hours of ELB. It includes the AWS S3 of 5 GB and AWS EBS general purpose volume upto 30 GB. PIOPS is not part of free usage tier. QUESTION NO: 58 A user has developed an application which is required to send the data to a NoSQL database. The user wants to decouple the data sending such that the application keeps processing and sending data but does not wait for an acknowledgement of DB. Which of the below mentioned applications helps in this scenario? A. AWS Simple Notification Service B. AWS Simple Workflow C. AWS Simple Queue Service D. AWS Simple Query Service Answer: C Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. In this case, the user can use AWS SQS to send messages which are received from an application and sent to DB. The application can continue processing data without waiting for any acknowledgement from DB. The user can use SQS to transmit any volume of data without losing messages or requiring other services to always be available. QUESTION NO: 59 An organization has created 50 IAM users. The organization has introduced a new policy which will change the access of an IAM user. How can the organization implement this effectively so that there is no need to apply the policy at the individual user level? A. Use the IAM groups and add users as per their role to different groups and apply policy to group B. The user can create a policy and apply it to multiple users in a single go with the AWS CLI C. Add each user to the IAM role as per their organization role to achieve effective policy setup D. Use the IAM role and implement access at the role level Answer: A Explanation: With AWS IAM, a group is a collection of IAM users. A group allows the user to specify permissions for a collection of users, which can make it easier to manage the permissions for those users. A group helps an organization manage access in a better way; instead of applying at the individual level, the organization can apply at the group level which is applicable to all the users who are a part of that group. QUESTION NO: 60 A user is planning to use AWS Cloud formation for his automatic deployment requirements. Which of the below mentioned components are required as a part of the template? A. Parameters B. Outputs C. Template version D. Resources Answer: D Explanation: AWS Cloud formation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The template is a JSON-format, text-based file that describes all the AWS resources required to deploy and run an application. It can have option fields, such as Template Parameters, Output, Data tables, and Template file format version. The only mandatory value is Resource. The user can define the AWS services which will be used/ created by this template inside the Resource section QUESTION NO: 61 A user has recently started using EC2. The user launched one EC2 instance in the default subnet in EC2-VPC Which of the below mentioned options is not attached or available with the EC2 instance when it is launched? A. Public IP address B. Internet gateway C. Elastic IP D. Private IP address Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to a user\u2019s AWS account. A subnet is a range of IP addresses in the VPC. The user can launch the AWS resources into a subnet. There are two supported platforms into which a user can launch instances: EC2-Classic and EC2-VPC (default subnet.. A default VPC has all the benefits of EC2-VPC and the ease of use of EC2- Classic. Each instance that the user launches into a default subnet has a private IP address and a public IP address. These instances can communicate with the internet through an internet gateway. An internet gateway enables the EC2 instances to connect to the internet through the Amazon EC2 network edge. QUESTION NO: 62 A user has launched an EC2 instance. The user is planning to setup the CloudWatch alarm. Which of the below mentioned actions is not supported by the CloudWatch alarm? A. Notify the Auto Scaling launch config to scale up B. Send an SMS using SNS C. Notify the Auto Scaling group to scale down D. Stop the EC2 instance Answer: B Explanation: A user can create a CloudWatch alarm that takes various actions when the alarm changes state. An alarm watches a single metric over the time period that the user has specified, and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The actions could be sending a notification to an Amazon Simple Notification Service topic (SMS, Email, and HTTP end point.,notifying the Auto Scaling policy or changing the state of the instance to Stop/Terminate. QUESTION NO: 63 A user is trying to delete an Auto Scaling group from CLI. Which of the below mentioned steps are to be performed by the user? A. Terminate the instances with the ec2-terminate-instance command B. Terminate the Auto Scaling instances with the as-terminate-instance command C. Set the minimum size and desired capacity to 0 D. There is no need to change the capacity. Run the as-delete-group command and it will reset all values to 0 Answer: C Explanation: If the user wants to delete the Auto Scaling group, the user should manually set the values of the minimum and desired capacity to 0. Otherwise Auto Scaling will not allow for the deletion of the group from CLI. While trying from the AWS console, the user need not set the values to 0 as the Auto Scaling console will automatically do so. QUESTION NO: 64 An organization is planning to create 5 different AWS accounts considering various security requirements. The organization wants to use a single payee account by using the consolidated billing option. Which of the below mentioned statements is true with respect to the above information? A. Master (Payee. account will get only the total bill and cannot see the cost incurred by each account B. Master (Payee. account can view only the AWS billing details of the linked accounts C. It is not recommended to use consolidated billing since the payee account will have access to the linked accounts D. Each AWS account needs to create an AWS billing policy to provide permission to the payee account Answer: B Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. Consolidated billing enables the organization to see a combined view of the AWS charges incurred by each account as well as obtain a detailed cost report for each of the individual AWS accounts associated with the paying account. The payee account will not have any other access than billing data of linked accounts. QUESTIONNO: 64-A A user has deployed an application on his private cloud. The user is using his own monitoring tool. He wants to configure that whenever there is an error, the monitoring tool should notify him via SMS. Which of the below mentioned AWS services will help in this scenario? A. None because the user infrastructure is in the private cloud/ B. AWS SNS C. AWS SES D. AWS SMS Answer: B Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can be used to make push notifications to mobile devices. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. In this case user can use the SNS apis to send SMS. QUESTION NO: 65 A user has created a web application with Auto Scaling. The user is regularly monitoring the application and he observed that the traffic is highest on Thursday and Friday between 8 AM to 6 PM. What is the best solution to handle scaling in this case? A. Add a new instance manually by 8 AM Thursday and terminate the same by 6 PM Friday B. Schedule Auto Scaling to scale up by 8 AM Thursday and scale down after 6 PM on Friday C. Schedule a policy which may scale up every day at 8 AM and scales down by 6 PM D. Configure a batch process to add a instance by 8 AM and remove it by Friday 6 PM Answer: B Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. In this case the load increases by Thursday and decreases by Friday. Thus, the user can setup the scaling activity based on the predictable traffic patterns of the web application using Auto Scaling scale by Schedule. QUESTION NO: 66 A user has setup a CloudWatch alarm on an EC2 action when the CPU utilization is above 75%. The alarm sends a notification to SNS on the alarm state. If the user wants to simulate the alarm action how can he achieve this? A. Run activities on the CPU such that its utilization reaches above 75% B. From the AWS console change the state to \u2018Alarm\u2019 C. The user can set the alarm state to \u2018Alarm\u2019 using CLI D. Run the SNS action manually Answer: C Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods.The user can test an alarm by setting it to any state using the SetAlarmState API (mon-set-alarm-state command.. This temporary state change lasts only until the next alarm comparison occurs. QUESTION 13 A user is trying to setup a scheduled scaling activity using Auto Scaling. The user wants to setup the recurring schedule. Which of the below mentioned parameters is not required in this case? A. Maximum size B. Auto Scaling group name C. End time D. Recurrence value Answer: A Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can also configure the recurring schedule action which will follow the Linux cron format. If the user is setting a recurring event, it is required that the user specifies the Recurrence value (in a cron format., end time (not compulsory but recurrence will stop after this. and the Auto Scaling group for which the scaling activity is to be scheduled. QUESTION NO: 67 A user has setup a billing alarm using CloudWatch for $200. The usage of AWS exceeded $200 after some days. The user wants to increase the limit from $200 to $400? What should the user do? A. Create a new alarm of $400 and link it with the first alarm B. It is not possible to modify the alarm once it has crossed the usage limit C. Update the alarm to set the limit at $400 instead of $200 D. Create a new alarm for the additional $200 amount Answer: C Explanation: AWS CloudWatch supports enabling the billing alarm on the total AWS charges. The estimated charges are calculated and sent several times daily to CloudWatch in the form of metric data. This data will be stored for 14 days. This data also includes the estimated charges for every service in AWS used by the user, as well as the estimated overall AWS charges. If the user wants to increase the limit, the user can modify the alarm and specify a new threshold. QUESTION NO: 68 A sys admin has created the below mentioned policy and applied to an S3 object named aws.jpg. The aws.jpg is inside a bucket named cloudacademy. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg\"] }] A. It is not possible to define a policy at the object level B. It will make all the objects of the bucket cloudacademy as public C. It will make the bucket cloudacademy as public D. the aws.jpg object as public Answer: A Explanation: A system admin can grant permission to the S3 objects or buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. It cannot be applied at the object level. QUESTION NO: 69 A user is trying to save some cost on the AWS services. Which of the below mentioned options will not help him save cost? A. Delete the unutilized EBS volumes once the instance is terminated B. Delete the AutoScaling launch configuration after the instances are terminated C. Release the elastic IP if not required once the instance is terminated D. Delete the AWS ELB after the instances are terminated Answer: B Explanation: AWS bills the user on a as pay as you go model. AWS will charge the user once the AWS resource is allocated. Even though the user is not using the resource, AWS will charge if it is in service or allocated. Thus, it is advised that once the user\u2019s work is completed he should: Terminate the EC2 instance Delete the EBS volumes Release the unutilized Elastic IPs Delete ELB The AutoScaling launch configuration does not cost the user. Thus, it will not make any difference to the cost whether it is deleted or not. QUESTION NO: 70 A user is trying to aggregate all the CloudWatch metric data of the last 1 week. Which of the below mentioned statistics is not available for the user as a part of data aggregation? A. Aggregate B. Sum C. Sample data D. Average Answer: A Explanation: Amazon CloudWatch is basically a metrics repository. Either the user can send the custom data or an AWS product can put metrics into the repository, and the user can retrieve the statistics based on those metrics. The statistics are metric data aggregations over specified periods of time. Aggregations are made using the namespace, metric name, dimensions, and the data point unit of measure, within the time period that is specified by the user. CloudWatch supports Sum, Min, Max, Sample Data and Average statistics aggregation. QUESTION NO: 71 An organization is planning to use AWS for their production roll out. The organization wants to implement automation for deployment such that it will automatically create a LAMP stack, download the latest PHP installable from S3 and setup the ELB. Which of the below mentioned AWS services meets the quirement for making an orderly deployment of the software? A. AWS Elastic Beanstalk B. AWS Cloudfront C. AWS Cloudformation D. AWS DevOps Answer: C Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. Cloudformation provides an easy way to create and delete the collection of related AWS resources and provision them in an orderly way. AWS CloudFormation automates and simplifies the task of repeatedly and predictably creating groups of related resources that power the user\u2019s applications. AWS Cloudfront is a CDN; Elastic Beanstalk does quite a few of the required tasks. However, it is a PAAS which uses a ready AMI. AWS Elastic Beanstalk provides an environment to easily develop and run applications in the cloud. QUESTION NO: 72 A user has created a subnet with VPC and launched an EC2 instance in that subnet with only default settings.Which of the below mentioned options is ready to use on the EC2 instance as soon as it is launched? A. Elastic IP B. Private IP C. Public IP D. I nternet gateway Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to a user\u2019s AWS account. A subnet is a range of IP addresses in the VPC. The user can launch the AWS resources into a subnet. There are two supported platforms into which a user can launch instances: EC2-Classic and EC2-VPC. When the user launches an instance which is not a part of the non-default subnet, it will only have a private IP assigned to it. The instances part of a subnet can communicate with each other but cannot communicate over the internet or to the AWS services, such as RDS / S3. QUESTION NO: 73 An organization is setting up programmatic billing access for their AWS account. Which of the below mentioned services is not required or enabled when the organization wants to use programmatic access? A. Programmatic access B. AWS bucket to hold the billing report C. AWS billing alerts D. Monthly Billing report Answer: C Explanation: AWS provides an option to have programmatic access to billing. Programmatic Billing Access leverages the existing Amazon Simple Storage Service (Amazon S3. APIs. Thus, the user can build applications that reference his billing data from a CSV (comma-separated value. file stored in an Amazon S3 bucket. To enable programmatic access, the user has to first enable the monthly billing report. Then the user needs to provide an AWS bucket name where the billing CSV will be uploaded. The user should also enable the Programmatic access option. QUESTION NO: 74 A user has configured the Auto Scaling group with the minimum capacity as 3 and the maximum capacity as 5. When the user configures the AS group, how many instances will Auto Scaling launch? A. 3 B. 0 C. 5 D. 2 Answer: C Explanation: When the user configures the launch configuration and the Auto Scaling group, the Auto Scaling group will start instances by launching the minimum number (or the desired number, if specified. of EC2 instances. If there are no other scaling conditions attached to the Auto Scaling group, it will maintain the minimum number of running instances at all times. QUESTION NO: 75 An admin is planning to monitor the ELB. Which of the below mentioned services does not help the admin capture the monitoring information about the ELB activity? A. ELB Access logs B. ELB health check C. CloudWatch metrics D. ELB API calls with CloudTrail Answer: B Explanation: The admin can capture information about Elastic Load Balancer using either: CloudWatch Metrics ELB Logs files which are stored in the S3 bucket CloudTrail with API calls which can notify the user as well generate logs for each API calls The health check is internally performed by ELB and does not help the admin get the ELB activity. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 76 A user is planning to use AWS Cloudformation. Which of the below mentioned functionalities does not help him to correctly understand Cloudfromation? A. Cloudformation follows the DevOps model for the creation of Dev & Test B. AWS Cloudfromation does not charge the user for its service but only charges for the AWS resources created with it C. Cloudformation works with a wide variety of AWS services, such as EC2, EBS, VPC, IAM, S3, RDS, ELB, etc D. CloudFormation provides a set of application bootstrapping scripts which enables the user to install Software Answer: A Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. It supports a wide variety of AWS services, such as EC2, EBS, AS, ELB, RDS, VPC, etc. It also provides application bootstrapping scripts which enable the user to install software packages or create folders. It is free of the cost and only charges the user for the services created with it. The only challenge is that it does not follow any model, such as DevOps; instead customers can define templates and use them to provision and manage the AWS resources in an orderly way. QUESTION NO: 77 A user has launched 10 instances from the same AMI ID using Auto Scaling. The user is trying to see the average CPU utilization across all instances of the last 2 weeks under the CloudWatch console. How can the user achieve this? A. View the Auto Scaling CPU metrics B. Aggregate the data over the instance AMI ID C. The user has to use the CloudWatchanalyser to find the average data across instances D. It is not possible to see the average CPU utilization of the same AMI ID since the instance ID is different Answer: B Explanation: Amazon CloudWatch is basically a metrics repository. Either the user can send the custom data or an AWS product can put metrics into the repository, and the user can retrieve the statistics based on those metrics. The statistics are metric data aggregations over specified periods of time. Aggregations are made using the namespace, metric name, dimensions, and the data point unit of measure, within the time period that is specified by the user. To aggregate the data across instances launched with AMI, the user should select the AMI ID under EC2 metrics and select the aggregate average to view the data. QUESTION NO: 78 A user is trying to understand AWS SNS. To which of the below mentioned end points is SNS unable to send a notification? A. Email JSON B. HTTP C. AWS SQS D. AWS SES Answer: D Explanation: Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can select one the following transports as part of the subscription requests: \u201cHTTP\u201d, \u201cHTTPS\u201d,\u201dEmail\u201d, \u201cEmailJSON\u201d, \u201cSQS\u201d, \u201cand SMS\u201d. QUESTION NO: 79 A user has configured an Auto Scaling group with ELB. The user has enabled detailed CloudWatch monitoring on Auto Scaling. Which of the below mentioned statements will help the user understand the functionality better? A. It is not possible to setup detailed monitoring for Auto Scaling B. In this case, Auto Scaling will send data every minute and will charge the user extra C. Detailed monitoring will send data every minute without additional charges D. Auto Scaling sends data every minute only and does not charge the user Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Auto Scaling includes 7 metrics and 1 dimension, and sends data to CloudWatch every 5 minutes by default. The user can enable detailed monitoring for Auto Scaling, which sends data to CloudWatch every minute. However, this will have some extra-costs. QUESTION NO: 80 A system admin is planning to setup event notifications on RDS. Which of the below mentioned services will help the admin setup notifications? A. AWS SES B. AWS Cloudtrail C. AWS Cloudwatch D. AWS SNS Answer: D Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. These notifications can be in any notification form supported by Amazon SNS for an AWS region, such as an email, a text message or a call to an HTTP endpoint QUESTION NO: 81 You are building an online store on AWS that uses SQS to process your customer orders. Your backend system needs those messages in the same sequence the customer orders have been put in. How can you achieve that? A. It is not possible to do this with SQS B. You can use sequencing information on each message C. You can do this with SQS but you also need to use SWF D. Messages will arrive in the same order by default Answer: B Explanation: Amazon SQS is engineered to always be available and deliver messages. One of the resulting tradeoffs is that SQSdoes not guarantee first in, first out delivery of messages. For many distributed applications, each message can stand on its own, and as long as all messages are delivered, the order is not important. If your system requires that order be preserved, you can place sequencing information in each message, so that you can reorder the messages when the queue returns them. QUESTION NO: 82 An organization wants to move to Cloud. They are looking for a secure encrypted database storage option. Which of the below mentioned AWS functionalities helps them to achieve this? A. AWS MFA with EBS B. AWS EBS encryption C. Multi-tier encryption with Redshift D. AWS S3 server side storage Answer: B Explanation: AWS EBS supports encryption of the volume while creating new volumes. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. The data at rest, the I/O as well as all the snapshots of EBS will be encrypted. The encryption occurs on the servers that host the EC2 instances, providing encryption of data as it moves between the EC2 instances and EBS storage. EBS encryption is based on the AES-256 cryptographic algorithm, which is the industry standard QUESTION NO: 83 A user wants to disable connection draining on an existing ELB. Which of the below mentioned statements helps the user disable connection draining on the ELB? A. The user can only disable connection draining from CLI B. It is not possible to disable the connection draining feature once enabled C. The user can disable the connection draining feature from EC2 -> ELB console or from CLI D. The user needs to stop all instances before disabling connection draining Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. The user can enable or disable connection draining from the AWS EC2 console -> ELB or using CLI. QUESTION NO: 84 A user has a refrigerator plant. The user is measuring the temperature of the plant every 15 minutes. If the user wants to send the data to CloudWatch to view the data visually, which of the below mentioned statements is true with respect to the information given above? A. The user needs to use AWS CLI or API to upload the data B. The user can use the AWS Import Export facility to import data to CloudWatch C. The user will upload data from the AWS console D. The user cannot upload data to CloudWatch since it is not an AWS service metric Answer: A Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. While sending the data the user has to include the metric name, namespace and timezone as part of the request. QUESTION NO: 85 A system admin is managing buckets, objects and folders with AWS S3. Which of the below mentioned statements is true and should be taken in consideration by the sysadmin? A. The folders support only ACL B. Both the object and bucket can have an Access Policy but folder cannot have policy C. Folders can have a policy D. Both the object and bucket can have ACL but folders cannot have ACL Answer: A Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. It cannot be applied at the object level. The folders are similar to objects with no content. Thus, folders can have only ACL and cannot have a policy. QUESTION NO: 86 A user has created an ELB with three instances. How many security groups will ELB create by default? A. 3 B. 5 C. 2 D. 1 Answer: C Explanation: Elastic Load Balancing provides a special Amazon EC2 source security group that the user can use to ensure that back-end EC2 instances receive traffic only from Elastic Load Balancing. This feature needs two security groups: the source security group and a security group that defines the ingress rules for the back-end instances. To ensure that traffic only flows between the load balancer and the back-end instances, the user can add or modify a rule to the back-end security group which can limit the ingress traffic. Thus, it can come only from the source security group provided by Elastic load Balancing. QUESTION NO: 87 An organization has created 50 IAM users. The organization wants that each user can change their password but cannot change their access keys. How can the organization achieve this? A. The organization has to create a special password policy and attach it to each user B. The root account owner has to use CLI which forces each IAM user to change their password on first login C. By default each IAM user can modify their passwords D. The root account owner can set the policy from the IAM console under the password policy screen Answer: D Explanation: With AWS IAM, organizations can use the AWS Management Console to display, create, change or delete a password policy. As a part of managing the password policy, the user can enable all users to manage their own passwords. If the user has selected the option which allows the IAM users to modify their password, he does not need to set a separate policy for the users. This option in the AWS console allows changing only the password. QUESTION NO: 88 A user has created a photo editing software and hosted it on EC2. The software accepts requests from the user about the photo format and resolution and sends a message to S3 to enhance the picture accordingly.Which of the below mentioned AWS services will help make a scalable software with the AWS infrastructure in this scenario? A. AWS Glacier B. AWS Elastic Transcoder C. AWS Simple Notification Service D. AWS Simple Queue Service Answer: D Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. The user can configure SQS, which will decouple the call between the EC2 application and S3. Thus, the application does not keep waiting for S3 to provide the data. QUESTION NO: 89 An application is generating a log file every 5 minutes. The log file is not critical but may be required only for verification in case of some major issue. The file should be accessible over the internet whenever required. Which of the below mentioned options is a best possible storage solution for it? A. AWS S3 B. AWS Glacier C. AWS RDS D. AWS RRS Answer: D Explanation: Amazon S3 stores objects according to their storage class. There are three major storage classes: Standard, Reduced Redundancy Storage and Glacier. Standard is for AWS S3 and provides very high durability. However, the costs are a little higher. Glacier is for archival and the files are not available over the internet. Reduced Redundancy Storage is for less critical files. Reduced Redundancy is little cheaper as it provides less durability in comparison to S3. In this case since the log files are not mission critical files, RRS will be a better option. QUESTION NO: 90 A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25. The user is trying to create the private subnet with CIDR 20.0.0.128/25. Which of the below mentioned statements is true in this scenario? A. It will not allow the user to create the private subnet due to a CIDR overlap B. It will allow the user to create a private subnet with CIDR as 20.0.0.128/25 C. This statement is wrong as AWS does not allow CIDR 20.0.0.0/25 D. It will not allow the user to create a private subnet due to a wrong CIDR range Answer: B Explanation: When the user creates a subnet in VPC, he specifies the CIDR block for the subnet. The CIDR block of a subnet can be the same as the CIDR block for the VPC (for a single subnet in the VPC., or a subset (to enable multiple subnets.. If the user creates more than one subnet in a VPC, the CIDR blocks of the subnets must not overlap. Thus, in this case the user has created a VPC with the CIDR block 20.0.0.0/24, which supports 256 IP addresses (20.0.0.0 to 20.0.0.255.. The user can break this CIDR block into two subnets, each supporting 128 IP addresses. One subnet uses the CIDR block 20.0.0.0/25 (for addresses 20.0.0.0 - 20.0.0.127. and the other uses the CIDR block 20.0.0.128/25 (for addresses 20.0.0.128 - 20.0.0.255.. QUESTION NO: 91 A user has created an S3 bucket which is not publicly accessible. The bucket is having thirty objects which are also private. If the user wants to make the objects public, how can he configure this with minimal efforts? A. The user should select all objects from the console and apply a single policy to mark them public B. The user can write a program which programmatically makes all objects public using S3 SDK C. Set the AWS bucket policy which marks all objects as public D. Make the bucket ACL as public so it will also mark all objects as public Answer: C Explanation: A system admin can grant permission of the S3 objects or buckets to any user or make the objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. QUESTION NO: 92 A sys admin is maintaining an application on AWS. The application is installed on EC2 and user has configured ELB and Auto Scaling. Considering future load increase, the user is planning to launch new servers proactively so that they get registered with ELB. How can the user add these instances with Auto Scaling? A. Increase the desired capacity of the Auto Scaling group B. Increase the maximum limit of the Auto Scaling group C. Launch an instance manually and register it with ELB on the fly D. Decrease the minimum limit of the Auto Scaling grou Answer: A Explanation: A user can increase the desired capacity of the Auto Scaling group and Auto Scaling will launch a new instance as per the new capacity. The newly launched instances will be registered with ELB if Auto Scaling group is configured with ELB. If the user decreases the minimum size the instances will be removed from Auto Scaling. Increasing the maximum size will not add instances but only set the maximum instance cap. QUESTION NO: 93 An organization, which has the AWS account ID as 999988887777, has created 50 IAM users. All the users are added to the same group cloudacademy. If the organization has enabled that each IAM user can login with the AWS console, which AWS login URL will the IAM users use? A. https:// 999988887777.signin.aws.amazon.com/console/ B. https:// signin.aws.amazon.com/cloudacademy/ C. https:// cloudacademy.signin.aws.amazon.com/999988887777/console/ D. https:// 999988887777.aws.amazon.com/ cloudacademy/ Answer: A Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Once the organization has created the IAM users, they will have a separate AWS console URL to login to the AWS console. The console login URL for the IAM user will be https:// AWS_Account_ID.signin.aws.amazon.com/console/. It uses only the AWS account ID and does not depend on the group or user ID. QUESTION NO: 94 A user has setup connection draining with ELB to allow in-flight requests to continue while the instance is being deregistered through Auto Scaling. If the user has not specified the draining time, how long will ELB allow inflight requests traffic to continue? A. 600 seconds B. 3600 seconds C. 300 seconds D. 0 seconds Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. The user can specify a maximum time (3600 seconds. for the load balancer to keep the connections alive before reporting the instance as deregistered. If the user does not specify the maximum timeout period, by default, the load balancer will close the connections to the deregistering instance after 300 seconds. QUESTION NO: 95 A root AWS account owner is trying to understand various options to set the permission to AWS S3. Which of the below mentioned options is not the right option to grant permission for S3? A. User Access Policy B. S3 Object Access Policy C. S3 Bucket Access Policy D. S3 ACL Answer: B Explanation: Amazon S3 provides a set of operations to work with the Amazon S3 resources. Managing S3 resource access refers to granting others permissions to work with S3. There are three ways the root account owner can define access with S3: S3 ACL: The user can use ACLs to grant basic read/write permissions to other AWS accounts. S3 Bucket Policy: The policy is used to grant other AWS accounts or IAM users permissions for the bucket and the objects in it. User Access Policy: Define an IAM user and assign him the IAM policy which grants him access to S3. QUESTION NO: 96 A sys admin has created a shopping cart application and hosted it on EC2. The EC2 instances are running behind ELB. The admin wants to ensure that the end user request will always go to the EC2 instance where the user session has been created. How can the admin configure this? A. Enable ELB cross zone load balancing B. Enable ELB cookie setup C. Enable ELB sticky session D. Enable ELB connection draining Answer: C Explanation: Generally AWS ELB routes each request to a zone with the minimum load. The Elastic Load Balancer provides a feature called sticky session which binds the user\u2019s session with a specific EC2 instance. If the sticky session is enabled the first request from the user will be redirected to any of the EC2 instances. But, henceforth, all requests from the same user will be redirected to the same EC2 instance. This ensures that all requests coming from the user during the session will be sent to the same application instance. QUESTION NO: 97 A user has configured ELB with three instances. The user wants to achieve High Availabilityi as well as redundancy with ELB. Which of the below mentioned AWS services helps the user achieve this for ELB? A. Route 53 B. AWS Mechanical Turk C. Auto Scaling D. AWS EMR Answer: A Explanation: The user can provide high availability and redundancy for applications running behind Elastic Load Balancer by enabling the Amazon Route 53 Domain Name System (DNS. failover for the load balancers. Amazon Route 53 is a DNS service that provides reliable routing to the user\u2019s infrastructure. QUESTION NO: 98 An organization is using AWS since a few months. The finance team wants to visualize the pattern of AWS spending. Which of the below AWS tool will help for this requirement? A. AWS Cost Manager B. AWS Cost Explorer C. AWS CloudWatch D. AWS Consolidated Billing Answer: B Explanation: The AWS Billing and Cost Management console includes the Cost Explorer tool for viewing AWS cost data as a graph. It does not charge extra to user for this service. With Cost Explorer the user can filter graphs using resource tags or with services in AWS. If the organization is using Consolidated Billing it helps generate report based on linked accounts. This will help organization to identify areas that require further inquiry. The organization can view trends and use that to understand spend and to predict future costs. QUESTION NO: 99 A user has launched an ELB which has 5 instances registered with it. The user deletes the ELB by mistake. What will happen to the instances? A. ELB will ask the user whether to delete the instances or not B. Instances will be terminated C. ELB cannot be deleted if it has running instances registered with it D. Instances will keep running Answer: D Explanation: When the user deletes the Elastic Load Balancer, all the registered instances will be deregistered. However, they will continue to run. The user will incur charges if he does not take any action on those instances. QUESTION NO: 100 A user is planning to setup notifications on the RDS DB for a snapshot. Which of the below mentioned event categories is not supported by RDS for this snapshot source type? A. Backup B. Creation C. Deletion D. Restoration Answer: A Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. Event categories for a snapshot source type include: Creation, Deletion, and Restoration. The Backup is a part of DB instance source type. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 101 A customer is using AWS for Dev and Test. The customer wants to setup the Dev environment with Cloudformation. Which of the below mentioned steps are not required while using Cloudformation? A. Create a stack B. Configure a service C. Create and upload the template D. Provide the parameters configured as part of the template Answer: B Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. AWS CloudFormation introduces two concepts: the template and the stack. The template is a JSON-format, text-based file that describes all the AWS resources required to deploy and run an application. The stack is a collection of AWS resources which are created and managed as a single unit when AWS CloudFormation instantiates a template. While creating a stack, the user uploads the template and provides the data for the parameters if required. QUESTION NO: 102 A user has configured the AWS CloudWatch alarm for estimated usage charges in the US East region. Which of the below mentioned statements is not true with respect to the estimated charges? A. It will store the estimated charges data of the last 14 days B. It will include the estimated charges of every AWS service C. The metric data will represent the data of all the regions D. The metric data will show data specific to that region Answer: D Explanation: When the user has enabled the monitoring of estimated charges for the AWS account with AWS CloudWatch, the estimated charges are calculated and sent several times daily to CloudWatch in the form of metric data. This data will be stored for 14 days. The billing metric data is stored in the US East (Northern Virginia. Region and represents worldwide charges. This data also includes the estimated charges for every service in AWS used by the user, as well as the estimated overall AWS charges. QUESTION NO: 103 A user is accessing RDS from an application. The user has enabled the Multi AZ feature with the MS SQL RDS DB. During a planned outage how will AWS ensure that a switch from DB to a standby replica will not affect access to the application? A. RDS will have an internal IP which will redirect all requests to the new DB B. RDS uses DNS to switch over to stand by replica for seamless transition C. The switch over changes Hardware so RDS does not need to worry about access D. RDS will have both the DBs running independently and the user has to manually switch over Answer: B Explanation: In the event of a planned or unplanned outage of a DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone if the user has enabled Multi AZ. The automatic failover mechanism simply changes the DNS record of the DB instance to point to the standby DB instance. As a result, the user will need to re-establish any existing connections to the DB instance. However, as the DNS is the same, the application can access DB seamlessly. QUESTION NO: 104 An organization is generating digital policy files which are required by the admins for verification. Once the files are verified they may not be required in the future unless there is some compliance issue. If the organization wants to save them in a cost effective way, which is the best possible solution? A. AWS RRS B. AWS S3 C. AWS RDS D. AWS Glacier Answer: D Explanation: Amazon S3 stores objects according to their storage class. There are three major storage classes: Standard, Reduced Redundancy and Glacier. Standard is for AWS S3 and provides very high durability. However, the costs are a little higher. Reduced redundancy is for less critical files. Glacier is for archival and the files which are accessed infrequently. It is an extremely low-cost storage service that provides secure and durable storage for data archiving and backup. QUESTION NO: 105 A user has launched an EBS backed instance. The user started the instance at 9 AM in the morning. Between 9 AM to 10 AM, the user is testing some script. Thus, he stopped the instance twice and restarted it. In the same hour the user rebooted the instance once. For how many instance hours will AWS charge the user? A. 3 hours B. 4 hours C. 2 hours D. 1 hour Answer: A Explanation: A user can stop/start or reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. When the instance is rebooted AWS will not charge the user for the extra hours. In case the user stops the instance, AWS does not charge the running cost but charges only the EBS storage cost. If the user starts and stops the instance multiple times in a single hour, AWS will charge the user for every start and stop. In this case, since the instance was rebooted twice, it will cost the user for 3 instance hours. QUESTION NO: 106 An organization has configured the custom metric upload with CloudWatch. The organization has given permission to its employees to upload data using CLI as well SDK. How can the user track the calls made to CloudWatch? A. The user can enable logging with CloudWatch which logs all the activities B. Use CloudTrail to monitor the API calls C. Create an IAM user and allow each user to log the data using the S3 bucket D. Enable detailed monitoring with CloudWatch Answer: B Explanation: AWS CloudTrail is a web service which will allow the user to monitor the calls made to the Amazon CloudWatch API for the organization\u2019s account, including calls made by the AWS Management Console, Command Line Interface (CLI., and other services. When CloudTrail logging is turned on, CloudWatch will write log files into the Amazon S3 bucket, which is specified during the CloudTrail configuration. QUESTION NO: 107 A user has created a queue named \u201cmyqueue\u201d with SQS. There are four messages published to queue which are not received by the consumer yet. If the user tries to delete the queue, what will happen? A. A user can never delete a queue manually. AWS deletes it after 30 days of inactivity on queue B. It will delete the queue C. It will initiate the delete but wait for four days before deleting until all messages are deleted automatically. D. It will ask user to delete the messages first Answer: B Explanation: SQS allows the user to move data between distributed components of applications so they can perform different tasks without losing messages or requiring each component to be always available. The user can delete a queue at any time, whether it is empty or not. It is important to note that queues retain messages for a set period of time. By default, a queue retains messages for four days. QUESTION NO: 108 A user has launched a large EBS backed EC2 instance in the US-East-1a region. The user wants to achieve Disaster Recovery (DR. for that instance by creating another small instance in Europe. How can the user achieve DR? A. Copy the running instance using the \u201cInstance Copy\u201d command to the EU region B. Create AMI of the instance and copy the AMI to the EU region. Then launch the instance from the EU AMI. C. Copy the instance from the US East region to the EU region D. Use the \u201cLaunch more like this\u201d option to copy the instance from one region to another Answer: B Explanation: To launch an EC2 instance it is required to have an AMI in that region. If the AMI is not available in that region, then create a new AMI or use the copy command to copy the AMI from one region to the other region. QUESTION NO: 109 A user has created numerous EBS volumes. What is the general limit for each AWS account for the maximum number of EBS volumes that can be created? A. 10000 B. 5000 C. 100 D. 1000 Answer: B Explanation: A user can attach multiple EBS volumes to the same instance within the limits specified by his AWS account. Each AWS account has a limit on the number of Amazon EBS volumes that the user can create, and the total storage available. The default limit for the maximum number of volumes that can be created is 5000. QUESTION NO: 110 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24. and VPN only subnets CIDR (20.0.1.0/24. along with the VPN gateway (vgw-12345. to connect to the user\u2019s data centre. Which of the below mentioned options is a valid entry for the main route table in this scenario? A. Destination: 20.0.0.0/24 and Target: vgw-12345 B. Destination: 20.0.0.0/16 and Target: ALL C. Destination: 20.0.1.0/16 and Target: vgw-12345 D. Destination: 0.0.0.0/0 and Target: vgw-12345 Answer: D Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. Here are the valid entries for the main route table in this scenario: Destination: 0.0.0.0/0 & Target: vgw-12345 (To route all internet traffic to the VPN gateway. Destination: 20.0.0.0/16 & Target: local (To allow local routing in VPC. QUESTION NO: 111 A user has stored data on an encrypted EBS volume. The user wants to share the data with his friend\u2019s AWS account. How can user achieve this? A. Create an AMI from the volume and share the AMI B. Copy the data to an unencrypted volume and then share C. Take a snapshot and share the snapshot with a friend D. If both the accounts are using the same encryption key then the user can share the volume directly Answer: B Explanation: AWS EBS supports encryption of the volume. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. If the user is having data on an encrypted volume and is trying to share it with others, he has to copy the data from the encrypted volume to a new unencrypted volume. Only then can the user share it as an encrypted volume data. Otherwise the snapshot cannot be shared. QUESTION NO: 112 A user has enabled the Multi AZ feature with the MS SQL RDS database server. Which of the below mentioned statements will help the user understand the Multi AZ feature better? A. In a Multi AZ, AWS runs two DBs in parallel and copies the data asynchronously to the replica copy B. In a Multi AZ, AWS runs two DBs in parallel and copies the data synchronously to the replica copy C. In a Multi AZ, AWS runs just one DB but copies the data synchronously to the standby replica D. AWS MS SQL does not support the Multi AZ feature Answer: C Explanation: Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption.Note that the high-availability feature is not a scaling solution for read-only scenarios; you cannot use a standby replica to serve read traffic. To service read-only traffic, you should use a read replica. QUESTION NO: 113 An organization is using cost allocation tags to find the cost distribution of different departments and projects. One of the instances has two separate tags with the key/ value as \u201cInstanceName/ HR\u201d, \u201cCostCenter/HR\u201d. What will AWS do in this case? A. InstanceName is a reserved tag for AWS. Thus, AWS will not allow this tag B. AWS will not allow the tags as the value is the same for different keys C. AWS will allow tags but will not show correctly in the cost allocation report due to the same value of the two separate keys D. AWS will allow both the tags and show properly in the cost distribution report Answer: D Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources, AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. Each tag will have a key-value and can be applied to services, such as EC2, S3, RDS, EMR, etc. It is required that the key should be different for each tag. The value can be the same for different keys. In this case since the value is different, AWS will properly show the distribution report with the correct values. QUESTION NO: 114 A user is publishing custom metrics to CloudWatch. Which of the below mentioned statements will help the user understand the functionality better? A. The user can use the CloudWatch Import tool B. The user should be able to see the data in the console after around 15 minutes C. If the user is uploading the custom data, the user must supply the namespace, timezone, and metric name as part of the command D. The user can view as well as upload data using the console, CLI and APIs Answer: B Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user has to always include the namespace as a part of the request. However, the other parameters are optional. If the user has uploaded data using CLI, he can view it as a graph inside the console. The data will take around 2 minutes to upload but can be viewed only after around 15 minutes. QUESTION NO: 115 A user is launching an EC2 instance in the US East region. Which of the below mentioned options is recommended by AWS with respect to the selection of the availability zone? A. Always select the US-East-1-a zone for HA B. Do not select the AZ; instead let AWS select the AZ C. The user can never select the availability zone while launching an instance D. Always select the AZ while launching an instance Answer: B Explanation: When launching an instance with EC2, AWS recommends not to select the availability zone (AZ.. AWS specifies that the default Availability Zone should be accepted. This is because it enables AWS to select the best Availability Zone based on the system health and available capacity. If the user launches additional instances, only then an Availability Zone should be specified. This is to specify the same or different AZ from the running instances. QUESTION NO: 116 A user has created a VPC with CIDR 20.0.0.0/16 with only a private subnet and VPN connection using the VPC wizard. The user wants to connect to the instance in a private subnet over SSH. How should the user define the security rule for SSH? A. Allow Inbound traffic on port 22 from the user\u2019s network B. The user has to create an instance in EC2 Classic with an elastic IP and configure the security group of a private subnet to allow SSH from that elastic IP C. The user can connect to a instance in a private subnet using the NAT instance D. Allow Inbound traffic on port 80 and 22 to allow the user to connect to a private subnet over the Internet Answer: A Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, the user can setup a case with a VPN only subnet (private. which uses VPN access to connect with his data centre. When the user has configured this setup with Wizard, all network connections to the instances in the subnet will come from his data centre. The user has to configure the security group of the private subnet which allows the inbound traffic on SSH (port 22. from the data centre\u2019s network range. QUESTION NO: 117 A user has created an ELB with the availability zone US-East-1A. The user wants to add more zones to ELB to achieve High Availability. How can the user add more zones to the existing ELB? A. It is not possible to add more zones to the existing ELB B. The only option is to launch instances in different zones and add to ELB C. The user should stop the ELB and add zones and instances as required D. The user can add zones on the fly from the AWS console Answer: D Explanation: The user has created an Elastic Load Balancer with the availability zone and wants to add more zones to the existing ELB. The user can do so in two ways: From the console or CLI, add new zones to ELB; Launch instances in a separate AZ and add instances to the existing ELB. QUESTION NO: 118 A user has configured an Auto Scaling group with ELB. The user has enabled detailed CloudWatch monitoring on Elastic Load balancing. Which of the below mentioned statements will help the user understand this functionality better? A. ELB sends data to CloudWatch every minute only and does not charge the user B. ELB will send data every minute and will charge the user extra C. ELB is not supported by CloudWatch D. It is not possible to setup detailed monitoring for ELB Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Elastic Load Balancing includes 10 metrics and 2 dimensions, and sends data to CloudWatch every minute. This does not cost extra. QUESTION NO: 119 A user has configured ELB with two EBS backed EC2 instances. The user is trying to understand the DNS access and IP support for ELB. Which of the below mentioned statements may not help the user understand the IP mechanism supported by ELB? A. The client can connect over IPV4 or IPV6 using Dualstack B. ELB DNS supports both IPV4 and IPV6 C. Communication between the load balancer and back-end instances is always through IPV4 D. The ELB supports either IPV4 or IPV6 but not both Answer: D Explanation: Elastic Load Balancing supports both Internet Protocol version 6 (IPv6. and Internet Protocol version 4 (IPv4.. Clients can connect to the user\u2019s load balancer using either IPv4 or IPv6 (in EC2- Classic. DNS. However, communication between the load balancer and its back-end instances uses only IPv4. The user can use the Dualstack-prefixed DNS name to enable IPv6 support for communications between the client and the load balancers. Thus, the clients are able to access the load balancer using either IPv4 or IPv6 as their individual connectivity needs dictate. QUESTION NO: 120 A user has received a message from the support team that an issue occurred 1 week back between 3 AM to 4 AM and the EC2 server was not reachable. The user is checking the CloudWatch metrics of that instance. How can the user find the data easily using the CloudWatch console? A. The user can find the data by giving the exact values in the time Tab under CloudWatch metrics B. The user can find the data by filtering values of the last 1 week for a 1 hour period in the Relative tab under CloudWatch metrics C. It is not possible to find the exact time from the console. The user has to use CLI to provide the specific time D. The user can find the data by giving the exact values in the Absolute tab under CloudWatch metrics Answer: D Explanation: If the user is viewing the data inside the CloudWatch console, the console provides options to filter values either using the relative period, such as days /hours or using the Absolute tab where the user can provide data with a specific date and time. The console also provides the option to search using the local timezone under the time range caption in the console. QUESTION NO: 121 A user has setup Auto Scaling with ELB on the EC2 instances. The user wants to configure that whenever the CPU utilization is below 10%, Auto Scaling should remove one instance. How can the user configure this? A. The user can get an email using SNS when the CPU utilization is less than 10%. The user can use the desired capacity of Auto Scaling to remove the instance B. Use CloudWatch to monitor the data and Auto Scaling to remove the instances using scheduled actions C. Configure CloudWatch to send a notification to Auto Scaling Launch configuration when the CPU utilization is less than 10% and configure the Auto Scaling policy to remove the instance D. Configure CloudWatch to send a notification to the Auto Scaling group when the CPU Utilization is less than 10% and configure the Auto Scaling policy to remove the instance Answer: D Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup to receive a notification on the Auto Scaling group with the CloudWatch alarm when the CPU utilization is below a certain threshold. The user can configure the Auto Scaling policy to take action for removing the instance. When the CPU utilization is below 10% CloudWatch will send an alarm to the Auto Scaling group to execute the policy. QUESTION NO: 122 A user has enabled detailed CloudWatch metric monitoring on an Auto Scaling group. Which of the below mentioned metrics will help the user identify the total number of instances in an Auto Scaling group cluding pending, terminating and running instances? A. GroupTotalInstances B. GroupSumInstances C. It is not possible to get a count of all the three metrics together. The user has to find the individual number of running, terminating and pending instances and sum it D. GroupInstancesCount Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. For Auto Scaling, CloudWatch provides various metrics to get the group information, such as the Number of Pending, Running or Terminating instances at any moment. If the user wants to get the total number of Running, Pending and Terminating instances at any moment, he can use the GroupTotalInstances metric. QUESTION NO: 123 A user is trying to configure the CloudWatch billing alarm. Which of the below mentioned steps should be performed by the user for the first time alarm creation in the AWS Account Management section? A. Enable Receiving Billing Reports B. Enable Receiving Billing Alerts C. Enable AWS billing utility D. Enable CloudWatch Billing Threshold Answer: B Explanation: AWS CloudWatch supports enabling the billing alarm on the total AWS charges. Before the user can create an alarm on the estimated charges, he must enable monitoring of the estimated AWS charges, by selecting the option \u201cEnable receiving billing alerts\u201d. It takes about 15 minutes before the user can view the billing data. The user can then create the alarms. QUESTION NO: 124 A user is checking the CloudWatch metrics from the AWS console. The user notices that the CloudWatch data is coming in UTC. The user wants to convert the data to a local time zone. How can the user perform this? A. In the CloudWatch dashboard the user should set the local timezone so that CloudWatch shows the data only in the local time zone B. In the CloudWatch console select the local timezone under the Time Range tab to 712 view the data as per the local timezone C. The CloudWatch data is always in UTC; the user has to manually convert the data D. The user should have send the local timezone while uploading the data so that CloudWatch will show the data only in the local timezone Answer: B Explanation: If the user is viewing the data inside the CloudWatch console, the console provides options to filter values either using the relative period, such as days/hours or using the Absolute tab where the use can provide data with a specific date and time. The console also provides the option to search using the local timezone under the time range caption in the console because the time range tab allows the user to change the time zone. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 125 An organization (Account ID 123412341234. has attached the below mentioned IAM policy to a user. What does this policy statement entitle the user to perform? \"Statement\": [ { \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*AccessKey*\", ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] } ] Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (Account ID 123412341234. wants some of their users to manage keys (access and secret access keys. of all IAM users, the organization should set the below mentioned policy which entitles the IAM user to modify keys of all IAM users with CLI, SDK or API. \"Statement\": [ { \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*AccessKey*\", ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] } ] QUESTION NO: 126 A user is trying to connect to a running EC2 instance using SSH. However, the user gets a connection time out error. Which of the below mentioned options is not a possible reason for rejection? A. The access key to connect to the instance is wrong B. The security group is not configured properly C. The private key used to launch the instance is not correct D. The instance CPU is heavily loaded Answer: A Explanation: If the user is trying to connect to a Linux EC2 instance and receives the connection time out error the probable reasons are: - Security group is not configured with the SSH port - The private key pair is not right - The user name to login is wrong - The instance CPU is heavily loaded, so it does not allow more connections QUESTION NO: 127 A user has configured Elastic Load Balancing by enabling a Secure Socket Layer (SSL) negotiation configuration known as a Security Policy. Which of the below mentioned options is not part of this secure policy while negotiating the SSL connection between the user and the client? A. SSL Protocols B. Client Order Preference C. SSL Ciphers D. Server Order Preference Answer: B Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the load balancer. A security policy is a combination of SSL Protocols, SSL Ciphers, and the Server order Preference option. QUESTION NO: 128 A user has configured CloudWatch monitoring on an EBS backed EC2 instance. If the user has not attached any additional device, which of the below mentioned metrics will always show a 0 value? A. DiskReadBytes 7456 B. NetworkIn C. NetworkOut D. CPUUtilization Answer: A Explanation: CloudWatch is used to monitor AWS as the well custom services. For EC2 when the user is monitoring the EC2 instances, it will capture the 7 Instance level and 3 system check parameters for the EC2 instance. Since this is an EBS backed instance, it will not have ephermal storage attached to it. Out of the 7 EC2 metrics, the 4 metrics DiskReadOps, DiskWriteOps, DiskReadBytes and DiskWriteBytes are disk related data and available only when there is ephermal storage attached to an instance. For an EBS backed instance without any additional device, this data will be 0. QUESTION NO: 129 A user has launched an EBS backed EC2 instance. What will be the difference while performing the restart or stop/start options on that instance? A. For restart it does not charge for an extra hour, while every stop/start it will be charged as a separate hour B. Every restart is charged by AWS as a separate hour, while multiple start/stop actions during a single hour will be counted as a single hour C. For every restart or start/stop it will be charged as a separate hour D. For restart it charges extra only once, while for every stop/start it will be charged as a separate hour Answer: A Explanation: For an EC2 instance launched with an EBS backed AMI, each time the instance state is changed from stop to start/ running, AWS charges a full instance hour, even if these transitions happen multiple times within a single hour. Anyway, rebooting an instance AWS does not charge a new instance billing hour. QUESTION NO: 130 A user has created a queue named \u201cmyqueue\u201d in US-East region with AWS SQS. The user\u2019s AWS account ID is 123456789012. If the user wants to perform some action on this queue, which of the below Queue URL should he use? A. http://sqs.us-east-1.amazonaws.com/123456789012/myqueue B. http://sqs.amazonaws.com/123456789012/myqueue C. http://sqs. 123456789012.us-east-1.amazonaws.com/myqueue D. http:// 123456789012.sqs. us-east-1.amazonaws.com/myqueue Answer: A Explanation: When creating a new queue in SQS, the user must provide a queue name that is unique within the scope of all queues of user\u2019s account. If the user creates queues using both the latest WSDL and a previous version, he will have a single namespace for all his queues. Amazon SQS assigns each queue created by user an identifier called a queue URL, which includes the queue name and other components that Amazon SQS determines. Whenever the user wants to perform an action on a queue, he must provide its queue URL. The queue URL for the account id 123456789012 & queue name \u201cmyqueue\u201d in US-East-1 region will be http:// sqs.us-east1.amazonaws.com/123456789012/myqueue. QUESTION NO: 131 A sys admin is trying to understand the Auto Scaling activities. Which of the below mentioned processes is not performed by Auto Scaling? A. Reboot Instance B. Schedule Actions C. Replace Unhealthy D. Availability Zone Balancing Answer: A Explanation: There are two primary types of Auto Scaling processes: Launch and Terminate, which launch or terminate instances, respectively. Some other actions performed by Auto Scaling are: AddToLoadbalancer, AlarmNotification, HealthCheck, AZRebalance, ReplaceUnHealthy, and ScheduledActions. QUESTION NO: 132 A sys admin is trying to understand EBS snapshots. Which of the below mentioned statements will not be useful to the admin to understand the concepts about a snapshot? A. The snapshot is synchronous B. It is recommended to stop the instance before taking a snapshot for consistent data C. The snapshot is incremental D. The snapshot captures the data that has been written to the hard disk when the snapshot command was executed Answer: A Explanation: The AWS snapshot is a point in time backup of an EBS volume. When the snapshot command is executed it will capture the current state of the data that is written on the drive and take a backup. For a better and consistent snapshot of the root EBS volume, AWS recommends stopping the instance. For additional volumes it is recommended to unmount the device. The snapshots are asynchronous and incremental. QUESTION NO: 133 A root account owner has created an S3 bucket testmycloud. The account owner wants to allow everyone to upload the objects as well as enforce that the person who uploaded the object should manage the permission of those objects. Which is the easiest way to achieve this? A. The root account owner should create a bucket policy which allows the IAM users to upload the object B. The root account owner should create the bucket policy which allows the other account owners to set the object policy of that bucket C. The root account should use ACL with the bucket to allow everyone to upload the object D. The root account should create the IAM users and provide them the permission to upload content to the bucket Answer: C Explanation: Each AWS S3 bucket and object has an ACL (Access Control List. associated with it. An ACL is a list of grants identifying the grantee and the permission granted. The user can use ACLs to grant basic read/write permissions to other AWS accounts. ACLs use an Amazon S3\u2013specific XML schema. The user cannot grant permissions to other users in his account. ACLs are suitable for specific scenarios. For example, if a bucket owner allows other AWS accounts to upload objects, permissions to these objects can only be managed using the object ACL by the AWS account that owns the object. QUESTION NO: 134 An organization has setup consolidated billing with 3 different AWS accounts. Which of the below mentioned advantages will organization receive in terms of the AWS pricing? A. The consolidated billing does not bring any cost advantage for the organization B. All AWS accounts will be charged for S3 storage by combining the total storage of each account C. The EC2 instances of each account will receive a total of 750*3 micro instance hours free D. The free usage tier for all the 3 accounts will be 3 years and not a single year Answer: B Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. For billing purposes, AWS treats all the accounts on the consolidated bill as one account. Some services, such as Amazon EC2 and Amazon S3 have volume pricing tiers across certain usage dimensions that give the user lower prices when he uses the service more. QUESTION NO: 135 A user has launched two EBS backed EC2 instances in the US-East-1a region. The user wants to change the zone of one of the instances. How can the user change it? A. Stop one of the instances and change the availability zone B. The zone can only be modified using the AWS CLI C. From the AWS EC2 console, select the Actions - > Change zones and specify new zone D. Create an AMI of the running instance and launch the instance in a separate AZ Answer: D Explanation: With AWS EC2, when a user is launching an instance he can select the availability zone (AZ. at the time of launch. If the zone is not selected, AWS selects it on behalf of the user. Once the instance is launched, the user cannot change the zone of that instance unless he creates an AMI of that instance and launches a new instance from it. QUESTION NO: 136 A user wants to make so that whenever the CPU utilization of the AWS EC2 instance is above 90%, the redlight of his bedroom turns on. Which of the below mentioned AWS services is helpful for this purpose? A. AWS CloudWatch + AWS SES B. AWS CloudWatch + AWS SNS C. None. It is not possible to configure the light with the AWS infrastructure services D. AWS CloudWatch and a dedicated software turning on the light Answer: B Explanation: Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can configure some sensor devices at his home which receives data on the HTTP end point (REST calls. and turn on the red light. The user can configure the CloudWatch alarm to send a notification to the AWS SNS HTTP end point (the sensor device. and it will turn the light red when there is an alarm condition. QUESTION NO: 137 An organization has added 3 of his AWS accounts to consolidated billing. One of the AWS accounts has purchased a Reserved Instance (RI. of a small instance size in the US-East-1a zone. All other AWS accounts are running instances of a small size in the same zone. What will happen in this case for the RI pricing? A. Only the account that has purchased the RI will get the advantage of RI pricing B. One instance of a small size and running in the US-East-1a zone of each AWS account will get the benefit of RI pricing C. Any single instance from all the three accounts can get the benefit of AWS RI pricing if they are running in the same zone and are of the same size D. If there are more than one instances of a small size running across multiple accounts in the same zone no one will get the benefit of RI Answer: C Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. For billing purposes, consolidated billing treats all the accounts on the consolidated bill as one account. This means that all accounts on a consolidated bill can receive the hourly cost benefit of the Amazon EC2 Reserved Instances purchased by any other account. In this case only one Reserved Instance has been purchased by one account. Thus, only a single instance from any of the accounts will get the advantage of RI. AWS will implement the blended rate for each instance if more than one instance is running concurrently. QUESTION NO: 138 An organization is planning to use AWS for 5 different departments. The finance department is responsible to pay for all the accounts. However, they want the cost separation for each account to map with the right cost centre. How can the finance department achieve this? A. Create 5 separate accounts and make them a part of one consolidate billing B. Create 5 separate accounts and use the IAM cross account access with the roles for better management C. Create 5 separate IAM users and set a different policy for their access D. Create 5 separate IAM groups and add users as per the department\u2019s employees Answer: A Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. Consolidated billing enables the organization to see a combined view of the AWS charges incurred by each account as well as obtain a detailed cost report for each of the individual AWS accounts associated with the paying account. QUESTION NO: 139 A user has setup an EBS backed instance and a CloudWatch alarm when the CPU utilization is more than 65%. The user has setup the alarm to watch it for 5 periods of 5 minutes each. The CPU utilization is 60% between 9 AM to 6 PM. The user has stopped the EC2 instance for 15 minutes between 11 AM to 11:15 AM. What will be the status of the alarm at 11:30 AM? A. Alarm B. OK C. Insufficient Data D. Error Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The state of the alarm will be OK for the whole day. When the user stops the instance for three periods the alarm may not receive the data QUESTION NO: 140 A user is running one instance for only 3 hours every day. The user wants to save some cost with the instance. Which of the below mentioned Reserved Instance categories is advised in this case? A. The user should not use RI; instead only go with the on-demand pricing B. The user should use the AWS high utilized RI C. The user should use the AWS medium utilized RI D. The user should use the AWS low utilized RI Answer: A Explanation: The AWS Reserved Instance provides the user with an option to save some money by paying a one-time fixed amount and then save on the hourly rate. It is advisable that if the user is having 30% or more usage of an instance per day, he should go for a RI. If the user is going to use an EC2 instance for more than 2200-2500 hours per year, RI will help the user save some cost. Here, the instance is not going to run for less than 1500 hours. Thus, it is advisable that the user should use the on-demand pricing. QUESTION NO: 141 A user has setup an RDS DB with Oracle. The user wants to get notifications when someone modifies the security group of that DB. How can the user configure that? A. It is not possible to get the notifications on a change in the security group B. Configure SNS to monitor security group changes C. Configure event notification on the DB security group D. Configure the CloudWatch alarm on the DB for a change in the security group Answer: C Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. These events can be configured for source categories, such as DB instance, DB security group, DB snapshot and DB parameter group. If the user is subscribed to a Configuration Change category for a DB security group, he will be notified when the DB security group is changed. QUESTION NO: 142 A user is trying to setup a recurring Auto Scaling process. The user has setup one process to scale up every day at 8 am and scale down at 7 PM. The user is trying to setup another recurring process which scales up on the 1st of every month at 8 AM and scales down the same day at 7 PM. What will Auto Scaling do in this scenario? A. Auto Scaling will execute both processes but will add just one instance on the 1st B. Auto Scaling will add two instances on the 1st of the month C. Auto Scaling will schedule both the processes but execute only one process randomly D. Auto Scaling will throw an error since there is a conflict in the schedule of two separate Auto Scaling Processes Answer: D Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can also configure the recurring schedule action which will follow the Linux cron format. As per Auto Scaling, a scheduled action must have a unique time value. If the user attempts to schedule an activity at a time when another existing activity is already scheduled, the call will be rejected with an error message noting the conflict. QUESTION NO: 143 A user is planning to setup infrastructure on AWS for the Christmas sales. The user is planning to use Auto Scaling based on the schedule for proactive scaling. What advise would you give to the user? A. It is good to schedule now because if the user forgets later on it will not scale up B. The scaling should be setup only one week before Christmas C. Wait till end of November before scheduling the activity D. It is not advisable to use scheduled based scaling Answer: C Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can specify any date in the future to scale up or down during that period. As per Auto Scaling the user can schedule an action for up to a month in the future. Thus, it is recommended to wait until end of November before scheduling for Christmas. QUESTION NO: 144 A user is trying to understand the ACL and policy for an S3 bucket. Which of the below mentioned policy permissions is equivalent to the WRITE ACL on a bucket? A. s3:GetObjectAcl B. s3:GetObjectVersion C. s3:ListBucketVersions D. s3:DeleteObject Answer: D Explanation: Amazon S3 provides a set of operations to work with the Amazon S3 resources. Each AWS S3 bucket can have an ACL (Access Control List. or bucket policy associated with it. The WRITE ACL list allows the other AWS accounts to write/modify to that bucket. The equivalent S3 bucket policy permission for it is s3:DeleteObject. QUESTION NO: 145 A user has created an ELB with Auto Scaling. Which of the below mentioned offerings from ELB helps the user to stop sending new requests traffic from the load balancer to the EC2 instance when the instance is being deregistered while continuing in-flight requests? A. ELB sticky session B. ELB deregistration check C. ELB connection draining D. ELB auto registration Off Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. QUESTION NO: 146 A user has launched an EC2 instance from an instance store backed AMI. The infrastructure team wants to create an AMI from the running instance. Which of the below mentioned steps will not be performed while creating the AMI? A. Define the AMI launch permissions B. Upload the bundled volume C. Register the AMI D. Bundle the volume Answer: A Explanation: When the user has launched an EC2 instance from an instance store backed AMI, it will need to follow certain steps, such as \u201cBundling the root volume\u201d, \u201cUploading the bundled volume\u201d and \u201cRegister the AMI\u201d. Once the AMI is created the user can setup the launch permission. However, it is not required to setup during the launch. QUESTION NO: 147 You are managing the AWS account of a big organization. The organization has more than 1000+ employees and they want to provide access to the various services to most of the employees. Which of the below mentioned options is the best possible solution in this case? A. The user should create a separate IAM user for each employee and provide access to them as per the policy B. The user should create an IAM role and attach STS with the role. The user should attach that role to the EC2 instance and setup AWS authentication on that server C. The user should create IAM groups as per the organization\u2019s departments and add each user to the group for better access control D. Attach an IAM role with the organization\u2019s authentication service to authorize each user for various AWS services Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user is managing an AWS account for an organization that already has an identity system, such as the login system for the corporate network (SSO.. In this case, instead of creating individual IAM users or groups for each user who need AWS access, it may be more practical to use a proxy server to translate the user identities from the organization network into the temporary AWS security credentials. This proxy server will attach an IAM role to the user after authentication. QUESTION NO: 148 A user has configured a VPC with a new subnet. The user has created a security group. The user wants to configure that instances of the same subnet communicate with each other. How can the user configure this with the security group? A. There is no need for a security group modification as all the instances can communicate with each other inside the same subnet B. Configure the subnet as the source in the security group and allow traffic on all the protocols and ports C. Configure the security group itself as the source and allow traffic on all the protocols and ports D. The user has to use VPC peering to configure this Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features that the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level. If the user is using the default security group it will have a rule which allows the instances to communicate with other. For a new security group the user has to specify the rule, add it to define the source as the security group itself, and select all the protocols and ports for that source. QUESTION NO: 149 A user is launching an instance. He is on the \u201cTag the instance\u201d screen. Which of the below mentioned information will not help the user understand the functionality of an AWS tag? A. Each tag will have a key and value B. The user can apply tags to the S3 bucket C. The maximum value of the tag key length is 64 unicode characters D. AWS tags are used to find the cost distribution of various resources Answer: C Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources, AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. Each tag will have a key-value and can be applied to services, such as EC2, S3, RDS, EMR, etc. The maximum size of a tag key is 128 unicode characters. QUESTION NO: 150 A user has created a VPC with CIDR 20.0.0.0/16. The user has created public and VPN only subnets along with hardware VPN access to connect to the user\u2019s datacenter. The user wants to make so that all traffic coming to the public subnet follows the organization\u2019s proxy policy. How can the user make this happen? A. Setting up a NAT with the proxy protocol and configure that the public subnet receives traffic from NAT B. Settin up a proxy policy in the internet gateway connected with the public subnet C. It is not possible to setup the proxy policy for a public subnet D. Setting the route table and security group of the public subnet which receives traffic from a virtual private gateway Answer: D Explanation: The user can create subnets within a VPC. If the user wants to connect to VPC from his own data centre, he can setup public and VPN only subnets which uses hardware VPN access to connect with his data centre. When the user has configured this setup, it will update the main route table used with the VPN-only subnet, create a custom route table and associate it with the public subnet. It also creates an internet gateway for the public subnet. By default the internet traffic of the VPN subnet is routed to a virtual private gateway while the internet traffic of the public subnet is routed through the internet gateway. The user can set up the route and security group rules. These rules enable the traffic to come from the organization\u2019s network over the virtual private gateway to the public subnet to allow proxy settings on that public subnet. Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"AWS Certified SysOps Administrator Questions- 1st"},{"location":"nightwolf-cotribution/aws/#aws-certified-sysops-administrator-questions-and-answers","text":"These are AWS interview questions for experienced professionals. You will find these questions very helpful in your AWS professional role interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 1 You are currently hosting multiple applications in a VPC and have logged numerous port scans coming in from a specific IP address block. Your security team has requested that all access from the offending IP address block be denied tor the next 24 hours. Which of the following is the best method to quickly and temporarily deny access from the specified IP address block? A. Create an AD policy to modify Windows Firewall settings on all hosts in the VPC to deny access from the IP address block B. Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP address block C. Add a rule to all of the VPC 5 Security Groups to deny access from the IP address block D. Modify the Windows Firewall settings on all Amazon Machine Images (AMIs) that your organization uses in that VPC to deny access from the IP address block Answer: C Reference: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html QUESTION NO: 2 When preparing for a compliance assessment of your system built inside of AWS. what are three best practices for you to prepare for anaudit? Choose any 3 answers. A. Gather evidence of your IT operational controls B. Request and obtain applicable third-party audited AWS compliance reports and certifications C. Request and obtain a compliance and security tour of an AWS data center for a pre-assessment security review D. Request and obtain approval from AWS to perform relevant network scans and indepth penetration tests of your system and endpoints E. Schedule meetings with AWS third-party auditors to provide evidence of AWS compliance that maps to your control objectives Answer: B,D,E QUESTION NO: 3 You have started a new job and are reviewing your company infrastructure on AWS You notice one web application where they have an Elastic Load Balancer (&B) in front of web instances in an Auto Scaling Group. When you check the metrics for the ELB in CloudWatch you see four healthy instances In Availability Zone (AZ) A and zero in AZ B There are zero unhealthy instances. What do you need to fix to balance the instances across AZs? A. Set the ELB to only be attached to another AZ B. Make sure Auto Scaling is configured to launch in both AZs C. Make sure your AMI is available in both AZs D. Make sure the maximum size of the Auto Scaling Group is greater than 4 Answer: B QUESTION NO: 4 You have been asked to leverage Amazon VPC BC2 and SOS to implement an application that submits and receives millions of messages per second to a message queue. You want to ensure your application has sufficient bandwidth between your EC2 instances and SQS Which option will provide (he most scalable solution for bandwidth between the application and SOS? A. Ensure the application instances are properly configured with an Elastic Load Balancer. B. Ensure the application instances are launched in private subnets with the EBS-optimized option enabled. C. Ensure the application instances are launched in public subnets with the associate-publicIP-address=true option enabled D. Launch application instances in private subnets with an Auto Scaling group and Auto Scaling triggers configured to watch the SOS queue size Answer: C Reference: http://www.cardinalpath.com/autoscaling-your-website-with-amazon-web-services-part-2/ QUESTION NO: 5 You have identified network throughput as a bottleneck on your ml small EC2 instance when uploading data into Amazon S3 In the same region. How do you remedy this situation? A. Add an additional ENI B. Change to a larger Instance C. Use DirectConnect between EC2 and S3 D. Use EBS PIOPS on the local volume Answer: B Reference: https://media.amazonwebservices.com/AWS_Amazon_EMR_Best_Practices.pdf QUESTION NO: 6 When attached to an Amazon VPC which two components provide connectivity with external networks? Choose 2 answers A. Elastic IPS (EIP) B. NAT Gateway (NAT) C. Internet Gateway {IGW) D. Virtual Private Gateway (VGW) Answer: C,D QUESTION NO: 7 Your application currently leverages AWS Auto Scaling to grow and shrink as load Increases/decreases and has been performing well. Your marketing team expects a steady ramp up in traffic to follow an upcoming campaign that will result in a 20x growth in traffic over 4 weeks. Your forecast for the approximate number of Amazon EC2 instances necessary to meet the peak demand is 175. What should you do to avoid potential service disruptions during the ramp up in traffic? 456789 A. Ensure that you have pre-allocated 175 Elastic IP addresses so that each server will be able to obtain one as it launches B. Check the service limits in Trusted Advisor and adjust as necessary so the forecasted count remains within limits. C. Change your Auto Scaling configuration to set a desired capacity of 175 prior to the launch of the marketing campaign. D. Pre-warm your Elastic Load Balancer to match the requests per second anticipated during peak demand prior to the marketing campaign. Answer: D QUESTION NO: 8 You have an Auto Scaling group associated with an Elastic Load Balancer (ELB). You have noticed that instances launched via the Auto Scaling group are being marked unhealthy due to an ELB health check, but these unhealthy instances are not being terminated. What do you need to do to ensure trial instances marked unhealthy by the ELB will be terminated and replaced? A. Change the thresholds set on the Auto Scaling group health check B. Add an Elastic Load Balancing health check to your Auto Scaling group C. Increase the value for the Health check interval set on the Elastic Load Balancer D. Change the health check set on the Elastic Load Balancer to use TCP rather than HTTP checks Answer: B Reference: http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/as-add-elb-healthcheck.html Add an Elastic Load Balancing Health Check to your Auto Scaling GroupBy default, an Auto Scaling group periodically reviews the results of EC2 instance status to determine the health state of each instance. However, if you have associated your Auto Scaling group with an Elastic Load Balancing load balancer, you can choose to use the Elastic Load Balancing health check. In this case, Auto Scaling determines the health status of your instances by checking the results of both the EC2 instance status check and the Elastic Load Balancing instance health check. For information about EC2 instance status checks, see Monitor Instances With Status Checks in the Amazon EC2 User Guide for Linux Instances. For information about Elastic Load Balancing health checks, see Health Check in the Elastic Load Balancing Developer Guide. This topic shows you how to add an Elastic Load Balancing health check to your Auto Scaling group, assuming that you have created a load balancer and have registered the load balancer with your Auto Scaling group. If you have not registered the load balancer with your Auto Scaling group, see Set Up a Scaled and Load-Balanced Application. Auto Scaling marks an instance unhealthy if the calls to the Amazon EC2 action DescribeInstanceStatus return any state other than running, the system status shows impaired, or the calls to Elastic Load Balancing action DescribeInstanceHealth returns OutOfService in the instance state field. If there are multiple load balancers associated with your Auto Scaling group, Auto Scaling checks the health state of your EC2 instances by making health check calls to each load balancer. For each call, if the Elastic Load Balancing action returns any state other than InService, the instance is marked as unhealthy. After Auto Scaling marks an instance as unhealthy, it remains in that state, even if subsequent calls from other load balancers return an InService state for the same instance. QUESTION NO: 9 Which two AWS services provide out-of-the-box user configurable automatic backup-as-a-service and backup rotation options? Choose any 2 answers. A. Amazon S3 B. Amazon RDS C. Amazon EBS D. Amazon Red shift Answer: C,D QUESTION NO: 10 An organization has configured a VPC with an Internet Gateway (IGW). pairs of public and private subnets (each with one subnet per Availability Zone), and an Elastic Load Balancer (ELB) configured to use the public subnets. The application web tier leverages the ELB, Auto Scaling and a mum-AZ RDS database instance. The Organization would like to eliminate any potential single points ft failure in this design. What step should you take to achieve this organization's objective? A. Nothing, there are no single points of failure in this architecture. B. Create and attach a second IGW to provide redundant internet connectivity. C. Create and configure a second Elastic Load Balancer to provide a redundant load balancer. D. Create a second multi-AZ RDS instance in another Availability Zone and configurereplication to provide a redundant database. Answer: C QUESTION NO: 11 Which of the following are characteristics of Amazon VPC subnets? Choose any 2 answers. A. Each subnet maps to a single Availability Zone B. A CIDR block mask of /25 is the smallest range supported C. Instances in a private subnet can communicate with the internet only if they have an Elastic IP. D. By default, all subnets can route between each other, whether they are private or public E. V Each subnet spans at least 2 Availability zones to provide a high-availability environment. Answer: C,E QUESTION NO: 12 You are creating an Auto Scaling group whose Instances need to insert a custom metric into CloudWatch. Which method would be the best way to authenticate your CloudWatch PUT request? A. Create an IAM role with the Put MetricData permission and modify the Auto Scaling launch configuration to launch instances in that role. B. Create an IAM user with the PutMetricData permission and modify the Auto Scaling launch configuration to inject the userscredentials into the instance User Data. C. Modify the appropriate Cloud Watch metric policies to allow the Put MetricData permission to instances from the Auto Scaling group. D. Create an IAM user with the PutMetricData permission and put the credentials in a private repository and have applications on the server pull the credentials as needed. Answer: B QUESTION NO: 13 When an EC2 instance that is backed by an S3-based AMI Is terminated, what happens to the data on me root volume? A. Data is automatically saved as an E8S volume. B. Data is automatically saved as an ESS snapshot. C. Data is automatically deleted. D. Data is unavailable until the instance is restarted. Answer: D Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html QUESTION NO: 14 You have a web application leveraging an Elastic Load Balancer (ELB) In front of the web servers deployed using an Auto Scaling Group Your database is running on Relational Database Service (RDS) The application serves out technical articles and responses to them in general there are more views of an article than there are responses to the article. On occasion, an article on the site becomes extremely popular resulting in significant traffic Increases that causes the site to go down. What could you do to help alleviate the pressure on the infrastructure while maintaining availability during these events? Choose 3 answers A. Leverage CloudFront for the delivery of the articles. B. Add RDS read-replicas for the read traffic going to your relational database C. Leverage ElastiCache for caching the most frequently used data. D. Use SOS to queue up the requests for the technical posts and deliver them out of the queue. E. Use Route53 health checks to fail over to an S3 bucket for an error page. Answer: A,C,E QUESTION NO: 15 The majority of your Infrastructure is on premises and you have a small footprint on AWS Your company has decided to roll out a new application that is heavily dependent on low latency connectivity to LOAP for authentication Your security policy requires minimal changes to the company's existing application user management processes. What option would you implement to successfully launch this application1? A. Create a second, independent LOAP server in AWS for your application to use for authentication B. Establish a VPN connection so your applications can authenticate against your existing on-premises LDAP servers C. Establish a VPN connection between your data center and AWS create a LDAP replica on AWS and configure your application to use the LDAP replica for authentication D. Create a second LDAP domain on AWS establish a VPN connection to establish a trust relationship between your new and existing domains and use the new domain for authentication Answer: D Reference: http://msdn.microsoft.com/en-us/library/azure/jj156090.aspx QUESTION NO: 16 You need to design a VPC for a web-application consisting of an Elastic Load Balancer (ELB). a fleet of web/application servers, and an RDS database The entire Infrastructure must be distributed over 2 availability zones. Which VPC configuration works while assuring the database is not available from the Internet? A. One public subnet for ELB one public subnet for the web-servers, and one private subnet for the database B. One public subnet for ELB two private subnets for the web-servers, two private subnets for RDS C. Two public subnets for ELB two private subnets for the web-servers and two private subnets for RDS D. Two public subnets for ELB two public subnets for the web-servers, and two public subnets for RDS Answer: A QUESTION NO: 17 An application that you are managing has EC2 instances & Dynamo OB tables deployed to several AWS Regions In order to monitor the performance of the application globally, you would like to see two graphs 1) Avg CPU Utilization across all EC2 instances and 2) Number of Throttled Requests for all DynamoDB tables. How can you accomplish this? A. Tag your resources with the application name, and select the tag name as the dimension in the Cloudwatch Management console to view the respective graphs B. Use the Cloud Watch CLI tools to pull the respective metrics from each regional endpoint Aggregate the data offline & store it for graphing in CloudWatch. C. Add SNMP traps to each instance and DynamoDB table Leverage a central monitoring server to capture data from each instance and table Put the aggregate data into Cloud Watch for graphing. D. Add a CloudWatch agent to each instance and attach one to each DynamoDB table. When configuring the agent set the appropriate application name & view the graphs in CloudWatch. Answer: C QUESTION NO: 18 When assessing an organization s use of AWS API access credentials which of the following three credentials should be evaluated? Choose 3 answers A. Key pairs B. Console passwords 10 C. Access keys D. Signing certificates E. Security Group memberships Answer: A,C,D Reference: http://media.amazonwebservices.com/AWS_Operational_Checklists.pdf QUESTION NO: 19 You have a Linux EC2 web server instance running inside a VPC The instance is In a public subnet and has an EIP associated with it so you can connect to It over the Internet via HTTP or SSH The instance was also fully accessible when you last logged in via SSH. and was also serving web requests on port 80. Now you are not able to SSH into the host nor does it respond to web requests on port 80 that were working fine last time you checked You have double-checked that all networking configuration parameters (security groups route tables. IGW'EIP. NACLs etc) are properly configured {and you haven\u2019t made any changes to those anyway since you were last able to reach the Instance). You look at the EC2 console and notice that system status check shows \"impaired.\" Which should be your next step in troubleshooting and attempting to get the instance back to a healthy state so that you can log in again? A. Stop and start the instance so that it will be able to be redeployed on a healthy host system that most likely will fix the \"impaired\" system status B. Reboot your instance so that the operating system will have a chance to boot in a clean healthy state that most likely will fix the 'impaired\" system status C. Add another dynamic private IP address to me instance and try to connect via mat new path, since the networking stack of the OS may be locked up causing the \u201cimpaired\u201d system status. D. Add another Elastic Network Interface to the instance and try to connect via that new path since the networking stack of the OS may be locked up causing the \"impaired\" system status E. un-map and then re-map the EIP to the instance, since the IGWVNAT gateway may not be working properly, causing the \"impaired\" system status Answer: B QUESTION NO: 20 What is a placement group? A. A collection of Auto Scaling groups in the same Region B. Feature that enables EC2 instances to interact with each other via nigh bandwidth, low latency connections C. A collection of Elastic Load Balancers in the same Region or Availability Zone D. A collection of authorized Cloud Front edge locations for a distribution Answer: C Reference: http://aws.amazon.com/ec2/faqs/ QUESTION NO: 21 Your entire AWS infrastructure lives inside of one Amazon VPC You have an Infrastructure monitoring application running on an Amazon instance in Availability Zone (AZ) A of the region, and another application instance running in AZ B. The monitoring application needs to make use of ICMP ping to confirm network reachability of the instance hosting the application. Can you configure the security groups for these instances to only allow the ICMP ping to pass from the monitoringinstance to the application instance and nothing else\" If so how? A. No Two instances in two different AZ's can't talk directly to each other via ICMP ping as that protocol is not allowed across subnet (iebroadcast) boundaries B. Yes Both the monitoring instance and the application instance have to be a part of the same security group, and that security group needs to allow inbound ICMP C. Yes, The security group for the monitoring instance needs to allow outbound ICMP and the application instance's security group needs to allow Inbound ICMP D. Yes, Both the monitoring instance's security group and the application instance's security group need to allow both inbound and outbound ICMP ping packets since ICMP is not a connection-oriented protocol Answer: D QUESTION NO: 22 You have two Elastic Compute Cloud (EC2) instances inside a Virtual Private Cloud (VPC) in the same Availability Zone (AZ) but in different subnets.One instance is running a database and the other instance an application that will interface with the database. You want to confirm that they can talk to each other for your application to work properly. Which two things do we need to confirm in the VPC settings so that these EC2 instances can communicate inside the VPC? Choose 2 answers A. A network ACL that allows communication between the two subnets. B. Both instances are the same instance class and using the same Key-pair. C. That the default route is set to a NAT instance or internet Gateway (IGW) for them to communicate. D. Security groups are set to allow the application host to talk to the database on the right port/protocol. Answer: A,C QUESTION NO: 23 Which services allow the customer to retain full administrative privileges of the underlying EC2 instances? Choose 2 answers A. Amazon Elastic Map Reduce B. Elastic Load Balancing C. AWS Elastic Beanstalk D. I\" Amazon Elasticache E. Amazon Relational Database service Answer: B,C QUESTION NO: 24 You have a web-style application with a stateless but CPU and memory-intensive web tier running on a cc2 8xlarge EC2 instance inside of a VPC The instance when under load is having 134 problems returning requests within the SLA as defined by your business The application maintains its state in a DynamoDB table, but the data tier is properly provisioned and responses are consistently fast. How can you best resolve the issue of the application responses not meeting your SLA? A. Add another cc2 8xlarge application instance, and put both behind an Elastic Load Balancer B. Move the cc2 8xlarge to the same Availability Zone as the DynamoDB table C. Cache the database responses in ElastiCache for more rapid access D. Move the database from DynamoDB to RDS MySQL in scale-out read-replica configuration Answer: B Reference: http://aws.amazon.com/elasticmapreduce/faqs/ QUESTION NO: 25 You are managing a legacy application Inside VPC with hard coded IP addresses in its configuration. Which two mechanisms will allow the application to failover to new instances without the need for reconfiguration? Choose 2 answers A. Create an ELB to reroute traffic to a failover instance B. Create a secondary ENI that can be moved to a failover instance C. Use Route53 health checks to fail traffic over to a failover instance D. Assign a secondary private IP address to the primary ENIO that can De moved to a failover instance Answer: A,D (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 26 You are designing a system that has a Bastion host. This component needs to be highly available without human intervention. Which of the following approaches would you select? A. Run the bastion on two instances one in each AZ B. Run the bastion on an active Instance in one AZ and have an AMI ready to boot up in the event of failure C. Configure the bastion instance in an Auto Scaling group Specify the Auto Scaling group to include multiple AZs but have a min-size of 1 and max-size of 1 D. Configure an ELB in front of the bastion instance Answer: C QUESTION NO: 27 Which of the following statements about this S3 bucket policy is true? 15 A. Denies the server with the IP address 192 168 100 0 full access to the \"mybucket\" bucket B. Denies the server with the IP address 192 168 100 188 full access to the \"mybucket\" bucket C. Grants all the servers within the 192 168 100 0/24 subnet full access to the \"mybucket\" bucket D. Grants all the servers within the 192 168 100 188/32 subnet full access to the \"mybucket\" bucket Answer: C QUESTION NO: 28 Which of the following requires a custom CloudWatch metric to monitor? A. Data transfer of an EC2 instance B. Disk usage activity of an EC2 instance C. Memory Utilization of an EC2 instance D. CPU Utilization of an EC2mstance Answer: B Reference: http://aws.amazon.com/cloudwatch/ QUESTION NO: 29 You run a web application where web servers on EC2 Instances are In an Auto Scaling group Monitoring over the last 6 months shows that 6 web servers are necessary to handle the minimum load During the day up to 12 servers are needed Five to six days per year, the number of web servers required might go up to 15. What would you recommend to minimize costs while being able to provide hill availability? A. 6 Reserved instances (heavy utilization). 6 Reserved instances {medium utilization), rest covered by On-Demand instances B. 6 Reserved instances (heavy utilization). 6 On-Demand instances, rest covered by Spot Instances C. 6 Reserved instances (heavy utilization) 6 Spot instances, rest covered by OnDemand instances D. 6 Reserved instances (heavy utilization) 6 Reserved instances (medium utilization) rest covered by Spot instances 1678 Answer: C QUESTION NO: 30 You have been asked to propose a multi-region deployment of a web-facing application where a controlled portion of your traffic is being processed by an alternate region. Which configuration would achieve that goal? A. Route53 record sets with weighted routing policy B. Route53 record sets with latency based routing policy C. Auto Scaling with scheduled scaling actions set D. Elastic Load Balancing with health checks enabled Answer: D Reference: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/TerminologyandKeyConcepts.html QUESTION NO: 31 You have set up Individual AWS accounts for each project. You have been asked to make sure your AWS Infrastructure costs do not exceed the budget set per project for each month. Which of the following approaches can help ensure that you do not exceed the budget each month? A. Consolidate your accounts so you have a single bill for all accounts and projects B. Set up auto scaling with CloudWatch alarms using SNS to notify you when you are running too many Instances in a given account C. Set up CloudWatch billing alerts for all AWS resources used by each project, with a notification occurring when the amount for each resource tagged to a particular project matches the budget allocated to the project. D. Set up CloudWatch billing alerts for all AWS resources used by each account, with email notifications when it hits 50%. 80% and 90% of its budgeted monthly spend Answer: C QUESTION NO: 32 When creation of an EBS snapshot Is initiated but not completed the EBS volume? A. Cannot De detached or attached to an EC2 instance until me snapshot completes B. Can be used in read-only mode while me snapshot is in progress C. Can be used while me snapshot Is in progress D. Cannot be used until the snapshot completes Answer: C Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html QUESTION NO: 33 You are using ElastiCache Memcached to store session state and cache database queries in your infrastructure You notice in Cloud Watch that Evictions and GetMisses are Doth very high. What two actions could you take to rectify this? Choose 2 answers A. Increase the number of nodes in your cluster B. Tweak the max-item-size parameter C. Shrink the number of nodes in your cluster D. Increase the size of the nodes in the duster Answer: B,D QUESTION NO: 34 You are running a database on an EC2 instance, with the data stored on Elastic Block Store (EBS) for persistence At times throughout the day, you are seeing large variance in the response times of the database queries Looking into the instance with the isolate command you see a lot of wait time on the disk volume that the database's data is stored on. What two ways can you improve the performance of the database's storage while maintaining the current persistence of the data? Choose 2 answers A. Move to an SSD backed instance B. Move the database to an EBS-Optimized Instance C. T Use Provisioned IOPs EBS D. Use the ephemeral storage on an m2 4xiarge Instance Instead Answer: A,B QUESTION NO: 35 Your EC2-Based Multi-tier application includes a monitoring instance that periodically makes application -level read only requests of various application components and if any of those fail more than three times 30 seconds calls CloudWatch lo fire an alarm, and the alarm notifies your operations team by email and SMS of a possible application health problem. However, you also need to watch the watcher -the monitoring instance itself - and be notified if it becomes unhealthy. Which of the following Is a simple way to achieve that goal? A. Run another monitoring instance that pings the monitoring instance and fires a could watch alarm mat notifies your operations teamshould the primary monitoring instance become unhealthy. B. Set a Cloud Watch alarm based on EC2 system and instance status checks and have the alarm notify your operations team of anydetected problem with the monitoring instance. C. Set a Cloud Watch alarm based on the CPU utilization of the monitoring instance and nave the alarm notify your operations team if C r the CPU usage exceeds 50% few more than one minute: then have your monitoring application go into a CPU-bound loop should itDetect any application problems. D. Have the monitoring instances post messages to an SOS queue and then dequeue those messages on another instance should D c- the queue cease to have new messages, the second instance should first terminate the original monitoring instance start anotherbackup monitoring instance and assume (he role of the previous monitoring instance and beginning adding messages to the SOSqueue. 19 Answer: D QUESTION NO: 36 You have decided to change the Instance type for instances running In your application tier that are using Auto Scaling. In which area below would you change the instance type definition? A. Auto Scaling launch configuration B. Auto Scaling group C. Auto Scaling policy D. Auto Scaling tags Answer: B Reference: http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/WhatIsAutoScaling.html QUESTION NO: 37 You are attempting to connect to an instance in Amazon VPC without success You have already verified that the VPC has an Internet Gateway (IGW) the instance has an associated Elastic IP (EIP) and correct security group rules are in place. Which VPC component should you evaluate next? A. The configuration of a MAT instance B. The configuration of the Routing Table C. The configuration of the internet Gateway (IGW) D. The configuration of SRC/DST checking Answer: C Reference: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/UserScenariosForVPC.html QUESTION NO: 38 You are tasked with the migration of a highly trafficked Node JS application to AWS In order to comply with organizational standards Chef recipes must be used to configure the application servers that host this application and to support application lifecycle events. Which deployment option meets these requirements while minimizing administrative burden? A. Create a new stack within Opsworks add the appropriate layers to the stack and deploy the application B. Create a new application within Elastic Beanstalk and deploy this application to a new environment C. Launch a Mode JS server from a community AMI and manually deploy the application to the launched EC2 instance D. Launch and configure Chef Server on an EC2 instance and leverage the AWS CLI to launch application servers and configure those instances using Chef. Answer: B Reference: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deployment.html QUESTION NO: 39 You have been asked to automate many routine systems administrator backup and recovery activities Your current plan is to leverage AWS-managed solutions as much as possible and automate the rest with the AWS CU and scripts. Which task would be best accomplished with a script? A. Creating daily EBS snapshots with a monthly rotation of snapshots B. Creating daily ROS snapshots with a monthly rotation of snapshots C. Automatically detect and stop unused or underutilized EC2 instances D. Automatically add Auto Scaled EC2 instances to an Amazon Elastic Load Balancer Answer: C QUESTION NO: 40 Your organization's security policy requires that all privileged users either use frequently rotated passwords or one-time access credentials in addition to username/password. Which two of the following options would allow an organization to enforce this policy for AWS users? Choose 2 answers A. Configure multi-factor authentication for privileged 1AM users B. Create 1AM users for privileged accounts C. Implement identity federation between your organization's Identity provider leveraging the 1AM Security Token Service D. Enable the 1AM single-use password policy option for privileged users Answer: C,D QUESTION NO: 41 What are characteristics of Amazon S3? Choose 2 answers A. Objects are directly accessible via a URL B. S3 should be used to host a relational database C. S3 allows you to store objects or virtually unlimited size D. S3 allows you to store virtually unlimited amounts of data E. S3 offers Provisioned IOPS Answer: B,C QUESTION NO: 42 You receive a frantic call from a new DBA who accidentally dropped a table containing all your customers. Which Amazon RDS feature will allow you to reliably restore your database to within 5 minutes of when the mistake was made? A. Multi-AZ RDS B. RDS snapshots C. RDS read replicas D. RDS automated backup Answer: B Reference: http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.BackingUpAndRestoringAmazonRDSInstances.html QUESTION NO: 43 A media company produces new video files on-premises every day with a total size of around 100GBS after compression All files have a size of 1 -2 GB and need to be uploaded to Amazon S3 every night in a fixed time window between 3am and 5am Current upload takes almost 3 hours, although less than half of the available bandwidth is used. What step(s) would ensure that the file uploads are able to complete in the allotted time window? A. Increase your network bandwidth to provide faster throughput to S3 B. Upload the files in parallel to S3 C. Pack all files into a single archive, upload it to S3, then extract the files in AWS D. Use AWS Import/Export to transfer the video files Answer: D Reference: http://aws.amazon.com/importexport/faqs/ QUESTION NO: 44 You are running a web-application on AWS consisting of the following components an Elastic Load Balancer (ELB) an Auto-Scaling Group of EC2 instances running Linux/PHP/Apache, and Relational DataBase Service (RDS) MySQL. Which security measures fall into AWS's responsibility? A. Protect the EC2 instances against unsolicited access by enforcing the principle of least-privilege access B. Protect against IP spoofing or packet sniffing C. Assure all communication between EC2 instances and ELB is encrypted D. Install latest security patches on ELB. RDS and EC2 instances Answer: B QUESTION NO: 45 You use S3 to store critical data for your company Several users within your group currently have lull permissions to your S3 buckets You need to come up with a solution mat does not impact your users and also protect against the accidental deletion of objects. Which two options will address this issue? Choose 2 answers A. Enable versioning on your S3 Buckets B. Configure your S3 Buckets with MFA delete C. Create a Bucket policy and only allow read only permissions to all users at the bucket level D. Enable object life cycle policies and configure the data older than 3 months to be archived in Glacier Answer: B,C QUESTION NO: 46 An organization's security policy requires multiple copies of all critical data to be replicated across at least a primary and backup data center. The organization has decided to store some critical data on Amazon S3. 245 Which option should you implement to ensure this requirement is met? A. Use the S3 copy API to replicate data between two S3 buckets in different regions B. You do not need to implement anything since S3 data is automatically replicated between regions C. Use the S3 copy API to replicate data between two S3 buckets in different facilities within an AWS Region D. You do not need to implement anything since S3 data is automatically replicated between multiple facilities within an AWS Region Answer: C QUESTION NO: 47 You are tasked with setting up a cluster of EC2 Instances for a NoSOL database The database requires random read 10 disk performance up to a 100.000 IOPS at 4KB block side per node Which of the following EC2 instances will perform the best for this workload? A. A High-Memory Quadruple Extra Large (m2 4xlarge) with EBS-Optimized set to true and a PIOPs EBS volume B. A Cluster Compute Eight Extra Large (cc2 8xlarge) using instance storage C. High I/O Quadruple Extra Large (hil 4xiarge) using instance storage D. A Cluster GPU Quadruple Extra Large (cg1 4xlarge) using four separate 4000 PIOPS EBS volumes in a RAID 0 configuration Answer: B Reference: http://aws.amazon.com/ec2/instance-types/ QUESTION NO: 48 When an EC2 EBS-backed (EBS root) instance is stopped, what happens to the data on any ephemeral store volumes? A. Data will be deleted and win no longer be accessible B. Data Is automatically saved in an EBS volume. C. Data Is automatically saved as an E8S snapshot D. Data is unavailable until the instance is restarted Answer: D QUESTION NO: 49 Your team Is excited about theuse of AWS because now they have access to programmable Infrastructure\" You have been asked to manage your AWS infrastructure In a manner similar to the way you might manage application code You want to be able to deploy exact copies of different versions of your infrastructure, stage changes into different environments, revert back to previous versions, and identify what versions are running at any particular time (development test QA. production). Which approach addresses this requirement? A. Use cost allocation reports and AWS Opsworks to deploy and manage your infrastructure. B. Use AWS CloudWatch metrics and alerts along with resource tagging to deploy and manage your infrastructure. C. Use AWS Beanstalk and a version control system like GIT to deploy and manage your infrastructure. D. Use AWS CloudFormation and a version control system like GIT to deploy and manage your infrastructure. Answer: B Reference: http://aws.amazon.com/opsworks/faqs/ QUESTION NO: 50 You have a server with a 5O0GB Amazon EBS data volume. The volume is 80% full. You need to back up the volume at regular intervals and be able to re-create the volume in a new Availability Zone in the shortest time possible. All applications using the volume can be paused for a period of a few minutes with no discernible user impact. Which of the following backup methods will best fulfill your requirements? A. Take periodic snapshots of the EBS volume 2678930 B. Use a third party Incremental backup application to back up to Amazon Glacier C. Periodically back up all data to a single compressed archive and archive to Amazon S3 using a parallelized multi-part upload D. Create another EBS volume in the second Availability Zone attach it to the Amazon EC2 instance, and use a disk manager to mirror me two disks Answer: D Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 51 Your company Is moving towards tracking web page users with a small tracking Image loaded on each page Currently you are serving this image out of US-East, but are starting to get concerned about the time It takes to load the image for users on the west coast. What are the two best ways to speed up serving this image? Choose 2 answers A. Use Route 53's Latency Based Routing and serve the image out of US-West-2 as well as US-East-1 B. Serve the image out through CloudFront C. Serve the image out of S3 so that it isn't being served oft of your web application tier D. Use EBS PIOPs to serve the image faster out of your EC2 instances Answer: A,B QUESTION NO: 52 If you want to launch Amazon Elastic Compute Cloud (EC2) Instances and assign each Instance a predetermined private IP address you should: A. Assign a group or sequential Elastic IP address to the instances B. Launch the instances in a Placement Group C. Launch the instances in the Amazon virtual Private Cloud (VPC). D. Use standard EC2 instances since each instance gets a private Domain Name Service (DNS) already E. Launch the Instance from a private Amazon Machine image (Mil) Answer: C Reference: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-ip-addressing.html QUESTION NO: 53 A customer has a web application that uses cookie Based sessions to track logged in users It Is deployed on AWS using ELB and Auto Scaling The customer observes that when load increases. Auto Scaling launches new Instances but the load on the easting Instances does not decrease, causing all existing users to have a sluggish experience. Which two answer choices independently describe a behavior that could be the cause of the sluggish user experience? Choose 2 answers A. ELB's normal behavior sends requests from the same user to the same backend instance B. ELB's behavior when sticky sessions are enabled causes ELB to send requests in the same session to the same backend instance C. A faulty browser is not honoring the TTL of the ELB DNS name. D. The web application uses long polling such as comet or websockets. Thereby keeping a connection open to a web server tor a long time E. The web application uses long polling such as comet or websockets. Thereby keeping a connection open to a web server for a long time. Answer: B,D QUESTION NO: 54 What would happen to an RDS (Relational Database Service) multi-Availability Zone deployment of the primary OB instance fails? A. The IP of the primary DB instance is switched to the standby OB instance B. The RDS (Relational Database Service) DB instance reboots C. A new DB instance is created in the standby availability zone D. The canonical name record (CNAME) is changed from primary to standby Answer: B QUESTION NO: 55 How can the domain's zone apex for example \"myzoneapexdomain com\" be pointed towards an Elastic Load Balancer? A. By using an AAAA record B. By using an A record C. By using an Amazon Route 53 CNAME record D. By using an Amazon Route 53 Alias record Answer: C Reference: http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosingalias-non-alias.html Topic 2, Volume B QUESTION NO: 56 An organization has created 5 IAM users. The organization wants to give them the same login ID but different passwords. How can the organization achieve this? A. The organization should create a separate login ID but give the IAM users the same alias so that each one can login with their alias B. The organization should create each user in a separate region so that they have their own URL to login C. It is not possible to have the same login ID for multiple IAM users of the same account D. The organization should create various groups and add each user with the same login ID to different groups. The user can login with their own group ID Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Whenever the organization is creating an IAM user, there should be a unique ID for each user. It is not possible to have the same login ID for multiple users. The names of users,groups, roles, instance profiles must be alphanumeric, including the following common characters: plus(+), equal(=), comma(,), period(.), at(@), and dash (-) QUESTION NO: 57 A user is planning to evaluate AWS for their internal use. The user does not want to incur any charge on his account during the evaluation. Which of the below mentioned AWS services would incur a charge if used? A. AWS S3 with 1 GB of storage B. AWS micro instance running 24 hours daily C. AWS ELB running 24 hours a day D. AWS PIOPS volume of 10 GB size Answer: D Explanation: AWS is introducing a free usage tier for one year to help the new AWS customers get started in Cloud. The free tier can be used for anything that the user wants to run in the Cloud. AWS offers a handful of AWS services as a part of this which includes 750 hours of free micro instances and 750 hours of ELB. It includes the AWS S3 of 5 GB and AWS EBS general purpose volume upto 30 GB. PIOPS is not part of free usage tier. QUESTION NO: 58 A user has developed an application which is required to send the data to a NoSQL database. The user wants to decouple the data sending such that the application keeps processing and sending data but does not wait for an acknowledgement of DB. Which of the below mentioned applications helps in this scenario? A. AWS Simple Notification Service B. AWS Simple Workflow C. AWS Simple Queue Service D. AWS Simple Query Service Answer: C Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. In this case, the user can use AWS SQS to send messages which are received from an application and sent to DB. The application can continue processing data without waiting for any acknowledgement from DB. The user can use SQS to transmit any volume of data without losing messages or requiring other services to always be available. QUESTION NO: 59 An organization has created 50 IAM users. The organization has introduced a new policy which will change the access of an IAM user. How can the organization implement this effectively so that there is no need to apply the policy at the individual user level? A. Use the IAM groups and add users as per their role to different groups and apply policy to group B. The user can create a policy and apply it to multiple users in a single go with the AWS CLI C. Add each user to the IAM role as per their organization role to achieve effective policy setup D. Use the IAM role and implement access at the role level Answer: A Explanation: With AWS IAM, a group is a collection of IAM users. A group allows the user to specify permissions for a collection of users, which can make it easier to manage the permissions for those users. A group helps an organization manage access in a better way; instead of applying at the individual level, the organization can apply at the group level which is applicable to all the users who are a part of that group. QUESTION NO: 60 A user is planning to use AWS Cloud formation for his automatic deployment requirements. Which of the below mentioned components are required as a part of the template? A. Parameters B. Outputs C. Template version D. Resources Answer: D Explanation: AWS Cloud formation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The template is a JSON-format, text-based file that describes all the AWS resources required to deploy and run an application. It can have option fields, such as Template Parameters, Output, Data tables, and Template file format version. The only mandatory value is Resource. The user can define the AWS services which will be used/ created by this template inside the Resource section QUESTION NO: 61 A user has recently started using EC2. The user launched one EC2 instance in the default subnet in EC2-VPC Which of the below mentioned options is not attached or available with the EC2 instance when it is launched? A. Public IP address B. Internet gateway C. Elastic IP D. Private IP address Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to a user\u2019s AWS account. A subnet is a range of IP addresses in the VPC. The user can launch the AWS resources into a subnet. There are two supported platforms into which a user can launch instances: EC2-Classic and EC2-VPC (default subnet.. A default VPC has all the benefits of EC2-VPC and the ease of use of EC2- Classic. Each instance that the user launches into a default subnet has a private IP address and a public IP address. These instances can communicate with the internet through an internet gateway. An internet gateway enables the EC2 instances to connect to the internet through the Amazon EC2 network edge. QUESTION NO: 62 A user has launched an EC2 instance. The user is planning to setup the CloudWatch alarm. Which of the below mentioned actions is not supported by the CloudWatch alarm? A. Notify the Auto Scaling launch config to scale up B. Send an SMS using SNS C. Notify the Auto Scaling group to scale down D. Stop the EC2 instance Answer: B Explanation: A user can create a CloudWatch alarm that takes various actions when the alarm changes state. An alarm watches a single metric over the time period that the user has specified, and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The actions could be sending a notification to an Amazon Simple Notification Service topic (SMS, Email, and HTTP end point.,notifying the Auto Scaling policy or changing the state of the instance to Stop/Terminate. QUESTION NO: 63 A user is trying to delete an Auto Scaling group from CLI. Which of the below mentioned steps are to be performed by the user? A. Terminate the instances with the ec2-terminate-instance command B. Terminate the Auto Scaling instances with the as-terminate-instance command C. Set the minimum size and desired capacity to 0 D. There is no need to change the capacity. Run the as-delete-group command and it will reset all values to 0 Answer: C Explanation: If the user wants to delete the Auto Scaling group, the user should manually set the values of the minimum and desired capacity to 0. Otherwise Auto Scaling will not allow for the deletion of the group from CLI. While trying from the AWS console, the user need not set the values to 0 as the Auto Scaling console will automatically do so. QUESTION NO: 64 An organization is planning to create 5 different AWS accounts considering various security requirements. The organization wants to use a single payee account by using the consolidated billing option. Which of the below mentioned statements is true with respect to the above information? A. Master (Payee. account will get only the total bill and cannot see the cost incurred by each account B. Master (Payee. account can view only the AWS billing details of the linked accounts C. It is not recommended to use consolidated billing since the payee account will have access to the linked accounts D. Each AWS account needs to create an AWS billing policy to provide permission to the payee account Answer: B Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. Consolidated billing enables the organization to see a combined view of the AWS charges incurred by each account as well as obtain a detailed cost report for each of the individual AWS accounts associated with the paying account. The payee account will not have any other access than billing data of linked accounts. QUESTIONNO: 64-A A user has deployed an application on his private cloud. The user is using his own monitoring tool. He wants to configure that whenever there is an error, the monitoring tool should notify him via SMS. Which of the below mentioned AWS services will help in this scenario? A. None because the user infrastructure is in the private cloud/ B. AWS SNS C. AWS SES D. AWS SMS Answer: B Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can be used to make push notifications to mobile devices. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. In this case user can use the SNS apis to send SMS. QUESTION NO: 65 A user has created a web application with Auto Scaling. The user is regularly monitoring the application and he observed that the traffic is highest on Thursday and Friday between 8 AM to 6 PM. What is the best solution to handle scaling in this case? A. Add a new instance manually by 8 AM Thursday and terminate the same by 6 PM Friday B. Schedule Auto Scaling to scale up by 8 AM Thursday and scale down after 6 PM on Friday C. Schedule a policy which may scale up every day at 8 AM and scales down by 6 PM D. Configure a batch process to add a instance by 8 AM and remove it by Friday 6 PM Answer: B Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. In this case the load increases by Thursday and decreases by Friday. Thus, the user can setup the scaling activity based on the predictable traffic patterns of the web application using Auto Scaling scale by Schedule. QUESTION NO: 66 A user has setup a CloudWatch alarm on an EC2 action when the CPU utilization is above 75%. The alarm sends a notification to SNS on the alarm state. If the user wants to simulate the alarm action how can he achieve this? A. Run activities on the CPU such that its utilization reaches above 75% B. From the AWS console change the state to \u2018Alarm\u2019 C. The user can set the alarm state to \u2018Alarm\u2019 using CLI D. Run the SNS action manually Answer: C Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods.The user can test an alarm by setting it to any state using the SetAlarmState API (mon-set-alarm-state command.. This temporary state change lasts only until the next alarm comparison occurs. QUESTION 13 A user is trying to setup a scheduled scaling activity using Auto Scaling. The user wants to setup the recurring schedule. Which of the below mentioned parameters is not required in this case? A. Maximum size B. Auto Scaling group name C. End time D. Recurrence value Answer: A Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can also configure the recurring schedule action which will follow the Linux cron format. If the user is setting a recurring event, it is required that the user specifies the Recurrence value (in a cron format., end time (not compulsory but recurrence will stop after this. and the Auto Scaling group for which the scaling activity is to be scheduled. QUESTION NO: 67 A user has setup a billing alarm using CloudWatch for $200. The usage of AWS exceeded $200 after some days. The user wants to increase the limit from $200 to $400? What should the user do? A. Create a new alarm of $400 and link it with the first alarm B. It is not possible to modify the alarm once it has crossed the usage limit C. Update the alarm to set the limit at $400 instead of $200 D. Create a new alarm for the additional $200 amount Answer: C Explanation: AWS CloudWatch supports enabling the billing alarm on the total AWS charges. The estimated charges are calculated and sent several times daily to CloudWatch in the form of metric data. This data will be stored for 14 days. This data also includes the estimated charges for every service in AWS used by the user, as well as the estimated overall AWS charges. If the user wants to increase the limit, the user can modify the alarm and specify a new threshold. QUESTION NO: 68 A sys admin has created the below mentioned policy and applied to an S3 object named aws.jpg. The aws.jpg is inside a bucket named cloudacademy. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg\"] }] A. It is not possible to define a policy at the object level B. It will make all the objects of the bucket cloudacademy as public C. It will make the bucket cloudacademy as public D. the aws.jpg object as public Answer: A Explanation: A system admin can grant permission to the S3 objects or buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. It cannot be applied at the object level. QUESTION NO: 69 A user is trying to save some cost on the AWS services. Which of the below mentioned options will not help him save cost? A. Delete the unutilized EBS volumes once the instance is terminated B. Delete the AutoScaling launch configuration after the instances are terminated C. Release the elastic IP if not required once the instance is terminated D. Delete the AWS ELB after the instances are terminated Answer: B Explanation: AWS bills the user on a as pay as you go model. AWS will charge the user once the AWS resource is allocated. Even though the user is not using the resource, AWS will charge if it is in service or allocated. Thus, it is advised that once the user\u2019s work is completed he should: Terminate the EC2 instance Delete the EBS volumes Release the unutilized Elastic IPs Delete ELB The AutoScaling launch configuration does not cost the user. Thus, it will not make any difference to the cost whether it is deleted or not. QUESTION NO: 70 A user is trying to aggregate all the CloudWatch metric data of the last 1 week. Which of the below mentioned statistics is not available for the user as a part of data aggregation? A. Aggregate B. Sum C. Sample data D. Average Answer: A Explanation: Amazon CloudWatch is basically a metrics repository. Either the user can send the custom data or an AWS product can put metrics into the repository, and the user can retrieve the statistics based on those metrics. The statistics are metric data aggregations over specified periods of time. Aggregations are made using the namespace, metric name, dimensions, and the data point unit of measure, within the time period that is specified by the user. CloudWatch supports Sum, Min, Max, Sample Data and Average statistics aggregation. QUESTION NO: 71 An organization is planning to use AWS for their production roll out. The organization wants to implement automation for deployment such that it will automatically create a LAMP stack, download the latest PHP installable from S3 and setup the ELB. Which of the below mentioned AWS services meets the quirement for making an orderly deployment of the software? A. AWS Elastic Beanstalk B. AWS Cloudfront C. AWS Cloudformation D. AWS DevOps Answer: C Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. Cloudformation provides an easy way to create and delete the collection of related AWS resources and provision them in an orderly way. AWS CloudFormation automates and simplifies the task of repeatedly and predictably creating groups of related resources that power the user\u2019s applications. AWS Cloudfront is a CDN; Elastic Beanstalk does quite a few of the required tasks. However, it is a PAAS which uses a ready AMI. AWS Elastic Beanstalk provides an environment to easily develop and run applications in the cloud. QUESTION NO: 72 A user has created a subnet with VPC and launched an EC2 instance in that subnet with only default settings.Which of the below mentioned options is ready to use on the EC2 instance as soon as it is launched? A. Elastic IP B. Private IP C. Public IP D. I nternet gateway Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to a user\u2019s AWS account. A subnet is a range of IP addresses in the VPC. The user can launch the AWS resources into a subnet. There are two supported platforms into which a user can launch instances: EC2-Classic and EC2-VPC. When the user launches an instance which is not a part of the non-default subnet, it will only have a private IP assigned to it. The instances part of a subnet can communicate with each other but cannot communicate over the internet or to the AWS services, such as RDS / S3. QUESTION NO: 73 An organization is setting up programmatic billing access for their AWS account. Which of the below mentioned services is not required or enabled when the organization wants to use programmatic access? A. Programmatic access B. AWS bucket to hold the billing report C. AWS billing alerts D. Monthly Billing report Answer: C Explanation: AWS provides an option to have programmatic access to billing. Programmatic Billing Access leverages the existing Amazon Simple Storage Service (Amazon S3. APIs. Thus, the user can build applications that reference his billing data from a CSV (comma-separated value. file stored in an Amazon S3 bucket. To enable programmatic access, the user has to first enable the monthly billing report. Then the user needs to provide an AWS bucket name where the billing CSV will be uploaded. The user should also enable the Programmatic access option. QUESTION NO: 74 A user has configured the Auto Scaling group with the minimum capacity as 3 and the maximum capacity as 5. When the user configures the AS group, how many instances will Auto Scaling launch? A. 3 B. 0 C. 5 D. 2 Answer: C Explanation: When the user configures the launch configuration and the Auto Scaling group, the Auto Scaling group will start instances by launching the minimum number (or the desired number, if specified. of EC2 instances. If there are no other scaling conditions attached to the Auto Scaling group, it will maintain the minimum number of running instances at all times. QUESTION NO: 75 An admin is planning to monitor the ELB. Which of the below mentioned services does not help the admin capture the monitoring information about the ELB activity? A. ELB Access logs B. ELB health check C. CloudWatch metrics D. ELB API calls with CloudTrail Answer: B Explanation: The admin can capture information about Elastic Load Balancer using either: CloudWatch Metrics ELB Logs files which are stored in the S3 bucket CloudTrail with API calls which can notify the user as well generate logs for each API calls The health check is internally performed by ELB and does not help the admin get the ELB activity. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 76 A user is planning to use AWS Cloudformation. Which of the below mentioned functionalities does not help him to correctly understand Cloudfromation? A. Cloudformation follows the DevOps model for the creation of Dev & Test B. AWS Cloudfromation does not charge the user for its service but only charges for the AWS resources created with it C. Cloudformation works with a wide variety of AWS services, such as EC2, EBS, VPC, IAM, S3, RDS, ELB, etc D. CloudFormation provides a set of application bootstrapping scripts which enables the user to install Software Answer: A Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. It supports a wide variety of AWS services, such as EC2, EBS, AS, ELB, RDS, VPC, etc. It also provides application bootstrapping scripts which enable the user to install software packages or create folders. It is free of the cost and only charges the user for the services created with it. The only challenge is that it does not follow any model, such as DevOps; instead customers can define templates and use them to provision and manage the AWS resources in an orderly way. QUESTION NO: 77 A user has launched 10 instances from the same AMI ID using Auto Scaling. The user is trying to see the average CPU utilization across all instances of the last 2 weeks under the CloudWatch console. How can the user achieve this? A. View the Auto Scaling CPU metrics B. Aggregate the data over the instance AMI ID C. The user has to use the CloudWatchanalyser to find the average data across instances D. It is not possible to see the average CPU utilization of the same AMI ID since the instance ID is different Answer: B Explanation: Amazon CloudWatch is basically a metrics repository. Either the user can send the custom data or an AWS product can put metrics into the repository, and the user can retrieve the statistics based on those metrics. The statistics are metric data aggregations over specified periods of time. Aggregations are made using the namespace, metric name, dimensions, and the data point unit of measure, within the time period that is specified by the user. To aggregate the data across instances launched with AMI, the user should select the AMI ID under EC2 metrics and select the aggregate average to view the data. QUESTION NO: 78 A user is trying to understand AWS SNS. To which of the below mentioned end points is SNS unable to send a notification? A. Email JSON B. HTTP C. AWS SQS D. AWS SES Answer: D Explanation: Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can select one the following transports as part of the subscription requests: \u201cHTTP\u201d, \u201cHTTPS\u201d,\u201dEmail\u201d, \u201cEmailJSON\u201d, \u201cSQS\u201d, \u201cand SMS\u201d. QUESTION NO: 79 A user has configured an Auto Scaling group with ELB. The user has enabled detailed CloudWatch monitoring on Auto Scaling. Which of the below mentioned statements will help the user understand the functionality better? A. It is not possible to setup detailed monitoring for Auto Scaling B. In this case, Auto Scaling will send data every minute and will charge the user extra C. Detailed monitoring will send data every minute without additional charges D. Auto Scaling sends data every minute only and does not charge the user Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Auto Scaling includes 7 metrics and 1 dimension, and sends data to CloudWatch every 5 minutes by default. The user can enable detailed monitoring for Auto Scaling, which sends data to CloudWatch every minute. However, this will have some extra-costs. QUESTION NO: 80 A system admin is planning to setup event notifications on RDS. Which of the below mentioned services will help the admin setup notifications? A. AWS SES B. AWS Cloudtrail C. AWS Cloudwatch D. AWS SNS Answer: D Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. These notifications can be in any notification form supported by Amazon SNS for an AWS region, such as an email, a text message or a call to an HTTP endpoint QUESTION NO: 81 You are building an online store on AWS that uses SQS to process your customer orders. Your backend system needs those messages in the same sequence the customer orders have been put in. How can you achieve that? A. It is not possible to do this with SQS B. You can use sequencing information on each message C. You can do this with SQS but you also need to use SWF D. Messages will arrive in the same order by default Answer: B Explanation: Amazon SQS is engineered to always be available and deliver messages. One of the resulting tradeoffs is that SQSdoes not guarantee first in, first out delivery of messages. For many distributed applications, each message can stand on its own, and as long as all messages are delivered, the order is not important. If your system requires that order be preserved, you can place sequencing information in each message, so that you can reorder the messages when the queue returns them. QUESTION NO: 82 An organization wants to move to Cloud. They are looking for a secure encrypted database storage option. Which of the below mentioned AWS functionalities helps them to achieve this? A. AWS MFA with EBS B. AWS EBS encryption C. Multi-tier encryption with Redshift D. AWS S3 server side storage Answer: B Explanation: AWS EBS supports encryption of the volume while creating new volumes. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. The data at rest, the I/O as well as all the snapshots of EBS will be encrypted. The encryption occurs on the servers that host the EC2 instances, providing encryption of data as it moves between the EC2 instances and EBS storage. EBS encryption is based on the AES-256 cryptographic algorithm, which is the industry standard QUESTION NO: 83 A user wants to disable connection draining on an existing ELB. Which of the below mentioned statements helps the user disable connection draining on the ELB? A. The user can only disable connection draining from CLI B. It is not possible to disable the connection draining feature once enabled C. The user can disable the connection draining feature from EC2 -> ELB console or from CLI D. The user needs to stop all instances before disabling connection draining Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. The user can enable or disable connection draining from the AWS EC2 console -> ELB or using CLI. QUESTION NO: 84 A user has a refrigerator plant. The user is measuring the temperature of the plant every 15 minutes. If the user wants to send the data to CloudWatch to view the data visually, which of the below mentioned statements is true with respect to the information given above? A. The user needs to use AWS CLI or API to upload the data B. The user can use the AWS Import Export facility to import data to CloudWatch C. The user will upload data from the AWS console D. The user cannot upload data to CloudWatch since it is not an AWS service metric Answer: A Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. While sending the data the user has to include the metric name, namespace and timezone as part of the request. QUESTION NO: 85 A system admin is managing buckets, objects and folders with AWS S3. Which of the below mentioned statements is true and should be taken in consideration by the sysadmin? A. The folders support only ACL B. Both the object and bucket can have an Access Policy but folder cannot have policy C. Folders can have a policy D. Both the object and bucket can have ACL but folders cannot have ACL Answer: A Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. It cannot be applied at the object level. The folders are similar to objects with no content. Thus, folders can have only ACL and cannot have a policy. QUESTION NO: 86 A user has created an ELB with three instances. How many security groups will ELB create by default? A. 3 B. 5 C. 2 D. 1 Answer: C Explanation: Elastic Load Balancing provides a special Amazon EC2 source security group that the user can use to ensure that back-end EC2 instances receive traffic only from Elastic Load Balancing. This feature needs two security groups: the source security group and a security group that defines the ingress rules for the back-end instances. To ensure that traffic only flows between the load balancer and the back-end instances, the user can add or modify a rule to the back-end security group which can limit the ingress traffic. Thus, it can come only from the source security group provided by Elastic load Balancing. QUESTION NO: 87 An organization has created 50 IAM users. The organization wants that each user can change their password but cannot change their access keys. How can the organization achieve this? A. The organization has to create a special password policy and attach it to each user B. The root account owner has to use CLI which forces each IAM user to change their password on first login C. By default each IAM user can modify their passwords D. The root account owner can set the policy from the IAM console under the password policy screen Answer: D Explanation: With AWS IAM, organizations can use the AWS Management Console to display, create, change or delete a password policy. As a part of managing the password policy, the user can enable all users to manage their own passwords. If the user has selected the option which allows the IAM users to modify their password, he does not need to set a separate policy for the users. This option in the AWS console allows changing only the password. QUESTION NO: 88 A user has created a photo editing software and hosted it on EC2. The software accepts requests from the user about the photo format and resolution and sends a message to S3 to enhance the picture accordingly.Which of the below mentioned AWS services will help make a scalable software with the AWS infrastructure in this scenario? A. AWS Glacier B. AWS Elastic Transcoder C. AWS Simple Notification Service D. AWS Simple Queue Service Answer: D Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. The user can configure SQS, which will decouple the call between the EC2 application and S3. Thus, the application does not keep waiting for S3 to provide the data. QUESTION NO: 89 An application is generating a log file every 5 minutes. The log file is not critical but may be required only for verification in case of some major issue. The file should be accessible over the internet whenever required. Which of the below mentioned options is a best possible storage solution for it? A. AWS S3 B. AWS Glacier C. AWS RDS D. AWS RRS Answer: D Explanation: Amazon S3 stores objects according to their storage class. There are three major storage classes: Standard, Reduced Redundancy Storage and Glacier. Standard is for AWS S3 and provides very high durability. However, the costs are a little higher. Glacier is for archival and the files are not available over the internet. Reduced Redundancy Storage is for less critical files. Reduced Redundancy is little cheaper as it provides less durability in comparison to S3. In this case since the log files are not mission critical files, RRS will be a better option. QUESTION NO: 90 A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25. The user is trying to create the private subnet with CIDR 20.0.0.128/25. Which of the below mentioned statements is true in this scenario? A. It will not allow the user to create the private subnet due to a CIDR overlap B. It will allow the user to create a private subnet with CIDR as 20.0.0.128/25 C. This statement is wrong as AWS does not allow CIDR 20.0.0.0/25 D. It will not allow the user to create a private subnet due to a wrong CIDR range Answer: B Explanation: When the user creates a subnet in VPC, he specifies the CIDR block for the subnet. The CIDR block of a subnet can be the same as the CIDR block for the VPC (for a single subnet in the VPC., or a subset (to enable multiple subnets.. If the user creates more than one subnet in a VPC, the CIDR blocks of the subnets must not overlap. Thus, in this case the user has created a VPC with the CIDR block 20.0.0.0/24, which supports 256 IP addresses (20.0.0.0 to 20.0.0.255.. The user can break this CIDR block into two subnets, each supporting 128 IP addresses. One subnet uses the CIDR block 20.0.0.0/25 (for addresses 20.0.0.0 - 20.0.0.127. and the other uses the CIDR block 20.0.0.128/25 (for addresses 20.0.0.128 - 20.0.0.255.. QUESTION NO: 91 A user has created an S3 bucket which is not publicly accessible. The bucket is having thirty objects which are also private. If the user wants to make the objects public, how can he configure this with minimal efforts? A. The user should select all objects from the console and apply a single policy to mark them public B. The user can write a program which programmatically makes all objects public using S3 SDK C. Set the AWS bucket policy which marks all objects as public D. Make the bucket ACL as public so it will also mark all objects as public Answer: C Explanation: A system admin can grant permission of the S3 objects or buckets to any user or make the objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. QUESTION NO: 92 A sys admin is maintaining an application on AWS. The application is installed on EC2 and user has configured ELB and Auto Scaling. Considering future load increase, the user is planning to launch new servers proactively so that they get registered with ELB. How can the user add these instances with Auto Scaling? A. Increase the desired capacity of the Auto Scaling group B. Increase the maximum limit of the Auto Scaling group C. Launch an instance manually and register it with ELB on the fly D. Decrease the minimum limit of the Auto Scaling grou Answer: A Explanation: A user can increase the desired capacity of the Auto Scaling group and Auto Scaling will launch a new instance as per the new capacity. The newly launched instances will be registered with ELB if Auto Scaling group is configured with ELB. If the user decreases the minimum size the instances will be removed from Auto Scaling. Increasing the maximum size will not add instances but only set the maximum instance cap. QUESTION NO: 93 An organization, which has the AWS account ID as 999988887777, has created 50 IAM users. All the users are added to the same group cloudacademy. If the organization has enabled that each IAM user can login with the AWS console, which AWS login URL will the IAM users use? A. https:// 999988887777.signin.aws.amazon.com/console/ B. https:// signin.aws.amazon.com/cloudacademy/ C. https:// cloudacademy.signin.aws.amazon.com/999988887777/console/ D. https:// 999988887777.aws.amazon.com/ cloudacademy/ Answer: A Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Once the organization has created the IAM users, they will have a separate AWS console URL to login to the AWS console. The console login URL for the IAM user will be https:// AWS_Account_ID.signin.aws.amazon.com/console/. It uses only the AWS account ID and does not depend on the group or user ID. QUESTION NO: 94 A user has setup connection draining with ELB to allow in-flight requests to continue while the instance is being deregistered through Auto Scaling. If the user has not specified the draining time, how long will ELB allow inflight requests traffic to continue? A. 600 seconds B. 3600 seconds C. 300 seconds D. 0 seconds Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. The user can specify a maximum time (3600 seconds. for the load balancer to keep the connections alive before reporting the instance as deregistered. If the user does not specify the maximum timeout period, by default, the load balancer will close the connections to the deregistering instance after 300 seconds. QUESTION NO: 95 A root AWS account owner is trying to understand various options to set the permission to AWS S3. Which of the below mentioned options is not the right option to grant permission for S3? A. User Access Policy B. S3 Object Access Policy C. S3 Bucket Access Policy D. S3 ACL Answer: B Explanation: Amazon S3 provides a set of operations to work with the Amazon S3 resources. Managing S3 resource access refers to granting others permissions to work with S3. There are three ways the root account owner can define access with S3: S3 ACL: The user can use ACLs to grant basic read/write permissions to other AWS accounts. S3 Bucket Policy: The policy is used to grant other AWS accounts or IAM users permissions for the bucket and the objects in it. User Access Policy: Define an IAM user and assign him the IAM policy which grants him access to S3. QUESTION NO: 96 A sys admin has created a shopping cart application and hosted it on EC2. The EC2 instances are running behind ELB. The admin wants to ensure that the end user request will always go to the EC2 instance where the user session has been created. How can the admin configure this? A. Enable ELB cross zone load balancing B. Enable ELB cookie setup C. Enable ELB sticky session D. Enable ELB connection draining Answer: C Explanation: Generally AWS ELB routes each request to a zone with the minimum load. The Elastic Load Balancer provides a feature called sticky session which binds the user\u2019s session with a specific EC2 instance. If the sticky session is enabled the first request from the user will be redirected to any of the EC2 instances. But, henceforth, all requests from the same user will be redirected to the same EC2 instance. This ensures that all requests coming from the user during the session will be sent to the same application instance. QUESTION NO: 97 A user has configured ELB with three instances. The user wants to achieve High Availabilityi as well as redundancy with ELB. Which of the below mentioned AWS services helps the user achieve this for ELB? A. Route 53 B. AWS Mechanical Turk C. Auto Scaling D. AWS EMR Answer: A Explanation: The user can provide high availability and redundancy for applications running behind Elastic Load Balancer by enabling the Amazon Route 53 Domain Name System (DNS. failover for the load balancers. Amazon Route 53 is a DNS service that provides reliable routing to the user\u2019s infrastructure. QUESTION NO: 98 An organization is using AWS since a few months. The finance team wants to visualize the pattern of AWS spending. Which of the below AWS tool will help for this requirement? A. AWS Cost Manager B. AWS Cost Explorer C. AWS CloudWatch D. AWS Consolidated Billing Answer: B Explanation: The AWS Billing and Cost Management console includes the Cost Explorer tool for viewing AWS cost data as a graph. It does not charge extra to user for this service. With Cost Explorer the user can filter graphs using resource tags or with services in AWS. If the organization is using Consolidated Billing it helps generate report based on linked accounts. This will help organization to identify areas that require further inquiry. The organization can view trends and use that to understand spend and to predict future costs. QUESTION NO: 99 A user has launched an ELB which has 5 instances registered with it. The user deletes the ELB by mistake. What will happen to the instances? A. ELB will ask the user whether to delete the instances or not B. Instances will be terminated C. ELB cannot be deleted if it has running instances registered with it D. Instances will keep running Answer: D Explanation: When the user deletes the Elastic Load Balancer, all the registered instances will be deregistered. However, they will continue to run. The user will incur charges if he does not take any action on those instances. QUESTION NO: 100 A user is planning to setup notifications on the RDS DB for a snapshot. Which of the below mentioned event categories is not supported by RDS for this snapshot source type? A. Backup B. Creation C. Deletion D. Restoration Answer: A Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. Event categories for a snapshot source type include: Creation, Deletion, and Restoration. The Backup is a part of DB instance source type. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 101 A customer is using AWS for Dev and Test. The customer wants to setup the Dev environment with Cloudformation. Which of the below mentioned steps are not required while using Cloudformation? A. Create a stack B. Configure a service C. Create and upload the template D. Provide the parameters configured as part of the template Answer: B Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. AWS CloudFormation introduces two concepts: the template and the stack. The template is a JSON-format, text-based file that describes all the AWS resources required to deploy and run an application. The stack is a collection of AWS resources which are created and managed as a single unit when AWS CloudFormation instantiates a template. While creating a stack, the user uploads the template and provides the data for the parameters if required. QUESTION NO: 102 A user has configured the AWS CloudWatch alarm for estimated usage charges in the US East region. Which of the below mentioned statements is not true with respect to the estimated charges? A. It will store the estimated charges data of the last 14 days B. It will include the estimated charges of every AWS service C. The metric data will represent the data of all the regions D. The metric data will show data specific to that region Answer: D Explanation: When the user has enabled the monitoring of estimated charges for the AWS account with AWS CloudWatch, the estimated charges are calculated and sent several times daily to CloudWatch in the form of metric data. This data will be stored for 14 days. The billing metric data is stored in the US East (Northern Virginia. Region and represents worldwide charges. This data also includes the estimated charges for every service in AWS used by the user, as well as the estimated overall AWS charges. QUESTION NO: 103 A user is accessing RDS from an application. The user has enabled the Multi AZ feature with the MS SQL RDS DB. During a planned outage how will AWS ensure that a switch from DB to a standby replica will not affect access to the application? A. RDS will have an internal IP which will redirect all requests to the new DB B. RDS uses DNS to switch over to stand by replica for seamless transition C. The switch over changes Hardware so RDS does not need to worry about access D. RDS will have both the DBs running independently and the user has to manually switch over Answer: B Explanation: In the event of a planned or unplanned outage of a DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone if the user has enabled Multi AZ. The automatic failover mechanism simply changes the DNS record of the DB instance to point to the standby DB instance. As a result, the user will need to re-establish any existing connections to the DB instance. However, as the DNS is the same, the application can access DB seamlessly. QUESTION NO: 104 An organization is generating digital policy files which are required by the admins for verification. Once the files are verified they may not be required in the future unless there is some compliance issue. If the organization wants to save them in a cost effective way, which is the best possible solution? A. AWS RRS B. AWS S3 C. AWS RDS D. AWS Glacier Answer: D Explanation: Amazon S3 stores objects according to their storage class. There are three major storage classes: Standard, Reduced Redundancy and Glacier. Standard is for AWS S3 and provides very high durability. However, the costs are a little higher. Reduced redundancy is for less critical files. Glacier is for archival and the files which are accessed infrequently. It is an extremely low-cost storage service that provides secure and durable storage for data archiving and backup. QUESTION NO: 105 A user has launched an EBS backed instance. The user started the instance at 9 AM in the morning. Between 9 AM to 10 AM, the user is testing some script. Thus, he stopped the instance twice and restarted it. In the same hour the user rebooted the instance once. For how many instance hours will AWS charge the user? A. 3 hours B. 4 hours C. 2 hours D. 1 hour Answer: A Explanation: A user can stop/start or reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. When the instance is rebooted AWS will not charge the user for the extra hours. In case the user stops the instance, AWS does not charge the running cost but charges only the EBS storage cost. If the user starts and stops the instance multiple times in a single hour, AWS will charge the user for every start and stop. In this case, since the instance was rebooted twice, it will cost the user for 3 instance hours. QUESTION NO: 106 An organization has configured the custom metric upload with CloudWatch. The organization has given permission to its employees to upload data using CLI as well SDK. How can the user track the calls made to CloudWatch? A. The user can enable logging with CloudWatch which logs all the activities B. Use CloudTrail to monitor the API calls C. Create an IAM user and allow each user to log the data using the S3 bucket D. Enable detailed monitoring with CloudWatch Answer: B Explanation: AWS CloudTrail is a web service which will allow the user to monitor the calls made to the Amazon CloudWatch API for the organization\u2019s account, including calls made by the AWS Management Console, Command Line Interface (CLI., and other services. When CloudTrail logging is turned on, CloudWatch will write log files into the Amazon S3 bucket, which is specified during the CloudTrail configuration. QUESTION NO: 107 A user has created a queue named \u201cmyqueue\u201d with SQS. There are four messages published to queue which are not received by the consumer yet. If the user tries to delete the queue, what will happen? A. A user can never delete a queue manually. AWS deletes it after 30 days of inactivity on queue B. It will delete the queue C. It will initiate the delete but wait for four days before deleting until all messages are deleted automatically. D. It will ask user to delete the messages first Answer: B Explanation: SQS allows the user to move data between distributed components of applications so they can perform different tasks without losing messages or requiring each component to be always available. The user can delete a queue at any time, whether it is empty or not. It is important to note that queues retain messages for a set period of time. By default, a queue retains messages for four days. QUESTION NO: 108 A user has launched a large EBS backed EC2 instance in the US-East-1a region. The user wants to achieve Disaster Recovery (DR. for that instance by creating another small instance in Europe. How can the user achieve DR? A. Copy the running instance using the \u201cInstance Copy\u201d command to the EU region B. Create AMI of the instance and copy the AMI to the EU region. Then launch the instance from the EU AMI. C. Copy the instance from the US East region to the EU region D. Use the \u201cLaunch more like this\u201d option to copy the instance from one region to another Answer: B Explanation: To launch an EC2 instance it is required to have an AMI in that region. If the AMI is not available in that region, then create a new AMI or use the copy command to copy the AMI from one region to the other region. QUESTION NO: 109 A user has created numerous EBS volumes. What is the general limit for each AWS account for the maximum number of EBS volumes that can be created? A. 10000 B. 5000 C. 100 D. 1000 Answer: B Explanation: A user can attach multiple EBS volumes to the same instance within the limits specified by his AWS account. Each AWS account has a limit on the number of Amazon EBS volumes that the user can create, and the total storage available. The default limit for the maximum number of volumes that can be created is 5000. QUESTION NO: 110 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24. and VPN only subnets CIDR (20.0.1.0/24. along with the VPN gateway (vgw-12345. to connect to the user\u2019s data centre. Which of the below mentioned options is a valid entry for the main route table in this scenario? A. Destination: 20.0.0.0/24 and Target: vgw-12345 B. Destination: 20.0.0.0/16 and Target: ALL C. Destination: 20.0.1.0/16 and Target: vgw-12345 D. Destination: 0.0.0.0/0 and Target: vgw-12345 Answer: D Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. Here are the valid entries for the main route table in this scenario: Destination: 0.0.0.0/0 & Target: vgw-12345 (To route all internet traffic to the VPN gateway. Destination: 20.0.0.0/16 & Target: local (To allow local routing in VPC. QUESTION NO: 111 A user has stored data on an encrypted EBS volume. The user wants to share the data with his friend\u2019s AWS account. How can user achieve this? A. Create an AMI from the volume and share the AMI B. Copy the data to an unencrypted volume and then share C. Take a snapshot and share the snapshot with a friend D. If both the accounts are using the same encryption key then the user can share the volume directly Answer: B Explanation: AWS EBS supports encryption of the volume. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. If the user is having data on an encrypted volume and is trying to share it with others, he has to copy the data from the encrypted volume to a new unencrypted volume. Only then can the user share it as an encrypted volume data. Otherwise the snapshot cannot be shared. QUESTION NO: 112 A user has enabled the Multi AZ feature with the MS SQL RDS database server. Which of the below mentioned statements will help the user understand the Multi AZ feature better? A. In a Multi AZ, AWS runs two DBs in parallel and copies the data asynchronously to the replica copy B. In a Multi AZ, AWS runs two DBs in parallel and copies the data synchronously to the replica copy C. In a Multi AZ, AWS runs just one DB but copies the data synchronously to the standby replica D. AWS MS SQL does not support the Multi AZ feature Answer: C Explanation: Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption.Note that the high-availability feature is not a scaling solution for read-only scenarios; you cannot use a standby replica to serve read traffic. To service read-only traffic, you should use a read replica. QUESTION NO: 113 An organization is using cost allocation tags to find the cost distribution of different departments and projects. One of the instances has two separate tags with the key/ value as \u201cInstanceName/ HR\u201d, \u201cCostCenter/HR\u201d. What will AWS do in this case? A. InstanceName is a reserved tag for AWS. Thus, AWS will not allow this tag B. AWS will not allow the tags as the value is the same for different keys C. AWS will allow tags but will not show correctly in the cost allocation report due to the same value of the two separate keys D. AWS will allow both the tags and show properly in the cost distribution report Answer: D Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources, AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. Each tag will have a key-value and can be applied to services, such as EC2, S3, RDS, EMR, etc. It is required that the key should be different for each tag. The value can be the same for different keys. In this case since the value is different, AWS will properly show the distribution report with the correct values. QUESTION NO: 114 A user is publishing custom metrics to CloudWatch. Which of the below mentioned statements will help the user understand the functionality better? A. The user can use the CloudWatch Import tool B. The user should be able to see the data in the console after around 15 minutes C. If the user is uploading the custom data, the user must supply the namespace, timezone, and metric name as part of the command D. The user can view as well as upload data using the console, CLI and APIs Answer: B Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user has to always include the namespace as a part of the request. However, the other parameters are optional. If the user has uploaded data using CLI, he can view it as a graph inside the console. The data will take around 2 minutes to upload but can be viewed only after around 15 minutes. QUESTION NO: 115 A user is launching an EC2 instance in the US East region. Which of the below mentioned options is recommended by AWS with respect to the selection of the availability zone? A. Always select the US-East-1-a zone for HA B. Do not select the AZ; instead let AWS select the AZ C. The user can never select the availability zone while launching an instance D. Always select the AZ while launching an instance Answer: B Explanation: When launching an instance with EC2, AWS recommends not to select the availability zone (AZ.. AWS specifies that the default Availability Zone should be accepted. This is because it enables AWS to select the best Availability Zone based on the system health and available capacity. If the user launches additional instances, only then an Availability Zone should be specified. This is to specify the same or different AZ from the running instances. QUESTION NO: 116 A user has created a VPC with CIDR 20.0.0.0/16 with only a private subnet and VPN connection using the VPC wizard. The user wants to connect to the instance in a private subnet over SSH. How should the user define the security rule for SSH? A. Allow Inbound traffic on port 22 from the user\u2019s network B. The user has to create an instance in EC2 Classic with an elastic IP and configure the security group of a private subnet to allow SSH from that elastic IP C. The user can connect to a instance in a private subnet using the NAT instance D. Allow Inbound traffic on port 80 and 22 to allow the user to connect to a private subnet over the Internet Answer: A Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, the user can setup a case with a VPN only subnet (private. which uses VPN access to connect with his data centre. When the user has configured this setup with Wizard, all network connections to the instances in the subnet will come from his data centre. The user has to configure the security group of the private subnet which allows the inbound traffic on SSH (port 22. from the data centre\u2019s network range. QUESTION NO: 117 A user has created an ELB with the availability zone US-East-1A. The user wants to add more zones to ELB to achieve High Availability. How can the user add more zones to the existing ELB? A. It is not possible to add more zones to the existing ELB B. The only option is to launch instances in different zones and add to ELB C. The user should stop the ELB and add zones and instances as required D. The user can add zones on the fly from the AWS console Answer: D Explanation: The user has created an Elastic Load Balancer with the availability zone and wants to add more zones to the existing ELB. The user can do so in two ways: From the console or CLI, add new zones to ELB; Launch instances in a separate AZ and add instances to the existing ELB. QUESTION NO: 118 A user has configured an Auto Scaling group with ELB. The user has enabled detailed CloudWatch monitoring on Elastic Load balancing. Which of the below mentioned statements will help the user understand this functionality better? A. ELB sends data to CloudWatch every minute only and does not charge the user B. ELB will send data every minute and will charge the user extra C. ELB is not supported by CloudWatch D. It is not possible to setup detailed monitoring for ELB Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Elastic Load Balancing includes 10 metrics and 2 dimensions, and sends data to CloudWatch every minute. This does not cost extra. QUESTION NO: 119 A user has configured ELB with two EBS backed EC2 instances. The user is trying to understand the DNS access and IP support for ELB. Which of the below mentioned statements may not help the user understand the IP mechanism supported by ELB? A. The client can connect over IPV4 or IPV6 using Dualstack B. ELB DNS supports both IPV4 and IPV6 C. Communication between the load balancer and back-end instances is always through IPV4 D. The ELB supports either IPV4 or IPV6 but not both Answer: D Explanation: Elastic Load Balancing supports both Internet Protocol version 6 (IPv6. and Internet Protocol version 4 (IPv4.. Clients can connect to the user\u2019s load balancer using either IPv4 or IPv6 (in EC2- Classic. DNS. However, communication between the load balancer and its back-end instances uses only IPv4. The user can use the Dualstack-prefixed DNS name to enable IPv6 support for communications between the client and the load balancers. Thus, the clients are able to access the load balancer using either IPv4 or IPv6 as their individual connectivity needs dictate. QUESTION NO: 120 A user has received a message from the support team that an issue occurred 1 week back between 3 AM to 4 AM and the EC2 server was not reachable. The user is checking the CloudWatch metrics of that instance. How can the user find the data easily using the CloudWatch console? A. The user can find the data by giving the exact values in the time Tab under CloudWatch metrics B. The user can find the data by filtering values of the last 1 week for a 1 hour period in the Relative tab under CloudWatch metrics C. It is not possible to find the exact time from the console. The user has to use CLI to provide the specific time D. The user can find the data by giving the exact values in the Absolute tab under CloudWatch metrics Answer: D Explanation: If the user is viewing the data inside the CloudWatch console, the console provides options to filter values either using the relative period, such as days /hours or using the Absolute tab where the user can provide data with a specific date and time. The console also provides the option to search using the local timezone under the time range caption in the console. QUESTION NO: 121 A user has setup Auto Scaling with ELB on the EC2 instances. The user wants to configure that whenever the CPU utilization is below 10%, Auto Scaling should remove one instance. How can the user configure this? A. The user can get an email using SNS when the CPU utilization is less than 10%. The user can use the desired capacity of Auto Scaling to remove the instance B. Use CloudWatch to monitor the data and Auto Scaling to remove the instances using scheduled actions C. Configure CloudWatch to send a notification to Auto Scaling Launch configuration when the CPU utilization is less than 10% and configure the Auto Scaling policy to remove the instance D. Configure CloudWatch to send a notification to the Auto Scaling group when the CPU Utilization is less than 10% and configure the Auto Scaling policy to remove the instance Answer: D Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup to receive a notification on the Auto Scaling group with the CloudWatch alarm when the CPU utilization is below a certain threshold. The user can configure the Auto Scaling policy to take action for removing the instance. When the CPU utilization is below 10% CloudWatch will send an alarm to the Auto Scaling group to execute the policy. QUESTION NO: 122 A user has enabled detailed CloudWatch metric monitoring on an Auto Scaling group. Which of the below mentioned metrics will help the user identify the total number of instances in an Auto Scaling group cluding pending, terminating and running instances? A. GroupTotalInstances B. GroupSumInstances C. It is not possible to get a count of all the three metrics together. The user has to find the individual number of running, terminating and pending instances and sum it D. GroupInstancesCount Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. For Auto Scaling, CloudWatch provides various metrics to get the group information, such as the Number of Pending, Running or Terminating instances at any moment. If the user wants to get the total number of Running, Pending and Terminating instances at any moment, he can use the GroupTotalInstances metric. QUESTION NO: 123 A user is trying to configure the CloudWatch billing alarm. Which of the below mentioned steps should be performed by the user for the first time alarm creation in the AWS Account Management section? A. Enable Receiving Billing Reports B. Enable Receiving Billing Alerts C. Enable AWS billing utility D. Enable CloudWatch Billing Threshold Answer: B Explanation: AWS CloudWatch supports enabling the billing alarm on the total AWS charges. Before the user can create an alarm on the estimated charges, he must enable monitoring of the estimated AWS charges, by selecting the option \u201cEnable receiving billing alerts\u201d. It takes about 15 minutes before the user can view the billing data. The user can then create the alarms. QUESTION NO: 124 A user is checking the CloudWatch metrics from the AWS console. The user notices that the CloudWatch data is coming in UTC. The user wants to convert the data to a local time zone. How can the user perform this? A. In the CloudWatch dashboard the user should set the local timezone so that CloudWatch shows the data only in the local time zone B. In the CloudWatch console select the local timezone under the Time Range tab to 712 view the data as per the local timezone C. The CloudWatch data is always in UTC; the user has to manually convert the data D. The user should have send the local timezone while uploading the data so that CloudWatch will show the data only in the local timezone Answer: B Explanation: If the user is viewing the data inside the CloudWatch console, the console provides options to filter values either using the relative period, such as days/hours or using the Absolute tab where the use can provide data with a specific date and time. The console also provides the option to search using the local timezone under the time range caption in the console because the time range tab allows the user to change the time zone. (adsbygoogle = window.adsbygoogle || []).push({}); QUESTION NO: 125 An organization (Account ID 123412341234. has attached the below mentioned IAM policy to a user. What does this policy statement entitle the user to perform? \"Statement\": [ { \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*AccessKey*\", ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] } ] Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (Account ID 123412341234. wants some of their users to manage keys (access and secret access keys. of all IAM users, the organization should set the below mentioned policy which entitles the IAM user to modify keys of all IAM users with CLI, SDK or API. \"Statement\": [ { \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*AccessKey*\", ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] } ] QUESTION NO: 126 A user is trying to connect to a running EC2 instance using SSH. However, the user gets a connection time out error. Which of the below mentioned options is not a possible reason for rejection? A. The access key to connect to the instance is wrong B. The security group is not configured properly C. The private key used to launch the instance is not correct D. The instance CPU is heavily loaded Answer: A Explanation: If the user is trying to connect to a Linux EC2 instance and receives the connection time out error the probable reasons are: - Security group is not configured with the SSH port - The private key pair is not right - The user name to login is wrong - The instance CPU is heavily loaded, so it does not allow more connections QUESTION NO: 127 A user has configured Elastic Load Balancing by enabling a Secure Socket Layer (SSL) negotiation configuration known as a Security Policy. Which of the below mentioned options is not part of this secure policy while negotiating the SSL connection between the user and the client? A. SSL Protocols B. Client Order Preference C. SSL Ciphers D. Server Order Preference Answer: B Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the load balancer. A security policy is a combination of SSL Protocols, SSL Ciphers, and the Server order Preference option. QUESTION NO: 128 A user has configured CloudWatch monitoring on an EBS backed EC2 instance. If the user has not attached any additional device, which of the below mentioned metrics will always show a 0 value? A. DiskReadBytes 7456 B. NetworkIn C. NetworkOut D. CPUUtilization Answer: A Explanation: CloudWatch is used to monitor AWS as the well custom services. For EC2 when the user is monitoring the EC2 instances, it will capture the 7 Instance level and 3 system check parameters for the EC2 instance. Since this is an EBS backed instance, it will not have ephermal storage attached to it. Out of the 7 EC2 metrics, the 4 metrics DiskReadOps, DiskWriteOps, DiskReadBytes and DiskWriteBytes are disk related data and available only when there is ephermal storage attached to an instance. For an EBS backed instance without any additional device, this data will be 0. QUESTION NO: 129 A user has launched an EBS backed EC2 instance. What will be the difference while performing the restart or stop/start options on that instance? A. For restart it does not charge for an extra hour, while every stop/start it will be charged as a separate hour B. Every restart is charged by AWS as a separate hour, while multiple start/stop actions during a single hour will be counted as a single hour C. For every restart or start/stop it will be charged as a separate hour D. For restart it charges extra only once, while for every stop/start it will be charged as a separate hour Answer: A Explanation: For an EC2 instance launched with an EBS backed AMI, each time the instance state is changed from stop to start/ running, AWS charges a full instance hour, even if these transitions happen multiple times within a single hour. Anyway, rebooting an instance AWS does not charge a new instance billing hour. QUESTION NO: 130 A user has created a queue named \u201cmyqueue\u201d in US-East region with AWS SQS. The user\u2019s AWS account ID is 123456789012. If the user wants to perform some action on this queue, which of the below Queue URL should he use? A. http://sqs.us-east-1.amazonaws.com/123456789012/myqueue B. http://sqs.amazonaws.com/123456789012/myqueue C. http://sqs. 123456789012.us-east-1.amazonaws.com/myqueue D. http:// 123456789012.sqs. us-east-1.amazonaws.com/myqueue Answer: A Explanation: When creating a new queue in SQS, the user must provide a queue name that is unique within the scope of all queues of user\u2019s account. If the user creates queues using both the latest WSDL and a previous version, he will have a single namespace for all his queues. Amazon SQS assigns each queue created by user an identifier called a queue URL, which includes the queue name and other components that Amazon SQS determines. Whenever the user wants to perform an action on a queue, he must provide its queue URL. The queue URL for the account id 123456789012 & queue name \u201cmyqueue\u201d in US-East-1 region will be http:// sqs.us-east1.amazonaws.com/123456789012/myqueue. QUESTION NO: 131 A sys admin is trying to understand the Auto Scaling activities. Which of the below mentioned processes is not performed by Auto Scaling? A. Reboot Instance B. Schedule Actions C. Replace Unhealthy D. Availability Zone Balancing Answer: A Explanation: There are two primary types of Auto Scaling processes: Launch and Terminate, which launch or terminate instances, respectively. Some other actions performed by Auto Scaling are: AddToLoadbalancer, AlarmNotification, HealthCheck, AZRebalance, ReplaceUnHealthy, and ScheduledActions. QUESTION NO: 132 A sys admin is trying to understand EBS snapshots. Which of the below mentioned statements will not be useful to the admin to understand the concepts about a snapshot? A. The snapshot is synchronous B. It is recommended to stop the instance before taking a snapshot for consistent data C. The snapshot is incremental D. The snapshot captures the data that has been written to the hard disk when the snapshot command was executed Answer: A Explanation: The AWS snapshot is a point in time backup of an EBS volume. When the snapshot command is executed it will capture the current state of the data that is written on the drive and take a backup. For a better and consistent snapshot of the root EBS volume, AWS recommends stopping the instance. For additional volumes it is recommended to unmount the device. The snapshots are asynchronous and incremental. QUESTION NO: 133 A root account owner has created an S3 bucket testmycloud. The account owner wants to allow everyone to upload the objects as well as enforce that the person who uploaded the object should manage the permission of those objects. Which is the easiest way to achieve this? A. The root account owner should create a bucket policy which allows the IAM users to upload the object B. The root account owner should create the bucket policy which allows the other account owners to set the object policy of that bucket C. The root account should use ACL with the bucket to allow everyone to upload the object D. The root account should create the IAM users and provide them the permission to upload content to the bucket Answer: C Explanation: Each AWS S3 bucket and object has an ACL (Access Control List. associated with it. An ACL is a list of grants identifying the grantee and the permission granted. The user can use ACLs to grant basic read/write permissions to other AWS accounts. ACLs use an Amazon S3\u2013specific XML schema. The user cannot grant permissions to other users in his account. ACLs are suitable for specific scenarios. For example, if a bucket owner allows other AWS accounts to upload objects, permissions to these objects can only be managed using the object ACL by the AWS account that owns the object. QUESTION NO: 134 An organization has setup consolidated billing with 3 different AWS accounts. Which of the below mentioned advantages will organization receive in terms of the AWS pricing? A. The consolidated billing does not bring any cost advantage for the organization B. All AWS accounts will be charged for S3 storage by combining the total storage of each account C. The EC2 instances of each account will receive a total of 750*3 micro instance hours free D. The free usage tier for all the 3 accounts will be 3 years and not a single year Answer: B Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. For billing purposes, AWS treats all the accounts on the consolidated bill as one account. Some services, such as Amazon EC2 and Amazon S3 have volume pricing tiers across certain usage dimensions that give the user lower prices when he uses the service more. QUESTION NO: 135 A user has launched two EBS backed EC2 instances in the US-East-1a region. The user wants to change the zone of one of the instances. How can the user change it? A. Stop one of the instances and change the availability zone B. The zone can only be modified using the AWS CLI C. From the AWS EC2 console, select the Actions - > Change zones and specify new zone D. Create an AMI of the running instance and launch the instance in a separate AZ Answer: D Explanation: With AWS EC2, when a user is launching an instance he can select the availability zone (AZ. at the time of launch. If the zone is not selected, AWS selects it on behalf of the user. Once the instance is launched, the user cannot change the zone of that instance unless he creates an AMI of that instance and launches a new instance from it. QUESTION NO: 136 A user wants to make so that whenever the CPU utilization of the AWS EC2 instance is above 90%, the redlight of his bedroom turns on. Which of the below mentioned AWS services is helpful for this purpose? A. AWS CloudWatch + AWS SES B. AWS CloudWatch + AWS SNS C. None. It is not possible to configure the light with the AWS infrastructure services D. AWS CloudWatch and a dedicated software turning on the light Answer: B Explanation: Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can configure some sensor devices at his home which receives data on the HTTP end point (REST calls. and turn on the red light. The user can configure the CloudWatch alarm to send a notification to the AWS SNS HTTP end point (the sensor device. and it will turn the light red when there is an alarm condition. QUESTION NO: 137 An organization has added 3 of his AWS accounts to consolidated billing. One of the AWS accounts has purchased a Reserved Instance (RI. of a small instance size in the US-East-1a zone. All other AWS accounts are running instances of a small size in the same zone. What will happen in this case for the RI pricing? A. Only the account that has purchased the RI will get the advantage of RI pricing B. One instance of a small size and running in the US-East-1a zone of each AWS account will get the benefit of RI pricing C. Any single instance from all the three accounts can get the benefit of AWS RI pricing if they are running in the same zone and are of the same size D. If there are more than one instances of a small size running across multiple accounts in the same zone no one will get the benefit of RI Answer: C Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. For billing purposes, consolidated billing treats all the accounts on the consolidated bill as one account. This means that all accounts on a consolidated bill can receive the hourly cost benefit of the Amazon EC2 Reserved Instances purchased by any other account. In this case only one Reserved Instance has been purchased by one account. Thus, only a single instance from any of the accounts will get the advantage of RI. AWS will implement the blended rate for each instance if more than one instance is running concurrently. QUESTION NO: 138 An organization is planning to use AWS for 5 different departments. The finance department is responsible to pay for all the accounts. However, they want the cost separation for each account to map with the right cost centre. How can the finance department achieve this? A. Create 5 separate accounts and make them a part of one consolidate billing B. Create 5 separate accounts and use the IAM cross account access with the roles for better management C. Create 5 separate IAM users and set a different policy for their access D. Create 5 separate IAM groups and add users as per the department\u2019s employees Answer: A Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. Consolidated billing enables the organization to see a combined view of the AWS charges incurred by each account as well as obtain a detailed cost report for each of the individual AWS accounts associated with the paying account. QUESTION NO: 139 A user has setup an EBS backed instance and a CloudWatch alarm when the CPU utilization is more than 65%. The user has setup the alarm to watch it for 5 periods of 5 minutes each. The CPU utilization is 60% between 9 AM to 6 PM. The user has stopped the EC2 instance for 15 minutes between 11 AM to 11:15 AM. What will be the status of the alarm at 11:30 AM? A. Alarm B. OK C. Insufficient Data D. Error Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The state of the alarm will be OK for the whole day. When the user stops the instance for three periods the alarm may not receive the data QUESTION NO: 140 A user is running one instance for only 3 hours every day. The user wants to save some cost with the instance. Which of the below mentioned Reserved Instance categories is advised in this case? A. The user should not use RI; instead only go with the on-demand pricing B. The user should use the AWS high utilized RI C. The user should use the AWS medium utilized RI D. The user should use the AWS low utilized RI Answer: A Explanation: The AWS Reserved Instance provides the user with an option to save some money by paying a one-time fixed amount and then save on the hourly rate. It is advisable that if the user is having 30% or more usage of an instance per day, he should go for a RI. If the user is going to use an EC2 instance for more than 2200-2500 hours per year, RI will help the user save some cost. Here, the instance is not going to run for less than 1500 hours. Thus, it is advisable that the user should use the on-demand pricing. QUESTION NO: 141 A user has setup an RDS DB with Oracle. The user wants to get notifications when someone modifies the security group of that DB. How can the user configure that? A. It is not possible to get the notifications on a change in the security group B. Configure SNS to monitor security group changes C. Configure event notification on the DB security group D. Configure the CloudWatch alarm on the DB for a change in the security group Answer: C Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. These events can be configured for source categories, such as DB instance, DB security group, DB snapshot and DB parameter group. If the user is subscribed to a Configuration Change category for a DB security group, he will be notified when the DB security group is changed. QUESTION NO: 142 A user is trying to setup a recurring Auto Scaling process. The user has setup one process to scale up every day at 8 am and scale down at 7 PM. The user is trying to setup another recurring process which scales up on the 1st of every month at 8 AM and scales down the same day at 7 PM. What will Auto Scaling do in this scenario? A. Auto Scaling will execute both processes but will add just one instance on the 1st B. Auto Scaling will add two instances on the 1st of the month C. Auto Scaling will schedule both the processes but execute only one process randomly D. Auto Scaling will throw an error since there is a conflict in the schedule of two separate Auto Scaling Processes Answer: D Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can also configure the recurring schedule action which will follow the Linux cron format. As per Auto Scaling, a scheduled action must have a unique time value. If the user attempts to schedule an activity at a time when another existing activity is already scheduled, the call will be rejected with an error message noting the conflict. QUESTION NO: 143 A user is planning to setup infrastructure on AWS for the Christmas sales. The user is planning to use Auto Scaling based on the schedule for proactive scaling. What advise would you give to the user? A. It is good to schedule now because if the user forgets later on it will not scale up B. The scaling should be setup only one week before Christmas C. Wait till end of November before scheduling the activity D. It is not advisable to use scheduled based scaling Answer: C Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can specify any date in the future to scale up or down during that period. As per Auto Scaling the user can schedule an action for up to a month in the future. Thus, it is recommended to wait until end of November before scheduling for Christmas. QUESTION NO: 144 A user is trying to understand the ACL and policy for an S3 bucket. Which of the below mentioned policy permissions is equivalent to the WRITE ACL on a bucket? A. s3:GetObjectAcl B. s3:GetObjectVersion C. s3:ListBucketVersions D. s3:DeleteObject Answer: D Explanation: Amazon S3 provides a set of operations to work with the Amazon S3 resources. Each AWS S3 bucket can have an ACL (Access Control List. or bucket policy associated with it. The WRITE ACL list allows the other AWS accounts to write/modify to that bucket. The equivalent S3 bucket policy permission for it is s3:DeleteObject. QUESTION NO: 145 A user has created an ELB with Auto Scaling. Which of the below mentioned offerings from ELB helps the user to stop sending new requests traffic from the load balancer to the EC2 instance when the instance is being deregistered while continuing in-flight requests? A. ELB sticky session B. ELB deregistration check C. ELB connection draining D. ELB auto registration Off Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. QUESTION NO: 146 A user has launched an EC2 instance from an instance store backed AMI. The infrastructure team wants to create an AMI from the running instance. Which of the below mentioned steps will not be performed while creating the AMI? A. Define the AMI launch permissions B. Upload the bundled volume C. Register the AMI D. Bundle the volume Answer: A Explanation: When the user has launched an EC2 instance from an instance store backed AMI, it will need to follow certain steps, such as \u201cBundling the root volume\u201d, \u201cUploading the bundled volume\u201d and \u201cRegister the AMI\u201d. Once the AMI is created the user can setup the launch permission. However, it is not required to setup during the launch. QUESTION NO: 147 You are managing the AWS account of a big organization. The organization has more than 1000+ employees and they want to provide access to the various services to most of the employees. Which of the below mentioned options is the best possible solution in this case? A. The user should create a separate IAM user for each employee and provide access to them as per the policy B. The user should create an IAM role and attach STS with the role. The user should attach that role to the EC2 instance and setup AWS authentication on that server C. The user should create IAM groups as per the organization\u2019s departments and add each user to the group for better access control D. Attach an IAM role with the organization\u2019s authentication service to authorize each user for various AWS services Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user is managing an AWS account for an organization that already has an identity system, such as the login system for the corporate network (SSO.. In this case, instead of creating individual IAM users or groups for each user who need AWS access, it may be more practical to use a proxy server to translate the user identities from the organization network into the temporary AWS security credentials. This proxy server will attach an IAM role to the user after authentication. QUESTION NO: 148 A user has configured a VPC with a new subnet. The user has created a security group. The user wants to configure that instances of the same subnet communicate with each other. How can the user configure this with the security group? A. There is no need for a security group modification as all the instances can communicate with each other inside the same subnet B. Configure the subnet as the source in the security group and allow traffic on all the protocols and ports C. Configure the security group itself as the source and allow traffic on all the protocols and ports D. The user has to use VPC peering to configure this Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features that the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level. If the user is using the default security group it will have a rule which allows the instances to communicate with other. For a new security group the user has to specify the rule, add it to define the source as the security group itself, and select all the protocols and ports for that source. QUESTION NO: 149 A user is launching an instance. He is on the \u201cTag the instance\u201d screen. Which of the below mentioned information will not help the user understand the functionality of an AWS tag? A. Each tag will have a key and value B. The user can apply tags to the S3 bucket C. The maximum value of the tag key length is 64 unicode characters D. AWS tags are used to find the cost distribution of various resources Answer: C Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources, AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. Each tag will have a key-value and can be applied to services, such as EC2, S3, RDS, EMR, etc. The maximum size of a tag key is 128 unicode characters. QUESTION NO: 150 A user has created a VPC with CIDR 20.0.0.0/16. The user has created public and VPN only subnets along with hardware VPN access to connect to the user\u2019s datacenter. The user wants to make so that all traffic coming to the public subnet follows the organization\u2019s proxy policy. How can the user make this happen? A. Setting up a NAT with the proxy protocol and configure that the public subnet receives traffic from NAT B. Settin up a proxy policy in the internet gateway connected with the public subnet C. It is not possible to setup the proxy policy for a public subnet D. Setting the route table and security group of the public subnet which receives traffic from a virtual private gateway Answer: D Explanation: The user can create subnets within a VPC. If the user wants to connect to VPC from his own data centre, he can setup public and VPN only subnets which uses hardware VPN access to connect with his data centre. When the user has configured this setup, it will update the main route table used with the VPN-only subnet, create a custom route table and associate it with the public subnet. It also creates an internet gateway for the public subnet. By default the internet traffic of the VPN subnet is routed to a virtual private gateway while the internet traffic of the public subnet is routed through the internet gateway. The user can set up the route and security group rules. These rules enable the traffic to come from the organization\u2019s network over the virtual private gateway to the public subnet to allow proxy settings on that public subnet.","title":"AWS Certified SysOps Administrator - Questions and Answers"},{"location":"nightwolf-cotribution/azure_interview_questions/","text":"Microsoft Azure Administrator \uf0c1 QUESTION 1: You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Dev, you assign the Contributor role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: A Explanation: The Contributor role can manage all resources (and add resources) in a Resource Group. QUESTION 2 You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Subscription1, you assign the Logic App Operator role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: B Explanation: You would need the Logic App Contributor role. References: - https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles - https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 3 You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Dev, you assign the Logic App Contributor role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: A Explanation: The Logic App Contributor role lets you manage logic app, but not access to them. It provides access to view, edit, and update a logic app. References: - https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles - https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 4 You have an Azure subscription named Subscription1 that contains an Azure Log Analytics workspace named Workspace1. You need to view the error events from a table named Event. Which query should you run in Workspace1? A. Get-Event Event | where ($_.EventType \u2013eq \"error\") B. Get-Event Event | where ($_.EventType == \"error\") C. search in (Event) * | where EventType \u2013eq \"error\" D. search in (Event) \"error\" E. select *from Event where EventType == \"error\" F. Event | where EventType is \"error\" Correct Answer: D Explanation: To search a term in a specific table, add in (table-name) just after the search operator References: - https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/search-queries - https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-portal QUESTION 5 You have an Azure subscription named Subscription1. Subscription1 contains the resource groups in the following table. RG1 has a web app named WebApp1. WebApp1 is located in West Europe. You move WebApp1 to RG2. What is the effect of the move? A. The App Service plan for WebApp1 moves to North Europe. Policy2 applies to WebApp1. B. The App Service plan for WebApp1 remains in West Europe. Policy2 applies to WebApp1. C. The App Service plan for WebApp1 moves to North Europe. Policy1 applies to WebApp1. D. The App Service plan for WebApp1 remains in West Europe. Policy1 applies to WebApp1. Correct Answer: B Section: [none] Explanation Explanation: You can move an app to another App Service plan, as long as the source plan and the target plan are in the same resource group and geographical region. The region in which your app runs is the region of the App Service plan it's in. However, you cannot change an App Service plan's region. References: - https://docs.microsoft.com/en-us/azure/app-service/app-service-plan-manage QUESTION 6 You have an Azure subscription that contains a resource group named RG1. RG1 contains 100 virtual machines. Your company has three cost centers named Manufacturing, Sales, and Finance. You need to associate each virtual machine to a specific cost center. What should you do? A. Configure locks for the virtual machine. B. Add an extension to the virtual machines. C. Assign tags to the virtual machines. D. Modify the inventory settings of the virtual machine. Correct Answer: C Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/billing/billing-getting-started https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-using-tags QUESTION 7 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the Subscriptions blade, you select the subscription, and then click Programmatic deployment. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 8 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the Subscriptions blade, you select the subscription, and then click Resource providers. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 9 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the RG1 blade, you click Automation script. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 10 HOTSPOT You have an Azure subscription. You plan to use Azure Resource Manager templates to deploy 50 Azure virtual machines that will be part of the same availability set. You need to ensure that as many virtual machines as possible are available if the fabric fails or during servicing. How should you configure the template? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Use two fault domains. 2 or 3 is max value, depending on which region you are in. Use 20 for platformUpdateDomainCount Increasing the update domain (platformUpdateDomainCount) helps with capacity and availability planning when the platform reboots nodes. A higher number for the pool (20 is max) means that fewer of their nodes in any given availability set would be rebooted at once. References: https://www.itprotoday.com/microsoft-azure/check-if-azure-region-supports-2-or-3-fault-domains-managed- disks https://github.com/Azure/acs-engine/issues/1030 QUESTION 11 HOTSPOT You have an Azure subscription named Subscription1 that has a subscription ID of c276fc76-9cd4-44c9- 99a7-4fd71546436e. You need to create a custom RBAC role named CR1 that meets the following requirements: Can be assigned only to the resource groups in Subscription1 Prevents the management of the access permissions for the resource groups Allows the viewing, creating, modifying, and deleting of resource within the resource groups What should you specify in the assignable scopes and the permission elements of the definition of CR1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/role-based-access-control/custom-roles https://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider- operations#microsoftresources QUESTION 12 HOTSPOT You have an Azure Active Directory (Azure AD) tenant that contains three global administrators named Admin1, Admin2, and Admin3. The tenant is associated to an Azure subscription. Access control for the subscription is configured as shown in the Access control exhibit. (Click the Exhibit tab.) You sign in to the Azure portal as Admin1 and configure the tenant as shown in the Tenant exhibit. (Click the Exhibit tab.) For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 13 You have an Azure policy as shown in the following exhibit. What is the effect of the policy? A. You are prevented from creating Azure SQL Servers in ContosoRG1 only. B. You can create Azure SQL servers in ContosoRG1 only. C. You can create Azure SQL servers in any resource group within Subscription1. D. You are prevented from creating Azure SQL servers anywhere in Subscription1. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You are prevented from creating Azure SQL servers anywhere in Subscription 1 with the exception of ContosoRG1 QUESTION 14 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Subscription1, you assign the DevTest Labs User role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: DevTest Labs User role only lets you connect, start, restart, and shutdown virtual machines in your Azure DevTest Labs. The Logic App Contributor role lets you manage logic app, but not access to them. It provides access to view, edit, and update a logic app. References: https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 15 You have an Azure Active Directory (Azure AD) tenant that contains 5,000 user accounts. You create a new user account named AdminUser1. You need to assign the User administrator administrative role to AdminUser1. What should you do from the user account properties? A. From the Directory role blade, modify the directory role. B. From the Licenses blade, assign a new license. C. From the Groups blade, invite the user account to a new group. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Assign a role to a user 1. Sign in to the Azure portal with an account that's a global admin or privileged role admin for the directory. 2. Select Azure Active Directory, select Users, and then select a specific user from the list. 3. For the selected user, select Directory role, select Add role, and then pick the appropriate admin roles from the Directory roles list, such as Conditional access administrator. 4. Press Select to save. References: https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-users-assign-role- azure-portal QUESTION 16 HOTSPOT You have an Azure subscription named Subscription1. You plan to deploy an Ubuntu Server virtual machine named VM1 to Subscription1. You need to perform a custom deployment of the virtual machine. A specific trusted root certification authority (CA) must be added during the deployment. What should you do? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Cloud-init.txt Cloud-init.txt is used to customize a Linux VM on first boot up. It can be used to install packages and write files, or to configure users and security. No additional steps or agents are required to apply your configuration. Box 2: The az vm create command Once Cloud-init.txt has been created, you can deploy the VM with az vm create cmdlet, sing the --custom- data parameter to provide the full path to the cloud-init.txt file. References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/tutorial-automate-vm-deployment QUESTION 17 You have an Azure subscription named Subscription1. In Subscription1, you create an alert rule named Alert1. The Alert1 action group is configured as shown in the following exhibit. Alert1 alert criteria is triggered every minute. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: 60 One alert per minute will trigger one email per minute. Box 2: 12 No more than 1 SMS every 5 minutes can be send, which equals 12 per hour. Note: Rate limiting is a suspension of notifications that occurs when too many are sent to a particular phone number, email address or device. Rate limiting ensures that alerts are manageable and actionable. The rate limit thresholds are: SMS: No more than 1 SMS every 5 minutes. Voice: No more than 1 Voice call every 5 minutes. Email: No more than 100 emails in an hour. Other actions are not rate limited. References: https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/monitoring-and-diagnostics/monitoring- overview-alerts.md Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 You need to resolve the licensing issue before you attempt to assign the license again. What should you do? A. From the Groups blade, invite the user accounts to a new group. B. From the Profile blade, modify the usage location. C. From the Directory role blade, modify the directory role. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: License cannot be assigned to a user without a usage location specified. Scenario: Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License agreement failed for one user.\" You verify that the Azure subscription has the available licenses. QUESTION 2 You need to resolve the Active Directory issue. What should you do? A. Run the IdFix tool then use the Update actions. B. From Active Directory Domains and Trusts, modify the list of UPN suffixes. C. From Azure AD Connect, modify the outbound synchronization rule. D. From Active Directory Users and Computers, select the user accounts and then modify the UPN suffix value. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: IdFix is used to perform discovery and remediation of identity objects and their attributes in an on-premises Active Directory environment in preparation for migration to Azure Active Directory. IdFix is intended for the Active Directory administrators responsible for directory synchronization with Azure Active Directory. Scenario: Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. References: https://www.microsoft.com/en-us/download/details.aspx?id=36832 QUESTION 3 You need to define a custom domain name for Azure AD to support the planned infrastructure. Which domain name should you use? A. ad.humongousinsurance.com B. humingousinsurance.onmicrosoft.com C. humongousinsurance.com D. humongousinsurance.local Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: Every Azure AD directory comes with an initial domain name in the form of domainname.onmicrosoft.com. The initial domain name cannot be changed or deleted, but you can add your corporate domain name to Azure AD as well. For example, your organization probably has other domain names used to do business and users who sign in using your corporate domain name. Adding custom domain names to Azure AD allows you to assign user names in the directory that are familiar to your users, such as \u2018alice@contoso.com.\u2019 instead of 'alice@domain name.onmicrosoft.com'. Scenario: Network Infrastructure: Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com Planned Azure AD Infrastructure: The on-premises Active Directory domain will be synchronized to Azure AD. References: https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/add-custom-domain Question Set 1 QUESTION 1 You plan to use the Azure Import/Export service to copy files to a storage account. Which two files should you create before you prepare the drives for the import job? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. a driveset CSV file B. a JSON configuration file C. a PowerShell PS1 file D. an XML manifest file E. a dataset CSV file Correct Answer: AE Section: [none] Explanation Explanation/Reference: Explanation: A: Modify the driveset.csv file in the root folder where the tool resides. E: Modify the dataset.csv file in the root folder where the tool resides. Depending on whether you want to import a file or folder or both, add entries in the dataset.csv file References: https://docs.microsoft.com/en-us/azure/storage/common/storage-import-export-data-to-files QUESTION 2 DRAG DROP You have an on-premises file server named Server1 that runs Windows Server 2016. You have an Azure subscription that contains an Azure file share. You deploy an Azure File Sync Storage Sync Service, and you create a sync group. You need to synchronize files from Server1 to Azure. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Install the Azure File Sync agent on Server1 The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share Step 2: Register Server1. Register Windows Server with Storage Sync Service Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server (or cluster) and the Storage Sync Service. Step 3: Add a server endpoint Create a sync group and a cloud endpoint. A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud endpoint, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-sync-files-deployment-guide QUESTION 3 You create an Azure Storage account named contosostorage. You plan to create a file share named data. Users need to map a drive to the data file share from home computers that run Windows 10. Which outbound port should you open between the home computers and the data file share? A. 80 B. 443 C. 445 D. 3389 Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: Ensure port 445 is open: The SMB protocol requires TCP port 445 to be open; connections will fail if port 445 is blocked. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows QUESTION 4 HOTSPOT You have several Azure virtual machines on a virtual network named VNet1. You configure an Azure Storage account as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: always Endpoint status is enabled. Box 2: Never After you configure firewall and virtual network settings for your storage account, select Allow trusted Microsoft services to access this storage account as an exception to enable Azure Backup service to access the network restricted storage account. Reference: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows https://azure.microsoft.com/en-us/blog/azure-backup-now-supports-storage-accounts-secured-with-azure- storage-firewalls-and-virtual-networks/ QUESTION 5 HOTSPOT You have an Azure subscription named Subscription1 that contains the resources shown in the following table. The status of VM1 is Running. You assign an Azure policy as shown in the exhibit. (Click the Exhibit tab.) You assign the policy by using the following parameters: For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 6 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 2 You plan to store media files in the rg1lod9172796 storage account. You need to configure the storage account to store the media files. The solution must ensure that only users who have access keys can download the media files and that the files are accessible only over HTTPS. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: We should create an Azure file share. Step 1: In the Azure portal, select All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. On the Storage Accounts window that appears. Step 2: Locate the rg1lod9172796 storage account. Step 3: On the storage account page, in the Services section, select Files. Step 4: On the menu at the top of the File service page, click + File share. The New file share page drops down. Step 5: In Name type myshare. Click OK to create the Azure file share. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-portal QUESTION 7 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 1 You plan to migrate a large amount of corporate data to Azure Storage and to back up files stored on old hardware to Azure Storage. You need to create a storage account named corpdata9172795n1 in the corpdatalod9172795 resource group. The solution must meet the following requirements: Corpdata9172795n1 must be able to host the virtual disk files for Azure virtual machines. The cost of accessing the files must be minimized. Replication costs must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: In the Azure portal, click All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. Step 2: On the Storage Accounts window that appears, choose Add. Step 3: Select the subscription in which to create the storage account. Step 4: Under the Resource group field, select corpdatalod9172795. Step 5: Enter a name for your storage account: corpdata9172795n1 Step 6: For Account kind select: General-purpose v2 accounts (recommended for most scenarios) General-purpose v2 accounts is recommended for most scenarios. . General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. Step 7: For replication select: Read-access geo-redundant storage (RA-GRS) Read-access geo-redundant storage (RA-GRS) maximizes availability for your storage account. RA-GRS provides read-only access to the data in the secondary location, in addition to geo-replication across two regions. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview QUESTION 8 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 2 You plan to move backup files and documents from an on-premises Windows file server to Azure Storage. The backup files will be stored as blobs. You need to create a storage account named corpdata9172795n2. The solution must meet the following requirements: Ensure that the documents are accessible via drive mappings from Azure virtual machines that run Windows Server 2016. Provide the highest possible redundancy for the documents. Minimize storage access costs. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: In the Azure portal, click All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. Step 2: On the Storage Accounts window that appears, choose Add. Step 3: Select the subscription in which to create the storage account. Step 4: Under the Resource group field, select Create New. Create a new Resource Step 5: Enter a name for your storage account: corpdata9172795n2 Step 6: For Account kind select: General-purpose v2 accounts (recommended for most scenarios) General-purpose v2 accounts is recommended for most scenarios. General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. Step 7: For replication select: Read-access geo-redundant storage (RA-GRS) Read-access geo-redundant storage (RA-GRS) maximizes availability for your storage account. RA-GRS provides read-only access to the data in the secondary location, in addition to geo-replication across two regions. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview QUESTION 9 You have an Azure subscription that contains the resources in the following table. Store1 contains a file share named Data. Data contains 5,000 files. You need to synchronize the files in Data to an on-premises server named Server1. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Download an automation script. B. Register Server1. C. Create a sync group. D. Create a container instance. E. Install the Azure File Sync agent on Server1. Correct Answer: BCE Section: [none] Explanation Explanation/Reference: Explanation: Step 1 (E): Install the Azure File Sync agent on Server1 The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share Step 2 (B): Register Server1. Register Windows Server with Storage Sync Service Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server (or cluster) and the Storage Sync Service. Step 3 (C): Create a sync group and a cloud endpoint. A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud endpoint, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-sync-files-deployment-guide QUESTION 10 DRAG DROP You have an Azure subscription named Subscription1. You create an Azure Storage account named contosostorage, and then you create a file share named data. Which UNC path should you include in a script that references files from the data file share? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: contosostorage The name of account Box 2: file.core.windows.net Box 3: data The name of the file share is data. Example: References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows QUESTION 11 DRAG DROP You have an Azure subscription that contains a storage account. You have an on-premises server named Server1 that runs Windows Server 2016. Server1 has 2 TB of data. You need to transfer the data to the storage account by using the Azure Import/Export service. In which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order. NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: At a high level, an import job involves the following steps: Step 1: Attach an external disk to Server1 and then run waimportexport.exe Determine data to be imported, number of drives you need, destination blob location for your data in Azure storage. Use the WAImportExport tool to copy data to disk drives. Encrypt the disk drives with BitLocker. Step 2: From the Azure portal, create an import job. Create an import job in your target storage account in Azure portal. Upload the drive journal files. Step 3: Detach the external disks from Server1 and ship the disks to an Azure data center. Provide the return address and carrier account number for shipping the drives back to you. Ship the disk drives to the shipping address provided during job creation. Step 4: From the Azure portal, update the import job Update the delivery tracking number in the import job details and submit the import job. The drives are received and processed at the Azure data center. The drives are shipped using your carrier account to the return address provided in the import job. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-import-export-service QUESTION 12 You have the Azure virtual machines shown in the following table. You have a Recovery Services vault that protects VM1 and VM2. You need to protect VM3 and VM4 by using Recovery Services. What should you do first? A. Create a new backup policy. B. Configure the extensions for VM3 and VM4. C. Create a storage account. D. Create a new Recovery Services vault. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: A Recovery Services vault is a storage entity in Azure that houses data. The data is typically copies of data, or configuration information for virtual machines (VMs), workloads, servers, or workstations. You can use Recovery Services vaults to hold backup data for various Azure services References: https://docs.microsoft.com/en-us/azure/site-recovery/azure-to-azure-tutorial-enable-replication QUESTION 13 HOTSPOT You have an Azure subscription named Subscription1 that is associated to an Azure Active Directory (Azure AD) tenant named AAD1. Subscription1 contains the objects in the following table. You plan to create a single backup policy for Vault1. To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Share1 only Box 2: 99 years With the latest update to Azure Backup, customers can retain their data for up to 99 years in Azure. Note: A backup policy defines a matrix of when the data snapshots are taken, and how long those snapshots are retained. The backup policy interface looks like this: References: https://docs.microsoft.com/en-us/azure/backup/backup-azure-files https://docs.microsoft.com/en-us/azure/backup/backup-azure-vms-first-look-arm#defining-a-backup-policy https://blogs.microsoft.com/firehose/2015/02/16/february-update-to-azure-backup-includes-data-retention- up-to-99-years-offline-backup-and-more/ QUESTION 14 HOTSPOT You have an Azure subscription named Subscription1. In Subscription1, you create an Azure file share named share1. You create a shared access signature (SAS) named SAS1 as shown in the following exhibit. To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Will have no access The IP 193.77.134.1 does not have access on the SAS. Box 2: Will have read, write, and list access The net use command is used to connect to file shares. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1 https://docs.microsoft.com/en-us/azure/vs-azure-tools-storage-manage-with-storage-explorer? tabs=windows QUESTION 15 HOTSPOT You have Azure Storage accounts as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: storageaccount1 and storageaccount2 only Box 2: All the storage accounts Note: The three different storage account options are: General-purpose v2 (GPv2) accounts, General- purpose v1 (GPv1) accounts, and Blob storage accounts. General-purpose v2 (GPv2) accounts are storage accounts that support all of the latest features for blobs, files, queues, and tables. Blob storage accounts support all the same block blob features as GPv2, but are limited to supporting only block blobs. General-purpose v1 (GPv1) accounts provide access to all Azure Storage services, but may not have the latest features or the lowest per gigabyte pricing. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-account-options QUESTION 16 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 1 You plan to prevent users from accidentally deleting blob data from Azure. You need to ensure that administrators can recover any blob data that is deleted accidentally from the storagelod9272261 storage account for 14 days after the deletion occurred. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Task A: Create a Recovery Services vault (if a vault already exists skip this task, go to Task B below) A1. From Azure Portal, On the Hub menu, click All services and in the list of resources, type Recovery Services and click Recovery Services vaults. If there are recovery services vaults in the subscription, the vaults are listed. A2. On the Recovery Services vaults menu, click Add. A3. The Recovery Services vault blade opens, prompting you to provide a Name, Subscription, Resource group, and Location Task B. Create a backup goal B1. On the Recovery Services vault blade (for the vault you just created), in the Getting Started section, click Backup, then on the Getting Started with Backup blade, select Backup goal. The Backup Goal blade opens. If the Recovery Services vault has been previously configured, then the Backup Goal blades opens when you click Backup on the Recovery Services vault blade. B2. From the Where is your workload running? drop-down menu, select Azure. B3. From the What do you want to backup? menu, select Blob Storage, and click OK. B4. Finish the Wizard. Task C. create a backup schedule C1. Open the Microsoft Azure Backup agent. You can find it by searching your machine for Microsoft Azure Backup. C2. In the Backup agent's Actions pane, click Schedule Backup to launch the Schedule Backup Wizard. C3. On the Getting started page of the Schedule Backup Wizard, click Next. C4. On the Select Items to Backup page, click Add Items. The Select Items dialog opens. C5. Select Blob Storage you want to protect, and then click OK. C6. In the Select Items to Backup page, click Next. On the Specify Backup Schedule page, specify Schedule a backup every day, and click Next. C7. On the Select Retention Policy page, set it to 14 days, and click Next. C8. Finish the Wizard. References: https://docs.microsoft.com/en-us/azure/backup/backup-configure-vault QUESTION 17 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 2 Your company plans to store several documents on a public website. You need to create a container named bios that will host the documents in the storagelod9272261 storage account. The solution must ensure anonymous access and must ensure that users can browse folders in the container. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Azure portal create public container To create a container in the Azure portal, follow these steps: Step 1: Navigate to your new storage account in the Azure portal. Step 2: In the left menu for the storage account, scroll to the lob service section, then select Blobs. Select the + Container button. Type a name for your new container: bios Set the level of public access to the container: Select anonymous access. Step 3: Select OK to create the container. References: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal QUESTION 18 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 3 Your company plans to host in Azure the source files of several line-of-business applications. You need to create an Azure file share named corpsoftware in the storagelod9272261 storage account. The solution must ensure that corpsoftware can store only up to 250 GB of data. What should you do from the Azure portal? Correct Answer: See explanation below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Go to the Storage Account blade on the Azure portal: Step 2: Click on add File Share button: Step 3: Provide Name (storagelod9272261) and Quota (250 GB). References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-create-file-share QUESTION 19 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 6 You recently created a virtual machine named Web01. You need to attach a new 80-GB standard data disk named Web01-Disk1 to Web01. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Add a data disk Step 1: In the Azure portal, from the menu on the left, select Virtual machines. Step 2: Select the Web01 virtual machine from the list. Step 3: On the Virtual machine page, , in Essentials, select Disks. Step 4: On the Disks page, select the Web01-Disk1 from the list of existing disks. Step 5: In the Disks pane, click + Add data disk. Step 6: Click the drop-down menu for Name to view a list of existing managed disks accessible to your Azure subscription. Select the managed disk Web01-Disk1 to attach: References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/attach-disk-portal Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a manufacturing company that has offices worldwide. Contoso works with partner organizations to bring products to market. Contoso products are manufactured by using blueprint files that the company authors and maintains. Existing Environment Currently, Contoso uses multiple types of servers for business operations, including the following: File servers Domain controllers Microsoft SQL Server servers Your network contains an Active Directory forest named contoso.com. All servers and client computers are joined to Active Directory. You have a public-facing application named App1. App1 is comprised of the following three tiers: A SQL database A web front end A processing middle tier Each tier is comprised of five virtual machines. Users access the web front end by using HTTPS only. Requirements Planned Changes Contoso plans to implement the following changes to the infrastructure: Move all the tiers of App1 to Azure. Move the existing product blueprint files to Azure Blob storage. Create a hybrid directory to support an upcoming Microsoft Office 365 migration project. Technical Requirements Contoso must meet the following technical requirements: Move all the virtual machines for App1 to Azure. Minimize the number of open ports between the App1 tiers. Ensure that all the virtual machines for App1 are protected by backups. Copy the blueprint files to Azure over the Internet. Ensure that the blueprint files are stored in the archive storage tier. Ensure that partner access to the blueprint files is secured and temporary. Prevent user passwords or hashes of passwprds from being stored in Azure. Use unmanaged standard storage for the hard disks of the virtual machines. Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity Minimize administrative effort whenever possible User Requirements Contoso identifies the following requirements for users: Ensure that only users who are part of a group named Pilot can join devices to Azure AD. Designate a new user named Admin1 as the service admin for the Azure subscription. Admin1 must receive email alerts regarding service outages. Ensure that a new user named User3 can create network objects for the Azure subscription. QUESTION 1 HOTSPOT You need to identify the storage requirements for Contoso. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes Contoso is moving the existing product blueprint files to Azure Blob storage. Use unmanaged standard storage for the hard disks of the virtual machines. We use Page Blobs for these. Box 2: No Box 3: No QUESTION 2 You need to move the blueprint files to Azure. What should you do? A. Use Azure Storage Explorer to copy the files. B. Use the Azure Import/Export service. C. Generate a shared access signature (SAS). Map a drive, and then copy the files by using File Explorer. D. Generate an access key. Map a drive, and then copy the files by using File Explorer. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Azure Storage Explorer is a free tool from Microsoft that allows you to work with Azure Storage data on Windows, macOS, and Linux. You can use it to upload and download data from Azure blob storage. Scenario: Planned Changes include: move the existing product blueprint files to Azure Blob storage. Technical Requirements include: Copy the blueprint files to Azure over the Internet. References: https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-data-to-azure- blob-using-azure-storage-explorer QUESTION 3 You need to implement a backup solution for App1 after the application is moved. What should you create first? A. a recovery plan B. a Recovery Services vault C. an Azure Backup Server D. a backup policy Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: A Recovery Services vault is a logical container that stores the backup data for each protected resource, such as Azure VMs. When the backup job for a protected resource runs, it creates a recovery point inside the Recovery Services vault. Scenario: There are three application tiers, each with five virtual machines. Move all the virtual machines for App1 to Azure. Ensure that all the virtual machines for App1 are protected by backups. References: https://docs.microsoft.com/en-us/azure/backup/quick-backup-vm-portal Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 DRAG DROP You need to prepare the environment to ensure that the web administrators can deploy the web apps as quickly as possible. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: First you create a storage account using the Azure portal. Step 2: Select Automation options at the bottom of the screen. The portal shows the template on the Template tab. Add the storage account to the library. Step 3: Share the template. Scenario: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. References: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-quickstart-create- templates-use-the-portal Question Set 1 QUESTION 1 You plan to automate the deployment of a virtual machine scale set that uses the Windows Server 2016 Datacenter image. You need to ensure that when the scale set virtual machines are provisioned, they have web server components installed. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Modify the extensionProfile section of the Azure Resource Manager template. B. Create an automation account. C. Upload a configuration script. D. Create a new virtual machine scale set in the Azure portal. E. Create an Azure policy. Correct Answer: AD Section: [none] Explanation Explanation/Reference: Explanation: Virtual Machine Scale Sets can be used with the Azure Desired State Configuration (DSC) extension handler. Virtual machine scale sets provide a way to deploy and manage large numbers of virtual machines, and can elastically scale in and out in response to load. DSC is used to configure the VMs as they come online so they are running the production software. References: https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-dsc QUESTION 2 DRAG DROP You have two Azure virtual machines named VM1 and VM2. VM1 has a single data disk named Disk1. You need to attach Disk1 to VM2. The solution must minimize downtime for both virtual machines. Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Stop VM1. Step 2: Detach Disk1 from VM1. Step 3: Attach Disk1 to VM2 Attach an existing disk Follow these steps to reattach an existing available data disk to a running VM. 1. Select a running VM for which you want to reattach a data disk. 2. From the menu on the left, select Disks. 3. Select Attach existing to attach an available data disk to the VM. 4. From the Attach existing disk pane, select OK. Step 4: Start VM1. Detach a data disk using the portal 1. In the left menu, select Virtual Machines. 2. Select the virtual machine that has the data disk you want to detach and click Stop to deallocate the VM. 3. In the virtual machine pane, select Disks. 4. At the top of the Disks pane, select Edit. 5. In the Disks pane, to the far right of the data disk that you would like to detach, click the Detach button image detach button. 6. After the disk has been removed, click Save on the top of the pane. 7. In the virtual machine pane, click Overview and then click the Start button at the top of the pane to restart the VM. 8. The disk stays in storage but is no longer attached to a virtual machine. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/detach-disk https://docs.microsoft.com/en-us/azure/lab-services/devtest-lab-attach-detach-data-disk QUESTION 3 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains a virtual machine named VM1. You install and configure a web server and a DNS server on VM1. VM1 has the effective network security rules shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Rule2 blocks ports 50-60, which includes port 53, the DNS port. Internet users can reach the Web server, since it uses port 80. Box 2: If Rule2 is removed internet users can reach the DNS server as well. Note: Rules are processed in priority order, with lower numbers processed before higher numbers, because lower numbers have higher priority. Once traffic matches a rule, processing stops. As a result, any rules that exist with lower priorities (higher numbers) that have the same attributes as rules with higher priorities are not processed. References: https://docs.microsoft.com/en-us/azure/virtual-network/security-overview QUESTION 4 DRAG DROP You have an Azure Linux virtual machine that is protected by Azure Backup. One week ago, two files were deleted from the virtual machine. You need to restore the deleted files to an on-premises computer as quickly as possible. Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: To restore files or folders from the recovery point, go to the virtual machine and choose the desired recovery point. Step 0. In the virtual machine's menu, click Backup to open the Backup dashboard. Step 1. In the Backup dashboard menu, click File Recovery. Step 2. From the Select recovery point drop-down menu, select the recovery point that holds the files you want. By default, the latest recovery point is already selected. Step 3: To download the software used to copy files from the recovery point, click Download Executable (for Windows Azure VM) or Download Script (for Linux Azure VM, a python script is generated). Step 4: Copy the files by using AzCopy AzCopy is a command-line utility designed for copying data to/from Microsoft Azure Blob, File, and Table storage, using simple commands designed for optimal performance. You can copy data between a file system and a storage account, or between storage accounts. References: https://docs.microsoft.com/en-us/azure/backup/backup-azure-restore-files-from-vm https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy QUESTION 5 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 3 You plan to protect on-premises virtual machines and Azure virtual machines by using Azure Backup. You need to prepare the backup infrastructure in Azure. The solution must minimize the cost of storing the backups in Azure. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: First, create Recovery Services vault. Step 1: On the left-hand menu, select All services and in the services list, type Recovery Services. As you type, the list of resources filters. When you see Recovery Services vaults in the list, select it to open the Recovery Services vaults menu. Step 2: In the Recovery Services vaults menu, click Add to open the Recovery Services vault menu. Step 3: In the Recovery Services vault menu, for example, Type myRecoveryServicesVault in Name. The current subscription ID appears in Subscription. If you have additional subscriptions, you could choose another subscription for the new vault. For Resource group select Use existing and choose myResourceGroup. If myResourceGroup doesn't exist, select Create new and type myResourceGroup. From the Location drop-down menu, choose West Europe. Click Create to create your Recovery Services vault. References: https://docs.microsoft.com/en-us/azure/backup/tutorial-backup-vm-at-scale QUESTION 6 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 3 You need to deploy two Azure virtual machines named VM1003a and VM1003b based on an Ubuntu Server image. The deployment must meet the following requirements: Provide a Service Level Agreement (SLA) of 99.95 percent availability. Use managed disks. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Open the Azure portal. Step 2: On the left menu, select All resources. You can sort the resources by Type to easily find your images. Step 3: Select the image you want to use from the list. The image Overview page opens. Step 4: Select Create VM from the menu. Step 5: Enter the virtual machine information. Select VM1003a as the name for the first Virtual machine. The user name and password entered here will be used to log in to the virtual machine. When complete, select OK. You can create the new VM in an existing resource group, or choose Create new to create a new resource group to store the VM. Step 6: Select a size for the VM. To see more sizes, select View all or change the Supported disk type filter. Step 7: Under Settings, make changes as necessary and select OK. Step 8: On the summary page, you should see your image name listed as a Private image. Select Ok to start the virtual machine deployment. Repeat the procedure for the second VM and name it VM1003b. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/create-vm-generalized-managed QUESTION 7 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 4 You need to deploy an Azure virtual machine named VM1004a based on an Ubuntu Server image, and then configure VM1004a to meet the following requirements: The virtual machine must contain data disks that can store at least 15 TB of data. The data disks must be able to provide at least 2.000 IOPS. Storage costs must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Open the Azure portal. Step 2: On the left menu, select All resources. You can sort the resources by Type to easily find your images. Step 3: Select the image you want to use from the list. The image Overview page opens. Step 4: Select Create VM from the menu. Step 5: Enter the virtual machine information. Select VM1004a as the name for the first Virtual machine. The user name and password entered here will be used to log in to the virtual machine. When complete, select OK. You can create the new VM in an existing resource group, or choose Create new to create a new resource group to store the VM. Step 6: Select a size for the VM. To see more sizes, select View all or change the Supported disk type filter. To support 15 TB of data you would need a Premium disk. Step 7: Under Settings, make changes as necessary and select OK. Step 8: On the summary page, you should see your image name listed as a Private image. Select Ok to start the virtual machine deployment. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/create-vm-generalized-managed QUESTION 8 You have an Azure subscription that contains a virtual machine named VM1. VM1 hosts a line-of-business application that is available 24 hours a day. VM1 has one network interface and one managed disk. VM1 uses the D4s v3 size. You plan to make the following changes to VM1: Change the size to D8s v3. Add a 500-GB managed disk. Add the Puppet Agent extension. Attach an additional network interface. Which change will cause downtime for VM1? A. Add the Puppet Agent extension. B. Change the size to D8s v3. C. Add a 500-GB managed disk. D. Attach an additional network interface. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: While resizing the VM it must be in a stopped state. References: https://azure.microsoft.com/en-us/blog/resize-virtual-machines/ QUESTION 9 You have an Azure virtual machine named VM1 that you use for testing. VM1 is protected by Azure Backup. You delete VM1. You need to remove the backup data stored for VM1. What should you do first? A. Delete the Recovery Services vault. B. Delete the storage account. C. Stop the backup D. Modify the backup policy. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Azure Backup provides backup for virtual machines \u2014 created through both the classic deployment model and the Azure Resource Manager deployment model \u2014 by using custom-defined backup policies in a Recovery Services vault. With the release of backup policy management, customers can manage backup policies and model them to meet their changing requirements from a single window. Customers can edit a policy, associate more virtual machines to a policy, and delete unnecessary policies to meet their compliance requirements. Incorrect Answers: B: You can't delete a Recovery Services vault if it is registered to a server and holds backup data. If you try to delete a vault, but can't, the vault is still configured to receive backup data. References: https://azure.microsoft.com/en-in/updates/azure-vm-backup-policy-management/ QUESTION 10 You have an Azure subscription named Subscription1. You deploy a Linux virtual machine named VM1 to Subscription1. You need to monitor the metrics and the logs of VM1. What should you use? A. the AzurePerformanceDiagnostics extension B. Azure HDInsight C. Linux Diagnostic Extension (LAD) 3.0 D. Azure Analysis Services Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: You can use extensions to configure diagnostics on your VMs to collect additional metric data. The basic host metrics are available, but to see more granular and VM-specific metrics, you need to install the Azure diagnostics extension on the VM. The Azure diagnostics extension allows additional monitoring and diagnostics data to be retrieved from the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/tutorial-monitoring QUESTION 11 DRAG DROP You have an availability set named AS1 that contains three virtual machines named VM1, VM2, and VM3. You attempt to reconfigure VM1 to use a larger size. The operation fails and you receive an allocation failure message. You need to ensure that the resize operation succeeds. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Stop VM1, VM, and VM3. If the VM you wish to resize is part of an availability set, then you must stop all VMs in the availability set before changing the size of any VM in the availability set. The reason all VMs in the availability set must be stopped before performing the resize operation to a size that requires different hardware is that all running VMs in the availability set must be using the same physical hardware cluster. Therefore, if a change of physical hardware cluster is required to change the VM size then all VMs must be first stopped and then restarted one-by-one to a different physical hardware clusters. Step 2: Resize VM1. Step 3: Start VM1, VM2, and VM3. References: https://azure.microsoft.com/es-es/blog/resize-virtual-machines/ QUESTION 12 You plan to back up an Azure virtual machine named VM1. You discover that the Backup Pre-Check status displays a status of Warning. What is a possible cause of the Warning status? A. VM1 is stopped. B. VM1 does not have the latest version of WaAppAgent.exe installed. C. VM1 has an unmanaged disk. D. A Recovery Services vault is unavailable. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Warning state indicates one or more issues in VM\u2019s configuration that might lead to backup failures and provides recommended steps to ensure successful backups. Not having the latest VM Agent installed, for example, can cause backups to fail intermittently and falls in this class of issues. References: https://azure.microsoft.com/en-us/blog/azure-vm-backup-pre-checks/ QUESTION 13 You have an Azure subscription named Subscription1 that is used by several departments at your company. Subscription1 contains the resources in the following table. Another administrator deploys a virtual machine named VM1 and an Azure Storage account named Storage2 by using a single Azure Resource Manager template. You need to view the template used for the deployment. From which blade can you view the template that was used for the deployment? A. Container1 B. RG1 C. VM1 D. Storage2 Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: View template from deployment history 1. Go to the resource group for your new resource group. Notice that the portal shows the result of the last deployment. Select this link. 2. You see a history of deployments for the group. In your case, the portal probably lists only one deployment. Select this deployment. 3. The portal displays a summary of the deployment. The summary includes the status of the deployment and its operations and the values that you provided for parameters. To see the template that you used for the deployment, select View template. References: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-export-template QUESTION 14 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Overview blade, you move the virtual machine to a different subscription. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You would need to redeploy the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 15 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Redeploy blade, you click Redeploy. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: When you redeploy a VM, it moves the VM to a new node within the Azure infrastructure and then powers it back on, retaining all your configuration options and associated resources. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 16 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Update management blade, you click Enable. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You would need to redeploy the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 17 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains two Azure virtual machines named VM1 and VM2. VM1 and VM2 run Windows Server 2016. VM1 is backed up daily by Azure Backup without using the Azure Backup agent. VM1 is affected by ransomware that encrypts data. You need to restore the latest backup of VM1. To which location can you restore the backup? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 18 You download an Azure Resource Manager template based on an existing virtual machine. The template will be used to deploy 100 virtual machines. You need to modify the template to reference an administrative password. You must prevent the password from being stored in plain text. What should you create to store the password? A. an Azure Key Vault and an access policy B. a Recovery Services vault and a backup policy C. Azure Active Directory (AD) Identity Protection and an Azure policy D. an Azure Storage account and an access policy Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: You can use a template that allows you to deploy a simple Windows VM by retrieving the password that is stored in a Key Vault. Therefore, the password is never put in plain text in the template parameter file. References: https://azure.microsoft.com/en-us/resources/templates/101-vm-secure-password/ QUESTION 19 HOTSPOT You create a virtual machine scale set named Scale1. Scale1 is configured as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: The Autoscale scale out rule increases the number of VMs by 2 if the CPU threshold is 80% or higher. The initial instance count is 4 and rises to 6 when the 2 extra instances of VMs are added. Box 2: The Autoscale scale in rule decreases the number of VMs by 4 if the CPU threshold is 30% or lower. The initial instance count is 4 and thus cannot be reduced to 0 as the minimum instances is set to 2. Instances are only added when the CPU threshold reaches 80%. References: https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-overview https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-best-practices https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-common-scale-patterns QUESTION 20 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 4 You plan to back up all the Azure virtual machines in your Azure subscription at 02:00 Coordinated Universal Time (UTC) daily. You need to prepare the Azure environment to ensure that any new virtual machines can be configured quickly for backup. The solution must ensure that all the daily backups performed at 02:00 UTC are stored for only 90 days. What should you do from your Recovery Services vault on the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Task A: Create a Recovery Services vault (if a vault already exists skip this task, go to Task B below) A1. From Azure Portal, On the Hub menu, click All services and in the list of resources, type Recovery Services and click Recovery Services vaults. If there are recovery services vaults in the subscription, the vaults are listed. A2. On the Recovery Services vaults menu, click Add. A3. The Recovery Services vault blade opens, prompting you to provide a Name, Subscription, Resource group, and Location Task B. B1. On the Recovery Services vault blade (for the vault you just created), in the Getting Started section, click Backup, then on the Getting Started with Backup blade, select Backup goal. The Backup Goal blade opens. If the Recovery Services vault has been previously configured, then the Backup Goal blades opens when you click Backup on the Recovery Services vault blade. B2. From the Where is your workload running? drop-down menu, select Azure. B3. From the What do you want to backup? menu, select Virtual Machine, and click OK. B4. Finish the Wizard. Task C. create a backup schedule C1. Open the Microsoft Azure Backup agent. You can find it by searching your machine for Microsoft Azure Backup. C2. In the Backup agent's Actions pane, click Schedule Backup to launch the Schedule Backup Wizard. C3. On the Getting started page of the Schedule Backup Wizard, click Next. C4. On the Select Items to Backup page, click Add Items. The Select Items dialog opens. C5. Select Blob Storage you want to protect, and then click OK. C6. In the Select Items to Backup page, click Next. On the Specify Backup Schedule page, specify Schedule a backup every: day At the following times: 2.00 AM C7. On the Select Retention Policy page, set it to 90 days, and click Next. C8. Finish the Wizard. References: https://docs.microsoft.com/en-us/azure/backup/backup-configure-vault QUESTION 21 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task5 You plan to connect several virtual machines to the VNET01-USEA2 virtual network. In the Web-RGlod9272261 resource group, you need to create a virtual machine that uses the Standard_B2ms size named Web01 that runs Windows Server 2016. Web01 must be added to an availability set. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Choose Create a resource in the upper left-hand corner of the Azure portal. Step 2: In the Basics tab, under Project details, make sure the correct subscription is selected and then choose Web-RGlod9272261 resource group Step 3: Under Instance details type/select: Virtual machine name: Web01 Image: Windows Server 2016 Size: Standard_B2ms size Leave the other defaults. Step 4: Finish the Wizard Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a consulting company that has a main office in Montreal and two branch offices in Seattle and New York. The Montreal office has 2,000 employees. The Seattle office has 1,000 employees. The New York office has 200 employees. All the resources used by Contoso are hosted on-premises. Contoso creates a new Azure subscription. The Azure Active Directory (Azure AD) tenant uses a domain named contoso.onmicrosoft.com. The tenant uses the P1 pricing tier. Existing Environment The network contains an Active Directory forest named contoso.com. All domain controllers are configured as DNS servers and host the contoso.com DNS zone. Contoso has finance, human resources, sales, research, and information technology departments. Each department has an organizational unit (OU) that contains all the accounts of that respective department. All the user accounts have the department attribute set to their respective department. New users are added frequently. Contoso.com contains a user named User1. All the offices connect by using private links. Contoso has data centers in the Montreal and Seattle offices. Each data center has a firewall that can be configured as a VPN device. All infrastructure servers are virtualized. The virtualization environment contains the servers in the following table. Contoso uses two web applications named App1 and App2. Each instance on each web application requires 1GB of memory. The Azure subscription contains the resources in the following table. The network security team implements several network security groups (NSGs). Planned Changes Contoso plans to implement the following changes: Deploy Azure ExpressRoute to the Montreal office. Migrate the virtual machines hosted on Server1 and Server2 to Azure. Synchronize on-premises Active Directory to Azure Active Directory (Azure AD). Migrate App1 and App2 to two Azure web apps named WebApp1 and WebApp2. Technical requirements Contoso must meet the following technical requirements: Ensure that WebApp1 can adjust the number of instances automatically based on the load and can scale up to five instances. Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. Ensure that routing information is exchanged automatically between Azure and the routers in the Montreal office. Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. Ensure that webapp2.azurewebsites.net can be accessed by using the name app2.contoso.com Connect the New York office to VNet1 over the Internet by using an encrypted connection. Create a workflow to send an email message when the settings of VM4 are modified. Create a custom Azure role named Role1 that is based on the Reader role. Minimize costs whenever possible. QUESTION 1 You discover that VM3 does NOT meet the technical requirements. You need to verify whether the issue relates to the NSGs. What should you use? A. Diagram in VNet1 B. the security recommendations in Azure Advisor C. Diagnostic settings in Azure Monitor D. Diagnose and solve problems in Traffic Manager profiles E. IP flow verify in Azure Network Watcher Correct Answer: E Section: [none] Explanation Explanation/Reference: Explanation: Scenario: Contoso must meet technical requirements including: Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. IP flow verify checks if a packet is allowed or denied to or from a virtual machine. The information consists of direction, protocol, local IP, remote IP, local port, and remote port. If the packet is denied by a security group, the name of the rule that denied the packet is returned. While any source or destination IP can be chosen, IP flow verify helps administrators quickly diagnose connectivity issues from or to the internet and from or to the on-premises environment. References: https://docs.microsoft.com/en-us/azure/network-watcher/network-watcher-ip-flow-verify-overview Question Set 1 QUESTION 1 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You modify the name servers at the domain registrar. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Modify the Name Server (NS) record. References: https://docs.microsoft.com/en-us/azure/dns/dns-delegate-domain-azure-dns QUESTION 2 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You modify the SOA record in the contoso.com zone. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Modify the NS record, not the SOA record. Note: The SOA record stores information about the name of the server that supplied the data for the zone; the administrator of the zone; the current version of the data file; the number of seconds a secondary name server should wait before checking for updates; the number of seconds a secondary name server should wait before retrying a failed zone transfer; the maximum number of seconds that a secondary name server can use data before it must either be refreshed or expire; and a default number of seconds for the time-to- live file on resource records. References: https://searchnetworking.techtarget.com/definition/start-of-authority-record QUESTION 3 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You add an NS record to the contoso.com Azure DNS zone. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Before you can delegate your DNS zone to Azure DNS, you need to know the name servers for your zone. The NS record set contains the names of the Azure DNS name servers assigned to the zone. References: https://docs.microsoft.com/en-us/azure/dns/dns-delegate-domain-azure-dns QUESTION 4 You are troubleshooting a performance issue for an Azure Application Gateway. You need to compare the total requests to the failed requests during the past six hours. What should you use? A. NSG flow logs in Azure Network Watcher B. Metrics in Application Gateway C. Connection monitor in Azure Network Watcher D. Diagnostics logs in Application Gateway Correct Answer: B Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-diagnostics#metrics QUESTION 5 You have two subscriptions named Subscription1 and Subscription2. Each subscription is associated to a different Azure AD tenant. Subscription1 contains a virtual network named VNet1. VNet1 contains an Azure virtual machine named VM1 and has an IP address space of 10.0.0.0/16. Subscription2 contains a virtual network named VNet2. VNet2 contains an Azure virtual machine named VM2 and has an IP address space of 10.10.0.0/24. You need to connect VNet1 to VNet2. What should you do first? A. Move VM1 to Subscription2. B. Modify the IP address space of VNet2. C. Provision virtual network gateways. D. Move VNet1 to Subscription2. Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: The virtual networks can be in the same or different regions, and from the same or different subscriptions. When connecting VNets from different subscriptions, the subscriptions do not need to be associated with the same Active Directory tenant. Configuring a VNet-to-VNet connection is a good way to easily connect VNets. Connecting a virtual network to another virtual network using the VNet-to-VNet connection type (VNet2VNet) is similar to creating a Site- to-Site IPsec connection to an on-premises location. Both connectivity types use a VPN gateway to provide a secure tunnel using IPsec/IKE, and both function the same way when communicating. The local network gateway for each VNet treats the other VNet as a local site. This lets you specify additional address space for the local network gateway in order to route traffic. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager- portal QUESTION 6 DRAG DROP You have an Azure subscription that contains two virtual networks named VNet1 and VNet2. Virtual machines connect to the virtual networks. The virtual networks have the address spaces and the subnets configured as shown in the following table. You need to add the address space of 10.33.0.0/16 to VNet1. The solution must ensure that the hosts on VNet1 and VNet2 can communicate. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Remove peering between Vnet1 and VNet2. You can't add address ranges to, or delete address ranges from a virtual network's address space once a virtual network is peered with another virtual network. To add or remove address ranges, delete the peering, add or remove the address ranges, then re-create the peering. Step 2: Add the 10.44.0.0/16 address space to VNet1. Step 3: Recreate peering between VNet1 and VNet2 References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-manage-peering QUESTION 7 You have an Azure subscription that contains the resources in the following table. To which subnets can you apply NSG1? A. the subnets on VNet2 only B. the subnets on VNet2 and VNet3 only C. the subnets on VNet1, VNet2, and VNet3 D. the subnets on VNet1 only E. the subnets on VNet3 only Correct Answer: E Section: [none] Explanation Explanation/Reference: Explanation: All Azure resources are created in an Azure region and subscription. A resource can only be created in a virtual network that exists in the same region and subscription as the resource. References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-vnet-plan-design-arm QUESTION 8 HOTSPOT You have an Azure virtual machine named VM1 that connects to a virtual network named VNet1. VM1 has the following configurations: Subnet 10.0.0.0/24 Availability set: AVSet Network security group (NSG): None Private IP address: 10.0.0.4 (dynamic) Public IP address: 40.90.219.6 (dynamic) You deploy a standard, Internet-facing load balancer named slb1. You need to configure slb1 to allow connectivity to VM1. Which changes should you apply to VM1 as you configure slb1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 9 You have five Azure virtual machines that run Windows Server 2016. The virtual machines are configured as web servers. You have an Azure load balancer named LB1 that provides load balancing services for the virtual machines. You need to ensure that visitors are serviced by the same web server for each request. What should you configure? A. Protocol to UDP B. Session persistence to None C. Session persistence to Client IP D. Idle Time-out (minutes) to 20 Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: You can set the sticky session in load balancer rules with setting the session persistence as the client IP. References: https://cloudopszone.com/configure-azure-load-balancer-for-sticky-sessions/ QUESTION 10 You have the Azure virtual networks shown in the following table. To which virtual networks can you establish a peering connection from VNet1? A. VNet2 and VNet3 only B. VNet2 only C. VNet3 and VNet4 only D. VNet2, VNet3, and VNet4 Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-connect-virtual-networks-portal QUESTION 11 You have an Azure subscription that contains a policy-based virtual network gateway named GW1 and a virtual network named VNet1. You need to ensure that you can configure a point-to-site connection from VNet1 to an on-premises computer. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Reset GW1. B. Create a route-based virtual network gateway. C. Delete GW1. D. Add a public IP address space to VNet1. E. Add a connection to GW1. F. Add a service endpoint to VNet1. Correct Answer: BC Section: [none] Explanation Explanation/Reference: Explanation: B: A VPN gateway is used when creating a VPN connection to your on-premises network. Route-based VPN devices use any-to-any (wildcard) traffic selectors, and let routing/forwarding tables direct traffic to different IPsec tunnels. It is typically built on router platforms where each IPsec tunnel is modeled as a network interface or VTI (virtual tunnel interface). C: Policy-based VPN devices use the combinations of prefixes from both networks to define how traffic is encrypted/decrypted through IPsec tunnels. It is typically built on firewall devices that perform packet filtering. IPsec tunnel encryption and decryption are added to the packet filtering and processing engine. Incorrect Answers: D: Point-to-Site connections do not require a VPN device or a public-facing IP address. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/create-routebased-vpn-gateway-portal https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-connect-multiple-policybased-rm-ps QUESTION 12 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains the virtual networks in the following table. Subscription1 contains the virtual machines in the following table. The firewalls on all the virtual machines are configured to allow all ICMP traffic. You add the peerings in the following table. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes Vnet1 and Vnet3 are peers. Box 2: Yes Vnet2 and Vnet3 are peers. Box 3: No Peering connections are non-transitive. References: https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/hybrid-networking/hub-spoke QUESTION 13 You have an Azure subscription named Subscription1 that contains the resource groups shown in the following table. In RG1, you create a virtual machine named VM1 in the East Asia location. You plan to create a virtual network named VNET1. You need to create VNET1, and then connect VM1 to VNET1. What are two possible ways to achieve this goal? Each correct answer presents a complete solution. NOTE: Each correct selection is worth one point. A. Create VNET1 in RG2, and then set East Asia as the location. B. Create VNET1 in a new resource group in the West US location, and then set West US as the location. C. Create VNET1 in RG1, and then set East US as the location. D. Create VNET1 in RG2, and then set East US as the location. E. Create VNET1 in RG1, and then set East Asia as the location. Correct Answer: AE Section: [none] Explanation Explanation/Reference: QUESTION 14 You have an Azure subscription that contains a virtual network named VNet1. VNet1 contains four subnets named Gateway, Perimeter, NVA, and Production. The NVA subnet contains two network virtual appliances (NVAs) that will perform network traffic inspection between the Perimeter subnet and the Production subnet. You need to implement an Azure load balancer for the NVAs. The solution must meet the following requirements: The NVAs must run in an active-active configuration that uses automatic failover. The NVAs must load balance traffic to two services on the Profuction subnet. The services have different IP addresses. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Add two load balancing rules that have HA Ports enabled and Floating IP disabled. B. Add a frontend IP configuration, two backend pools, and a health probe. C. Add two load balancing rules that have HA Ports and Floating IP enabled. D. Deploy a standard load balancer. E. Deploy a basic load balancer. F. Add a frontend IP configuration a backend pool, and a health probe. Correct Answer: BCD Section: [none] Explanation Explanation/Reference: Explanation: A standard load balancer is required for the HA ports. Two backend pools are needed as there are two services with different IP addresses. Floating IP rule is used where backend ports are reused. Incorrect Answers: F: HA Ports are not available for the basic load balancer. References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-overview https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-multivip-overview QUESTION 15 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 1 Your on-premises network uses an IP address range of 131.107.2.0 to 131.107.2.255. You need to ensure that only device from the on-premises network can connect to the rg1lod9172796n1 storage account. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Navigate to the rg1lod9172796n1 storage account. Step 2: Click on the settings menu called Firewalls and virtual networks. Step 3: Ensure that you have elected to allow access from 'Selected networks'. Step 4: To grant access to an internet IP range, enter the address range of 131.107.2.0 to 131.107.2.255 (in CIDR format) under Firewall, Address Ranges. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security QUESTION 16 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 4 Another administrator attempts to establish connectivity between two virtual networks named VNET1 and VNET2. The administrator reports that connections across the virtual networks fail. You need to ensure that network connections can be established successfully between VNET1 and VNET2 as quickly as possible. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can connect one VNet to another VNet using either a Virtual network peering, or an Azure VPN Gateway. To create a virtual network gateway Step 1: In the portal, on the left side, click +Create a resource and type 'virtual network gateway' in search. Locate Virtual network gateway in the search return and click the entry. On the Virtual network gateway page, click Create at the bottom of the page to open the Create virtual network gateway page. Step 2: On the Create virtual network gateway page, fill in the values for your virtual network gateway. Name: Name your gateway. This is not the same as naming a gateway subnet. It's the name of the gateway object you are creating. Gateway type: Select VPN. VPN gateways use the virtual network gateway type VPN. Virtual network: Choose the virtual network to which you want to add this gateway. Click Virtual network to open the 'Choose a virtual network' page. Select the VNet. If you don't see your VNet, make sure the Location field is pointing to the region in which your virtual network is located. Gateway subnet address range: You will only see this setting if you did not previously create a gateway subnet for your virtual network. If you previously created a valid gateway subnet, this setting will not appear. Step 4: Select Create New to create a Gateway subnet. Step 5: Click Create to begin creating the VPN gateway. The settings are validated and you'll see the \"Deploying Virtual network gateway\" tile on the dashboard. Creating a gateway can take up to 45 minutes. You may need to refresh your portal page to see the completed status. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager- portal? QUESTION 17 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 5 You plan to configure VM1 to be accessible from the Internet. You need to add a public IP address to the network interface used by VM1. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can add private and public IP addresses to an Azure network interface by completing the steps that follow. Step 1: In Azure portal, click More services > type virtual machines in the filter box, and then click Virtual machines. Step 2: In the Virtual machines pane, click the VM you want to add IP addresses to. Click Network interfaces in the virtual machine pane that appears, and then select the network interface you want to add the IP addresses to. In the example shown in the following picture, the NIC named myNIC from the VM named myVM is selected: Step 3: In the pane that appears for the NIC you selected, click IP configurations. Step 4: Click Create public IP address. Step 5: In the Create public IP address pane that appears, enter a Name, select an IP address assignment type, a Subscription, a Resource group, and a Location, then click Create, as shown in the following picture: References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-multiple-ip-addresses-portal QUESTION 18 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 6 You need to allow RDP connections over TCP port 3389 to VM1 from the Internet. The solution must prevent connections from the Internet over all other TCP ports. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Create a new network security group Step 2: Select your new network security group. Step 3: Select Inbound security rules. Under Add inbound security rule, enter the following Destination: Select Network security group, and then select the security group you created previously. Destination port ranges: 3389 Protocol: Select TCP References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-filter-network-traffic QUESTION 19 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 5 You plan to create 100 Azure virtual machines on each of the following three virtual networks: VNET1005a VNET1005b VNET1005c All the network traffic between the three virtual networks will be routed through VNET1005a. You need to create the virtual networks, and then to ensure that all the Azure virtual machines can connect to other virtual machines by using their private IP address. The solution must NOT require any virtual network gateways and must minimize the number of peerings. What should you do from the Azure portal before you configure IP routing? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Click Create a resource in the portal. Step 2: Enter Virtual network in the Search the Marketplace box at the top of the New pane that appears. Click Virtual network when it appears in the search results. Step 3: Select Classic in the Select a deployment model box in the Virtual Network pane that appears, then click Create. Step 4: Enter the following values on the Create virtual network (classic) pane and then click Create: Name: VNET1005a Address space: 10.0.0.0/16 Subnet name: subnet0 Resource group: Create new Subnet address range: 10.0.0.0/24 Subscription and location: Select your subscription and location. Step 5: Repeat steps 3-5 for VNET1005b (10.1.0.0/16, 10.1.0.0/24), and for VNET1005c 10.2.0.0/16, 10.2.0.0/24). References: https://docs.microsoft.com/en-us/azure/virtual-network/create-virtual-network-classic QUESTION 20 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 6 You plan to create several virtual machines in different availability zones, and then to configure the virtual machines for load balanced connections from the Internet. You need to create an IP address resource named ip1006 to support the planned load balancing solution. The solution must minimize costs. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: We should create a public IP address. Step 1: At the top, left corner of the portal, select + Create a resource. Step 2: Enter public ip address in the Search the Marketplace box. When Public IP address appears in the search results, select it. Step 3: Under Public IP address, select Create. Step 4: Enter, or select values for the following settings, under Create public IP address, then select Create: Name: ip1006 SKU: Basic SKU IP Version: IPv6 IP address assignment: Dynamic Subscription: Select appropriate Resource group: Select appropriate References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-public-ip-address QUESTION 21 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 8 You need to create a virtual network named VNET1008 that contains three subnets named subnet0, subnet1, and subnet2. The solution must meet the following requirements: Connections from any of the subnets to the Internet must be blocked. Connections from the Internet to any of the subnets must be blocked. The number of network security groups (NSGs) and NSG rules must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Click Create a resource in the portal. Step 2: Enter Virtual network in the Search the Marketplace box at the top of the New pane that appears. Click Virtual network when it appears in the search results. Step 3: Select Classic in the Select a deployment model box in the Virtual Network pane that appears, then click Create. Step 4: Enter the following values on the Create virtual network (classic) pane and then click Create: Name: VNET1008 Address space: 10.0.0.0/16 Subnet name: subnet0 Resource group: Create new Subnet address range: 10.0.0.0/24 Subscription and location: Select your subscription and location. Step 5: In the portal, you can create only one subnet when you create a virtual network. Click Subnets (in the SETTINGS section) on the Create virtual network (classic) pane that appears. Click +Add on the VNET1008 - Subnets pane that appears. Step 6: Enter subnet1 for Name on the Add subnet pane. Enter 10.0.1.0/24 for Address range. Click OK. Step 7: Create the third subnet: Click +Add on the VNET1008 - Subnets pane that appears. Enter subnet2 for Name on the Add subnet pane. Enter 10.0.2.0/24 for Address range. Click OK. References: https://docs.microsoft.com/en-us/azure/virtual-network/create-virtual-network-classic QUESTION 22 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Azure Network Watcher, you create a connection monitor. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: References: https://azure.microsoft.com/en-us/updates/general-availability-azure-network-watcher-connection-monitor- in-all-public-regions/ QUESTION 23 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Azure Network Watcher, you create a packet capture. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Use the Connection Monitor feature of Azure Network Watcher. References: https://azure.microsoft.com/en-us/updates/general-availability-azure-network-watcher-connection-monitor- in-all-public-regions/ QUESTION 24 HOTSPOT Subscription1 contains the virtual machines in the following table. In Subscription1, you create a load balancer that has the following configurations: Name: LB1 SKU: Basic Type: Internal Subnet: Subnet12 Virtual network: VNET1 For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-overview QUESTION 25 You have an Azure subscription named Subscription1 that contains two Azure virtual networks named VNet1 and VNet2. VNet1 contains a VPN gateway named VPNGW1 that uses static routing. There is a site- to-site VPN connection between your on-premises network and VNet1. On a computer named Client1 that runs Windows 10, you configure a point-to-site VPN connection to VNet1. You configure virtual network peering between VNet1 and VNet2. You verify that you can connect to VNet2 from the on-premises network. Client1 is unable to connect to VNet2. You need to ensure that you can connect Client1 to VNet2. What should you do? A. Select Allow gateway transit on VNet2. B. Enable BGP on VPNGW1. C. Select Allow gateway transit on VNet1. D. Download and re-install the VPN client configuration package on Client1. Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-point-to-site-routing QUESTION 26 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Performance Monitor, you create a Data Collector Set (DCS). Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Use the Connection Monitor feature of Azure Network Watcher. References: https://docs.microsoft.com/en-us/azure/network-watcher/network-watcher-monitoring-overview QUESTION 27 HOTSPOT You have a virtual network named VNet1 that has the configuration shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: add an address space Your IaaS virtual machines (VMs) and PaaS role instances in a virtual network automatically receive a private IP address from a range that you specify, based on the address space of the subnet they are connected to. We need to add the 192.168.1.0/24 address space. Box 2: add a network interface The 10.2.1.0/24 network exists. We need to add a network interface. References: https://docs.microsoft.com/en-us/office365/enterprise/designing-networking-for-microsoft-azure-iaas https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-static-private-ip-arm-pportal QUESTION 28 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains the virtual machines in the following table. Subscription1 contains a virtual network named VNet1 that has the subnets in the following table. VM3 has multiple network adapters, including a network adapter named NIC3. IP forwarding is enabled on NIC3. Routing is enabled on VM3. You create a route table named RT1 that contains the routes in the following table. You apply RT1 to Subnet1 and Subnet2. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: IP forwarding enables the virtual machine a network interface is attached to: Receive network traffic not destined for one of the IP addresses assigned to any of the IP configurations assigned to the network interface. Send network traffic with a different source IP address than the one assigned to one of a network interface's IP configurations. The setting must be enabled for every network interface that is attached to the virtual machine that receives traffic that the virtual machine needs to forward. A virtual machine can forward traffic whether it has multiple network interfaces or a single network interface attached to it. Box 1: Yes The routing table allows connections from VM3 to VM1 and VM2. And as IP forwarding is enabled on VM3, VM3 can connect to VM1. Box 2: No VM3, which has IP forwarding, must be turned on, in order for VM2 to connect to VM1. Box 3: Yes The routing table allows connections from VM1 and VM2 to VM3. IP forwarding on VM3 allows VM1 to connect to VM2 via VM3. References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-udr-overview https://www.quora.com/What-is-IP-forwarding QUESTION 29 You have an Azure subscription that contains the resources in the following table. VM1 and VM2 are deployed from the same template and host line-of-business applications accessed by using Remote Desktop. You configure the network security group (NSG) shown in the exhibit. (Click the Exhibit tab.) You need to prevent users of VM2 and VM2 from accessing websites on the Internet over TCP port 80. What should you do? A. Change the DenyWebSites outbound security rule. B. Change the Port_80 inbound security rule. C. Disassociate the NSG from a network interface. D. Associate the NSG to Subnet1. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: You can associate or dissociate a network security group from a network interface or subnet. The NSG has the appropriate rule to block users from accessing the Internet. We just need to associate it with Subnet1. References: https://docs.microsoft.com/en-us/azure/virtual-network/manage-network-security-group QUESTION 30 HOTSPOT You are creating an Azure load balancer. You need to add an IPv6 load balancing rule to the load balancer. How should you complete the Azure PowerShell script? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-ipv6-internet-ps QUESTION 31 HOTSPOT You have an Azure subscription named Subscription1 that contains a resource group named RG1. In RG1, you create an internal load balancer named LB1 and a public load balancer named LB2. You need to ensure that an administrator named Admin1 can manage LB1 and LB2. The solution must follow the principle of least privilege. Which role should you assign to Admin1 for each task? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 32 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 7 You plan to allow connections between the VNET01-USEA2 and VNET01-USWE2 virtual networks. You need to ensure that virtual machines can communicate across both virtual networks by using their private IP address. The solution must NOT require any virtual network gateways. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Virtual network peering enables you to seamlessly connect two Azure virtual networks. Once peered, the virtual networks appear as one, for connectivity purposes. Peer virtual networks Step 1. In the Search box at the top of the Azure portal, begin typing VNET01-USEA2. When VNET01- USEA2 appears in the search results, select it. Step 2. Select Peerings, under SETTINGS, and then select + Add, as shown in the following picture: Step 3. Enter, or select, the following information, accept the defaults for the remaining settings, and then select OK. Name: myVirtualNetwork1-myVirtualNetwork2 (for example) Subscription: elect your subscription. Virtual network: VNET01-USWE2 - To select the VNET01-USWE2 virtual network, select Virtual network, then select VNET01-USWE2. You can select a virtual network in the same region or in a different region. Now we need to repeat steps 1-3 for the other network VNET01-USWE2: Step 4. In the Search box at the top of the Azure portal, begin typing VNET01- USEA2. When VNET01- USEA2 appears in the search results, select it. Step 5. Select Peerings, under SETTINGS, and then select + Add. References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-connect-virtual-networks-portal QUESTION 33 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 8 You plan to host several secured websites on Web01. You need to allow HTTPS over TCP port 443 to Web01 and to prevent HTTP over TCP port 80 to Web01. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can filter network traffic to and from Azure resources in an Azure virtual network with a network security group. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. Step A: Create a network security group A1. Search for and select the resource group for the VM, choose Add, then search for and select Network security group. A2. Select Create. The Create network security group window opens. A3. Create a network security group Enter a name for your network security group. Select or create a resource group, then select a location. A4. Select Create to create the network security group. Step B: Create an inbound security rule to allows HTTPS over TCP port 443 B1. Select your new network security group. B2. Select Inbound security rules, then select Add. B3. Add inbound rule B4. Select Advanced. From the drop-down menu, select HTTPS. You can also verify by clicking Custom and selecting TCP port, and 443. B5. Select Add to create the rule. Repeat step B2-B5 to deny TCP port 80 B6. Select Inbound security rules, then select Add. B7. Add inbound rule B8. Select Advanced. Clicking Custom and selecting TCP port, and 80. B9. Select Deny. Step C: Associate your network security group with a subnet Your final step is to associate your network security group with a subnet or a specific network interface. C1. In the Search resources, services, and docs box at the top of the portal, begin typing Web01. When the Web01 VM appears in the search results, select it. C2. Under SETTINGS, select Networking. Select Configure the application security groups, select the Security Group you created in Step A, and then select Save, as shown in the following picture: References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-filter-network-traffic Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 HOTSPOT You are evaluating the name resolution for the virtual machines after the planned implementation of the Azure networking infrastructure. For each of the following statements, select Yes if the statement is true. Otherwise, select No. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes All client computers in the Paris office will be joined to an Azure AD domain. A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 Box 2: Yes A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Box 3: No Only VMs in the registration network, here the ClientResources-VNet, will be able to register hostname records. References: https://docs.microsoft.com/en-us/azure/dns/private-dns-overview Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a consulting company that has a main office in Montreal and two branch offices in Seattle and New York. The Montreal office has 2,000 employees. The Seattle office has 1,000 employees. The New York office has 200 employees. All the resources used by Contoso are hosted on-premises. Contoso creates a new Azure subscription. The Azure Active Directory (Azure AD) tenant uses a domain named contoso.onmicrosoft.com. The tenant uses the P1 pricing tier. Existing Environment The network contains an Active Directory forest named contoso.com. All domain controllers are configured as DNS servers and host the contoso.com DNS zone. Contoso has finance, human resources, sales, research, and information technology departments. Each department has an organizational unit (OU) that contains all the accounts of that respective department. All the user accounts have the department attribute set to their respective department. New users are added frequently. Contoso.com contains a user named User1. All the offices connect by using private links. Contoso has data centers in the Montreal and Seattle offices. Each data center has a firewall that can be configured as a VPN device. All infrastructure servers are virtualized. The virtualization environment contains the servers in the following table. Contoso uses two web applications named App1 and App2. Each instance on each web application requires 1GB of memory. The Azure subscription contains the resources in the following table. The network security team implements several network security groups (NSGs). Planned Changes Contoso plans to implement the following changes: Deploy Azure ExpressRoute to the Montreal office. Migrate the virtual machines hosted on Server1 and Server2 to Azure. Synchronize on-premises Active Directory to Azure Active Directory (Azure AD). Migrate App1 and App2 to two Azure web apps named WebApp1 and WebApp2. Technical requirements Contoso must meet the following technical requirements: Ensure that WebApp1 can adjust the number of instances automatically based on the load and can scale up to five instances. Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. Ensure that routing information is exchanged automatically between Azure and the routers in the Montreal office. Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. Ensure that webapp2.azurewebsites.net can be accessed by using the name app2.contoso.com Connect the New York office to VNet1 over the Internet by using an encrypted connection. Create a workflow to send an email message when the settings of VM4 are modified. Create a custom Azure role named Role1 that is based on the Reader role. Minimize costs whenever possible. QUESTION 1 HOTSPOT You need to meet the connection requirements for the New York office. What should you do? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Create a virtual network gateway and a local network gateway. Azure VPN gateway. The VPN gateway service enables you to connect the VNet to the on-premises network through a VPN appliance. For more information, see Connect an on-premises network to a Microsoft Azure virtual network. The VPN gateway includes the following elements: Virtual network gateway. A resource that provides a virtual VPN appliance for the VNet. It is responsible for routing traffic from the on-premises network to the VNet. Local network gateway. An abstraction of the on-premises VPN appliance. Network traffic from the cloud application to the on-premises network is routed through this gateway. Connection. The connection has properties that specify the connection type (IPSec) and the key shared with the on-premises VPN appliance to encrypt traffic. Gateway subnet. The virtual network gateway is held in its own subnet, which is subject to various requirements, described in the Recommendations section below. Box 2: Configure a site-to-site VPN connection On premises create a site-to-site connection for the virtual network gateway and the local network gateway. Scenario: Connect the New York office to VNet1 over the Internet by using an encrypted connection. Incorrect Answers: Azure ExpressRoute: Established between your network and Azure, through an ExpressRoute partner. This connection is private. Traffic does not go over the internet. References: https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/hybrid-networking/vpn Question Set 1 QUESTION 1 You have an Azure Active Directory (Azure AD) tenant named contoso.onmicrosoft.com. The User administrator role is assigned to a user named Admin1. An external partner has a Microsoft account that uses the user1@outlook.com sign in. Admin1 attempts to invite the external partner to sign in to the Azure AD tenant and receives the following error message: \u201cUnable to invite user user1@outlook.com \u2013 Generic authorization exception.\u201d You need to ensure that Admin1 can invite the external partner to sign in to the Azure AD tenant. What should you do? A. From the Roles and administrators blade, assign the Security administrator role to Admin1. B. From the Organizational relationships blade, add an identity provider. C. From the Custom domain names blade, add a custom domain. D. From the Users blade, modify the External collaboration settings. Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://techcommunity.microsoft.com/t5/Azure-Active-Directory/Generic-authorization-exception-inviting- Azure-AD-gests/td-p/274742 QUESTION 2 Your company has an Azure Active Directory (Azure AD) tenant named contoso.com that is configured for hybrid coexistence with the on-premises Active Directory domain. The tenant contains the users shown in the following table. Whenever possible, you need to enable Azure Multi-Factor Authentication (MFA) for the users in contoso.com. Which users should you enable for Azure MFA? A. User1 only B. User1, User2, and User3 only C. User1 and User2 only D. User1, User2, User3, and User4 E. User2 only Correct Answer: D Section: [none] Explanation Explanation/Reference: QUESTION 3 You have two Azure Active Directory (Azure AD) tenants named contoso.com and fabrikam.com. You have a Microsoft account that you use to sign in to both tenants. You need to configure the default sign-in tenant for the Azure portal. What should you do? A. From Azure Cloud Shell, run Set-AzureRmSubscription. B. From Azure Cloud Shell, run Set-AzureRmContext. C. From the Azure portal, configure the portal settings. D. From the Azure portal, change the directory. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Set-AzureRmContext cmdlet sets authentication information for cmdlets that you run in the current session. The context includes tenant, subscription, and environment information. References: https://docs.microsoft.com/en-us/powershell/module/azurerm.profile/set-azurermcontext QUESTION 4 You have an Azure Active Directory (Azure AD) tenant. All administrators must enter a verification code to access the Azure portal. You need to ensure that the administrators can access the Azure portal only from your on-premises network. What should you configure? A. an Azure AD Identity Protection user risk policy. B. the multi-factor authentication service settings. C. the default for all the roles in Azure AD Privileged Identity Management D. an Azure AD Identity Protection sign-in risk policy Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 5 You have an Active Directory forest named contoso.com. You install and configure Azure AD Connect to use password hash synchronization as the single sign-on (SSO) method. Staging mode is enabled. You review the synchronization results and discover that the Synchronization Service Manager does not display any sync jobs. You need to ensure that the synchronization completes successfully. What should you do? A. Run Azure AD Connect and set the SSO method to Pass-through Authentication. B. From Synchronization Service Manager, run a full import. C. From Azure PowerShell, run Start-AdSyncSyncCycle \u2013PolicyType Initial. D. Run Azure AD Connect and disable staging mode. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Staging mode must be disabled. If the Azure AD Connect server is in staging mode, password hash synchronization is temporarily disabled. References: https://docs.microsoft.com/en-us/azure/active-directory/connect/active-directory-aadconnectsync- troubleshoot-password-hash-synchronization#no-passwords-are-synchronized-troubleshoot-by-using-the- troubleshooting-task QUESTION 6 You have an Azure Active Directory (Azure AD) tenant named contoso.onmicrosoft.com. You hire a temporary vendor. The vendor uses a Microsoft account that has a sign-in of user1@outlook.com. You need to ensure that the vendor can authenticate to the tenant by using user1@outlook.com. What should you do? A. From the Azure portal, add a custom domain name, create a new Azure AD user, and then specify user1@outlook.com as the username. B. From Azure Cloud Shell, run the New-AzureADUser cmdlet and specify the \u2013UserPrincipalName user1@outlook.com parameter. C. From the Azure portal, add a new guest user, and then specify user1@outlook.com as the email address. D. From Windows PowerShell, run the New-AzureADUser cmdlet and specify the \u2013UserPrincipalName user1@outlook.com parameter. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: UserPrincipalName - contains the UserPrincipalName (UPN) of this user. The UPN is what the user will use when they sign in into Azure AD. The common structure is @, so for Abby Brown in Contoso.com, the UPN would be AbbyB@contoso.com Example: To create the user, call the New-AzureADUser cmdlet with the parameter values: powershell New-AzureADUser -AccountEnabled $True -DisplayName \"Abby Brown\" -PasswordProfile $PasswordProfile -MailNickName \"AbbyB\" -UserPrincipalName \"AbbyB@contoso.com\" References: https://docs.microsoft.com/bs-cyrl-ba/powershell/azure/active-directory/new-user-sample?view=azureadps- 2.0 QUESTION 7 You have an Azure Active Directory (Azure AD) tenant named contosocloud.onmicrosoft.com. Your company has a public DNS zone for contoso.com. You add contoso.com as a custom domain name to Azure AD. You need to ensure that Azure can verify the domain name. Which type of DNS record should you create? A. MX B. SRV C. DNSKEY D. NSEC Correct Answer: A Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/dns/dns-web-sites-custom-domain QUESTION 8 You set the multi-factor authentication status for a user named admin1@contoso.com to Enabled. Admin1 accesses the Azure portal by using a web browser. Which additional security verifications can Admin1 use when accessing the Azure portal? A. a phone call, a text message that contains a verification code, and a notification or a verification code sent from the Microsoft Authenticator app B. an app password, a text message that contains a verification code, and a notification sent from the Microsoft Authenticator app C. an app password, a text message that contains a verification code, and a verification code sent from the Microsoft Authenticator app D. a phone call, an email message that contains a verification code, and a text message that contains an app password Correct Answer: A Section: [none] Explanation Explanation/Reference: QUESTION 9 DRAG DROP You have an Azure Active Directory (Azure AD) tenant that has the initial domain name. You have a domain name of contoso.com registered at a third-party registrar. You need to ensure that you can create Azure AD users that have names containing a suffix of @contoso.com. Which three actions should you perform in sequence? To answer, move the appropriate cmdlets from the list of cmdlets to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/dns/dns-web-sites-custom-domain QUESTION 10 Your company has a main office in London that contains 100 client computers. Three years ago, you migrated to Azure Active Directory (Azure AD). The company\u2019s security policy states that all personal devices and corporate-owned devices must be registered or joined to Azure AD. A remote user named User1 is unable to join a personal device to Azure AD from a home network. You verify that other users can join their devices to Azure AD. You need to ensure that User1 can join the device to Azure AD. What should you do? A. From the Device settings blade, modify the Users may join devices to Azure AD setting. B. From the Device settings blade, modify the Maximum number of devices per user setting. C. Create a point-to-site VPN from the home network of User1 to Azure. D. Assign the User administrator role to User1. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Maximum number of devices setting enables you to select the maximum number of devices that a user can have in Azure AD. If a user reaches this quota, they will not be able to add additional devices until one or more of the existing devices are removed. Incorrect Answers: A: The Users may join devices to Azure AD setting enables you to select the users who can join devices to Azure AD. Options are All, Selected and None. The default is All. C: Azure AD Join enables users to join their devices to Active Directory from anywhere as long as they have connectivity with the Internet. References: https://docs.microsoft.com/en-us/azure/active-directory/devices/device-management-azure-portal http://techgenix.com/pros-and-cons-azure-ad-join/ QUESTION 11 You have an Azure DNS zone named adatum.com. You need to delegate a subdomain named research.adatum.com to a different DNS server in Azure. What should you do? A. Create an A record named *.research in the adatum.com zone. B. Create a PTR record named research in the adatum.com zone. C. Modify the SOA record of adatum.com. D. Create an NS record named research in the adatum.com zone. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: You need to create a name server (NS) record for the zone. References: https://docs.microsoft.com/en-us/azure/dns/delegate-subdomain QUESTION 12 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 7 You plan to deploy several Azure virtual machines and to connect them to a virtual network named VNET1007. You need to ensure that future virtual machines on VNET1007 can register their name in an internal DNS zone named corp9172795.com. The zone must NOT be hosted on a virtual machine. What should you do from Azure Cloud Shell? To complete this task, start Azure Cloud Shell and select PowerShell (Linux). Click Show Advanced Settings, and then enter corp9172795n1 in the Storage account text box and File1 in the File share text box. Click Create storage, and then complete the task. Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: New-AzureRMResourceGroup -name MyResourceGroup Before you create the DNS zone, create a resource group to contain the DNS zone. Step 2: New-AzureRmDnsZone -Name corp9172795.com -ResourceGroupName MyResourceGroup A DNS zone is created by using the New-AzureRmDnsZone cmdlet. This creates a DNS zone called corp9172795.com in the resource group called MyResourceGroup. References: https://docs.microsoft.com/en-us/azure/dns/dns-getstarted-powershell QUESTION 13 HOTSPOT You have an Azure Active Directory (Azure AD) tenant named adatum.com. Adatum.com contains the groups in the following table: You create two user accounts that are configured as shown in the following table. To which groups do User1 and User2 belong? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Group 1 only First rule applies Box 2: Group1 and Group2 only Both membership rules apply. References: https://docs.microsoft.com/en-us/sccm/core/clients/manage/collections/create-collections Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a manufacturing company that has offices worldwide. Contoso works with partner organizations to bring products to market. Contoso products are manufactured by using blueprint files that the company authors and maintains. Existing Environment Currently, Contoso uses multiple types of servers for business operations, including the following: File servers Domain controllers Microsoft SQL Server servers Your network contains an Active Directory forest named contoso.com. All servers and client computers are joined to Active Directory. You have a public-facing application named App1. App1 is comprised of the following three tiers: A SQL database A web front end A processing middle tier Each tier is comprised of five virtual machines. Users access the web front end by using HTTPS only. Requirements Planned Changes Contoso plans to implement the following changes to the infrastructure: Move all the tiers of App1 to Azure. Move the existing product blueprint files to Azure Blob storage. Create a hybrid directory to support an upcoming Microsoft Office 365 migration project. Technical Requirements Contoso must meet the following technical requirements: Move all the virtual machines for App1 to Azure. Minimize the number of open ports between the App1 tiers. Ensure that all the virtual machines for App1 are protected by backups. Copy the blueprint files to Azure over the Internet. Ensure that the blueprint files are stored in the archive storage tier. Ensure that partner access to the blueprint files is secured and temporary. Prevent user passwords or hashes of passwprds from being stored in Azure. Use unmanaged standard storage for the hard disks of the virtual machines. Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity Minimize administrative effort whenever possible User Requirements Contoso identifies the following requirements for users: Ensure that only users who are part of a group named Pilot can join devices to Azure AD. Designate a new user named Admin1 as the service admin for the Azure subscription. Admin1 must receive email alerts regarding service outages. Ensure that a new user named User3 can create network objects for the Azure subscription. QUESTION 1 HOTSPOT You need to configure the Device settings to meet the technical requirements and the user requirements. Which two settings should you modify? To answer, select the appropriate settings in the answer area. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Selected Only selected users should be able to join devices Box 2: Yes Require Multi-Factor Auth to join devices. From scenario: Ensure that only users who are part of a group named Pilot can join devices to Azure AD Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity. QUESTION 2 You need to recommend a solution to automate the configuration for the finance department users. The solution must meet the technical requirements. What should you include in the recommendation? A. Azure AD B2C B. Azure AD Identity Protection C. an Azure logic app and the Microsoft Identity Management (MIM) client D. dynamic groups and conditional access policies Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Scenario: Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. The recommendation is to use conditional access policies that can then be targeted to groups of users, specific applications, or other conditions. References: https://docs.microsoft.com/en-us/azure/active-directory/authentication/howto-mfa-userstates QUESTION 3 HOTSPOT You need to implement Role1. Which command should you run before you create Role1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 You need to prepare the environment to meet the authentication requirements. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Install the Active Directory Federation Services (AD FS) role on a domain controller in the Miami office. B. Allow inbound TCP port 8080 to the domain controllers in the Miami office. C. Join the client computers in the Miami office to Azure AD. D. Add http://autologon.microsoftazuread-sso.com to the intranet zone of each client computer in the Miami office. E. Install Azure AD Connect on a server in the Miami office and enable Pass-through Authentication. Correct Answer: DE Section: [none] Explanation Explanation/Reference: Explanation: D: You can gradually roll out Seamless SSO to your users. You start by adding the following Azure AD URL to all or selected users' Intranet zone settings by using Group Policy in Active Directory: https:// autologon.microsoftazuread-sso.com E: Seamless SSO works with any method of cloud authentication - Password Hash Synchronization or Pass-through Authentication, and can be enabled via Azure AD Connect. Incorrect Answers: A: Seamless SSO is not applicable to Active Directory Federation Services (ADFS). B: Azure AD connect does not port 8080. It uses port 443. C: Seamless SSO needs the user's device to be domain-joined, but doesn't need for the device to be Azure AD Joined. Scenario: Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. Planned Azure AD Infrastructure include: The on-premises Active Directory domain will be synchronized to Azure AD. References: https://docs.microsoft.com/en-us/azure/active-directory/connect/active-directory-aadconnect-sso-quick-start (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Azure interview questions"},{"location":"nightwolf-cotribution/azure_interview_questions/#microsoft-azure-administrator","text":"QUESTION 1: You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Dev, you assign the Contributor role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: A Explanation: The Contributor role can manage all resources (and add resources) in a Resource Group. QUESTION 2 You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Subscription1, you assign the Logic App Operator role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: B Explanation: You would need the Logic App Contributor role. References: - https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles - https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 3 You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Dev, you assign the Logic App Contributor role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: A Explanation: The Logic App Contributor role lets you manage logic app, but not access to them. It provides access to view, edit, and update a logic app. References: - https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles - https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 4 You have an Azure subscription named Subscription1 that contains an Azure Log Analytics workspace named Workspace1. You need to view the error events from a table named Event. Which query should you run in Workspace1? A. Get-Event Event | where ($_.EventType \u2013eq \"error\") B. Get-Event Event | where ($_.EventType == \"error\") C. search in (Event) * | where EventType \u2013eq \"error\" D. search in (Event) \"error\" E. select *from Event where EventType == \"error\" F. Event | where EventType is \"error\" Correct Answer: D Explanation: To search a term in a specific table, add in (table-name) just after the search operator References: - https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/search-queries - https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-portal QUESTION 5 You have an Azure subscription named Subscription1. Subscription1 contains the resource groups in the following table. RG1 has a web app named WebApp1. WebApp1 is located in West Europe. You move WebApp1 to RG2. What is the effect of the move? A. The App Service plan for WebApp1 moves to North Europe. Policy2 applies to WebApp1. B. The App Service plan for WebApp1 remains in West Europe. Policy2 applies to WebApp1. C. The App Service plan for WebApp1 moves to North Europe. Policy1 applies to WebApp1. D. The App Service plan for WebApp1 remains in West Europe. Policy1 applies to WebApp1. Correct Answer: B Section: [none] Explanation Explanation: You can move an app to another App Service plan, as long as the source plan and the target plan are in the same resource group and geographical region. The region in which your app runs is the region of the App Service plan it's in. However, you cannot change an App Service plan's region. References: - https://docs.microsoft.com/en-us/azure/app-service/app-service-plan-manage QUESTION 6 You have an Azure subscription that contains a resource group named RG1. RG1 contains 100 virtual machines. Your company has three cost centers named Manufacturing, Sales, and Finance. You need to associate each virtual machine to a specific cost center. What should you do? A. Configure locks for the virtual machine. B. Add an extension to the virtual machines. C. Assign tags to the virtual machines. D. Modify the inventory settings of the virtual machine. Correct Answer: C Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/billing/billing-getting-started https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-using-tags QUESTION 7 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the Subscriptions blade, you select the subscription, and then click Programmatic deployment. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 8 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the Subscriptions blade, you select the subscription, and then click Resource providers. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 9 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the RG1 blade, you click Automation script. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 10 HOTSPOT You have an Azure subscription. You plan to use Azure Resource Manager templates to deploy 50 Azure virtual machines that will be part of the same availability set. You need to ensure that as many virtual machines as possible are available if the fabric fails or during servicing. How should you configure the template? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Use two fault domains. 2 or 3 is max value, depending on which region you are in. Use 20 for platformUpdateDomainCount Increasing the update domain (platformUpdateDomainCount) helps with capacity and availability planning when the platform reboots nodes. A higher number for the pool (20 is max) means that fewer of their nodes in any given availability set would be rebooted at once. References: https://www.itprotoday.com/microsoft-azure/check-if-azure-region-supports-2-or-3-fault-domains-managed- disks https://github.com/Azure/acs-engine/issues/1030 QUESTION 11 HOTSPOT You have an Azure subscription named Subscription1 that has a subscription ID of c276fc76-9cd4-44c9- 99a7-4fd71546436e. You need to create a custom RBAC role named CR1 that meets the following requirements: Can be assigned only to the resource groups in Subscription1 Prevents the management of the access permissions for the resource groups Allows the viewing, creating, modifying, and deleting of resource within the resource groups What should you specify in the assignable scopes and the permission elements of the definition of CR1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/role-based-access-control/custom-roles https://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider- operations#microsoftresources QUESTION 12 HOTSPOT You have an Azure Active Directory (Azure AD) tenant that contains three global administrators named Admin1, Admin2, and Admin3. The tenant is associated to an Azure subscription. Access control for the subscription is configured as shown in the Access control exhibit. (Click the Exhibit tab.) You sign in to the Azure portal as Admin1 and configure the tenant as shown in the Tenant exhibit. (Click the Exhibit tab.) For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 13 You have an Azure policy as shown in the following exhibit. What is the effect of the policy? A. You are prevented from creating Azure SQL Servers in ContosoRG1 only. B. You can create Azure SQL servers in ContosoRG1 only. C. You can create Azure SQL servers in any resource group within Subscription1. D. You are prevented from creating Azure SQL servers anywhere in Subscription1. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You are prevented from creating Azure SQL servers anywhere in Subscription 1 with the exception of ContosoRG1 QUESTION 14 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Subscription1, you assign the DevTest Labs User role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: DevTest Labs User role only lets you connect, start, restart, and shutdown virtual machines in your Azure DevTest Labs. The Logic App Contributor role lets you manage logic app, but not access to them. It provides access to view, edit, and update a logic app. References: https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 15 You have an Azure Active Directory (Azure AD) tenant that contains 5,000 user accounts. You create a new user account named AdminUser1. You need to assign the User administrator administrative role to AdminUser1. What should you do from the user account properties? A. From the Directory role blade, modify the directory role. B. From the Licenses blade, assign a new license. C. From the Groups blade, invite the user account to a new group. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Assign a role to a user 1. Sign in to the Azure portal with an account that's a global admin or privileged role admin for the directory. 2. Select Azure Active Directory, select Users, and then select a specific user from the list. 3. For the selected user, select Directory role, select Add role, and then pick the appropriate admin roles from the Directory roles list, such as Conditional access administrator. 4. Press Select to save. References: https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-users-assign-role- azure-portal QUESTION 16 HOTSPOT You have an Azure subscription named Subscription1. You plan to deploy an Ubuntu Server virtual machine named VM1 to Subscription1. You need to perform a custom deployment of the virtual machine. A specific trusted root certification authority (CA) must be added during the deployment. What should you do? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Cloud-init.txt Cloud-init.txt is used to customize a Linux VM on first boot up. It can be used to install packages and write files, or to configure users and security. No additional steps or agents are required to apply your configuration. Box 2: The az vm create command Once Cloud-init.txt has been created, you can deploy the VM with az vm create cmdlet, sing the --custom- data parameter to provide the full path to the cloud-init.txt file. References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/tutorial-automate-vm-deployment QUESTION 17 You have an Azure subscription named Subscription1. In Subscription1, you create an alert rule named Alert1. The Alert1 action group is configured as shown in the following exhibit. Alert1 alert criteria is triggered every minute. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: 60 One alert per minute will trigger one email per minute. Box 2: 12 No more than 1 SMS every 5 minutes can be send, which equals 12 per hour. Note: Rate limiting is a suspension of notifications that occurs when too many are sent to a particular phone number, email address or device. Rate limiting ensures that alerts are manageable and actionable. The rate limit thresholds are: SMS: No more than 1 SMS every 5 minutes. Voice: No more than 1 Voice call every 5 minutes. Email: No more than 100 emails in an hour. Other actions are not rate limited. References: https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/monitoring-and-diagnostics/monitoring- overview-alerts.md Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 You need to resolve the licensing issue before you attempt to assign the license again. What should you do? A. From the Groups blade, invite the user accounts to a new group. B. From the Profile blade, modify the usage location. C. From the Directory role blade, modify the directory role. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: License cannot be assigned to a user without a usage location specified. Scenario: Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License agreement failed for one user.\" You verify that the Azure subscription has the available licenses. QUESTION 2 You need to resolve the Active Directory issue. What should you do? A. Run the IdFix tool then use the Update actions. B. From Active Directory Domains and Trusts, modify the list of UPN suffixes. C. From Azure AD Connect, modify the outbound synchronization rule. D. From Active Directory Users and Computers, select the user accounts and then modify the UPN suffix value. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: IdFix is used to perform discovery and remediation of identity objects and their attributes in an on-premises Active Directory environment in preparation for migration to Azure Active Directory. IdFix is intended for the Active Directory administrators responsible for directory synchronization with Azure Active Directory. Scenario: Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. References: https://www.microsoft.com/en-us/download/details.aspx?id=36832 QUESTION 3 You need to define a custom domain name for Azure AD to support the planned infrastructure. Which domain name should you use? A. ad.humongousinsurance.com B. humingousinsurance.onmicrosoft.com C. humongousinsurance.com D. humongousinsurance.local Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: Every Azure AD directory comes with an initial domain name in the form of domainname.onmicrosoft.com. The initial domain name cannot be changed or deleted, but you can add your corporate domain name to Azure AD as well. For example, your organization probably has other domain names used to do business and users who sign in using your corporate domain name. Adding custom domain names to Azure AD allows you to assign user names in the directory that are familiar to your users, such as \u2018alice@contoso.com.\u2019 instead of 'alice@domain name.onmicrosoft.com'. Scenario: Network Infrastructure: Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com Planned Azure AD Infrastructure: The on-premises Active Directory domain will be synchronized to Azure AD. References: https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/add-custom-domain Question Set 1 QUESTION 1 You plan to use the Azure Import/Export service to copy files to a storage account. Which two files should you create before you prepare the drives for the import job? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. a driveset CSV file B. a JSON configuration file C. a PowerShell PS1 file D. an XML manifest file E. a dataset CSV file Correct Answer: AE Section: [none] Explanation Explanation/Reference: Explanation: A: Modify the driveset.csv file in the root folder where the tool resides. E: Modify the dataset.csv file in the root folder where the tool resides. Depending on whether you want to import a file or folder or both, add entries in the dataset.csv file References: https://docs.microsoft.com/en-us/azure/storage/common/storage-import-export-data-to-files QUESTION 2 DRAG DROP You have an on-premises file server named Server1 that runs Windows Server 2016. You have an Azure subscription that contains an Azure file share. You deploy an Azure File Sync Storage Sync Service, and you create a sync group. You need to synchronize files from Server1 to Azure. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Install the Azure File Sync agent on Server1 The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share Step 2: Register Server1. Register Windows Server with Storage Sync Service Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server (or cluster) and the Storage Sync Service. Step 3: Add a server endpoint Create a sync group and a cloud endpoint. A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud endpoint, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-sync-files-deployment-guide QUESTION 3 You create an Azure Storage account named contosostorage. You plan to create a file share named data. Users need to map a drive to the data file share from home computers that run Windows 10. Which outbound port should you open between the home computers and the data file share? A. 80 B. 443 C. 445 D. 3389 Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: Ensure port 445 is open: The SMB protocol requires TCP port 445 to be open; connections will fail if port 445 is blocked. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows QUESTION 4 HOTSPOT You have several Azure virtual machines on a virtual network named VNet1. You configure an Azure Storage account as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: always Endpoint status is enabled. Box 2: Never After you configure firewall and virtual network settings for your storage account, select Allow trusted Microsoft services to access this storage account as an exception to enable Azure Backup service to access the network restricted storage account. Reference: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows https://azure.microsoft.com/en-us/blog/azure-backup-now-supports-storage-accounts-secured-with-azure- storage-firewalls-and-virtual-networks/ QUESTION 5 HOTSPOT You have an Azure subscription named Subscription1 that contains the resources shown in the following table. The status of VM1 is Running. You assign an Azure policy as shown in the exhibit. (Click the Exhibit tab.) You assign the policy by using the following parameters: For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 6 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 2 You plan to store media files in the rg1lod9172796 storage account. You need to configure the storage account to store the media files. The solution must ensure that only users who have access keys can download the media files and that the files are accessible only over HTTPS. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: We should create an Azure file share. Step 1: In the Azure portal, select All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. On the Storage Accounts window that appears. Step 2: Locate the rg1lod9172796 storage account. Step 3: On the storage account page, in the Services section, select Files. Step 4: On the menu at the top of the File service page, click + File share. The New file share page drops down. Step 5: In Name type myshare. Click OK to create the Azure file share. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-portal QUESTION 7 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 1 You plan to migrate a large amount of corporate data to Azure Storage and to back up files stored on old hardware to Azure Storage. You need to create a storage account named corpdata9172795n1 in the corpdatalod9172795 resource group. The solution must meet the following requirements: Corpdata9172795n1 must be able to host the virtual disk files for Azure virtual machines. The cost of accessing the files must be minimized. Replication costs must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: In the Azure portal, click All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. Step 2: On the Storage Accounts window that appears, choose Add. Step 3: Select the subscription in which to create the storage account. Step 4: Under the Resource group field, select corpdatalod9172795. Step 5: Enter a name for your storage account: corpdata9172795n1 Step 6: For Account kind select: General-purpose v2 accounts (recommended for most scenarios) General-purpose v2 accounts is recommended for most scenarios. . General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. Step 7: For replication select: Read-access geo-redundant storage (RA-GRS) Read-access geo-redundant storage (RA-GRS) maximizes availability for your storage account. RA-GRS provides read-only access to the data in the secondary location, in addition to geo-replication across two regions. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview QUESTION 8 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 2 You plan to move backup files and documents from an on-premises Windows file server to Azure Storage. The backup files will be stored as blobs. You need to create a storage account named corpdata9172795n2. The solution must meet the following requirements: Ensure that the documents are accessible via drive mappings from Azure virtual machines that run Windows Server 2016. Provide the highest possible redundancy for the documents. Minimize storage access costs. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: In the Azure portal, click All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. Step 2: On the Storage Accounts window that appears, choose Add. Step 3: Select the subscription in which to create the storage account. Step 4: Under the Resource group field, select Create New. Create a new Resource Step 5: Enter a name for your storage account: corpdata9172795n2 Step 6: For Account kind select: General-purpose v2 accounts (recommended for most scenarios) General-purpose v2 accounts is recommended for most scenarios. General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. Step 7: For replication select: Read-access geo-redundant storage (RA-GRS) Read-access geo-redundant storage (RA-GRS) maximizes availability for your storage account. RA-GRS provides read-only access to the data in the secondary location, in addition to geo-replication across two regions. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview QUESTION 9 You have an Azure subscription that contains the resources in the following table. Store1 contains a file share named Data. Data contains 5,000 files. You need to synchronize the files in Data to an on-premises server named Server1. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Download an automation script. B. Register Server1. C. Create a sync group. D. Create a container instance. E. Install the Azure File Sync agent on Server1. Correct Answer: BCE Section: [none] Explanation Explanation/Reference: Explanation: Step 1 (E): Install the Azure File Sync agent on Server1 The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share Step 2 (B): Register Server1. Register Windows Server with Storage Sync Service Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server (or cluster) and the Storage Sync Service. Step 3 (C): Create a sync group and a cloud endpoint. A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud endpoint, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-sync-files-deployment-guide QUESTION 10 DRAG DROP You have an Azure subscription named Subscription1. You create an Azure Storage account named contosostorage, and then you create a file share named data. Which UNC path should you include in a script that references files from the data file share? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: contosostorage The name of account Box 2: file.core.windows.net Box 3: data The name of the file share is data. Example: References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows QUESTION 11 DRAG DROP You have an Azure subscription that contains a storage account. You have an on-premises server named Server1 that runs Windows Server 2016. Server1 has 2 TB of data. You need to transfer the data to the storage account by using the Azure Import/Export service. In which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order. NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: At a high level, an import job involves the following steps: Step 1: Attach an external disk to Server1 and then run waimportexport.exe Determine data to be imported, number of drives you need, destination blob location for your data in Azure storage. Use the WAImportExport tool to copy data to disk drives. Encrypt the disk drives with BitLocker. Step 2: From the Azure portal, create an import job. Create an import job in your target storage account in Azure portal. Upload the drive journal files. Step 3: Detach the external disks from Server1 and ship the disks to an Azure data center. Provide the return address and carrier account number for shipping the drives back to you. Ship the disk drives to the shipping address provided during job creation. Step 4: From the Azure portal, update the import job Update the delivery tracking number in the import job details and submit the import job. The drives are received and processed at the Azure data center. The drives are shipped using your carrier account to the return address provided in the import job. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-import-export-service QUESTION 12 You have the Azure virtual machines shown in the following table. You have a Recovery Services vault that protects VM1 and VM2. You need to protect VM3 and VM4 by using Recovery Services. What should you do first? A. Create a new backup policy. B. Configure the extensions for VM3 and VM4. C. Create a storage account. D. Create a new Recovery Services vault. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: A Recovery Services vault is a storage entity in Azure that houses data. The data is typically copies of data, or configuration information for virtual machines (VMs), workloads, servers, or workstations. You can use Recovery Services vaults to hold backup data for various Azure services References: https://docs.microsoft.com/en-us/azure/site-recovery/azure-to-azure-tutorial-enable-replication QUESTION 13 HOTSPOT You have an Azure subscription named Subscription1 that is associated to an Azure Active Directory (Azure AD) tenant named AAD1. Subscription1 contains the objects in the following table. You plan to create a single backup policy for Vault1. To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Share1 only Box 2: 99 years With the latest update to Azure Backup, customers can retain their data for up to 99 years in Azure. Note: A backup policy defines a matrix of when the data snapshots are taken, and how long those snapshots are retained. The backup policy interface looks like this: References: https://docs.microsoft.com/en-us/azure/backup/backup-azure-files https://docs.microsoft.com/en-us/azure/backup/backup-azure-vms-first-look-arm#defining-a-backup-policy https://blogs.microsoft.com/firehose/2015/02/16/february-update-to-azure-backup-includes-data-retention- up-to-99-years-offline-backup-and-more/ QUESTION 14 HOTSPOT You have an Azure subscription named Subscription1. In Subscription1, you create an Azure file share named share1. You create a shared access signature (SAS) named SAS1 as shown in the following exhibit. To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Will have no access The IP 193.77.134.1 does not have access on the SAS. Box 2: Will have read, write, and list access The net use command is used to connect to file shares. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1 https://docs.microsoft.com/en-us/azure/vs-azure-tools-storage-manage-with-storage-explorer? tabs=windows QUESTION 15 HOTSPOT You have Azure Storage accounts as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: storageaccount1 and storageaccount2 only Box 2: All the storage accounts Note: The three different storage account options are: General-purpose v2 (GPv2) accounts, General- purpose v1 (GPv1) accounts, and Blob storage accounts. General-purpose v2 (GPv2) accounts are storage accounts that support all of the latest features for blobs, files, queues, and tables. Blob storage accounts support all the same block blob features as GPv2, but are limited to supporting only block blobs. General-purpose v1 (GPv1) accounts provide access to all Azure Storage services, but may not have the latest features or the lowest per gigabyte pricing. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-account-options QUESTION 16 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 1 You plan to prevent users from accidentally deleting blob data from Azure. You need to ensure that administrators can recover any blob data that is deleted accidentally from the storagelod9272261 storage account for 14 days after the deletion occurred. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Task A: Create a Recovery Services vault (if a vault already exists skip this task, go to Task B below) A1. From Azure Portal, On the Hub menu, click All services and in the list of resources, type Recovery Services and click Recovery Services vaults. If there are recovery services vaults in the subscription, the vaults are listed. A2. On the Recovery Services vaults menu, click Add. A3. The Recovery Services vault blade opens, prompting you to provide a Name, Subscription, Resource group, and Location Task B. Create a backup goal B1. On the Recovery Services vault blade (for the vault you just created), in the Getting Started section, click Backup, then on the Getting Started with Backup blade, select Backup goal. The Backup Goal blade opens. If the Recovery Services vault has been previously configured, then the Backup Goal blades opens when you click Backup on the Recovery Services vault blade. B2. From the Where is your workload running? drop-down menu, select Azure. B3. From the What do you want to backup? menu, select Blob Storage, and click OK. B4. Finish the Wizard. Task C. create a backup schedule C1. Open the Microsoft Azure Backup agent. You can find it by searching your machine for Microsoft Azure Backup. C2. In the Backup agent's Actions pane, click Schedule Backup to launch the Schedule Backup Wizard. C3. On the Getting started page of the Schedule Backup Wizard, click Next. C4. On the Select Items to Backup page, click Add Items. The Select Items dialog opens. C5. Select Blob Storage you want to protect, and then click OK. C6. In the Select Items to Backup page, click Next. On the Specify Backup Schedule page, specify Schedule a backup every day, and click Next. C7. On the Select Retention Policy page, set it to 14 days, and click Next. C8. Finish the Wizard. References: https://docs.microsoft.com/en-us/azure/backup/backup-configure-vault QUESTION 17 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 2 Your company plans to store several documents on a public website. You need to create a container named bios that will host the documents in the storagelod9272261 storage account. The solution must ensure anonymous access and must ensure that users can browse folders in the container. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Azure portal create public container To create a container in the Azure portal, follow these steps: Step 1: Navigate to your new storage account in the Azure portal. Step 2: In the left menu for the storage account, scroll to the lob service section, then select Blobs. Select the + Container button. Type a name for your new container: bios Set the level of public access to the container: Select anonymous access. Step 3: Select OK to create the container. References: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal QUESTION 18 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 3 Your company plans to host in Azure the source files of several line-of-business applications. You need to create an Azure file share named corpsoftware in the storagelod9272261 storage account. The solution must ensure that corpsoftware can store only up to 250 GB of data. What should you do from the Azure portal? Correct Answer: See explanation below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Go to the Storage Account blade on the Azure portal: Step 2: Click on add File Share button: Step 3: Provide Name (storagelod9272261) and Quota (250 GB). References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-create-file-share QUESTION 19 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 6 You recently created a virtual machine named Web01. You need to attach a new 80-GB standard data disk named Web01-Disk1 to Web01. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Add a data disk Step 1: In the Azure portal, from the menu on the left, select Virtual machines. Step 2: Select the Web01 virtual machine from the list. Step 3: On the Virtual machine page, , in Essentials, select Disks. Step 4: On the Disks page, select the Web01-Disk1 from the list of existing disks. Step 5: In the Disks pane, click + Add data disk. Step 6: Click the drop-down menu for Name to view a list of existing managed disks accessible to your Azure subscription. Select the managed disk Web01-Disk1 to attach: References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/attach-disk-portal Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a manufacturing company that has offices worldwide. Contoso works with partner organizations to bring products to market. Contoso products are manufactured by using blueprint files that the company authors and maintains. Existing Environment Currently, Contoso uses multiple types of servers for business operations, including the following: File servers Domain controllers Microsoft SQL Server servers Your network contains an Active Directory forest named contoso.com. All servers and client computers are joined to Active Directory. You have a public-facing application named App1. App1 is comprised of the following three tiers: A SQL database A web front end A processing middle tier Each tier is comprised of five virtual machines. Users access the web front end by using HTTPS only. Requirements Planned Changes Contoso plans to implement the following changes to the infrastructure: Move all the tiers of App1 to Azure. Move the existing product blueprint files to Azure Blob storage. Create a hybrid directory to support an upcoming Microsoft Office 365 migration project. Technical Requirements Contoso must meet the following technical requirements: Move all the virtual machines for App1 to Azure. Minimize the number of open ports between the App1 tiers. Ensure that all the virtual machines for App1 are protected by backups. Copy the blueprint files to Azure over the Internet. Ensure that the blueprint files are stored in the archive storage tier. Ensure that partner access to the blueprint files is secured and temporary. Prevent user passwords or hashes of passwprds from being stored in Azure. Use unmanaged standard storage for the hard disks of the virtual machines. Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity Minimize administrative effort whenever possible User Requirements Contoso identifies the following requirements for users: Ensure that only users who are part of a group named Pilot can join devices to Azure AD. Designate a new user named Admin1 as the service admin for the Azure subscription. Admin1 must receive email alerts regarding service outages. Ensure that a new user named User3 can create network objects for the Azure subscription. QUESTION 1 HOTSPOT You need to identify the storage requirements for Contoso. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes Contoso is moving the existing product blueprint files to Azure Blob storage. Use unmanaged standard storage for the hard disks of the virtual machines. We use Page Blobs for these. Box 2: No Box 3: No QUESTION 2 You need to move the blueprint files to Azure. What should you do? A. Use Azure Storage Explorer to copy the files. B. Use the Azure Import/Export service. C. Generate a shared access signature (SAS). Map a drive, and then copy the files by using File Explorer. D. Generate an access key. Map a drive, and then copy the files by using File Explorer. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Azure Storage Explorer is a free tool from Microsoft that allows you to work with Azure Storage data on Windows, macOS, and Linux. You can use it to upload and download data from Azure blob storage. Scenario: Planned Changes include: move the existing product blueprint files to Azure Blob storage. Technical Requirements include: Copy the blueprint files to Azure over the Internet. References: https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-data-to-azure- blob-using-azure-storage-explorer QUESTION 3 You need to implement a backup solution for App1 after the application is moved. What should you create first? A. a recovery plan B. a Recovery Services vault C. an Azure Backup Server D. a backup policy Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: A Recovery Services vault is a logical container that stores the backup data for each protected resource, such as Azure VMs. When the backup job for a protected resource runs, it creates a recovery point inside the Recovery Services vault. Scenario: There are three application tiers, each with five virtual machines. Move all the virtual machines for App1 to Azure. Ensure that all the virtual machines for App1 are protected by backups. References: https://docs.microsoft.com/en-us/azure/backup/quick-backup-vm-portal Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 DRAG DROP You need to prepare the environment to ensure that the web administrators can deploy the web apps as quickly as possible. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: First you create a storage account using the Azure portal. Step 2: Select Automation options at the bottom of the screen. The portal shows the template on the Template tab. Add the storage account to the library. Step 3: Share the template. Scenario: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. References: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-quickstart-create- templates-use-the-portal Question Set 1 QUESTION 1 You plan to automate the deployment of a virtual machine scale set that uses the Windows Server 2016 Datacenter image. You need to ensure that when the scale set virtual machines are provisioned, they have web server components installed. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Modify the extensionProfile section of the Azure Resource Manager template. B. Create an automation account. C. Upload a configuration script. D. Create a new virtual machine scale set in the Azure portal. E. Create an Azure policy. Correct Answer: AD Section: [none] Explanation Explanation/Reference: Explanation: Virtual Machine Scale Sets can be used with the Azure Desired State Configuration (DSC) extension handler. Virtual machine scale sets provide a way to deploy and manage large numbers of virtual machines, and can elastically scale in and out in response to load. DSC is used to configure the VMs as they come online so they are running the production software. References: https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-dsc QUESTION 2 DRAG DROP You have two Azure virtual machines named VM1 and VM2. VM1 has a single data disk named Disk1. You need to attach Disk1 to VM2. The solution must minimize downtime for both virtual machines. Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Stop VM1. Step 2: Detach Disk1 from VM1. Step 3: Attach Disk1 to VM2 Attach an existing disk Follow these steps to reattach an existing available data disk to a running VM. 1. Select a running VM for which you want to reattach a data disk. 2. From the menu on the left, select Disks. 3. Select Attach existing to attach an available data disk to the VM. 4. From the Attach existing disk pane, select OK. Step 4: Start VM1. Detach a data disk using the portal 1. In the left menu, select Virtual Machines. 2. Select the virtual machine that has the data disk you want to detach and click Stop to deallocate the VM. 3. In the virtual machine pane, select Disks. 4. At the top of the Disks pane, select Edit. 5. In the Disks pane, to the far right of the data disk that you would like to detach, click the Detach button image detach button. 6. After the disk has been removed, click Save on the top of the pane. 7. In the virtual machine pane, click Overview and then click the Start button at the top of the pane to restart the VM. 8. The disk stays in storage but is no longer attached to a virtual machine. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/detach-disk https://docs.microsoft.com/en-us/azure/lab-services/devtest-lab-attach-detach-data-disk QUESTION 3 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains a virtual machine named VM1. You install and configure a web server and a DNS server on VM1. VM1 has the effective network security rules shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Rule2 blocks ports 50-60, which includes port 53, the DNS port. Internet users can reach the Web server, since it uses port 80. Box 2: If Rule2 is removed internet users can reach the DNS server as well. Note: Rules are processed in priority order, with lower numbers processed before higher numbers, because lower numbers have higher priority. Once traffic matches a rule, processing stops. As a result, any rules that exist with lower priorities (higher numbers) that have the same attributes as rules with higher priorities are not processed. References: https://docs.microsoft.com/en-us/azure/virtual-network/security-overview QUESTION 4 DRAG DROP You have an Azure Linux virtual machine that is protected by Azure Backup. One week ago, two files were deleted from the virtual machine. You need to restore the deleted files to an on-premises computer as quickly as possible. Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: To restore files or folders from the recovery point, go to the virtual machine and choose the desired recovery point. Step 0. In the virtual machine's menu, click Backup to open the Backup dashboard. Step 1. In the Backup dashboard menu, click File Recovery. Step 2. From the Select recovery point drop-down menu, select the recovery point that holds the files you want. By default, the latest recovery point is already selected. Step 3: To download the software used to copy files from the recovery point, click Download Executable (for Windows Azure VM) or Download Script (for Linux Azure VM, a python script is generated). Step 4: Copy the files by using AzCopy AzCopy is a command-line utility designed for copying data to/from Microsoft Azure Blob, File, and Table storage, using simple commands designed for optimal performance. You can copy data between a file system and a storage account, or between storage accounts. References: https://docs.microsoft.com/en-us/azure/backup/backup-azure-restore-files-from-vm https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy QUESTION 5 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 3 You plan to protect on-premises virtual machines and Azure virtual machines by using Azure Backup. You need to prepare the backup infrastructure in Azure. The solution must minimize the cost of storing the backups in Azure. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: First, create Recovery Services vault. Step 1: On the left-hand menu, select All services and in the services list, type Recovery Services. As you type, the list of resources filters. When you see Recovery Services vaults in the list, select it to open the Recovery Services vaults menu. Step 2: In the Recovery Services vaults menu, click Add to open the Recovery Services vault menu. Step 3: In the Recovery Services vault menu, for example, Type myRecoveryServicesVault in Name. The current subscription ID appears in Subscription. If you have additional subscriptions, you could choose another subscription for the new vault. For Resource group select Use existing and choose myResourceGroup. If myResourceGroup doesn't exist, select Create new and type myResourceGroup. From the Location drop-down menu, choose West Europe. Click Create to create your Recovery Services vault. References: https://docs.microsoft.com/en-us/azure/backup/tutorial-backup-vm-at-scale QUESTION 6 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 3 You need to deploy two Azure virtual machines named VM1003a and VM1003b based on an Ubuntu Server image. The deployment must meet the following requirements: Provide a Service Level Agreement (SLA) of 99.95 percent availability. Use managed disks. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Open the Azure portal. Step 2: On the left menu, select All resources. You can sort the resources by Type to easily find your images. Step 3: Select the image you want to use from the list. The image Overview page opens. Step 4: Select Create VM from the menu. Step 5: Enter the virtual machine information. Select VM1003a as the name for the first Virtual machine. The user name and password entered here will be used to log in to the virtual machine. When complete, select OK. You can create the new VM in an existing resource group, or choose Create new to create a new resource group to store the VM. Step 6: Select a size for the VM. To see more sizes, select View all or change the Supported disk type filter. Step 7: Under Settings, make changes as necessary and select OK. Step 8: On the summary page, you should see your image name listed as a Private image. Select Ok to start the virtual machine deployment. Repeat the procedure for the second VM and name it VM1003b. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/create-vm-generalized-managed QUESTION 7 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 4 You need to deploy an Azure virtual machine named VM1004a based on an Ubuntu Server image, and then configure VM1004a to meet the following requirements: The virtual machine must contain data disks that can store at least 15 TB of data. The data disks must be able to provide at least 2.000 IOPS. Storage costs must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Open the Azure portal. Step 2: On the left menu, select All resources. You can sort the resources by Type to easily find your images. Step 3: Select the image you want to use from the list. The image Overview page opens. Step 4: Select Create VM from the menu. Step 5: Enter the virtual machine information. Select VM1004a as the name for the first Virtual machine. The user name and password entered here will be used to log in to the virtual machine. When complete, select OK. You can create the new VM in an existing resource group, or choose Create new to create a new resource group to store the VM. Step 6: Select a size for the VM. To see more sizes, select View all or change the Supported disk type filter. To support 15 TB of data you would need a Premium disk. Step 7: Under Settings, make changes as necessary and select OK. Step 8: On the summary page, you should see your image name listed as a Private image. Select Ok to start the virtual machine deployment. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/create-vm-generalized-managed QUESTION 8 You have an Azure subscription that contains a virtual machine named VM1. VM1 hosts a line-of-business application that is available 24 hours a day. VM1 has one network interface and one managed disk. VM1 uses the D4s v3 size. You plan to make the following changes to VM1: Change the size to D8s v3. Add a 500-GB managed disk. Add the Puppet Agent extension. Attach an additional network interface. Which change will cause downtime for VM1? A. Add the Puppet Agent extension. B. Change the size to D8s v3. C. Add a 500-GB managed disk. D. Attach an additional network interface. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: While resizing the VM it must be in a stopped state. References: https://azure.microsoft.com/en-us/blog/resize-virtual-machines/ QUESTION 9 You have an Azure virtual machine named VM1 that you use for testing. VM1 is protected by Azure Backup. You delete VM1. You need to remove the backup data stored for VM1. What should you do first? A. Delete the Recovery Services vault. B. Delete the storage account. C. Stop the backup D. Modify the backup policy. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Azure Backup provides backup for virtual machines \u2014 created through both the classic deployment model and the Azure Resource Manager deployment model \u2014 by using custom-defined backup policies in a Recovery Services vault. With the release of backup policy management, customers can manage backup policies and model them to meet their changing requirements from a single window. Customers can edit a policy, associate more virtual machines to a policy, and delete unnecessary policies to meet their compliance requirements. Incorrect Answers: B: You can't delete a Recovery Services vault if it is registered to a server and holds backup data. If you try to delete a vault, but can't, the vault is still configured to receive backup data. References: https://azure.microsoft.com/en-in/updates/azure-vm-backup-policy-management/ QUESTION 10 You have an Azure subscription named Subscription1. You deploy a Linux virtual machine named VM1 to Subscription1. You need to monitor the metrics and the logs of VM1. What should you use? A. the AzurePerformanceDiagnostics extension B. Azure HDInsight C. Linux Diagnostic Extension (LAD) 3.0 D. Azure Analysis Services Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: You can use extensions to configure diagnostics on your VMs to collect additional metric data. The basic host metrics are available, but to see more granular and VM-specific metrics, you need to install the Azure diagnostics extension on the VM. The Azure diagnostics extension allows additional monitoring and diagnostics data to be retrieved from the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/tutorial-monitoring QUESTION 11 DRAG DROP You have an availability set named AS1 that contains three virtual machines named VM1, VM2, and VM3. You attempt to reconfigure VM1 to use a larger size. The operation fails and you receive an allocation failure message. You need to ensure that the resize operation succeeds. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Stop VM1, VM, and VM3. If the VM you wish to resize is part of an availability set, then you must stop all VMs in the availability set before changing the size of any VM in the availability set. The reason all VMs in the availability set must be stopped before performing the resize operation to a size that requires different hardware is that all running VMs in the availability set must be using the same physical hardware cluster. Therefore, if a change of physical hardware cluster is required to change the VM size then all VMs must be first stopped and then restarted one-by-one to a different physical hardware clusters. Step 2: Resize VM1. Step 3: Start VM1, VM2, and VM3. References: https://azure.microsoft.com/es-es/blog/resize-virtual-machines/ QUESTION 12 You plan to back up an Azure virtual machine named VM1. You discover that the Backup Pre-Check status displays a status of Warning. What is a possible cause of the Warning status? A. VM1 is stopped. B. VM1 does not have the latest version of WaAppAgent.exe installed. C. VM1 has an unmanaged disk. D. A Recovery Services vault is unavailable. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Warning state indicates one or more issues in VM\u2019s configuration that might lead to backup failures and provides recommended steps to ensure successful backups. Not having the latest VM Agent installed, for example, can cause backups to fail intermittently and falls in this class of issues. References: https://azure.microsoft.com/en-us/blog/azure-vm-backup-pre-checks/ QUESTION 13 You have an Azure subscription named Subscription1 that is used by several departments at your company. Subscription1 contains the resources in the following table. Another administrator deploys a virtual machine named VM1 and an Azure Storage account named Storage2 by using a single Azure Resource Manager template. You need to view the template used for the deployment. From which blade can you view the template that was used for the deployment? A. Container1 B. RG1 C. VM1 D. Storage2 Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: View template from deployment history 1. Go to the resource group for your new resource group. Notice that the portal shows the result of the last deployment. Select this link. 2. You see a history of deployments for the group. In your case, the portal probably lists only one deployment. Select this deployment. 3. The portal displays a summary of the deployment. The summary includes the status of the deployment and its operations and the values that you provided for parameters. To see the template that you used for the deployment, select View template. References: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-export-template QUESTION 14 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Overview blade, you move the virtual machine to a different subscription. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You would need to redeploy the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 15 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Redeploy blade, you click Redeploy. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: When you redeploy a VM, it moves the VM to a new node within the Azure infrastructure and then powers it back on, retaining all your configuration options and associated resources. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 16 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Update management blade, you click Enable. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You would need to redeploy the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 17 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains two Azure virtual machines named VM1 and VM2. VM1 and VM2 run Windows Server 2016. VM1 is backed up daily by Azure Backup without using the Azure Backup agent. VM1 is affected by ransomware that encrypts data. You need to restore the latest backup of VM1. To which location can you restore the backup? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 18 You download an Azure Resource Manager template based on an existing virtual machine. The template will be used to deploy 100 virtual machines. You need to modify the template to reference an administrative password. You must prevent the password from being stored in plain text. What should you create to store the password? A. an Azure Key Vault and an access policy B. a Recovery Services vault and a backup policy C. Azure Active Directory (AD) Identity Protection and an Azure policy D. an Azure Storage account and an access policy Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: You can use a template that allows you to deploy a simple Windows VM by retrieving the password that is stored in a Key Vault. Therefore, the password is never put in plain text in the template parameter file. References: https://azure.microsoft.com/en-us/resources/templates/101-vm-secure-password/ QUESTION 19 HOTSPOT You create a virtual machine scale set named Scale1. Scale1 is configured as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: The Autoscale scale out rule increases the number of VMs by 2 if the CPU threshold is 80% or higher. The initial instance count is 4 and rises to 6 when the 2 extra instances of VMs are added. Box 2: The Autoscale scale in rule decreases the number of VMs by 4 if the CPU threshold is 30% or lower. The initial instance count is 4 and thus cannot be reduced to 0 as the minimum instances is set to 2. Instances are only added when the CPU threshold reaches 80%. References: https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-overview https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-best-practices https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-common-scale-patterns QUESTION 20 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 4 You plan to back up all the Azure virtual machines in your Azure subscription at 02:00 Coordinated Universal Time (UTC) daily. You need to prepare the Azure environment to ensure that any new virtual machines can be configured quickly for backup. The solution must ensure that all the daily backups performed at 02:00 UTC are stored for only 90 days. What should you do from your Recovery Services vault on the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Task A: Create a Recovery Services vault (if a vault already exists skip this task, go to Task B below) A1. From Azure Portal, On the Hub menu, click All services and in the list of resources, type Recovery Services and click Recovery Services vaults. If there are recovery services vaults in the subscription, the vaults are listed. A2. On the Recovery Services vaults menu, click Add. A3. The Recovery Services vault blade opens, prompting you to provide a Name, Subscription, Resource group, and Location Task B. B1. On the Recovery Services vault blade (for the vault you just created), in the Getting Started section, click Backup, then on the Getting Started with Backup blade, select Backup goal. The Backup Goal blade opens. If the Recovery Services vault has been previously configured, then the Backup Goal blades opens when you click Backup on the Recovery Services vault blade. B2. From the Where is your workload running? drop-down menu, select Azure. B3. From the What do you want to backup? menu, select Virtual Machine, and click OK. B4. Finish the Wizard. Task C. create a backup schedule C1. Open the Microsoft Azure Backup agent. You can find it by searching your machine for Microsoft Azure Backup. C2. In the Backup agent's Actions pane, click Schedule Backup to launch the Schedule Backup Wizard. C3. On the Getting started page of the Schedule Backup Wizard, click Next. C4. On the Select Items to Backup page, click Add Items. The Select Items dialog opens. C5. Select Blob Storage you want to protect, and then click OK. C6. In the Select Items to Backup page, click Next. On the Specify Backup Schedule page, specify Schedule a backup every: day At the following times: 2.00 AM C7. On the Select Retention Policy page, set it to 90 days, and click Next. C8. Finish the Wizard. References: https://docs.microsoft.com/en-us/azure/backup/backup-configure-vault QUESTION 21 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task5 You plan to connect several virtual machines to the VNET01-USEA2 virtual network. In the Web-RGlod9272261 resource group, you need to create a virtual machine that uses the Standard_B2ms size named Web01 that runs Windows Server 2016. Web01 must be added to an availability set. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Choose Create a resource in the upper left-hand corner of the Azure portal. Step 2: In the Basics tab, under Project details, make sure the correct subscription is selected and then choose Web-RGlod9272261 resource group Step 3: Under Instance details type/select: Virtual machine name: Web01 Image: Windows Server 2016 Size: Standard_B2ms size Leave the other defaults. Step 4: Finish the Wizard Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a consulting company that has a main office in Montreal and two branch offices in Seattle and New York. The Montreal office has 2,000 employees. The Seattle office has 1,000 employees. The New York office has 200 employees. All the resources used by Contoso are hosted on-premises. Contoso creates a new Azure subscription. The Azure Active Directory (Azure AD) tenant uses a domain named contoso.onmicrosoft.com. The tenant uses the P1 pricing tier. Existing Environment The network contains an Active Directory forest named contoso.com. All domain controllers are configured as DNS servers and host the contoso.com DNS zone. Contoso has finance, human resources, sales, research, and information technology departments. Each department has an organizational unit (OU) that contains all the accounts of that respective department. All the user accounts have the department attribute set to their respective department. New users are added frequently. Contoso.com contains a user named User1. All the offices connect by using private links. Contoso has data centers in the Montreal and Seattle offices. Each data center has a firewall that can be configured as a VPN device. All infrastructure servers are virtualized. The virtualization environment contains the servers in the following table. Contoso uses two web applications named App1 and App2. Each instance on each web application requires 1GB of memory. The Azure subscription contains the resources in the following table. The network security team implements several network security groups (NSGs). Planned Changes Contoso plans to implement the following changes: Deploy Azure ExpressRoute to the Montreal office. Migrate the virtual machines hosted on Server1 and Server2 to Azure. Synchronize on-premises Active Directory to Azure Active Directory (Azure AD). Migrate App1 and App2 to two Azure web apps named WebApp1 and WebApp2. Technical requirements Contoso must meet the following technical requirements: Ensure that WebApp1 can adjust the number of instances automatically based on the load and can scale up to five instances. Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. Ensure that routing information is exchanged automatically between Azure and the routers in the Montreal office. Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. Ensure that webapp2.azurewebsites.net can be accessed by using the name app2.contoso.com Connect the New York office to VNet1 over the Internet by using an encrypted connection. Create a workflow to send an email message when the settings of VM4 are modified. Create a custom Azure role named Role1 that is based on the Reader role. Minimize costs whenever possible. QUESTION 1 You discover that VM3 does NOT meet the technical requirements. You need to verify whether the issue relates to the NSGs. What should you use? A. Diagram in VNet1 B. the security recommendations in Azure Advisor C. Diagnostic settings in Azure Monitor D. Diagnose and solve problems in Traffic Manager profiles E. IP flow verify in Azure Network Watcher Correct Answer: E Section: [none] Explanation Explanation/Reference: Explanation: Scenario: Contoso must meet technical requirements including: Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. IP flow verify checks if a packet is allowed or denied to or from a virtual machine. The information consists of direction, protocol, local IP, remote IP, local port, and remote port. If the packet is denied by a security group, the name of the rule that denied the packet is returned. While any source or destination IP can be chosen, IP flow verify helps administrators quickly diagnose connectivity issues from or to the internet and from or to the on-premises environment. References: https://docs.microsoft.com/en-us/azure/network-watcher/network-watcher-ip-flow-verify-overview Question Set 1 QUESTION 1 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You modify the name servers at the domain registrar. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Modify the Name Server (NS) record. References: https://docs.microsoft.com/en-us/azure/dns/dns-delegate-domain-azure-dns QUESTION 2 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You modify the SOA record in the contoso.com zone. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Modify the NS record, not the SOA record. Note: The SOA record stores information about the name of the server that supplied the data for the zone; the administrator of the zone; the current version of the data file; the number of seconds a secondary name server should wait before checking for updates; the number of seconds a secondary name server should wait before retrying a failed zone transfer; the maximum number of seconds that a secondary name server can use data before it must either be refreshed or expire; and a default number of seconds for the time-to- live file on resource records. References: https://searchnetworking.techtarget.com/definition/start-of-authority-record QUESTION 3 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You add an NS record to the contoso.com Azure DNS zone. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Before you can delegate your DNS zone to Azure DNS, you need to know the name servers for your zone. The NS record set contains the names of the Azure DNS name servers assigned to the zone. References: https://docs.microsoft.com/en-us/azure/dns/dns-delegate-domain-azure-dns QUESTION 4 You are troubleshooting a performance issue for an Azure Application Gateway. You need to compare the total requests to the failed requests during the past six hours. What should you use? A. NSG flow logs in Azure Network Watcher B. Metrics in Application Gateway C. Connection monitor in Azure Network Watcher D. Diagnostics logs in Application Gateway Correct Answer: B Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-diagnostics#metrics QUESTION 5 You have two subscriptions named Subscription1 and Subscription2. Each subscription is associated to a different Azure AD tenant. Subscription1 contains a virtual network named VNet1. VNet1 contains an Azure virtual machine named VM1 and has an IP address space of 10.0.0.0/16. Subscription2 contains a virtual network named VNet2. VNet2 contains an Azure virtual machine named VM2 and has an IP address space of 10.10.0.0/24. You need to connect VNet1 to VNet2. What should you do first? A. Move VM1 to Subscription2. B. Modify the IP address space of VNet2. C. Provision virtual network gateways. D. Move VNet1 to Subscription2. Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: The virtual networks can be in the same or different regions, and from the same or different subscriptions. When connecting VNets from different subscriptions, the subscriptions do not need to be associated with the same Active Directory tenant. Configuring a VNet-to-VNet connection is a good way to easily connect VNets. Connecting a virtual network to another virtual network using the VNet-to-VNet connection type (VNet2VNet) is similar to creating a Site- to-Site IPsec connection to an on-premises location. Both connectivity types use a VPN gateway to provide a secure tunnel using IPsec/IKE, and both function the same way when communicating. The local network gateway for each VNet treats the other VNet as a local site. This lets you specify additional address space for the local network gateway in order to route traffic. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager- portal QUESTION 6 DRAG DROP You have an Azure subscription that contains two virtual networks named VNet1 and VNet2. Virtual machines connect to the virtual networks. The virtual networks have the address spaces and the subnets configured as shown in the following table. You need to add the address space of 10.33.0.0/16 to VNet1. The solution must ensure that the hosts on VNet1 and VNet2 can communicate. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Remove peering between Vnet1 and VNet2. You can't add address ranges to, or delete address ranges from a virtual network's address space once a virtual network is peered with another virtual network. To add or remove address ranges, delete the peering, add or remove the address ranges, then re-create the peering. Step 2: Add the 10.44.0.0/16 address space to VNet1. Step 3: Recreate peering between VNet1 and VNet2 References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-manage-peering QUESTION 7 You have an Azure subscription that contains the resources in the following table. To which subnets can you apply NSG1? A. the subnets on VNet2 only B. the subnets on VNet2 and VNet3 only C. the subnets on VNet1, VNet2, and VNet3 D. the subnets on VNet1 only E. the subnets on VNet3 only Correct Answer: E Section: [none] Explanation Explanation/Reference: Explanation: All Azure resources are created in an Azure region and subscription. A resource can only be created in a virtual network that exists in the same region and subscription as the resource. References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-vnet-plan-design-arm QUESTION 8 HOTSPOT You have an Azure virtual machine named VM1 that connects to a virtual network named VNet1. VM1 has the following configurations: Subnet 10.0.0.0/24 Availability set: AVSet Network security group (NSG): None Private IP address: 10.0.0.4 (dynamic) Public IP address: 40.90.219.6 (dynamic) You deploy a standard, Internet-facing load balancer named slb1. You need to configure slb1 to allow connectivity to VM1. Which changes should you apply to VM1 as you configure slb1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 9 You have five Azure virtual machines that run Windows Server 2016. The virtual machines are configured as web servers. You have an Azure load balancer named LB1 that provides load balancing services for the virtual machines. You need to ensure that visitors are serviced by the same web server for each request. What should you configure? A. Protocol to UDP B. Session persistence to None C. Session persistence to Client IP D. Idle Time-out (minutes) to 20 Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: You can set the sticky session in load balancer rules with setting the session persistence as the client IP. References: https://cloudopszone.com/configure-azure-load-balancer-for-sticky-sessions/ QUESTION 10 You have the Azure virtual networks shown in the following table. To which virtual networks can you establish a peering connection from VNet1? A. VNet2 and VNet3 only B. VNet2 only C. VNet3 and VNet4 only D. VNet2, VNet3, and VNet4 Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-connect-virtual-networks-portal QUESTION 11 You have an Azure subscription that contains a policy-based virtual network gateway named GW1 and a virtual network named VNet1. You need to ensure that you can configure a point-to-site connection from VNet1 to an on-premises computer. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Reset GW1. B. Create a route-based virtual network gateway. C. Delete GW1. D. Add a public IP address space to VNet1. E. Add a connection to GW1. F. Add a service endpoint to VNet1. Correct Answer: BC Section: [none] Explanation Explanation/Reference: Explanation: B: A VPN gateway is used when creating a VPN connection to your on-premises network. Route-based VPN devices use any-to-any (wildcard) traffic selectors, and let routing/forwarding tables direct traffic to different IPsec tunnels. It is typically built on router platforms where each IPsec tunnel is modeled as a network interface or VTI (virtual tunnel interface). C: Policy-based VPN devices use the combinations of prefixes from both networks to define how traffic is encrypted/decrypted through IPsec tunnels. It is typically built on firewall devices that perform packet filtering. IPsec tunnel encryption and decryption are added to the packet filtering and processing engine. Incorrect Answers: D: Point-to-Site connections do not require a VPN device or a public-facing IP address. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/create-routebased-vpn-gateway-portal https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-connect-multiple-policybased-rm-ps QUESTION 12 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains the virtual networks in the following table. Subscription1 contains the virtual machines in the following table. The firewalls on all the virtual machines are configured to allow all ICMP traffic. You add the peerings in the following table. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes Vnet1 and Vnet3 are peers. Box 2: Yes Vnet2 and Vnet3 are peers. Box 3: No Peering connections are non-transitive. References: https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/hybrid-networking/hub-spoke QUESTION 13 You have an Azure subscription named Subscription1 that contains the resource groups shown in the following table. In RG1, you create a virtual machine named VM1 in the East Asia location. You plan to create a virtual network named VNET1. You need to create VNET1, and then connect VM1 to VNET1. What are two possible ways to achieve this goal? Each correct answer presents a complete solution. NOTE: Each correct selection is worth one point. A. Create VNET1 in RG2, and then set East Asia as the location. B. Create VNET1 in a new resource group in the West US location, and then set West US as the location. C. Create VNET1 in RG1, and then set East US as the location. D. Create VNET1 in RG2, and then set East US as the location. E. Create VNET1 in RG1, and then set East Asia as the location. Correct Answer: AE Section: [none] Explanation Explanation/Reference: QUESTION 14 You have an Azure subscription that contains a virtual network named VNet1. VNet1 contains four subnets named Gateway, Perimeter, NVA, and Production. The NVA subnet contains two network virtual appliances (NVAs) that will perform network traffic inspection between the Perimeter subnet and the Production subnet. You need to implement an Azure load balancer for the NVAs. The solution must meet the following requirements: The NVAs must run in an active-active configuration that uses automatic failover. The NVAs must load balance traffic to two services on the Profuction subnet. The services have different IP addresses. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Add two load balancing rules that have HA Ports enabled and Floating IP disabled. B. Add a frontend IP configuration, two backend pools, and a health probe. C. Add two load balancing rules that have HA Ports and Floating IP enabled. D. Deploy a standard load balancer. E. Deploy a basic load balancer. F. Add a frontend IP configuration a backend pool, and a health probe. Correct Answer: BCD Section: [none] Explanation Explanation/Reference: Explanation: A standard load balancer is required for the HA ports. Two backend pools are needed as there are two services with different IP addresses. Floating IP rule is used where backend ports are reused. Incorrect Answers: F: HA Ports are not available for the basic load balancer. References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-overview https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-multivip-overview QUESTION 15 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 1 Your on-premises network uses an IP address range of 131.107.2.0 to 131.107.2.255. You need to ensure that only device from the on-premises network can connect to the rg1lod9172796n1 storage account. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Navigate to the rg1lod9172796n1 storage account. Step 2: Click on the settings menu called Firewalls and virtual networks. Step 3: Ensure that you have elected to allow access from 'Selected networks'. Step 4: To grant access to an internet IP range, enter the address range of 131.107.2.0 to 131.107.2.255 (in CIDR format) under Firewall, Address Ranges. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security QUESTION 16 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 4 Another administrator attempts to establish connectivity between two virtual networks named VNET1 and VNET2. The administrator reports that connections across the virtual networks fail. You need to ensure that network connections can be established successfully between VNET1 and VNET2 as quickly as possible. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can connect one VNet to another VNet using either a Virtual network peering, or an Azure VPN Gateway. To create a virtual network gateway Step 1: In the portal, on the left side, click +Create a resource and type 'virtual network gateway' in search. Locate Virtual network gateway in the search return and click the entry. On the Virtual network gateway page, click Create at the bottom of the page to open the Create virtual network gateway page. Step 2: On the Create virtual network gateway page, fill in the values for your virtual network gateway. Name: Name your gateway. This is not the same as naming a gateway subnet. It's the name of the gateway object you are creating. Gateway type: Select VPN. VPN gateways use the virtual network gateway type VPN. Virtual network: Choose the virtual network to which you want to add this gateway. Click Virtual network to open the 'Choose a virtual network' page. Select the VNet. If you don't see your VNet, make sure the Location field is pointing to the region in which your virtual network is located. Gateway subnet address range: You will only see this setting if you did not previously create a gateway subnet for your virtual network. If you previously created a valid gateway subnet, this setting will not appear. Step 4: Select Create New to create a Gateway subnet. Step 5: Click Create to begin creating the VPN gateway. The settings are validated and you'll see the \"Deploying Virtual network gateway\" tile on the dashboard. Creating a gateway can take up to 45 minutes. You may need to refresh your portal page to see the completed status. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager- portal? QUESTION 17 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 5 You plan to configure VM1 to be accessible from the Internet. You need to add a public IP address to the network interface used by VM1. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can add private and public IP addresses to an Azure network interface by completing the steps that follow. Step 1: In Azure portal, click More services > type virtual machines in the filter box, and then click Virtual machines. Step 2: In the Virtual machines pane, click the VM you want to add IP addresses to. Click Network interfaces in the virtual machine pane that appears, and then select the network interface you want to add the IP addresses to. In the example shown in the following picture, the NIC named myNIC from the VM named myVM is selected: Step 3: In the pane that appears for the NIC you selected, click IP configurations. Step 4: Click Create public IP address. Step 5: In the Create public IP address pane that appears, enter a Name, select an IP address assignment type, a Subscription, a Resource group, and a Location, then click Create, as shown in the following picture: References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-multiple-ip-addresses-portal QUESTION 18 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 6 You need to allow RDP connections over TCP port 3389 to VM1 from the Internet. The solution must prevent connections from the Internet over all other TCP ports. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Create a new network security group Step 2: Select your new network security group. Step 3: Select Inbound security rules. Under Add inbound security rule, enter the following Destination: Select Network security group, and then select the security group you created previously. Destination port ranges: 3389 Protocol: Select TCP References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-filter-network-traffic QUESTION 19 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 5 You plan to create 100 Azure virtual machines on each of the following three virtual networks: VNET1005a VNET1005b VNET1005c All the network traffic between the three virtual networks will be routed through VNET1005a. You need to create the virtual networks, and then to ensure that all the Azure virtual machines can connect to other virtual machines by using their private IP address. The solution must NOT require any virtual network gateways and must minimize the number of peerings. What should you do from the Azure portal before you configure IP routing? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Click Create a resource in the portal. Step 2: Enter Virtual network in the Search the Marketplace box at the top of the New pane that appears. Click Virtual network when it appears in the search results. Step 3: Select Classic in the Select a deployment model box in the Virtual Network pane that appears, then click Create. Step 4: Enter the following values on the Create virtual network (classic) pane and then click Create: Name: VNET1005a Address space: 10.0.0.0/16 Subnet name: subnet0 Resource group: Create new Subnet address range: 10.0.0.0/24 Subscription and location: Select your subscription and location. Step 5: Repeat steps 3-5 for VNET1005b (10.1.0.0/16, 10.1.0.0/24), and for VNET1005c 10.2.0.0/16, 10.2.0.0/24). References: https://docs.microsoft.com/en-us/azure/virtual-network/create-virtual-network-classic QUESTION 20 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 6 You plan to create several virtual machines in different availability zones, and then to configure the virtual machines for load balanced connections from the Internet. You need to create an IP address resource named ip1006 to support the planned load balancing solution. The solution must minimize costs. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: We should create a public IP address. Step 1: At the top, left corner of the portal, select + Create a resource. Step 2: Enter public ip address in the Search the Marketplace box. When Public IP address appears in the search results, select it. Step 3: Under Public IP address, select Create. Step 4: Enter, or select values for the following settings, under Create public IP address, then select Create: Name: ip1006 SKU: Basic SKU IP Version: IPv6 IP address assignment: Dynamic Subscription: Select appropriate Resource group: Select appropriate References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-public-ip-address QUESTION 21 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 8 You need to create a virtual network named VNET1008 that contains three subnets named subnet0, subnet1, and subnet2. The solution must meet the following requirements: Connections from any of the subnets to the Internet must be blocked. Connections from the Internet to any of the subnets must be blocked. The number of network security groups (NSGs) and NSG rules must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Click Create a resource in the portal. Step 2: Enter Virtual network in the Search the Marketplace box at the top of the New pane that appears. Click Virtual network when it appears in the search results. Step 3: Select Classic in the Select a deployment model box in the Virtual Network pane that appears, then click Create. Step 4: Enter the following values on the Create virtual network (classic) pane and then click Create: Name: VNET1008 Address space: 10.0.0.0/16 Subnet name: subnet0 Resource group: Create new Subnet address range: 10.0.0.0/24 Subscription and location: Select your subscription and location. Step 5: In the portal, you can create only one subnet when you create a virtual network. Click Subnets (in the SETTINGS section) on the Create virtual network (classic) pane that appears. Click +Add on the VNET1008 - Subnets pane that appears. Step 6: Enter subnet1 for Name on the Add subnet pane. Enter 10.0.1.0/24 for Address range. Click OK. Step 7: Create the third subnet: Click +Add on the VNET1008 - Subnets pane that appears. Enter subnet2 for Name on the Add subnet pane. Enter 10.0.2.0/24 for Address range. Click OK. References: https://docs.microsoft.com/en-us/azure/virtual-network/create-virtual-network-classic QUESTION 22 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Azure Network Watcher, you create a connection monitor. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: References: https://azure.microsoft.com/en-us/updates/general-availability-azure-network-watcher-connection-monitor- in-all-public-regions/ QUESTION 23 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Azure Network Watcher, you create a packet capture. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Use the Connection Monitor feature of Azure Network Watcher. References: https://azure.microsoft.com/en-us/updates/general-availability-azure-network-watcher-connection-monitor- in-all-public-regions/ QUESTION 24 HOTSPOT Subscription1 contains the virtual machines in the following table. In Subscription1, you create a load balancer that has the following configurations: Name: LB1 SKU: Basic Type: Internal Subnet: Subnet12 Virtual network: VNET1 For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-overview QUESTION 25 You have an Azure subscription named Subscription1 that contains two Azure virtual networks named VNet1 and VNet2. VNet1 contains a VPN gateway named VPNGW1 that uses static routing. There is a site- to-site VPN connection between your on-premises network and VNet1. On a computer named Client1 that runs Windows 10, you configure a point-to-site VPN connection to VNet1. You configure virtual network peering between VNet1 and VNet2. You verify that you can connect to VNet2 from the on-premises network. Client1 is unable to connect to VNet2. You need to ensure that you can connect Client1 to VNet2. What should you do? A. Select Allow gateway transit on VNet2. B. Enable BGP on VPNGW1. C. Select Allow gateway transit on VNet1. D. Download and re-install the VPN client configuration package on Client1. Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-point-to-site-routing QUESTION 26 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Performance Monitor, you create a Data Collector Set (DCS). Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Use the Connection Monitor feature of Azure Network Watcher. References: https://docs.microsoft.com/en-us/azure/network-watcher/network-watcher-monitoring-overview QUESTION 27 HOTSPOT You have a virtual network named VNet1 that has the configuration shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: add an address space Your IaaS virtual machines (VMs) and PaaS role instances in a virtual network automatically receive a private IP address from a range that you specify, based on the address space of the subnet they are connected to. We need to add the 192.168.1.0/24 address space. Box 2: add a network interface The 10.2.1.0/24 network exists. We need to add a network interface. References: https://docs.microsoft.com/en-us/office365/enterprise/designing-networking-for-microsoft-azure-iaas https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-static-private-ip-arm-pportal QUESTION 28 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains the virtual machines in the following table. Subscription1 contains a virtual network named VNet1 that has the subnets in the following table. VM3 has multiple network adapters, including a network adapter named NIC3. IP forwarding is enabled on NIC3. Routing is enabled on VM3. You create a route table named RT1 that contains the routes in the following table. You apply RT1 to Subnet1 and Subnet2. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: IP forwarding enables the virtual machine a network interface is attached to: Receive network traffic not destined for one of the IP addresses assigned to any of the IP configurations assigned to the network interface. Send network traffic with a different source IP address than the one assigned to one of a network interface's IP configurations. The setting must be enabled for every network interface that is attached to the virtual machine that receives traffic that the virtual machine needs to forward. A virtual machine can forward traffic whether it has multiple network interfaces or a single network interface attached to it. Box 1: Yes The routing table allows connections from VM3 to VM1 and VM2. And as IP forwarding is enabled on VM3, VM3 can connect to VM1. Box 2: No VM3, which has IP forwarding, must be turned on, in order for VM2 to connect to VM1. Box 3: Yes The routing table allows connections from VM1 and VM2 to VM3. IP forwarding on VM3 allows VM1 to connect to VM2 via VM3. References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-udr-overview https://www.quora.com/What-is-IP-forwarding QUESTION 29 You have an Azure subscription that contains the resources in the following table. VM1 and VM2 are deployed from the same template and host line-of-business applications accessed by using Remote Desktop. You configure the network security group (NSG) shown in the exhibit. (Click the Exhibit tab.) You need to prevent users of VM2 and VM2 from accessing websites on the Internet over TCP port 80. What should you do? A. Change the DenyWebSites outbound security rule. B. Change the Port_80 inbound security rule. C. Disassociate the NSG from a network interface. D. Associate the NSG to Subnet1. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: You can associate or dissociate a network security group from a network interface or subnet. The NSG has the appropriate rule to block users from accessing the Internet. We just need to associate it with Subnet1. References: https://docs.microsoft.com/en-us/azure/virtual-network/manage-network-security-group QUESTION 30 HOTSPOT You are creating an Azure load balancer. You need to add an IPv6 load balancing rule to the load balancer. How should you complete the Azure PowerShell script? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-ipv6-internet-ps QUESTION 31 HOTSPOT You have an Azure subscription named Subscription1 that contains a resource group named RG1. In RG1, you create an internal load balancer named LB1 and a public load balancer named LB2. You need to ensure that an administrator named Admin1 can manage LB1 and LB2. The solution must follow the principle of least privilege. Which role should you assign to Admin1 for each task? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 32 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 7 You plan to allow connections between the VNET01-USEA2 and VNET01-USWE2 virtual networks. You need to ensure that virtual machines can communicate across both virtual networks by using their private IP address. The solution must NOT require any virtual network gateways. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Virtual network peering enables you to seamlessly connect two Azure virtual networks. Once peered, the virtual networks appear as one, for connectivity purposes. Peer virtual networks Step 1. In the Search box at the top of the Azure portal, begin typing VNET01-USEA2. When VNET01- USEA2 appears in the search results, select it. Step 2. Select Peerings, under SETTINGS, and then select + Add, as shown in the following picture: Step 3. Enter, or select, the following information, accept the defaults for the remaining settings, and then select OK. Name: myVirtualNetwork1-myVirtualNetwork2 (for example) Subscription: elect your subscription. Virtual network: VNET01-USWE2 - To select the VNET01-USWE2 virtual network, select Virtual network, then select VNET01-USWE2. You can select a virtual network in the same region or in a different region. Now we need to repeat steps 1-3 for the other network VNET01-USWE2: Step 4. In the Search box at the top of the Azure portal, begin typing VNET01- USEA2. When VNET01- USEA2 appears in the search results, select it. Step 5. Select Peerings, under SETTINGS, and then select + Add. References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-connect-virtual-networks-portal QUESTION 33 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 8 You plan to host several secured websites on Web01. You need to allow HTTPS over TCP port 443 to Web01 and to prevent HTTP over TCP port 80 to Web01. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can filter network traffic to and from Azure resources in an Azure virtual network with a network security group. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. Step A: Create a network security group A1. Search for and select the resource group for the VM, choose Add, then search for and select Network security group. A2. Select Create. The Create network security group window opens. A3. Create a network security group Enter a name for your network security group. Select or create a resource group, then select a location. A4. Select Create to create the network security group. Step B: Create an inbound security rule to allows HTTPS over TCP port 443 B1. Select your new network security group. B2. Select Inbound security rules, then select Add. B3. Add inbound rule B4. Select Advanced. From the drop-down menu, select HTTPS. You can also verify by clicking Custom and selecting TCP port, and 443. B5. Select Add to create the rule. Repeat step B2-B5 to deny TCP port 80 B6. Select Inbound security rules, then select Add. B7. Add inbound rule B8. Select Advanced. Clicking Custom and selecting TCP port, and 80. B9. Select Deny. Step C: Associate your network security group with a subnet Your final step is to associate your network security group with a subnet or a specific network interface. C1. In the Search resources, services, and docs box at the top of the portal, begin typing Web01. When the Web01 VM appears in the search results, select it. C2. Under SETTINGS, select Networking. Select Configure the application security groups, select the Security Group you created in Step A, and then select Save, as shown in the following picture: References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-filter-network-traffic Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 HOTSPOT You are evaluating the name resolution for the virtual machines after the planned implementation of the Azure networking infrastructure. For each of the following statements, select Yes if the statement is true. Otherwise, select No. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes All client computers in the Paris office will be joined to an Azure AD domain. A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 Box 2: Yes A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Box 3: No Only VMs in the registration network, here the ClientResources-VNet, will be able to register hostname records. References: https://docs.microsoft.com/en-us/azure/dns/private-dns-overview Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a consulting company that has a main office in Montreal and two branch offices in Seattle and New York. The Montreal office has 2,000 employees. The Seattle office has 1,000 employees. The New York office has 200 employees. All the resources used by Contoso are hosted on-premises. Contoso creates a new Azure subscription. The Azure Active Directory (Azure AD) tenant uses a domain named contoso.onmicrosoft.com. The tenant uses the P1 pricing tier. Existing Environment The network contains an Active Directory forest named contoso.com. All domain controllers are configured as DNS servers and host the contoso.com DNS zone. Contoso has finance, human resources, sales, research, and information technology departments. Each department has an organizational unit (OU) that contains all the accounts of that respective department. All the user accounts have the department attribute set to their respective department. New users are added frequently. Contoso.com contains a user named User1. All the offices connect by using private links. Contoso has data centers in the Montreal and Seattle offices. Each data center has a firewall that can be configured as a VPN device. All infrastructure servers are virtualized. The virtualization environment contains the servers in the following table. Contoso uses two web applications named App1 and App2. Each instance on each web application requires 1GB of memory. The Azure subscription contains the resources in the following table. The network security team implements several network security groups (NSGs). Planned Changes Contoso plans to implement the following changes: Deploy Azure ExpressRoute to the Montreal office. Migrate the virtual machines hosted on Server1 and Server2 to Azure. Synchronize on-premises Active Directory to Azure Active Directory (Azure AD). Migrate App1 and App2 to two Azure web apps named WebApp1 and WebApp2. Technical requirements Contoso must meet the following technical requirements: Ensure that WebApp1 can adjust the number of instances automatically based on the load and can scale up to five instances. Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. Ensure that routing information is exchanged automatically between Azure and the routers in the Montreal office. Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. Ensure that webapp2.azurewebsites.net can be accessed by using the name app2.contoso.com Connect the New York office to VNet1 over the Internet by using an encrypted connection. Create a workflow to send an email message when the settings of VM4 are modified. Create a custom Azure role named Role1 that is based on the Reader role. Minimize costs whenever possible. QUESTION 1 HOTSPOT You need to meet the connection requirements for the New York office. What should you do? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Create a virtual network gateway and a local network gateway. Azure VPN gateway. The VPN gateway service enables you to connect the VNet to the on-premises network through a VPN appliance. For more information, see Connect an on-premises network to a Microsoft Azure virtual network. The VPN gateway includes the following elements: Virtual network gateway. A resource that provides a virtual VPN appliance for the VNet. It is responsible for routing traffic from the on-premises network to the VNet. Local network gateway. An abstraction of the on-premises VPN appliance. Network traffic from the cloud application to the on-premises network is routed through this gateway. Connection. The connection has properties that specify the connection type (IPSec) and the key shared with the on-premises VPN appliance to encrypt traffic. Gateway subnet. The virtual network gateway is held in its own subnet, which is subject to various requirements, described in the Recommendations section below. Box 2: Configure a site-to-site VPN connection On premises create a site-to-site connection for the virtual network gateway and the local network gateway. Scenario: Connect the New York office to VNet1 over the Internet by using an encrypted connection. Incorrect Answers: Azure ExpressRoute: Established between your network and Azure, through an ExpressRoute partner. This connection is private. Traffic does not go over the internet. References: https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/hybrid-networking/vpn Question Set 1 QUESTION 1 You have an Azure Active Directory (Azure AD) tenant named contoso.onmicrosoft.com. The User administrator role is assigned to a user named Admin1. An external partner has a Microsoft account that uses the user1@outlook.com sign in. Admin1 attempts to invite the external partner to sign in to the Azure AD tenant and receives the following error message: \u201cUnable to invite user user1@outlook.com \u2013 Generic authorization exception.\u201d You need to ensure that Admin1 can invite the external partner to sign in to the Azure AD tenant. What should you do? A. From the Roles and administrators blade, assign the Security administrator role to Admin1. B. From the Organizational relationships blade, add an identity provider. C. From the Custom domain names blade, add a custom domain. D. From the Users blade, modify the External collaboration settings. Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://techcommunity.microsoft.com/t5/Azure-Active-Directory/Generic-authorization-exception-inviting- Azure-AD-gests/td-p/274742 QUESTION 2 Your company has an Azure Active Directory (Azure AD) tenant named contoso.com that is configured for hybrid coexistence with the on-premises Active Directory domain. The tenant contains the users shown in the following table. Whenever possible, you need to enable Azure Multi-Factor Authentication (MFA) for the users in contoso.com. Which users should you enable for Azure MFA? A. User1 only B. User1, User2, and User3 only C. User1 and User2 only D. User1, User2, User3, and User4 E. User2 only Correct Answer: D Section: [none] Explanation Explanation/Reference: QUESTION 3 You have two Azure Active Directory (Azure AD) tenants named contoso.com and fabrikam.com. You have a Microsoft account that you use to sign in to both tenants. You need to configure the default sign-in tenant for the Azure portal. What should you do? A. From Azure Cloud Shell, run Set-AzureRmSubscription. B. From Azure Cloud Shell, run Set-AzureRmContext. C. From the Azure portal, configure the portal settings. D. From the Azure portal, change the directory. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Set-AzureRmContext cmdlet sets authentication information for cmdlets that you run in the current session. The context includes tenant, subscription, and environment information. References: https://docs.microsoft.com/en-us/powershell/module/azurerm.profile/set-azurermcontext QUESTION 4 You have an Azure Active Directory (Azure AD) tenant. All administrators must enter a verification code to access the Azure portal. You need to ensure that the administrators can access the Azure portal only from your on-premises network. What should you configure? A. an Azure AD Identity Protection user risk policy. B. the multi-factor authentication service settings. C. the default for all the roles in Azure AD Privileged Identity Management D. an Azure AD Identity Protection sign-in risk policy Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 5 You have an Active Directory forest named contoso.com. You install and configure Azure AD Connect to use password hash synchronization as the single sign-on (SSO) method. Staging mode is enabled. You review the synchronization results and discover that the Synchronization Service Manager does not display any sync jobs. You need to ensure that the synchronization completes successfully. What should you do? A. Run Azure AD Connect and set the SSO method to Pass-through Authentication. B. From Synchronization Service Manager, run a full import. C. From Azure PowerShell, run Start-AdSyncSyncCycle \u2013PolicyType Initial. D. Run Azure AD Connect and disable staging mode. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Staging mode must be disabled. If the Azure AD Connect server is in staging mode, password hash synchronization is temporarily disabled. References: https://docs.microsoft.com/en-us/azure/active-directory/connect/active-directory-aadconnectsync- troubleshoot-password-hash-synchronization#no-passwords-are-synchronized-troubleshoot-by-using-the- troubleshooting-task QUESTION 6 You have an Azure Active Directory (Azure AD) tenant named contoso.onmicrosoft.com. You hire a temporary vendor. The vendor uses a Microsoft account that has a sign-in of user1@outlook.com. You need to ensure that the vendor can authenticate to the tenant by using user1@outlook.com. What should you do? A. From the Azure portal, add a custom domain name, create a new Azure AD user, and then specify user1@outlook.com as the username. B. From Azure Cloud Shell, run the New-AzureADUser cmdlet and specify the \u2013UserPrincipalName user1@outlook.com parameter. C. From the Azure portal, add a new guest user, and then specify user1@outlook.com as the email address. D. From Windows PowerShell, run the New-AzureADUser cmdlet and specify the \u2013UserPrincipalName user1@outlook.com parameter. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: UserPrincipalName - contains the UserPrincipalName (UPN) of this user. The UPN is what the user will use when they sign in into Azure AD. The common structure is @, so for Abby Brown in Contoso.com, the UPN would be AbbyB@contoso.com Example: To create the user, call the New-AzureADUser cmdlet with the parameter values: powershell New-AzureADUser -AccountEnabled $True -DisplayName \"Abby Brown\" -PasswordProfile $PasswordProfile -MailNickName \"AbbyB\" -UserPrincipalName \"AbbyB@contoso.com\" References: https://docs.microsoft.com/bs-cyrl-ba/powershell/azure/active-directory/new-user-sample?view=azureadps- 2.0 QUESTION 7 You have an Azure Active Directory (Azure AD) tenant named contosocloud.onmicrosoft.com. Your company has a public DNS zone for contoso.com. You add contoso.com as a custom domain name to Azure AD. You need to ensure that Azure can verify the domain name. Which type of DNS record should you create? A. MX B. SRV C. DNSKEY D. NSEC Correct Answer: A Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/dns/dns-web-sites-custom-domain QUESTION 8 You set the multi-factor authentication status for a user named admin1@contoso.com to Enabled. Admin1 accesses the Azure portal by using a web browser. Which additional security verifications can Admin1 use when accessing the Azure portal? A. a phone call, a text message that contains a verification code, and a notification or a verification code sent from the Microsoft Authenticator app B. an app password, a text message that contains a verification code, and a notification sent from the Microsoft Authenticator app C. an app password, a text message that contains a verification code, and a verification code sent from the Microsoft Authenticator app D. a phone call, an email message that contains a verification code, and a text message that contains an app password Correct Answer: A Section: [none] Explanation Explanation/Reference: QUESTION 9 DRAG DROP You have an Azure Active Directory (Azure AD) tenant that has the initial domain name. You have a domain name of contoso.com registered at a third-party registrar. You need to ensure that you can create Azure AD users that have names containing a suffix of @contoso.com. Which three actions should you perform in sequence? To answer, move the appropriate cmdlets from the list of cmdlets to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/dns/dns-web-sites-custom-domain QUESTION 10 Your company has a main office in London that contains 100 client computers. Three years ago, you migrated to Azure Active Directory (Azure AD). The company\u2019s security policy states that all personal devices and corporate-owned devices must be registered or joined to Azure AD. A remote user named User1 is unable to join a personal device to Azure AD from a home network. You verify that other users can join their devices to Azure AD. You need to ensure that User1 can join the device to Azure AD. What should you do? A. From the Device settings blade, modify the Users may join devices to Azure AD setting. B. From the Device settings blade, modify the Maximum number of devices per user setting. C. Create a point-to-site VPN from the home network of User1 to Azure. D. Assign the User administrator role to User1. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Maximum number of devices setting enables you to select the maximum number of devices that a user can have in Azure AD. If a user reaches this quota, they will not be able to add additional devices until one or more of the existing devices are removed. Incorrect Answers: A: The Users may join devices to Azure AD setting enables you to select the users who can join devices to Azure AD. Options are All, Selected and None. The default is All. C: Azure AD Join enables users to join their devices to Active Directory from anywhere as long as they have connectivity with the Internet. References: https://docs.microsoft.com/en-us/azure/active-directory/devices/device-management-azure-portal http://techgenix.com/pros-and-cons-azure-ad-join/ QUESTION 11 You have an Azure DNS zone named adatum.com. You need to delegate a subdomain named research.adatum.com to a different DNS server in Azure. What should you do? A. Create an A record named *.research in the adatum.com zone. B. Create a PTR record named research in the adatum.com zone. C. Modify the SOA record of adatum.com. D. Create an NS record named research in the adatum.com zone. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: You need to create a name server (NS) record for the zone. References: https://docs.microsoft.com/en-us/azure/dns/delegate-subdomain QUESTION 12 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 7 You plan to deploy several Azure virtual machines and to connect them to a virtual network named VNET1007. You need to ensure that future virtual machines on VNET1007 can register their name in an internal DNS zone named corp9172795.com. The zone must NOT be hosted on a virtual machine. What should you do from Azure Cloud Shell? To complete this task, start Azure Cloud Shell and select PowerShell (Linux). Click Show Advanced Settings, and then enter corp9172795n1 in the Storage account text box and File1 in the File share text box. Click Create storage, and then complete the task. Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: New-AzureRMResourceGroup -name MyResourceGroup Before you create the DNS zone, create a resource group to contain the DNS zone. Step 2: New-AzureRmDnsZone -Name corp9172795.com -ResourceGroupName MyResourceGroup A DNS zone is created by using the New-AzureRmDnsZone cmdlet. This creates a DNS zone called corp9172795.com in the resource group called MyResourceGroup. References: https://docs.microsoft.com/en-us/azure/dns/dns-getstarted-powershell QUESTION 13 HOTSPOT You have an Azure Active Directory (Azure AD) tenant named adatum.com. Adatum.com contains the groups in the following table: You create two user accounts that are configured as shown in the following table. To which groups do User1 and User2 belong? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Group 1 only First rule applies Box 2: Group1 and Group2 only Both membership rules apply. References: https://docs.microsoft.com/en-us/sccm/core/clients/manage/collections/create-collections Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a manufacturing company that has offices worldwide. Contoso works with partner organizations to bring products to market. Contoso products are manufactured by using blueprint files that the company authors and maintains. Existing Environment Currently, Contoso uses multiple types of servers for business operations, including the following: File servers Domain controllers Microsoft SQL Server servers Your network contains an Active Directory forest named contoso.com. All servers and client computers are joined to Active Directory. You have a public-facing application named App1. App1 is comprised of the following three tiers: A SQL database A web front end A processing middle tier Each tier is comprised of five virtual machines. Users access the web front end by using HTTPS only. Requirements Planned Changes Contoso plans to implement the following changes to the infrastructure: Move all the tiers of App1 to Azure. Move the existing product blueprint files to Azure Blob storage. Create a hybrid directory to support an upcoming Microsoft Office 365 migration project. Technical Requirements Contoso must meet the following technical requirements: Move all the virtual machines for App1 to Azure. Minimize the number of open ports between the App1 tiers. Ensure that all the virtual machines for App1 are protected by backups. Copy the blueprint files to Azure over the Internet. Ensure that the blueprint files are stored in the archive storage tier. Ensure that partner access to the blueprint files is secured and temporary. Prevent user passwords or hashes of passwprds from being stored in Azure. Use unmanaged standard storage for the hard disks of the virtual machines. Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity Minimize administrative effort whenever possible User Requirements Contoso identifies the following requirements for users: Ensure that only users who are part of a group named Pilot can join devices to Azure AD. Designate a new user named Admin1 as the service admin for the Azure subscription. Admin1 must receive email alerts regarding service outages. Ensure that a new user named User3 can create network objects for the Azure subscription. QUESTION 1 HOTSPOT You need to configure the Device settings to meet the technical requirements and the user requirements. Which two settings should you modify? To answer, select the appropriate settings in the answer area. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Selected Only selected users should be able to join devices Box 2: Yes Require Multi-Factor Auth to join devices. From scenario: Ensure that only users who are part of a group named Pilot can join devices to Azure AD Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity. QUESTION 2 You need to recommend a solution to automate the configuration for the finance department users. The solution must meet the technical requirements. What should you include in the recommendation? A. Azure AD B2C B. Azure AD Identity Protection C. an Azure logic app and the Microsoft Identity Management (MIM) client D. dynamic groups and conditional access policies Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Scenario: Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. The recommendation is to use conditional access policies that can then be targeted to groups of users, specific applications, or other conditions. References: https://docs.microsoft.com/en-us/azure/active-directory/authentication/howto-mfa-userstates QUESTION 3 HOTSPOT You need to implement Role1. Which command should you run before you create Role1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 You need to prepare the environment to meet the authentication requirements. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Install the Active Directory Federation Services (AD FS) role on a domain controller in the Miami office. B. Allow inbound TCP port 8080 to the domain controllers in the Miami office. C. Join the client computers in the Miami office to Azure AD. D. Add http://autologon.microsoftazuread-sso.com to the intranet zone of each client computer in the Miami office. E. Install Azure AD Connect on a server in the Miami office and enable Pass-through Authentication. Correct Answer: DE Section: [none] Explanation Explanation/Reference: Explanation: D: You can gradually roll out Seamless SSO to your users. You start by adding the following Azure AD URL to all or selected users' Intranet zone settings by using Group Policy in Active Directory: https:// autologon.microsoftazuread-sso.com E: Seamless SSO works with any method of cloud authentication - Password Hash Synchronization or Pass-through Authentication, and can be enabled via Azure AD Connect. Incorrect Answers: A: Seamless SSO is not applicable to Active Directory Federation Services (ADFS). B: Azure AD connect does not port 8080. It uses port 443. C: Seamless SSO needs the user's device to be domain-joined, but doesn't need for the device to be Azure AD Joined. Scenario: Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. Planned Azure AD Infrastructure include: The on-premises Active Directory domain will be synchronized to Azure AD. References: https://docs.microsoft.com/en-us/azure/active-directory/connect/active-directory-aadconnect-sso-quick-start (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Microsoft Azure Administrator"},{"location":"nightwolf-cotribution/dbms_interview_questions/","text":"TOP 20 DBMS Interview Questions \uf0c1 We have consolidated a list of frequently asked DBMS (Database Management Systems) interview questions for Freshers and Experienced Engineers. You will find these questions very helpful in your interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Basic DBMS Interview Questions \uf0c1 What is meant by DBMS and what is its utility? Explain RDBMS with examples. What is meant by a database? Mention the issues with traditional file-based systems that make DBMS a better choice? Explain a few advantages of a DBMS. Explain different languages present in DBMS. What is meant by ACID properties in DBMS? Are NULL values in a database the same as that of blank space or zero? (adsbygoogle = window.adsbygoogle || []).push({}); Intermediate DBMS Interview Questions \uf0c1 What is meant by Data Warehousing? Explain different levels of data abstraction in a DBMS. What is meant by an entity-relationship (E-R) model? Explain the terms Entity, Entity Type, and Entity Set in DBMS. Explain different types of relationships amongst tables in a DBMS. Explain the difference between intension and extension in a database. Explain the difference between the DELETE and TRUNCATE command in a DBMS. What is a lock. Explain the major difference between a shared lock and an exclusive lock during a transaction in a database. What is meant by normalization and denormalization? (adsbygoogle = window.adsbygoogle || []).push({}); Advanced DBMS Interview Questions \uf0c1 Explain different types of Normalization forms in a DBMS. Explain different types of keys in a database. Explain the difference between a 2-tier and 3-tier architecture in a DBMS. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"DBMS Interview Questions and Answers"},{"location":"nightwolf-cotribution/dbms_interview_questions/#top-20-dbms-interview-questions","text":"We have consolidated a list of frequently asked DBMS (Database Management Systems) interview questions for Freshers and Experienced Engineers. You will find these questions very helpful in your interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({});","title":"TOP 20 DBMS Interview Questions"},{"location":"nightwolf-cotribution/dbms_interview_questions/#basic-dbms-interview-questions","text":"What is meant by DBMS and what is its utility? Explain RDBMS with examples. What is meant by a database? Mention the issues with traditional file-based systems that make DBMS a better choice? Explain a few advantages of a DBMS. Explain different languages present in DBMS. What is meant by ACID properties in DBMS? Are NULL values in a database the same as that of blank space or zero? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Basic DBMS Interview Questions"},{"location":"nightwolf-cotribution/dbms_interview_questions/#intermediate-dbms-interview-questions","text":"What is meant by Data Warehousing? Explain different levels of data abstraction in a DBMS. What is meant by an entity-relationship (E-R) model? Explain the terms Entity, Entity Type, and Entity Set in DBMS. Explain different types of relationships amongst tables in a DBMS. Explain the difference between intension and extension in a database. Explain the difference between the DELETE and TRUNCATE command in a DBMS. What is a lock. Explain the major difference between a shared lock and an exclusive lock during a transaction in a database. What is meant by normalization and denormalization? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Intermediate DBMS Interview Questions"},{"location":"nightwolf-cotribution/dbms_interview_questions/#advanced-dbms-interview-questions","text":"Explain different types of Normalization forms in a DBMS. Explain different types of keys in a database. Explain the difference between a 2-tier and 3-tier architecture in a DBMS. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Advanced DBMS Interview Questions"},{"location":"nightwolf-cotribution/devops_interview_questions-2/","text":"DevOps Interview Question and Answered for Freshers and Experienced - 2 \uf0c1 We have consolidated a list of frequently asked DevOps interview questions for Freshers and Experienced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); List of Docker components? 1. Docker image: \u2013 Contains OS (very small) (almost negligible) + soft wares 2. Docker Container: \u2013 Container like a machine which is created from Docker image. 3. Docker file: \u2013 Describes steps to create a docker image. 4. Docker hub/registry: \u2013 Stores all docker images publicly. 5. Docker daemon: \u2013 Docker service runs at back end Above five components we call as Docker components What is Docker workflow? First we create Docker file by mentioning instructions to build docker image. Form this Docker image, we are going to create Docker container. This Docker image we can push to docker hub as well. This image can be pulled by others to create docker containers. We can create docker images from docker containers. Like this we can create Docker images form either docker file or docker containers. We can create docker containers from docker images. This is the work flow of docker. Sample Docker file instructions? FROM ubuntu WORKDIR /tmp RUN echo \u201cHello\u201d > /tmp/testfile ENV myname user1 COPY testfile1 /tmp ADD test.tar.gz /tmp What is the importance of volumes in Docker? 1. Volume is a directory inside your container \u2000 2. First declare directory as a volume and then share volume \u2000 3. Even if we stop container, still we can access volume \u2000 4. Volume will be created in one container \u2000 5. You can share one volume across any no of containers \u2000 6. Volume will not be included when you update an image \u2000 7. Map volumes in two ways \u2000 8. Share host \u2013 container \u2000 9. Share container \u2013 container What do you mean by port mapping in Docker? Suppose if you want to make any container as web server by installing web package in it, you need to provide containers IP address to public in order to access website which is running inside docker container. But Docker containers don\u2019t have an IP address. So, to address this issue, we have a concept called Docker port mapping. We map host port with container port and customers use public IP of host machine. Then their request will be routed from host port to container\u2019s port and will be loaded webpage which is running inside docker container. This is how we can access website which is running inside container through port mapping. What is Registry server in Docker? Registry server is our own docker hub created to store private docker images instead of storing in public Docker hub. Registry server is one of the docker containers. We create this Registry server from \u201cregistry\u201d image, especially provided by docker to create private docker hub. We can store any no of private docker images in this Registry server. We can give access to others, so that, they also can store their docker images whomever you provide access. Whenever we want, we can pull these images and can create containers out of these images. Important docker commands? 1. Docker ps (to see list of running containers) 2. Docker ps -a (to see list of all containers) 3. Docker images (to see list of all images) 4. Docker run (to create docker container) 5. Docker attach (to go inside container) 6. Docker stop (to stop container) 7. Docker start (to start container) 8. Docker commit (to create image out of docker file) 9. Docker rm (to delete container) 10. Docker rmi (to delete image) What is Ansible? Ansible is one of the configuration Management Tools. It is a method through we automate system admin tasks. Configuration refers to each and every minute details of a system. If we do any changes in system means we are changing the configuration of a machine. That means we are changing the configuration of the machine. All windows/Linux system administrators manage the configuration of a machine manually. All DevOps engineers are managing this configuration automatic way by using some tools which are available in the market. One such tool is Ansible. That\u2019s why we call Ansible as configuration management tool. Working process of Ansible? Here we crate file called playbook and inside playbook we write script in YAML format to create infrastructure. Once we execute this playbook, automatically code will be converted into Infrastructure. We call this process as IAC (Infrastructure as Code). We have open source and enterprise editions of Ansible. Enterprise edition we call Ansible Tower. The architecture of Ansible? We create Ansible server by installing Ansible package in it. Python is pre-requisite to install ansible. We need not to install ansible package in nodes. Because, communication establishes from server to node through \u201cssh\u201d client. By default all Linux machine will have \u201cssh\u201d client. Server is going to push the code to nodes that we write in playbooks. So Ansible follows pushing mechanism. Ansible components? 1. Server:\u2013 It is the place where we create playbooks and write code in YML format 2. Node:\u2013 It is the place where we apply code to create infrastructure. Server pushes code to nodes. 3. SSH:\u2013 It is an agent through ansible server pushes code to nodes. 4. Setup:\u2013 It is a module in ansible which gathers nodes information. 5. Inventory file:- In this file we keep IP/DNS of nodes. Disadvantages in other SCM (Source Code Management) tools? \u2000 - Huge overhead of Infrastructure setup \u2000 - Complicated setup \u2000 - Pull mechanism \u2000 - Lot of learning required Advantages of Ansible over other SCM (Source Code Management) tools? \u2000 - Agentless \u2000 - Relies on \u201cssh\u201d \u2000 - Uses python \u2000 - Push mechanism How does Ansible work? We give nodes IP addresses in hosts file by creating any group in ansible server why because, ansible doesn\u2019t recognize individual IP addresses of nodes. We create playbook and write code in YAML script. The group name we have to mention in a playbook and then we execute the playbook. By default, playbook will be executed in all those nodes which are under this group. This is how ansible converts code into infrastructure. What do you mean by Ad-Hoc commands in Ansible? These are simple one liner Linux commands we use to meet temporary requirements without actually saving for later. Here we don\u2019t use ansible modules. So there, Idempotency will not work with Ad-Hoc commands. If at all we don\u2019t get required YAML module to write to create infrastructure, then we go for it. Without using playbooks we can use these Ad-Hoc commands for temporary purpose. Differences between Chef and Ansible? Ansible Chef Playbook Recipe Module Resource Host Node Setup Ohai SSH Knife mechanism: Push mechanism: Pull What is Playbook in Ansible? Playbook is a file where we write YAML script to create infrastructure in nodes. Here, we use modules to create infrastructure. We create so many sections in playbook. We mention all modules in task section. You can create any no of playbooks. There is no limit. Each playbook defines one scenario. All sections begin with \u201c-\u201d & its attributes & parameters beneath it. Mention some list of sections that we mention in Playbook? 1. Target section 2. Task section 3. Variable section 4. Handler section What is Target section in Ansible playbook? This is one of the important sections in Playbook. In this section, we mention the group name which contains either IP addresses or Hostnames of nodes. When we execute playbook, then code will be pushed too all nodes which are there in the group that we mention in Target section. We use \u201call\u201d key word to refer all groups. What is Task section in Ansible playbook? This is second most important section in playbook after target section. In this section, we are going to mention list of all modules. All tasks we mention in this task section. We can mention any no of modules in one playbook. There is no limit. If there is only one task, then instead of going with big playbook, simply we can go with arbitrary command where we can use one module at a time. If more than one module, then there is no option except going with big playbook. What is Variable section? In this section we are going to mention variables. Instead of hard coding, we can mention as variables so that during runtime it pulls the actual value in place of key. We have this concept in each and every programming language and scripting language. We use \u201cvars\u201d key word to use variables. What is Handler section? All tasks we mention in tasks section. But some tasks where dependency is there, we should not mention in tasks section. That is not good practice. For example, installing package is one task and starting service is one more task. But there is dependency between them. I.e. after installing package only, we have to start service. Otherwise it throws error. These kind of tasks, we mention in handler section. In above example, package task we mention in task section and service task we mention in handler section so that after installing task only service will be started. What is Dry run in playbook? Dry run is to test playbook. Before executing playbook in nodes, we can test whether the code in playbook is written properly or not. Dry run won\u2019t actually executes playbook, but it shows output as` if it executed playbook. Then by seeing the output, we can come to know whether the playbook is written properly or not. It checks whether the playbook is formatted correctly or not. It tests how the playbook is going to behave without running the tasks. Why are we using loops concept in Ansible? Sometimes we might need to deal with multiple tasks. For instance, Installing multiple packages, Creating many users, creation many groups..etc. In this case, mentioning module for every task is complex process. So, to address this issue, we have a concept of loops. We have to use variables in combination with loops. Where do we use conditionals in Playbooks? Sometimes, your nodes could be mixture of different flavors of Linux OS. Linux commands vary in different Linux operating systems. In this case, we can\u2019t execute common set of commands in all machines, at the same time, we can\u2019t execute different commands in each node separately. To address this issue, we have conditionals concept where commands will be executed based up on certain condition that we give. What is Ansible vault? Sometimes, we use sensitive information in playbooks like passwords, keys \u2026etc. So any one can open these playbooks and get to know about this sensitive information. So we have to protect our playbooks from being read by others. So by using Ansible vault, we encrypt playbooks so that, those who ever is having password, only those can read this information. It is the way of protecting playbooks by encrypting them. What do you mean by Roles in Ansible? Adding more & more functionality to the playbooks will make it difficult to maintain in a single file. To address this issue, we organize playbooks into a directory structure called \u201croles\u201d. We create separate file to each section and we just mention the names of those sections in playbook instead of mentioning all modules in main playbook. When you call main playbook, main playbook will call all sections files respectively in the order whatever order you mention in playbook. So, by using this Roles, we can maintain small playbook without any complexity. Write a sample playbook to install any package? \u2014 # My First YAML playbook \u2013 hosts: demo user: ansible become: yes connection: ssh tasks: \u2013 name: Install HTTPD on centos 7 action: yum name=httpd state=installed Write a sample playbook by mentioning variables instead of hard coding? \u2014 # My First YAML playbook \u2013 hosts: demo user: nightwolf become: yes connection: ssh vars: pkgname: httpd tasks: \u2013 name: Install HTTPD server on centos 7 action: yum name=\u2018{{pkgname}}\u2019 state=installed What is CI & CD? CI means Continues Integration and CD means Continues Delivery/Deploy. Whenever developers write code, we integrate all that code of all developers at that point of time and we build, test and deliver/deploy to the client. This process we call CI & CD. Jenkins helps in achieving this. So instead of doing night builds, build as and when commit occurs by integrating all code in SCM tool, build, test and checking the quality of that code is what we call Continues Integration. Key terminology that we use in Jenkins? - Integrate: Combine all code written by developers till some point of time. - Build: Compile the code and make a small executable package. - Test: Test in all environments whether application is working properly or not. - Archived: Stored in an artifactory so that in future we may use/deliver again. - Deliver: Handing the product to Client Deploy: Installing product in client\u2019s machines. What is Jenkins Workflow? We attach Git, Maven, Selenium & Artifactory plug-ins to Jenkins. Once Developers put the code in Git, Jenkins pulls that code and send to Maven for build. Once build is done, Jenkins pulls that built code and send to selenium for testing. Once testing is done, then Jenkins will pull that code and send to Artifactory as per requirement and finally we can deliver the end product to client we call Continues delivery. We can also deploy with Jenkins into clients machine directly as per the requirement. This is what Jenkins work flow. What are the ways through which we can do Continues Integration? There are total three ways through which we can do Continues Integration: 1. Manually:\u2013 Manually write code, then do build manually and then test manually by writing test cases and deploy manually into clients machine. 2. Scripts:\u2013 Can do above process by writing scripts so that these scripts do CI&CD automatically. here complexity is, writing script is not so easy. 3. Tool:\u2013 Using tools like Jenkins is very handy. Everything is preconfigured in these type of tools. So less manual intervention. This is the most preferred way. Benefits of CI? 1. Detects bugs as soon as possible, so that bug will be rectified fast and development happens fast. 2. Complete automation. No need manual intervention. 3. We can intervene manually whenever we want i.e. we can stop any stage at any point of time so have better control. 4. Can establish complete and continues work flow. Why only Jenkins? \u2000 - It has so many plug-ins. - You can write your own plug-in \u2000 - You can use community plug-ins \u2000 - Jenkins is not just a tool. It is a framework i.e. you can do what ever you want. All you need is plugins. - We can attach slaves to Jenkins master. It instructs others(slaves) to do Job. - If slaves are not available, Jenkins itself does the job. \u2000 - Jenkins also acts as cron server replacement i.e. can do repeated tasks automatically. \u2000 - Running some scripts regularly E.g.: Automatic daily alarm. \u2000 - Can create Labels (Group of slaves) (Can restrict where the project has to run). What is Jenkins Architecture? Jenkins architecture is Client-Server model. Where ever, we install Jenkins, we call that server is Jenkins master. We can also create slaves in Jenkins, so that server load will be distributed to slaves. Jenkins Master randomly assigns tasks to slaves. But if you want to restrict any job to run in particular slave, then we can do it, so that particular job will be executed in that slave only. We can group some slaves by using \u201cLabel\u201d. How to install Jenkins? - You can install Jenkins in any OS. All OSs supports Jenkins. We access Jenkins through web page only. That\u2019s why it doesn\u2019t make any difference whether you install Jenkins in Windows or Linux. \u2000 - Choose Long Term Support release version, so that you will get support from Jenkins community. If you are using Jenkins for testing purpose, you can choose weekly release. But for production environments, we prefer Long Term Support release version. - Need to install JAVA. Java is pre-requisite to install Jenkins. \u2000 - Need to install web package. Because, we are going to access Jenkins through web page only. Does Jenkins open source? There are two editions in Jenkins 1. Open source 2. Enterprise edition Open source edition we call Jenkins. Here we get support from community if we need it. Enterprise edition we call Hudson. Here Jenkins company will provide support. How many types of configurations in Jenkins? There are total 3 types of configurations in Jenkins: 1. Global:\u2013 Here, whatever configuration changes we do, applicable to whole Jenkins including jobs as well as nodes. This configuration has high priority. 2. Job:\u2013 These configurations applicable to only Jobs. Jobs also we call as projects or items in Jenkins. 3. Node:\u2013 These configurations applicable to only nodes. Also we call Slaves. These are kind of helpers to Jenkins master to distribute the excessive load. What do you mean by workspace in Jenkins? The workspace is the location on your computer where Jenkins places all files related to the Jenkins project. By default each project or job is assigned a workspace location and it contains Jenkins-specific project metadata, temporary files like logs and any build artifacts, including transient build files. Jenkins web page acts like a window through which we are actually doing work in workspace. List of Jenkins services? \u2000 - localhost:8080/restart (to restart Jenkins).\u2000 - localhost:8080/stop (to stop Jenkins). - localhost:8080/start (to start Jenkins). How to create a free style project in Jenkins? \u2000 - Create project by giving any name \u2000 - Select Free style project \u2000 - Click on build \u2000 - Select execute windows batch command \u2000 - Give any command (echo \u201cHello Dear Students!!\u201d) \u2000 - Select Save \u2000 - Click on Build now \u2000 - Finally can see Console output What do you mean by Plugins in Jenkins? \u2000 - With Jenkins, nearly everything is a plugin and that nearly all functionality is provided by plugins. You can think of Jenkins as little more than an executor of plugins. \u2000 - Plugins are small libraries that add new abilities to Jenkins and can provide integration points to other tools. \u2000 - Since nearly everything Jenkins does is because of a plugin, Jenkins ships with a small set of default plugins, some of which can be upgraded independently of Jenkins. How to create Maven Project? \u2000 - Select new item - Copy the git hub maven project link and paste in git section in Jenkins \u2000 - Select build \u2000 - Click on clean package \u2000 - Select save \u2000 - Click on Build now \u2000 - Verify workspace contents with GitHub sideSee console output How can we Schedule projects? Sometimes, we might need some jobs to be executed after frequent intervals. To schedule a job: - Click on any project - Click on Configure \u2000 - Select on Build triggers \u2000 - Click on Build periodically \u2000 - Give timing (In format : * * * * * ) \u2000 - Select Save \u2000 - Can see automatic builds every 1 min \u2000 - You can manually trigger build as well if you want. What do you mean by Upstream and Downstream projects? We can also call them as linked projects. These are the ways through which, we connect jobs one with other. In Upstream jobs, first job will trigger second job after build is over. In Downstream jobs, second job will wait till first job finishes its build. As and when first job finishes its work, then second job will be triggered automatically. In Upstream, first job will be active. In Downstream jobs, second job will be active. We can use any one type to link multiple jobs. What is view in Jenkins? We can customize view as per our needs. We can modify Jenkins home page. We can segregate jobs as per the type of jobs like free style jobs and maven jobs and so on. To create custom view: - Select List of Related Projects \u2000 - Select Default views \u2000 - Click on All \u2000 - Click on + and select Freestyle - Select List Views \u2000 - Select Job filter \u2000 - Select required jobs to be segregated \u2000 - Now, you can see different view What is User Administration in Jenkins? In Jenkins, we can create users, groups and can assign limited privileges to them so that, we can have better control on Jenkins. Users will not install Jenkins in their machines. They access Jenkins as a user. Here we can\u2019t assign permissions directly to users. Instead we create \u201cRoles\u201d and assign permissions to those roles. These roles we attach to users so that users get the permissions whatever we assign to those roles. What is Global tool configuration in Jenkins? We install Java, Maven, Git and many other tools in our server. Whenever Jenkins need those tools, by default Jenkins will install them automatically every time. But it\u2019s not a good practice. That\u2019s why we give installed path of all these tools in Jenkins so that whenever Jenkins need them, automatically Jenkins pull them form local machine instead of downloading every time. This way of giving path of these tools in Jenkins we call \u201cGlobal tool configuration\u201d What is Build? Build means, Compile the source code, assembling of all class files and finally creating deliverable Compile:\u2013 Convert Source code into machine-readable format Assembly (Linking):\u2013 Grouping all class files Deliverable:\u2013 .war, .jar The above process is same for any type of code. This process we call Build. What is Maven? Maven is one of the Build tools. It is the most advance build tool in the market. In this, everything is already pre-configured. Maven belongs to Apache Company. We use maven to build Java code only. We can\u2019t build other codes by using Maven. By default, we get so many plugins with Maven. You can write your own plugins as well. Maven\u2019s local repository is \u201c.M2\u201d where we can get required compilers and dependencies. Maven\u2019s main configuration file is \u201cpom.xml\u201d where we keep all instructions to build. Advantages of Maven? \u2000 - Automated tasks (Mention all in pom.xml) \u2000 - Multiple Tasks at a time \u2000 - Quality product \u2000 - Minimize bad builds - Keep history - Save time \u2013 Save money \u2000 - Gives set of standards - Gives define project life cycle (Goals) \u2000 - Manage all dependencies \u2000 - Uniformity in all projects \u2000 - Re-usability List of Build tools available in Market? \u2000 - C and C++: Make file \u2000 - .Net: Visual studio \u2000 - Java: Ant, Maven What is the architecture of Maven? Maven main configuration file is pom.xml. For one project, there will be one workspace and one pom.xml. Requirements for build:\u2013 - Source code (Will be pulled from Git hub) - Compiler (Pulls from remote repo and then put them in local repo, from there, maven pulls into Workspace) - Dependencies (Pulls from remote repo and then put them in local repo, from there, maven pulls into Workspace) What is Maven\u2019s Build Life Cycle? In maven, we have different goals. These are: - Generate resources (Dependencies) - Compile code - Unit test - Package (Build) - Install (in to local repo & artifactory) - Deploy (to servers) - Clean (delete all run time files) What does POM.XML contains? POM.XML is maven\u2019s main configuration file where we keep all details related to project. It contains: - Metadata about that project - Dependencies required to build the project - The kind of project - Kind of output you want (.jar, .war) - Description about that project What is Multi-Module Project in Maven? - Dividing big project into small modules, we call Multi Module Project. - Each module must have its own SRC folder & pom.xml so that build will happen separately. - To build all modules with one command, there should be a parent pom.xml file. This calls all child pom.xml files automatically. - In parent pom.xml file, need to mention the child pom.xml files in an order. What is Nagios? Nagios is one of the monitoring tools. By using Nagios we can monitor and give alerts. Where ever you install Nagios that becomes Nagios server. Monitoring is important, because we need to make sure that our servers should never go down. If at all in some exceptional cases server goes down, immediately we need alert in the form of intimation so that we can take required action to bring the server up immediately. So for this purpose, we use Nagios. Why do we have to use Nagios? There are many advantages in using Nagios: - It is oldest & Latest (every now and then, it is getting upgraded as per current market requirements) - Stable (we have been using this since so many years and it is performing well) - By default, we get so many Plug-ins - It is having its own Database. - Nagios is both Monitoring & Alerting tool. How does Nagios works? - We mention all details in configuration files what data to be collected from which machine. - Nagios daemon reads those details about what data to be collected. - Daemon use NRPE (Nagios Remote Plug-in Executer) plug-in to collect data form nodes and stores in its own database. - Finally displays in Nagios dashboard. What is the Directory structure of Nagios? /usr/local/nagios/bin \u2013 binary files /usr/local/nagios/sbin \u2013 CGI files (to get web page) /usr/local/nagios/libexec \u2013 plugins /usr/local/nagios/share \u2013 PHP Files /usr/local/nagios/etc \u2013 configuration files /usr/local/nagios/var \u2013 logs /usr/local/nagios/var/status.dat(file) \u2013 database What are the Important Configuration files in Nagios? Nagios main configuration file is /usr/local/nagios/etc/nagios.cfg /usr/local/nagios/etc/objects/localhost.cfg (where we keep hosts information) /usr/local/nagios/etc/objects/contacts.cfg (whom to be informed (emails)) /usr/local/nagios/etc/objects/timeperiods.cfg (at what time to monitor) /usr/local/nagios/etc/objects/commands.cfg (plugins to use) /usr/local/nagios/etc/objects/templates.cfg (sample templates) (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"DevOps Interview Questions for Freshers and Experienced-2"},{"location":"nightwolf-cotribution/devops_interview_questions-2/#devops-interview-question-and-answered-for-freshers-and-experienced-2","text":"We have consolidated a list of frequently asked DevOps interview questions for Freshers and Experienced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); List of Docker components? 1. Docker image: \u2013 Contains OS (very small) (almost negligible) + soft wares 2. Docker Container: \u2013 Container like a machine which is created from Docker image. 3. Docker file: \u2013 Describes steps to create a docker image. 4. Docker hub/registry: \u2013 Stores all docker images publicly. 5. Docker daemon: \u2013 Docker service runs at back end Above five components we call as Docker components What is Docker workflow? First we create Docker file by mentioning instructions to build docker image. Form this Docker image, we are going to create Docker container. This Docker image we can push to docker hub as well. This image can be pulled by others to create docker containers. We can create docker images from docker containers. Like this we can create Docker images form either docker file or docker containers. We can create docker containers from docker images. This is the work flow of docker. Sample Docker file instructions? FROM ubuntu WORKDIR /tmp RUN echo \u201cHello\u201d > /tmp/testfile ENV myname user1 COPY testfile1 /tmp ADD test.tar.gz /tmp What is the importance of volumes in Docker? 1. Volume is a directory inside your container \u2000 2. First declare directory as a volume and then share volume \u2000 3. Even if we stop container, still we can access volume \u2000 4. Volume will be created in one container \u2000 5. You can share one volume across any no of containers \u2000 6. Volume will not be included when you update an image \u2000 7. Map volumes in two ways \u2000 8. Share host \u2013 container \u2000 9. Share container \u2013 container What do you mean by port mapping in Docker? Suppose if you want to make any container as web server by installing web package in it, you need to provide containers IP address to public in order to access website which is running inside docker container. But Docker containers don\u2019t have an IP address. So, to address this issue, we have a concept called Docker port mapping. We map host port with container port and customers use public IP of host machine. Then their request will be routed from host port to container\u2019s port and will be loaded webpage which is running inside docker container. This is how we can access website which is running inside container through port mapping. What is Registry server in Docker? Registry server is our own docker hub created to store private docker images instead of storing in public Docker hub. Registry server is one of the docker containers. We create this Registry server from \u201cregistry\u201d image, especially provided by docker to create private docker hub. We can store any no of private docker images in this Registry server. We can give access to others, so that, they also can store their docker images whomever you provide access. Whenever we want, we can pull these images and can create containers out of these images. Important docker commands? 1. Docker ps (to see list of running containers) 2. Docker ps -a (to see list of all containers) 3. Docker images (to see list of all images) 4. Docker run (to create docker container) 5. Docker attach (to go inside container) 6. Docker stop (to stop container) 7. Docker start (to start container) 8. Docker commit (to create image out of docker file) 9. Docker rm (to delete container) 10. Docker rmi (to delete image) What is Ansible? Ansible is one of the configuration Management Tools. It is a method through we automate system admin tasks. Configuration refers to each and every minute details of a system. If we do any changes in system means we are changing the configuration of a machine. That means we are changing the configuration of the machine. All windows/Linux system administrators manage the configuration of a machine manually. All DevOps engineers are managing this configuration automatic way by using some tools which are available in the market. One such tool is Ansible. That\u2019s why we call Ansible as configuration management tool. Working process of Ansible? Here we crate file called playbook and inside playbook we write script in YAML format to create infrastructure. Once we execute this playbook, automatically code will be converted into Infrastructure. We call this process as IAC (Infrastructure as Code). We have open source and enterprise editions of Ansible. Enterprise edition we call Ansible Tower. The architecture of Ansible? We create Ansible server by installing Ansible package in it. Python is pre-requisite to install ansible. We need not to install ansible package in nodes. Because, communication establishes from server to node through \u201cssh\u201d client. By default all Linux machine will have \u201cssh\u201d client. Server is going to push the code to nodes that we write in playbooks. So Ansible follows pushing mechanism. Ansible components? 1. Server:\u2013 It is the place where we create playbooks and write code in YML format 2. Node:\u2013 It is the place where we apply code to create infrastructure. Server pushes code to nodes. 3. SSH:\u2013 It is an agent through ansible server pushes code to nodes. 4. Setup:\u2013 It is a module in ansible which gathers nodes information. 5. Inventory file:- In this file we keep IP/DNS of nodes. Disadvantages in other SCM (Source Code Management) tools? \u2000 - Huge overhead of Infrastructure setup \u2000 - Complicated setup \u2000 - Pull mechanism \u2000 - Lot of learning required Advantages of Ansible over other SCM (Source Code Management) tools? \u2000 - Agentless \u2000 - Relies on \u201cssh\u201d \u2000 - Uses python \u2000 - Push mechanism How does Ansible work? We give nodes IP addresses in hosts file by creating any group in ansible server why because, ansible doesn\u2019t recognize individual IP addresses of nodes. We create playbook and write code in YAML script. The group name we have to mention in a playbook and then we execute the playbook. By default, playbook will be executed in all those nodes which are under this group. This is how ansible converts code into infrastructure. What do you mean by Ad-Hoc commands in Ansible? These are simple one liner Linux commands we use to meet temporary requirements without actually saving for later. Here we don\u2019t use ansible modules. So there, Idempotency will not work with Ad-Hoc commands. If at all we don\u2019t get required YAML module to write to create infrastructure, then we go for it. Without using playbooks we can use these Ad-Hoc commands for temporary purpose. Differences between Chef and Ansible? Ansible Chef Playbook Recipe Module Resource Host Node Setup Ohai SSH Knife mechanism: Push mechanism: Pull What is Playbook in Ansible? Playbook is a file where we write YAML script to create infrastructure in nodes. Here, we use modules to create infrastructure. We create so many sections in playbook. We mention all modules in task section. You can create any no of playbooks. There is no limit. Each playbook defines one scenario. All sections begin with \u201c-\u201d & its attributes & parameters beneath it. Mention some list of sections that we mention in Playbook? 1. Target section 2. Task section 3. Variable section 4. Handler section What is Target section in Ansible playbook? This is one of the important sections in Playbook. In this section, we mention the group name which contains either IP addresses or Hostnames of nodes. When we execute playbook, then code will be pushed too all nodes which are there in the group that we mention in Target section. We use \u201call\u201d key word to refer all groups. What is Task section in Ansible playbook? This is second most important section in playbook after target section. In this section, we are going to mention list of all modules. All tasks we mention in this task section. We can mention any no of modules in one playbook. There is no limit. If there is only one task, then instead of going with big playbook, simply we can go with arbitrary command where we can use one module at a time. If more than one module, then there is no option except going with big playbook. What is Variable section? In this section we are going to mention variables. Instead of hard coding, we can mention as variables so that during runtime it pulls the actual value in place of key. We have this concept in each and every programming language and scripting language. We use \u201cvars\u201d key word to use variables. What is Handler section? All tasks we mention in tasks section. But some tasks where dependency is there, we should not mention in tasks section. That is not good practice. For example, installing package is one task and starting service is one more task. But there is dependency between them. I.e. after installing package only, we have to start service. Otherwise it throws error. These kind of tasks, we mention in handler section. In above example, package task we mention in task section and service task we mention in handler section so that after installing task only service will be started. What is Dry run in playbook? Dry run is to test playbook. Before executing playbook in nodes, we can test whether the code in playbook is written properly or not. Dry run won\u2019t actually executes playbook, but it shows output as` if it executed playbook. Then by seeing the output, we can come to know whether the playbook is written properly or not. It checks whether the playbook is formatted correctly or not. It tests how the playbook is going to behave without running the tasks. Why are we using loops concept in Ansible? Sometimes we might need to deal with multiple tasks. For instance, Installing multiple packages, Creating many users, creation many groups..etc. In this case, mentioning module for every task is complex process. So, to address this issue, we have a concept of loops. We have to use variables in combination with loops. Where do we use conditionals in Playbooks? Sometimes, your nodes could be mixture of different flavors of Linux OS. Linux commands vary in different Linux operating systems. In this case, we can\u2019t execute common set of commands in all machines, at the same time, we can\u2019t execute different commands in each node separately. To address this issue, we have conditionals concept where commands will be executed based up on certain condition that we give. What is Ansible vault? Sometimes, we use sensitive information in playbooks like passwords, keys \u2026etc. So any one can open these playbooks and get to know about this sensitive information. So we have to protect our playbooks from being read by others. So by using Ansible vault, we encrypt playbooks so that, those who ever is having password, only those can read this information. It is the way of protecting playbooks by encrypting them. What do you mean by Roles in Ansible? Adding more & more functionality to the playbooks will make it difficult to maintain in a single file. To address this issue, we organize playbooks into a directory structure called \u201croles\u201d. We create separate file to each section and we just mention the names of those sections in playbook instead of mentioning all modules in main playbook. When you call main playbook, main playbook will call all sections files respectively in the order whatever order you mention in playbook. So, by using this Roles, we can maintain small playbook without any complexity. Write a sample playbook to install any package? \u2014 # My First YAML playbook \u2013 hosts: demo user: ansible become: yes connection: ssh tasks: \u2013 name: Install HTTPD on centos 7 action: yum name=httpd state=installed Write a sample playbook by mentioning variables instead of hard coding? \u2014 # My First YAML playbook \u2013 hosts: demo user: nightwolf become: yes connection: ssh vars: pkgname: httpd tasks: \u2013 name: Install HTTPD server on centos 7 action: yum name=\u2018{{pkgname}}\u2019 state=installed What is CI & CD? CI means Continues Integration and CD means Continues Delivery/Deploy. Whenever developers write code, we integrate all that code of all developers at that point of time and we build, test and deliver/deploy to the client. This process we call CI & CD. Jenkins helps in achieving this. So instead of doing night builds, build as and when commit occurs by integrating all code in SCM tool, build, test and checking the quality of that code is what we call Continues Integration. Key terminology that we use in Jenkins? - Integrate: Combine all code written by developers till some point of time. - Build: Compile the code and make a small executable package. - Test: Test in all environments whether application is working properly or not. - Archived: Stored in an artifactory so that in future we may use/deliver again. - Deliver: Handing the product to Client Deploy: Installing product in client\u2019s machines. What is Jenkins Workflow? We attach Git, Maven, Selenium & Artifactory plug-ins to Jenkins. Once Developers put the code in Git, Jenkins pulls that code and send to Maven for build. Once build is done, Jenkins pulls that built code and send to selenium for testing. Once testing is done, then Jenkins will pull that code and send to Artifactory as per requirement and finally we can deliver the end product to client we call Continues delivery. We can also deploy with Jenkins into clients machine directly as per the requirement. This is what Jenkins work flow. What are the ways through which we can do Continues Integration? There are total three ways through which we can do Continues Integration: 1. Manually:\u2013 Manually write code, then do build manually and then test manually by writing test cases and deploy manually into clients machine. 2. Scripts:\u2013 Can do above process by writing scripts so that these scripts do CI&CD automatically. here complexity is, writing script is not so easy. 3. Tool:\u2013 Using tools like Jenkins is very handy. Everything is preconfigured in these type of tools. So less manual intervention. This is the most preferred way. Benefits of CI? 1. Detects bugs as soon as possible, so that bug will be rectified fast and development happens fast. 2. Complete automation. No need manual intervention. 3. We can intervene manually whenever we want i.e. we can stop any stage at any point of time so have better control. 4. Can establish complete and continues work flow. Why only Jenkins? \u2000 - It has so many plug-ins. - You can write your own plug-in \u2000 - You can use community plug-ins \u2000 - Jenkins is not just a tool. It is a framework i.e. you can do what ever you want. All you need is plugins. - We can attach slaves to Jenkins master. It instructs others(slaves) to do Job. - If slaves are not available, Jenkins itself does the job. \u2000 - Jenkins also acts as cron server replacement i.e. can do repeated tasks automatically. \u2000 - Running some scripts regularly E.g.: Automatic daily alarm. \u2000 - Can create Labels (Group of slaves) (Can restrict where the project has to run). What is Jenkins Architecture? Jenkins architecture is Client-Server model. Where ever, we install Jenkins, we call that server is Jenkins master. We can also create slaves in Jenkins, so that server load will be distributed to slaves. Jenkins Master randomly assigns tasks to slaves. But if you want to restrict any job to run in particular slave, then we can do it, so that particular job will be executed in that slave only. We can group some slaves by using \u201cLabel\u201d. How to install Jenkins? - You can install Jenkins in any OS. All OSs supports Jenkins. We access Jenkins through web page only. That\u2019s why it doesn\u2019t make any difference whether you install Jenkins in Windows or Linux. \u2000 - Choose Long Term Support release version, so that you will get support from Jenkins community. If you are using Jenkins for testing purpose, you can choose weekly release. But for production environments, we prefer Long Term Support release version. - Need to install JAVA. Java is pre-requisite to install Jenkins. \u2000 - Need to install web package. Because, we are going to access Jenkins through web page only. Does Jenkins open source? There are two editions in Jenkins 1. Open source 2. Enterprise edition Open source edition we call Jenkins. Here we get support from community if we need it. Enterprise edition we call Hudson. Here Jenkins company will provide support. How many types of configurations in Jenkins? There are total 3 types of configurations in Jenkins: 1. Global:\u2013 Here, whatever configuration changes we do, applicable to whole Jenkins including jobs as well as nodes. This configuration has high priority. 2. Job:\u2013 These configurations applicable to only Jobs. Jobs also we call as projects or items in Jenkins. 3. Node:\u2013 These configurations applicable to only nodes. Also we call Slaves. These are kind of helpers to Jenkins master to distribute the excessive load. What do you mean by workspace in Jenkins? The workspace is the location on your computer where Jenkins places all files related to the Jenkins project. By default each project or job is assigned a workspace location and it contains Jenkins-specific project metadata, temporary files like logs and any build artifacts, including transient build files. Jenkins web page acts like a window through which we are actually doing work in workspace. List of Jenkins services? \u2000 - localhost:8080/restart (to restart Jenkins).\u2000 - localhost:8080/stop (to stop Jenkins). - localhost:8080/start (to start Jenkins). How to create a free style project in Jenkins? \u2000 - Create project by giving any name \u2000 - Select Free style project \u2000 - Click on build \u2000 - Select execute windows batch command \u2000 - Give any command (echo \u201cHello Dear Students!!\u201d) \u2000 - Select Save \u2000 - Click on Build now \u2000 - Finally can see Console output What do you mean by Plugins in Jenkins? \u2000 - With Jenkins, nearly everything is a plugin and that nearly all functionality is provided by plugins. You can think of Jenkins as little more than an executor of plugins. \u2000 - Plugins are small libraries that add new abilities to Jenkins and can provide integration points to other tools. \u2000 - Since nearly everything Jenkins does is because of a plugin, Jenkins ships with a small set of default plugins, some of which can be upgraded independently of Jenkins. How to create Maven Project? \u2000 - Select new item - Copy the git hub maven project link and paste in git section in Jenkins \u2000 - Select build \u2000 - Click on clean package \u2000 - Select save \u2000 - Click on Build now \u2000 - Verify workspace contents with GitHub sideSee console output How can we Schedule projects? Sometimes, we might need some jobs to be executed after frequent intervals. To schedule a job: - Click on any project - Click on Configure \u2000 - Select on Build triggers \u2000 - Click on Build periodically \u2000 - Give timing (In format : * * * * * ) \u2000 - Select Save \u2000 - Can see automatic builds every 1 min \u2000 - You can manually trigger build as well if you want. What do you mean by Upstream and Downstream projects? We can also call them as linked projects. These are the ways through which, we connect jobs one with other. In Upstream jobs, first job will trigger second job after build is over. In Downstream jobs, second job will wait till first job finishes its build. As and when first job finishes its work, then second job will be triggered automatically. In Upstream, first job will be active. In Downstream jobs, second job will be active. We can use any one type to link multiple jobs. What is view in Jenkins? We can customize view as per our needs. We can modify Jenkins home page. We can segregate jobs as per the type of jobs like free style jobs and maven jobs and so on. To create custom view: - Select List of Related Projects \u2000 - Select Default views \u2000 - Click on All \u2000 - Click on + and select Freestyle - Select List Views \u2000 - Select Job filter \u2000 - Select required jobs to be segregated \u2000 - Now, you can see different view What is User Administration in Jenkins? In Jenkins, we can create users, groups and can assign limited privileges to them so that, we can have better control on Jenkins. Users will not install Jenkins in their machines. They access Jenkins as a user. Here we can\u2019t assign permissions directly to users. Instead we create \u201cRoles\u201d and assign permissions to those roles. These roles we attach to users so that users get the permissions whatever we assign to those roles. What is Global tool configuration in Jenkins? We install Java, Maven, Git and many other tools in our server. Whenever Jenkins need those tools, by default Jenkins will install them automatically every time. But it\u2019s not a good practice. That\u2019s why we give installed path of all these tools in Jenkins so that whenever Jenkins need them, automatically Jenkins pull them form local machine instead of downloading every time. This way of giving path of these tools in Jenkins we call \u201cGlobal tool configuration\u201d What is Build? Build means, Compile the source code, assembling of all class files and finally creating deliverable Compile:\u2013 Convert Source code into machine-readable format Assembly (Linking):\u2013 Grouping all class files Deliverable:\u2013 .war, .jar The above process is same for any type of code. This process we call Build. What is Maven? Maven is one of the Build tools. It is the most advance build tool in the market. In this, everything is already pre-configured. Maven belongs to Apache Company. We use maven to build Java code only. We can\u2019t build other codes by using Maven. By default, we get so many plugins with Maven. You can write your own plugins as well. Maven\u2019s local repository is \u201c.M2\u201d where we can get required compilers and dependencies. Maven\u2019s main configuration file is \u201cpom.xml\u201d where we keep all instructions to build. Advantages of Maven? \u2000 - Automated tasks (Mention all in pom.xml) \u2000 - Multiple Tasks at a time \u2000 - Quality product \u2000 - Minimize bad builds - Keep history - Save time \u2013 Save money \u2000 - Gives set of standards - Gives define project life cycle (Goals) \u2000 - Manage all dependencies \u2000 - Uniformity in all projects \u2000 - Re-usability List of Build tools available in Market? \u2000 - C and C++: Make file \u2000 - .Net: Visual studio \u2000 - Java: Ant, Maven What is the architecture of Maven? Maven main configuration file is pom.xml. For one project, there will be one workspace and one pom.xml. Requirements for build:\u2013 - Source code (Will be pulled from Git hub) - Compiler (Pulls from remote repo and then put them in local repo, from there, maven pulls into Workspace) - Dependencies (Pulls from remote repo and then put them in local repo, from there, maven pulls into Workspace) What is Maven\u2019s Build Life Cycle? In maven, we have different goals. These are: - Generate resources (Dependencies) - Compile code - Unit test - Package (Build) - Install (in to local repo & artifactory) - Deploy (to servers) - Clean (delete all run time files) What does POM.XML contains? POM.XML is maven\u2019s main configuration file where we keep all details related to project. It contains: - Metadata about that project - Dependencies required to build the project - The kind of project - Kind of output you want (.jar, .war) - Description about that project What is Multi-Module Project in Maven? - Dividing big project into small modules, we call Multi Module Project. - Each module must have its own SRC folder & pom.xml so that build will happen separately. - To build all modules with one command, there should be a parent pom.xml file. This calls all child pom.xml files automatically. - In parent pom.xml file, need to mention the child pom.xml files in an order. What is Nagios? Nagios is one of the monitoring tools. By using Nagios we can monitor and give alerts. Where ever you install Nagios that becomes Nagios server. Monitoring is important, because we need to make sure that our servers should never go down. If at all in some exceptional cases server goes down, immediately we need alert in the form of intimation so that we can take required action to bring the server up immediately. So for this purpose, we use Nagios. Why do we have to use Nagios? There are many advantages in using Nagios: - It is oldest & Latest (every now and then, it is getting upgraded as per current market requirements) - Stable (we have been using this since so many years and it is performing well) - By default, we get so many Plug-ins - It is having its own Database. - Nagios is both Monitoring & Alerting tool. How does Nagios works? - We mention all details in configuration files what data to be collected from which machine. - Nagios daemon reads those details about what data to be collected. - Daemon use NRPE (Nagios Remote Plug-in Executer) plug-in to collect data form nodes and stores in its own database. - Finally displays in Nagios dashboard. What is the Directory structure of Nagios? /usr/local/nagios/bin \u2013 binary files /usr/local/nagios/sbin \u2013 CGI files (to get web page) /usr/local/nagios/libexec \u2013 plugins /usr/local/nagios/share \u2013 PHP Files /usr/local/nagios/etc \u2013 configuration files /usr/local/nagios/var \u2013 logs /usr/local/nagios/var/status.dat(file) \u2013 database What are the Important Configuration files in Nagios? Nagios main configuration file is /usr/local/nagios/etc/nagios.cfg /usr/local/nagios/etc/objects/localhost.cfg (where we keep hosts information) /usr/local/nagios/etc/objects/contacts.cfg (whom to be informed (emails)) /usr/local/nagios/etc/objects/timeperiods.cfg (at what time to monitor) /usr/local/nagios/etc/objects/commands.cfg (plugins to use) /usr/local/nagios/etc/objects/templates.cfg (sample templates) (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"DevOps Interview Question and Answered for Freshers and Experienced - 2"},{"location":"nightwolf-cotribution/devops_interview_questions/","text":"DevOps Interview Question and Answered for Freshers and Experienced \uf0c1 We have consolidated a list of frequently asked DevOps interview questions for Freshers and Experienced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is Source Code Management? It is a process through which we can store and manage any code. Developers write code, Testers write test cases and DevOps engineers write scripts. This code, we can store and manage in Source Code Management. Different teams can store code simultaneously. It saves all changes separately. We can retrieve this code at any point of time. What are the Advantages of Source Code Management ? - Helps in Achieving teamwork. - Can work on different features simultaneously. - Acts like pipeline b/w offshore & onshore teams. - Track changes (Minute level). - Different people from the same team, as well as different teams, can store code simultaneously. Available Source Code Management tools in the market? There are so many Source Code Management tools available in the market. Those are - Git - SVN - Perforce - Clear case Out of all these tools, Git is the most advanced tool in the market where we are getting so many advantages compared to other Source Code Management tools. What is Git? Git is one of the Source Code Management tools where we can store any type of code. Git is the most advanced tool in the market now. We also call Git is version control system because every update stored as a new version. At any point of time, we can get any previous version. We can go back to previous versions. Every version will have a unique number. That number we call commit-ID. By using this commit ID, we can track each change i.e. who did what at what time. For every version, it takes incremental backup instead of taking the whole backup. That\u2019s why Git occupies less space. Since it is occupying less space, it is very fast. What are the advantages of Git? Speed:- Git stores every update in the form of versions. For every version, it takes incremental backup instead of taking the whole backup. Since it is taking less space, Git is very fast. That incremental backup we call \u201cSnapshot\u201d. Parallel branching:- We can create any number of branches as per our requirement. No need to take prior permission from any one, unlike other Source Code Management tools. Branching is for parallel development. Git branches allow us to work simultaneously on multiple features. Fully Distributed:- A backup copy is available in multiple locations in each and everyone\u2019s server instead of keeping in one central location, unlike other Source Code Management tools. So even if we lose data from one server, we can recover it easily. That\u2019s why we call GIT as DVCS (Distributed Version Control System) What are the stages in Git? There are total of 4 stages in Git 1. Workspace:- It is the place where we can create files physically and modify. Being a Git user, we work in this work space. 2. Staging area/Indexing area:- In this area, Git takes a snapshot for every version. It is a buffer zone between workspace and local repository. We can\u2019t see this region because it is virtual. 3. Local repository:- It is the place where Git stores all commit locally. It is a hidden directory so that no one can delete it accidentally. Every commit will have unique commit ID. 4. Central repository:- It is the place where Git stores all commit centrally. It belongs to everyone who is working in your project. Git Hub is one of the central repositories. Used for storing the code and sharing the code to others in the team. What is the common branching strategy in Git? - Product is the same, so one repo. But different features. - Each feature has one separate branch - Finally, merge (code) all branches - For Parallel development - Can create any no of branches - Can create one branch on the basis of another branch - Changes are personal to that particular branch - Can put files only in branches (not in repo directly) - The default branch is \u201cMaster\u201d - Files created in a workspace will be visible in any of the branch workspaces until you commit. Once you commit, then that file belongs to that particular branch. How many types of repositories available in Git? There are two types of repositories available in Git Bare Repositories (Central) These repositories are only for Storing & Sharing the code All central repositories are bare repositories Non \u2013 Bare Repositories (Local) In these repositories, we can modify the files All local /user repositories are Bare Repositories Can you elaborate commit in Git? - Storing file permanently in the local repository we call commit. - For every commit, we get one commit ID. - It contains 40 long Alpha-numeric characters. - It uses the concept \u201cCheck some\u201d (It\u2019s a tool in Linux, generates binary value equal to the data present in file) - Even if you change one dot, Commit-ID will get changed. - Helps in tracking the changes What do you mean by \u201cSnapshot\u201d in Git? - It is a backup copy for each version git stores in a repository. - Snapshot is an incremental backup copy (only backup for new changes) - Snapshot represents some data of particular time so that, we can get data of particular time by taking that particular snapshot - This snapshot will be taken in Staging area in Git which is present between Git workspace and Git local repository. What is GitHub? Git hub is central git repository where we can store code centrally. Git hub belongs to Microsoft Company. We can create any number of repositories in Git hub. All public repositories are free and can be accessible by everyone. Private repositories are not free and can restrict public access for security. We can copy the repository from one account to other accounts also. This process we call as \u201cFork\u201d. In this repository also we can create branches. The default branch is \u201cMaster\u201d. What is Git merge? By default, we get one branch in git local repository called \u201cMaster\u201d. We can create any no of branches for parallel development. We write code for each feature in each branch so that development happens separately. Finally, we merge code off all branches in to Master and push to central repository. We can merge code to any other branch as well. But merging code into master is standard practice that being followed widely. Sometimes, while merging, conflict occurs. When same file is in different branches with different code, when try to merge those branches, conflict occurs. We need to resolve that conflict manually by rearranging the code. What is Git stash? We create multiple branches to work simultaneously on multiple features. But to work on multiple tasks simultaneously in one branch (i.e. on one feature), we use git stash. Stash is a temporary repository where we can store our content and bring it back whenever we want to continue with our work with that stored content. It removes content inside file from working directory and puts in stashing store and gives clean working directory so that we can start new work freshly. Later on you can bring back that stashed items to working directory and can resume your work on that file. Git stash applicable to modified files. Not new files. Once we finish our work, we can remove all stashed items form stash repository. What is Git Reset? Git Reset command is used to remove changes form staging area. This is bringing back file form staging area to work directory. We use this command before commit. Often we go with git add accidentally. In this case if we commit, that file will be committed. Once you commit, commit ID will be generated and it will be in the knowledge of everyone. So to avoid this one, we use Git reset. If you add \u201c\u2013hard\u201d flag to git reset command, in one go, file will be removed from staging area as well as working directory. We generally go with this one if we fell that something wrong in the file itself. What is Git Revert? Git Revert command is used to remove changes from all 3 stages (work directory, staging area and local repository). We use this command after commit. Sometimes, we commit accidentally and later on we realize that we shouldn\u2019t have done that. For this we use Git revert. This operation will generate new commit ID with some meaningful message to ignore previous commit where mistake is there. But, here we can\u2019t completely eliminate the commit where mistake is there. Because Git tracks each and every change. Difference between Git pull and Git clone? We use these two commands to get changes from central repository. For the first time if you want whole central repository in your local server, we use git clone. It brings entire repository to your local server. Next time onwards you might want only changes instead of whole repository. In this case, we use Git pull. - Git clone is to get whole copy of central repository - Git pull is to get only new changes from central repository (Incremental data) What is the difference between Git pull and Fetch? We use Git pull command to get changes from central repository. In this operation, internally two commands will get executed. One is Git fetch and another one is Git merge. Git fetch means, only bringing changes from central repo to local repo. But these changes will not be integrated to local repo which is there in your server. Git merge means, merging changes to your local repository which is there in your server. Then only you can see these changes. So Git pull is the combination of Git pull and Git merge. What is the difference between Git merge and rebase? We often use these commands to merge code in multiple branches. Both are almost same but few differences. When you run Git merge, one new merge commit will be generated which is having the history of both development branches. It preserves the history of both branches. By seeing this merge commit, everyone will come to know that we merged two branches. If you do Git rebase, commits in new branch will be applied on top of base branch tip. There won\u2019t be any merge commit here. It appears that you started working in one single branch form the beginning. This operation will not preserves the history of new branch. What is Git Bisect? Git Bisect we use to pick bad commit out of all good commits. Often developers do some mistakes. For them it is very difficult to pick that commit where mistake is there. They go with building all commits one by one to pick bad commit. But Git bisect made their lives easy. Git bisect divides all commits equally in to two parts (bisecting equally). Now instead of building each commit, they go with building both parts. Where ever bad commit is there, that part build will be failed. We do operation many times till we get bad commit. So Git bisect allows you to find a bad commit out of good commits. You don\u2019t have to trace down the bad commit by hand; git-bisect will do that for you. What is Git squash? To move multiple commits into its parent so that you end up with one commit. If you repeat this process multiple times, you can reduce \u201cn\u201d number of commits to a single one. Finally we will end up with only one parent commit. We use this operation just to reduce number of commits. What is Git hooks? We often call this as web hooks as well. By default we get some configuration files when you install git. These files we use to set some permissions and notification purpose. We have different types of hooks (pre commit hooks & post commit hooks) Pre-commit hooks:- Sometimes you would want every member in your team to follow certain pattern while giving commit message. Then only it should allow them to commit. These type of restrictions we call pre-commit hooks. Post-commit hooks:- Sometimes, being a manager you would want an email notification regarding every commit occurs in a central repository. This kind of things we call post-commit hooks. In simple terms, hooks are nothing but scripts to put some restrictions. What is Git cherry-pick? When you go with git merge, all commits which are there in new development branch will be merged into current branch where you are. But sometimes, requirement will be in such that you would want to get only one commit form development branch instead of merging all commits. In this case we go with git cherry-pick. Git cherry-pick will pick only one commit whatever you select and merges with commits which are there in your current branch. So picking particular commit and merging into your current branch we call git cherry-pick. What is the difference between Git and SVN? SVN:- It is centralized version control system (CVCS) where back up copy will be placed in only one central repository. There is no branching strategy in SVN. You can\u2019t create branches. So no parallel development. There is no local repository. So can\u2019t save anything locally. Every time after writing code you need to push that code to central repository immediately to save changes. Git:- It is a Distributed version control system where back up copy is available in everyone\u2019s machine\u2019s local repository as well as a central repository. We can create any no of branches as we want. So we can go in parallel development simultaneously. Every Git repository will have its own local repository. So we can save changes locally. At the end of our work finally, we can push code to a central repository. What is the commit message in Git? Every time we commit, while committing, we have to give commit message just to identify each commit. We can\u2019t remember to commit numbers because they contain 40 long alphanumeric characters. So, to remember commits easily, we give commit message. The format of commit message differs from company to company and individual to individual. We have one more way to identify commits. That is giving \u201cTags\u201d. Tag is a kind of meaningful name to a particular commit. Instead of referring to commit ID, we can refer to tags. Internally tag will refer to respective commit ID. These are the ways to get a particular commit easily. What is Configuration Management? It is a method through we automate admin tasks. Each and every minute details of a system, we call configuration details. If we do any change here means we are changing the configuration of a machine. That means we are managing the configuration of the machine. System administrators used to manage the configuration of machine through manually. DevOps engineers are managing this configuration through automated way by using some tools which are available in the market. That\u2019s why we call these tools as configuration management tools. What is IAC? IAC means Infrastructure As Code. It is the process through which we automate all admin tasks. Here we write code in Ruby script in chef. When you apply this code, automatically code will be converted into Infrastructure. So here we are getting so many advantages in writing the code. Those are: 1. Code is Testable (Testing code is easy compare to Infrastructure) 2. Code is Repeatable (Can re-use the same code again and again) 3. Code is Versionable (Can store in versions so that can get any previous versions at any time) What do you mean by IT Infrastructure?? IT Infrastructure is a composite of the following things 1. Software 2. Network 3. People 4. Process What are the problems that system admins used to face earlier when there were no configuration management tools? 1. Managing users & Groups is big hectic thing (create users and groups, delete, edit\u2026\u2026) 2. Dealing with packages (Installing, Upgrading & Uninstalling) 3. Taking backups on regular basis manually 4. Deploying all kinds of applications in servers 5. Configure services (Starting, stopping and restarting services) These are some problems that system administrators used to face earlier in their manual process of managing configuration of any machine. Why should we go with Configuration Management Tool? 1. By using the Configuration Management Tool, we can automate almost each and every admin task. 2. We can increase uptime so that can provide maximum user satisfaction. 3. Improve the performance of systems. 4. Ensure compliance 5. Prevent errors as tools won\u2019t do any errors 6. Reduce cost (Buy tool once and use 24/7) How this Configuration Management Tool works? Whatever system admins (Linux/windows) used to do manually, now we are automating all those tasks by using any Configuration Management Tool. We can use this tool whether your servers are in on-premises or in the cloud. It turns your code into infrastructure. So your code is versionable, repeatable and testable. You only need to tell what the desired configuration should be, not how to achieve it. Through automation, we get our desired state of server. This is unique feature of Configuration Management Tool. What is the architecture of Chef? Chef is an administration tool. In this we have total 3 stages: 1. Chef Workstation (It is the place where we write code) 2. Chef Server (It is the place where we store code) 3. Chef Node (It is the place where we apply code) We need to establish communication among workstation, server and nodes. You can have any no of nodes. There is no limit. Chef can manage any no of nodes effectively. Components of Chef? 1. Chef Workstation: Where you write the code 2. Chef Server: Where you upload the code 3. Chef Node: Where you apply the code 4. Knife: Tool to establish communication among workstation, server & node. 5. Chef-client: Tool runs on every chef node to pull code from chef server 6. Ohai: Maintains current state information of chef node (System Discovery Tool) 7. Idempotency: Tracking the state of system resources to ensure that the changes should not re-apply repeatedly. 8. Chef Supermarket: Where you get custom code How does Chef Works? We need to install chef package in workstation, server and nodes. We create cookbook in workstation. Inside cookbook, there will be a default recipe where you write code in ruby script. You can create any no of recipes. There is no limit. After writing code in recipe, we upload whole cookbook to chef server. Chef server acts as central hub storing code. Then, we need to add this cookbook\u2019s recipe to nodes run-list. Chef-client tool will be there in each and every chef node. It runs frequently. Chef-client comes to chef server and take that code and applies that code in node. This is how code will be converted into infrastructure. What is Idempotency? It is unique feature in all configuration management tools. It ensures that changes should not re-apply repeatedly. Once chef-client converted code into Infrastructure, then even chef-client runs again, it will not take any action. It won\u2019t do the same task again and again. If any new changes are there in that code, then only chef-client is going to take action. So it doesn\u2019t make any difference ever if you run chef-client any no of times. So tracking the system details to not to reapply changes again and again, we call Idempotency. What is Ohai and how does it works?? Ohai we call \u201cSystem Discovery Tool\u201d. It stores system information. It captures each and every minute details of system and updates it then and there if any new changes are there. Whenever chef-client converts code in infrastructure in node, immediately Ohai store will be updated. Next time onwards, before chef-client runs, it verifies in Ohai store to know about current state of information. So chef-client will come to know the current state of server. Then chef-client acts accordingly. If new changes are there, then only it will take action. If there are no new changes, then it won\u2019t take any action. Ohai tool helps in achieving this. How many types of chef server? Total there are 3 ways through which we can manage chef server. 1. Directly we can take chef server from Chef Company itself. In this case, everything will be managed by Chef Company. You will get support from chef. This type of server we call Managed/Hosted chef. This is completely Graphical User Interface (GUI). It\u2019s not free. We need to pay to Chef Company after exceeding free tier limit. 2. We can launch one server and we need to install chef server package. It is completely free package. It\u2019s GUI (Graphical User interface) 3. We can launch one server and we need to install chef server package. It is completely free package. It\u2019s CLI (Command Line Interface). What is there inside cookbook? Below mentioned files and folders will be there inside cookbook when you first create it: Chefignore: like .gitignore (to ignore files and folders) Kitchen.yml: for testing of cookbook Metadata.rb: name, author, version\u2026. etc of cookbook Readme.md: information about usage of cookbook Recipe: It is a file where you write code Spec: for unit test Test: for integration test What is Attributes concept in chef? Sometimes we might need to deploy web applications to in nodes and for that we need to know some host specific details of each server like IP Address, Hostname etc. Because we need to mention that in configuration files of each server. These files we call as Configuration files. This information will be vary from system to system. These host specific details that we mention in Configuration files,we call \u201cAttributes\u201d. Chef-client tool gathers these Attributes from Ohai store and puts in configuration files. Instead of hard coding these attributes, we mention as variables so that every time, file will be updated with latest details of their respective nodes. What is Run-list in Chef? This is an ordered list of recipes that we are going to apply to nodes. We mention all recipes in cookbook and then we upload that cookbook to chef server. Then, we attach all recipes to nodes run-list in sequence order. When chef-client runs, it applies all recipes to nodes in the same order whatever the order you mention in run-list. Because sometimes order is important especially when we deal with dependent recipes. What is bootstrap? It is the process of adding chef node to chef server or we can call, bringing any machine into chef environment. In this bootstrapping process total three action will be performed automatically. 1. Node gets connected to chef server. 2. Chef server will install chef package in chef node. 3. Cookbooks will be applied to chef node. It is only one time effort. As and when we purchase any new machine in company, immediately we add that server to chef server. At a time, we can only bootstrap one machine, not multiple machines. What is the workflow of Chef? We connect chef workstation, chef server and chef node with each other. After that, we create cookbook in chef workstation and inside that cookbook, we write code in recipe w.r.t. the infrastructure to be created. Then we upload entire cookbook to chef server and attach that cookbook\u2019s recipe to nodes run-list. Now we automate chef-client which will be there in all chef nodes. Chef-client runs frequently towards chef server for new code. So chef-client will get that code from server and finally applies to chef node. This is how, code is converted into infrastructure. If no changes are there in code, even if chef-client runs any no of time, it won\u2019t take any action until it finds some changes in code. This is what we call Idempotency. How does we connect Chef Workstation to Chef Server? First we download started kit from chef server. This will be downloaded in the form of zip file. If we extract this zip file, we will get chef-repo folder. This chef-repo folder we need to place in chef workstation. Inside chef-repo folder, we can see total three folders. They are .chef, cookbooks and roles. Out of these three, .chef folder is responsible to establish communication between chef server and chef workstation. Because, inside .chef folder, we can see two files. They are knife.rb and organization.pem. Inside kinfe.rb, there will be the url (address) of chef server. Because of this url, communication will be established between chef server and chef workstation. This is how we connect Chef Workstation to Chef Server. How does the chef-client runs automatically? By default, chef-client runs manually. So we need to automate this manually. For this, we use \u201ccron tool\u201d which is the default tool in all Linux machines use to schedule tasks to be executed automatically at frequent intervals. So in this \u201ccrontab\u201d file, we give chef-client command and we need to set the timing as per our requirement. Then onwards chef-client runs automatically after every frequent intervals. It is only one time effort. When we purchase any new server in company, along with bootstrap, we automate chef-client then and there. What is chef supermarket? Chef supermarket is the place where we get custom cookbooks. Every time we need not to create cookbooks and need not to write code from scratch. We can go with custom cookbooks which are available in chef supermarket being provided by chef organization and community. We can download these cookbooks and modify as per our needs. We get almost each and every cookbook from chef supermarket. They are safe to use. What is wrapper cookbook? Either we can download those chef supermarket cookbooks or without downloading, we can call these supermarket cookbooks during run time so that every time we get updates automatically for that cookbook if any new updates are there. Here, we use our own cookbook to call chef supermarket cookbook. This process of calling cookbook by using another cookbook, we call wrapper cookbook. Especially, we use this concept to automate chef-client. What is \u201croles\u201d in chef? Roles are nothing but a Custom run-list. We create role & upload to chef server & assign them to nodes. If we have so many nodes, need to add cookbook to run-list of all those nodes, it is very difficult to attach to all nodes run-list. So, we create role & attach that role to all those nodes once. Next time onwards, add cookbook to that role. Automatically, that cookbook will be attached to all those nodes. So role is one time effort. Instead of adding cookbooks to each & every node\u2019s run-list always, just create a role & attach that role to nodes. When we add cookbook to that role, it will be automatically applied to all nodes those assigned with that role. What is include_recipe in chef? By default, we can call one recipe at a time in one cookbook. But if you want to call multiple recipes from same cookbook, we use include_recipe concept. Here, we take default recipe and we mention all recipes to be called in this default recipe in an order. If we call default recipe, automatically default recipe will call all other recipes which are there inside default recipe. By using one recipe, we can call any number of recipes. This process of calling one recipe by using other recipe, we call as include_recipe. Here condition is we can call recipes from same cookbook, but not from different cookbooks. How to deploy a web server by using chef? package \u2018httpd\u2019 do action :install end file \u2018/var/www/html/index.html\u2019 do content \u2018Hello nightwolf Students!!\u2019 action :create end service \u2018httpd\u2019 do action [ :enable, :start ] end How to write ruby code to create file, directory? file \u2018/myfile\u2019 do content \u2018This is my second file\u2019 action :create owner \u2018root\u2019 group \u2018root\u2019 end directory \u2018/mydir\u2019 do action :create owner \u2018root\u2019 group \u2018root\u2019 end How to write ruby code to create user, group and install package? user \u2018user1\u2019 do action: create end group \u2018group1\u2019 do action :create members \u2018user1\u2019 append true end package \u2018httpd\u2019 do action: install end What is container? The container is like a virtual machine in which we can deploy any type of applications, softwares and libraries. It\u2019s a light weight virtual machine which uses OS in the form of image, which is having less in size compare to traditional VMware and oracle virtual box OS images. Container word has been taken from shipping containers. It has everything to run an application. What is virtualization? Logically dividing big machine into multiple virtual machines so that each virtual machine acts as new server and we can deploy any kind of applications in it. For this first we install any virtualization software on top of base OS. This virtualization software will divide base machine resources in to logical components. In a simple terms, logically dividing one machine into multiple machines we call virtualization. What is Docker? Docker is a tool by using which, we create containers in less time. Docker uses light weight OS in the form of docker images that we will get from docker hub. Docker is open source now. It became so popular because of its unique virtualization concept called \u201cContainerization\u201d which is not there in other tools. We can use docker in both windows and Linux machines. What do you mean by docker image? Docker image is light weight OS provided by docker company. We can get any type of docker image form docker hub. We use these docker images to create docker containers. This docker images may contain only OS or OS + other softwares as well. Each software in docker image, will be stored in the form of layer. Advantage of using docker images is, we can replicate the same environment any no of times. What are the ways through which we can create docker images? There are three ways through which we can create docker images. 1. We can take any type of docker image directly from docker hub being provided by docker company and docker community. 2. We can create our own docker images form our own docker containers i.e. first we create container form base docker image taken form docker hub and then by going inside container, we install all required soft wares and then create docker image from our own docker container. 3. We can create docker image form docker file. It is the most preferred way of creating docker images. What is docker file and why do we use it? It is a just normal text file with instructions in it to build docker image. It is the automated way of creating docker images. Once you build docker image, automatically docker file will be created. In this file, we mention required OS image and all required soft wares in the form of instructions. Once we build docker file, back end, docker container will be created and then docker image will be crated from that container and that container will be destroyed automatically. Difference between docker and VM Ware? VM Ware uses complete OS which contains GBs in size. But docker image size is MBs only. So it takes less size. That\u2019s why it takes less base machine resources. This docker image is compressed version of OS. The second advantage of docker is, there is no pre-allocation of RAM. During run time, it takes RAM as pre requirement from base machine and one\u2019s job is done, it release RAM. But in VM Ware, pre-allocation of RAM is there and it blocked whether it uses or not. So need more RAM for base machine if you want to use VM Ware unlike Docker. What is OS-Lever Virtualization? It is the unique feature of Docker which is not available in other virtualization soft wares. Docker takes most of UNIX features form host machine OS and it only takes extra layers of required OS in the form of docker image. So docker image contains only extra layers of required OS. For core UNIX kernel, it depends upon host OS, why because UNIX kernel is same in any of the UNIX and Linux flavors. In a simple terms, docker takes host OS virtually. That\u2019s why we call this concept as OS-Lever Virtualization. What is Layered file system/Union file system? Inside docker container, wheat ever we do, that forms as a new layer. For instance, creating files, directories, installing packages etc. This is what we call as layered file system. Each layer takes less space. We can create docker image form this container. In that docker image also we get all these layers and forms unity. That\u2019s why we also call Union File System. If we create container out of docker image, you can able to see all those files, directories and packages. This is what replication of same environment. What are the benefits of Docker? \u2000 1. Containerization (OS level virtualization) (No need guest OS) \u2000 2. No pre-allocation of RAM \u20003. Can replicate same environment \u2000 4. Less cost \u2000 5. Less weight (MB\u2019s in size) \u2000 6. Fast to fire up \u2000 7. Can run on physical/virtual/cloud \u2000 8. Can re-use (same image) \u2000 9. Can create containers in less time Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"DevOps Interview Questions for Freshers and Experienced"},{"location":"nightwolf-cotribution/devops_interview_questions/#devops-interview-question-and-answered-for-freshers-and-experienced","text":"We have consolidated a list of frequently asked DevOps interview questions for Freshers and Experienced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is Source Code Management? It is a process through which we can store and manage any code. Developers write code, Testers write test cases and DevOps engineers write scripts. This code, we can store and manage in Source Code Management. Different teams can store code simultaneously. It saves all changes separately. We can retrieve this code at any point of time. What are the Advantages of Source Code Management ? - Helps in Achieving teamwork. - Can work on different features simultaneously. - Acts like pipeline b/w offshore & onshore teams. - Track changes (Minute level). - Different people from the same team, as well as different teams, can store code simultaneously. Available Source Code Management tools in the market? There are so many Source Code Management tools available in the market. Those are - Git - SVN - Perforce - Clear case Out of all these tools, Git is the most advanced tool in the market where we are getting so many advantages compared to other Source Code Management tools. What is Git? Git is one of the Source Code Management tools where we can store any type of code. Git is the most advanced tool in the market now. We also call Git is version control system because every update stored as a new version. At any point of time, we can get any previous version. We can go back to previous versions. Every version will have a unique number. That number we call commit-ID. By using this commit ID, we can track each change i.e. who did what at what time. For every version, it takes incremental backup instead of taking the whole backup. That\u2019s why Git occupies less space. Since it is occupying less space, it is very fast. What are the advantages of Git? Speed:- Git stores every update in the form of versions. For every version, it takes incremental backup instead of taking the whole backup. Since it is taking less space, Git is very fast. That incremental backup we call \u201cSnapshot\u201d. Parallel branching:- We can create any number of branches as per our requirement. No need to take prior permission from any one, unlike other Source Code Management tools. Branching is for parallel development. Git branches allow us to work simultaneously on multiple features. Fully Distributed:- A backup copy is available in multiple locations in each and everyone\u2019s server instead of keeping in one central location, unlike other Source Code Management tools. So even if we lose data from one server, we can recover it easily. That\u2019s why we call GIT as DVCS (Distributed Version Control System) What are the stages in Git? There are total of 4 stages in Git 1. Workspace:- It is the place where we can create files physically and modify. Being a Git user, we work in this work space. 2. Staging area/Indexing area:- In this area, Git takes a snapshot for every version. It is a buffer zone between workspace and local repository. We can\u2019t see this region because it is virtual. 3. Local repository:- It is the place where Git stores all commit locally. It is a hidden directory so that no one can delete it accidentally. Every commit will have unique commit ID. 4. Central repository:- It is the place where Git stores all commit centrally. It belongs to everyone who is working in your project. Git Hub is one of the central repositories. Used for storing the code and sharing the code to others in the team. What is the common branching strategy in Git? - Product is the same, so one repo. But different features. - Each feature has one separate branch - Finally, merge (code) all branches - For Parallel development - Can create any no of branches - Can create one branch on the basis of another branch - Changes are personal to that particular branch - Can put files only in branches (not in repo directly) - The default branch is \u201cMaster\u201d - Files created in a workspace will be visible in any of the branch workspaces until you commit. Once you commit, then that file belongs to that particular branch. How many types of repositories available in Git? There are two types of repositories available in Git Bare Repositories (Central) These repositories are only for Storing & Sharing the code All central repositories are bare repositories Non \u2013 Bare Repositories (Local) In these repositories, we can modify the files All local /user repositories are Bare Repositories Can you elaborate commit in Git? - Storing file permanently in the local repository we call commit. - For every commit, we get one commit ID. - It contains 40 long Alpha-numeric characters. - It uses the concept \u201cCheck some\u201d (It\u2019s a tool in Linux, generates binary value equal to the data present in file) - Even if you change one dot, Commit-ID will get changed. - Helps in tracking the changes What do you mean by \u201cSnapshot\u201d in Git? - It is a backup copy for each version git stores in a repository. - Snapshot is an incremental backup copy (only backup for new changes) - Snapshot represents some data of particular time so that, we can get data of particular time by taking that particular snapshot - This snapshot will be taken in Staging area in Git which is present between Git workspace and Git local repository. What is GitHub? Git hub is central git repository where we can store code centrally. Git hub belongs to Microsoft Company. We can create any number of repositories in Git hub. All public repositories are free and can be accessible by everyone. Private repositories are not free and can restrict public access for security. We can copy the repository from one account to other accounts also. This process we call as \u201cFork\u201d. In this repository also we can create branches. The default branch is \u201cMaster\u201d. What is Git merge? By default, we get one branch in git local repository called \u201cMaster\u201d. We can create any no of branches for parallel development. We write code for each feature in each branch so that development happens separately. Finally, we merge code off all branches in to Master and push to central repository. We can merge code to any other branch as well. But merging code into master is standard practice that being followed widely. Sometimes, while merging, conflict occurs. When same file is in different branches with different code, when try to merge those branches, conflict occurs. We need to resolve that conflict manually by rearranging the code. What is Git stash? We create multiple branches to work simultaneously on multiple features. But to work on multiple tasks simultaneously in one branch (i.e. on one feature), we use git stash. Stash is a temporary repository where we can store our content and bring it back whenever we want to continue with our work with that stored content. It removes content inside file from working directory and puts in stashing store and gives clean working directory so that we can start new work freshly. Later on you can bring back that stashed items to working directory and can resume your work on that file. Git stash applicable to modified files. Not new files. Once we finish our work, we can remove all stashed items form stash repository. What is Git Reset? Git Reset command is used to remove changes form staging area. This is bringing back file form staging area to work directory. We use this command before commit. Often we go with git add accidentally. In this case if we commit, that file will be committed. Once you commit, commit ID will be generated and it will be in the knowledge of everyone. So to avoid this one, we use Git reset. If you add \u201c\u2013hard\u201d flag to git reset command, in one go, file will be removed from staging area as well as working directory. We generally go with this one if we fell that something wrong in the file itself. What is Git Revert? Git Revert command is used to remove changes from all 3 stages (work directory, staging area and local repository). We use this command after commit. Sometimes, we commit accidentally and later on we realize that we shouldn\u2019t have done that. For this we use Git revert. This operation will generate new commit ID with some meaningful message to ignore previous commit where mistake is there. But, here we can\u2019t completely eliminate the commit where mistake is there. Because Git tracks each and every change. Difference between Git pull and Git clone? We use these two commands to get changes from central repository. For the first time if you want whole central repository in your local server, we use git clone. It brings entire repository to your local server. Next time onwards you might want only changes instead of whole repository. In this case, we use Git pull. - Git clone is to get whole copy of central repository - Git pull is to get only new changes from central repository (Incremental data) What is the difference between Git pull and Fetch? We use Git pull command to get changes from central repository. In this operation, internally two commands will get executed. One is Git fetch and another one is Git merge. Git fetch means, only bringing changes from central repo to local repo. But these changes will not be integrated to local repo which is there in your server. Git merge means, merging changes to your local repository which is there in your server. Then only you can see these changes. So Git pull is the combination of Git pull and Git merge. What is the difference between Git merge and rebase? We often use these commands to merge code in multiple branches. Both are almost same but few differences. When you run Git merge, one new merge commit will be generated which is having the history of both development branches. It preserves the history of both branches. By seeing this merge commit, everyone will come to know that we merged two branches. If you do Git rebase, commits in new branch will be applied on top of base branch tip. There won\u2019t be any merge commit here. It appears that you started working in one single branch form the beginning. This operation will not preserves the history of new branch. What is Git Bisect? Git Bisect we use to pick bad commit out of all good commits. Often developers do some mistakes. For them it is very difficult to pick that commit where mistake is there. They go with building all commits one by one to pick bad commit. But Git bisect made their lives easy. Git bisect divides all commits equally in to two parts (bisecting equally). Now instead of building each commit, they go with building both parts. Where ever bad commit is there, that part build will be failed. We do operation many times till we get bad commit. So Git bisect allows you to find a bad commit out of good commits. You don\u2019t have to trace down the bad commit by hand; git-bisect will do that for you. What is Git squash? To move multiple commits into its parent so that you end up with one commit. If you repeat this process multiple times, you can reduce \u201cn\u201d number of commits to a single one. Finally we will end up with only one parent commit. We use this operation just to reduce number of commits. What is Git hooks? We often call this as web hooks as well. By default we get some configuration files when you install git. These files we use to set some permissions and notification purpose. We have different types of hooks (pre commit hooks & post commit hooks) Pre-commit hooks:- Sometimes you would want every member in your team to follow certain pattern while giving commit message. Then only it should allow them to commit. These type of restrictions we call pre-commit hooks. Post-commit hooks:- Sometimes, being a manager you would want an email notification regarding every commit occurs in a central repository. This kind of things we call post-commit hooks. In simple terms, hooks are nothing but scripts to put some restrictions. What is Git cherry-pick? When you go with git merge, all commits which are there in new development branch will be merged into current branch where you are. But sometimes, requirement will be in such that you would want to get only one commit form development branch instead of merging all commits. In this case we go with git cherry-pick. Git cherry-pick will pick only one commit whatever you select and merges with commits which are there in your current branch. So picking particular commit and merging into your current branch we call git cherry-pick. What is the difference between Git and SVN? SVN:- It is centralized version control system (CVCS) where back up copy will be placed in only one central repository. There is no branching strategy in SVN. You can\u2019t create branches. So no parallel development. There is no local repository. So can\u2019t save anything locally. Every time after writing code you need to push that code to central repository immediately to save changes. Git:- It is a Distributed version control system where back up copy is available in everyone\u2019s machine\u2019s local repository as well as a central repository. We can create any no of branches as we want. So we can go in parallel development simultaneously. Every Git repository will have its own local repository. So we can save changes locally. At the end of our work finally, we can push code to a central repository. What is the commit message in Git? Every time we commit, while committing, we have to give commit message just to identify each commit. We can\u2019t remember to commit numbers because they contain 40 long alphanumeric characters. So, to remember commits easily, we give commit message. The format of commit message differs from company to company and individual to individual. We have one more way to identify commits. That is giving \u201cTags\u201d. Tag is a kind of meaningful name to a particular commit. Instead of referring to commit ID, we can refer to tags. Internally tag will refer to respective commit ID. These are the ways to get a particular commit easily. What is Configuration Management? It is a method through we automate admin tasks. Each and every minute details of a system, we call configuration details. If we do any change here means we are changing the configuration of a machine. That means we are managing the configuration of the machine. System administrators used to manage the configuration of machine through manually. DevOps engineers are managing this configuration through automated way by using some tools which are available in the market. That\u2019s why we call these tools as configuration management tools. What is IAC? IAC means Infrastructure As Code. It is the process through which we automate all admin tasks. Here we write code in Ruby script in chef. When you apply this code, automatically code will be converted into Infrastructure. So here we are getting so many advantages in writing the code. Those are: 1. Code is Testable (Testing code is easy compare to Infrastructure) 2. Code is Repeatable (Can re-use the same code again and again) 3. Code is Versionable (Can store in versions so that can get any previous versions at any time) What do you mean by IT Infrastructure?? IT Infrastructure is a composite of the following things 1. Software 2. Network 3. People 4. Process What are the problems that system admins used to face earlier when there were no configuration management tools? 1. Managing users & Groups is big hectic thing (create users and groups, delete, edit\u2026\u2026) 2. Dealing with packages (Installing, Upgrading & Uninstalling) 3. Taking backups on regular basis manually 4. Deploying all kinds of applications in servers 5. Configure services (Starting, stopping and restarting services) These are some problems that system administrators used to face earlier in their manual process of managing configuration of any machine. Why should we go with Configuration Management Tool? 1. By using the Configuration Management Tool, we can automate almost each and every admin task. 2. We can increase uptime so that can provide maximum user satisfaction. 3. Improve the performance of systems. 4. Ensure compliance 5. Prevent errors as tools won\u2019t do any errors 6. Reduce cost (Buy tool once and use 24/7) How this Configuration Management Tool works? Whatever system admins (Linux/windows) used to do manually, now we are automating all those tasks by using any Configuration Management Tool. We can use this tool whether your servers are in on-premises or in the cloud. It turns your code into infrastructure. So your code is versionable, repeatable and testable. You only need to tell what the desired configuration should be, not how to achieve it. Through automation, we get our desired state of server. This is unique feature of Configuration Management Tool. What is the architecture of Chef? Chef is an administration tool. In this we have total 3 stages: 1. Chef Workstation (It is the place where we write code) 2. Chef Server (It is the place where we store code) 3. Chef Node (It is the place where we apply code) We need to establish communication among workstation, server and nodes. You can have any no of nodes. There is no limit. Chef can manage any no of nodes effectively. Components of Chef? 1. Chef Workstation: Where you write the code 2. Chef Server: Where you upload the code 3. Chef Node: Where you apply the code 4. Knife: Tool to establish communication among workstation, server & node. 5. Chef-client: Tool runs on every chef node to pull code from chef server 6. Ohai: Maintains current state information of chef node (System Discovery Tool) 7. Idempotency: Tracking the state of system resources to ensure that the changes should not re-apply repeatedly. 8. Chef Supermarket: Where you get custom code How does Chef Works? We need to install chef package in workstation, server and nodes. We create cookbook in workstation. Inside cookbook, there will be a default recipe where you write code in ruby script. You can create any no of recipes. There is no limit. After writing code in recipe, we upload whole cookbook to chef server. Chef server acts as central hub storing code. Then, we need to add this cookbook\u2019s recipe to nodes run-list. Chef-client tool will be there in each and every chef node. It runs frequently. Chef-client comes to chef server and take that code and applies that code in node. This is how code will be converted into infrastructure. What is Idempotency? It is unique feature in all configuration management tools. It ensures that changes should not re-apply repeatedly. Once chef-client converted code into Infrastructure, then even chef-client runs again, it will not take any action. It won\u2019t do the same task again and again. If any new changes are there in that code, then only chef-client is going to take action. So it doesn\u2019t make any difference ever if you run chef-client any no of times. So tracking the system details to not to reapply changes again and again, we call Idempotency. What is Ohai and how does it works?? Ohai we call \u201cSystem Discovery Tool\u201d. It stores system information. It captures each and every minute details of system and updates it then and there if any new changes are there. Whenever chef-client converts code in infrastructure in node, immediately Ohai store will be updated. Next time onwards, before chef-client runs, it verifies in Ohai store to know about current state of information. So chef-client will come to know the current state of server. Then chef-client acts accordingly. If new changes are there, then only it will take action. If there are no new changes, then it won\u2019t take any action. Ohai tool helps in achieving this. How many types of chef server? Total there are 3 ways through which we can manage chef server. 1. Directly we can take chef server from Chef Company itself. In this case, everything will be managed by Chef Company. You will get support from chef. This type of server we call Managed/Hosted chef. This is completely Graphical User Interface (GUI). It\u2019s not free. We need to pay to Chef Company after exceeding free tier limit. 2. We can launch one server and we need to install chef server package. It is completely free package. It\u2019s GUI (Graphical User interface) 3. We can launch one server and we need to install chef server package. It is completely free package. It\u2019s CLI (Command Line Interface). What is there inside cookbook? Below mentioned files and folders will be there inside cookbook when you first create it: Chefignore: like .gitignore (to ignore files and folders) Kitchen.yml: for testing of cookbook Metadata.rb: name, author, version\u2026. etc of cookbook Readme.md: information about usage of cookbook Recipe: It is a file where you write code Spec: for unit test Test: for integration test What is Attributes concept in chef? Sometimes we might need to deploy web applications to in nodes and for that we need to know some host specific details of each server like IP Address, Hostname etc. Because we need to mention that in configuration files of each server. These files we call as Configuration files. This information will be vary from system to system. These host specific details that we mention in Configuration files,we call \u201cAttributes\u201d. Chef-client tool gathers these Attributes from Ohai store and puts in configuration files. Instead of hard coding these attributes, we mention as variables so that every time, file will be updated with latest details of their respective nodes. What is Run-list in Chef? This is an ordered list of recipes that we are going to apply to nodes. We mention all recipes in cookbook and then we upload that cookbook to chef server. Then, we attach all recipes to nodes run-list in sequence order. When chef-client runs, it applies all recipes to nodes in the same order whatever the order you mention in run-list. Because sometimes order is important especially when we deal with dependent recipes. What is bootstrap? It is the process of adding chef node to chef server or we can call, bringing any machine into chef environment. In this bootstrapping process total three action will be performed automatically. 1. Node gets connected to chef server. 2. Chef server will install chef package in chef node. 3. Cookbooks will be applied to chef node. It is only one time effort. As and when we purchase any new machine in company, immediately we add that server to chef server. At a time, we can only bootstrap one machine, not multiple machines. What is the workflow of Chef? We connect chef workstation, chef server and chef node with each other. After that, we create cookbook in chef workstation and inside that cookbook, we write code in recipe w.r.t. the infrastructure to be created. Then we upload entire cookbook to chef server and attach that cookbook\u2019s recipe to nodes run-list. Now we automate chef-client which will be there in all chef nodes. Chef-client runs frequently towards chef server for new code. So chef-client will get that code from server and finally applies to chef node. This is how, code is converted into infrastructure. If no changes are there in code, even if chef-client runs any no of time, it won\u2019t take any action until it finds some changes in code. This is what we call Idempotency. How does we connect Chef Workstation to Chef Server? First we download started kit from chef server. This will be downloaded in the form of zip file. If we extract this zip file, we will get chef-repo folder. This chef-repo folder we need to place in chef workstation. Inside chef-repo folder, we can see total three folders. They are .chef, cookbooks and roles. Out of these three, .chef folder is responsible to establish communication between chef server and chef workstation. Because, inside .chef folder, we can see two files. They are knife.rb and organization.pem. Inside kinfe.rb, there will be the url (address) of chef server. Because of this url, communication will be established between chef server and chef workstation. This is how we connect Chef Workstation to Chef Server. How does the chef-client runs automatically? By default, chef-client runs manually. So we need to automate this manually. For this, we use \u201ccron tool\u201d which is the default tool in all Linux machines use to schedule tasks to be executed automatically at frequent intervals. So in this \u201ccrontab\u201d file, we give chef-client command and we need to set the timing as per our requirement. Then onwards chef-client runs automatically after every frequent intervals. It is only one time effort. When we purchase any new server in company, along with bootstrap, we automate chef-client then and there. What is chef supermarket? Chef supermarket is the place where we get custom cookbooks. Every time we need not to create cookbooks and need not to write code from scratch. We can go with custom cookbooks which are available in chef supermarket being provided by chef organization and community. We can download these cookbooks and modify as per our needs. We get almost each and every cookbook from chef supermarket. They are safe to use. What is wrapper cookbook? Either we can download those chef supermarket cookbooks or without downloading, we can call these supermarket cookbooks during run time so that every time we get updates automatically for that cookbook if any new updates are there. Here, we use our own cookbook to call chef supermarket cookbook. This process of calling cookbook by using another cookbook, we call wrapper cookbook. Especially, we use this concept to automate chef-client. What is \u201croles\u201d in chef? Roles are nothing but a Custom run-list. We create role & upload to chef server & assign them to nodes. If we have so many nodes, need to add cookbook to run-list of all those nodes, it is very difficult to attach to all nodes run-list. So, we create role & attach that role to all those nodes once. Next time onwards, add cookbook to that role. Automatically, that cookbook will be attached to all those nodes. So role is one time effort. Instead of adding cookbooks to each & every node\u2019s run-list always, just create a role & attach that role to nodes. When we add cookbook to that role, it will be automatically applied to all nodes those assigned with that role. What is include_recipe in chef? By default, we can call one recipe at a time in one cookbook. But if you want to call multiple recipes from same cookbook, we use include_recipe concept. Here, we take default recipe and we mention all recipes to be called in this default recipe in an order. If we call default recipe, automatically default recipe will call all other recipes which are there inside default recipe. By using one recipe, we can call any number of recipes. This process of calling one recipe by using other recipe, we call as include_recipe. Here condition is we can call recipes from same cookbook, but not from different cookbooks. How to deploy a web server by using chef? package \u2018httpd\u2019 do action :install end file \u2018/var/www/html/index.html\u2019 do content \u2018Hello nightwolf Students!!\u2019 action :create end service \u2018httpd\u2019 do action [ :enable, :start ] end How to write ruby code to create file, directory? file \u2018/myfile\u2019 do content \u2018This is my second file\u2019 action :create owner \u2018root\u2019 group \u2018root\u2019 end directory \u2018/mydir\u2019 do action :create owner \u2018root\u2019 group \u2018root\u2019 end How to write ruby code to create user, group and install package? user \u2018user1\u2019 do action: create end group \u2018group1\u2019 do action :create members \u2018user1\u2019 append true end package \u2018httpd\u2019 do action: install end What is container? The container is like a virtual machine in which we can deploy any type of applications, softwares and libraries. It\u2019s a light weight virtual machine which uses OS in the form of image, which is having less in size compare to traditional VMware and oracle virtual box OS images. Container word has been taken from shipping containers. It has everything to run an application. What is virtualization? Logically dividing big machine into multiple virtual machines so that each virtual machine acts as new server and we can deploy any kind of applications in it. For this first we install any virtualization software on top of base OS. This virtualization software will divide base machine resources in to logical components. In a simple terms, logically dividing one machine into multiple machines we call virtualization. What is Docker? Docker is a tool by using which, we create containers in less time. Docker uses light weight OS in the form of docker images that we will get from docker hub. Docker is open source now. It became so popular because of its unique virtualization concept called \u201cContainerization\u201d which is not there in other tools. We can use docker in both windows and Linux machines. What do you mean by docker image? Docker image is light weight OS provided by docker company. We can get any type of docker image form docker hub. We use these docker images to create docker containers. This docker images may contain only OS or OS + other softwares as well. Each software in docker image, will be stored in the form of layer. Advantage of using docker images is, we can replicate the same environment any no of times. What are the ways through which we can create docker images? There are three ways through which we can create docker images. 1. We can take any type of docker image directly from docker hub being provided by docker company and docker community. 2. We can create our own docker images form our own docker containers i.e. first we create container form base docker image taken form docker hub and then by going inside container, we install all required soft wares and then create docker image from our own docker container. 3. We can create docker image form docker file. It is the most preferred way of creating docker images. What is docker file and why do we use it? It is a just normal text file with instructions in it to build docker image. It is the automated way of creating docker images. Once you build docker image, automatically docker file will be created. In this file, we mention required OS image and all required soft wares in the form of instructions. Once we build docker file, back end, docker container will be created and then docker image will be crated from that container and that container will be destroyed automatically. Difference between docker and VM Ware? VM Ware uses complete OS which contains GBs in size. But docker image size is MBs only. So it takes less size. That\u2019s why it takes less base machine resources. This docker image is compressed version of OS. The second advantage of docker is, there is no pre-allocation of RAM. During run time, it takes RAM as pre requirement from base machine and one\u2019s job is done, it release RAM. But in VM Ware, pre-allocation of RAM is there and it blocked whether it uses or not. So need more RAM for base machine if you want to use VM Ware unlike Docker. What is OS-Lever Virtualization? It is the unique feature of Docker which is not available in other virtualization soft wares. Docker takes most of UNIX features form host machine OS and it only takes extra layers of required OS in the form of docker image. So docker image contains only extra layers of required OS. For core UNIX kernel, it depends upon host OS, why because UNIX kernel is same in any of the UNIX and Linux flavors. In a simple terms, docker takes host OS virtually. That\u2019s why we call this concept as OS-Lever Virtualization. What is Layered file system/Union file system? Inside docker container, wheat ever we do, that forms as a new layer. For instance, creating files, directories, installing packages etc. This is what we call as layered file system. Each layer takes less space. We can create docker image form this container. In that docker image also we get all these layers and forms unity. That\u2019s why we also call Union File System. If we create container out of docker image, you can able to see all those files, directories and packages. This is what replication of same environment. What are the benefits of Docker? \u2000 1. Containerization (OS level virtualization) (No need guest OS) \u2000 2. No pre-allocation of RAM \u20003. Can replicate same environment \u2000 4. Less cost \u2000 5. Less weight (MB\u2019s in size) \u2000 6. Fast to fire up \u2000 7. Can run on physical/virtual/cloud \u2000 8. Can re-use (same image) \u2000 9. Can create containers in less time","title":"DevOps Interview Question and Answered for Freshers and Experienced"},{"location":"nightwolf-cotribution/docker_interview_questions/","text":"Docker interview questions and Answers \uf0c1 We have consolidated a list of frequently asked Docker interview questions for Freshers and Experienced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Docker Basic Interview Questions \uf0c1 Can you tell something about docker container? - In simplest terms, docker containers consist of applications and all their dependencies. - They share the kernel and system resources with other containers and run asisolated systems in the host operating system. - The main aim of docker containers is to get rid of the infrastructure dependency while deploying and running applications. This means that any containerized application can run on any platform irrespective of the infrastructure being used beneath. - Technically, they are just the runtime instances of docker images. What are docker images? They are executable packages(bundled with application code & dependencies, soware packages, etc.) for the purpose of creating containers. Docker images can be deployed to any docker environment and the containers can be spun up there to run the application. What is a DockerFile? It is a text file that has all commands which need to be run for building a given image. Can you tell what is the functionality of a hypervisor? A hypervisor is a so\u2000\u2000ware that makes virtualization happen because of which is sometimes referred to as the Virtual Machine Monitor. This divides the resources of the host system and allocates them to each guest environment installed. - This means that multiple OS can be installed on a single host system. Hypervisors are of 2 types: 1. Native Hypervisor: This type is also called a Bare-metal Hypervisor and runs directly on the underlying host system which also ensures direct access to the host hardware which is why it does not require base OS. 2. Hosted Hypervisor: This type makes use of the underlying host operating system which has the existing OS installed. What can you tell about Docker Compose? It is a YAML file consisting of all the details regarding various services, networks, and volumes that are needed for setting up the Docker-based application. So, dockercompose is used for creating multiple containers, host them and establish communication between them. For the purpose of communication amongst the containers, ports are exposed by each and every container. Can you tell something about docker namespace? A namespace is basically a Linux feature that ensures OS resources partition in a mutually exclusive manner. This forms the core concept behind containerization as namespaces introduce a layer of isolation amongst the containers. In docker, the namespaces ensure that the containers are portable and they don't affect the underlying host. Examples for namespace types that are currently being supported by Docker \u2013 PID, Mount, User, Network, IPC. What is the docker command that lists the status of all docker containers? In order to get the status of all the containers, we run the below command: docker # ps -a On what circumstances will you lose data stored in a container? The data of a container remains in it until and unless you delete the container. What is docker image registry? - A Docker image registry, in simple terms, is an area where the docker images are stored. Instead of converting the applications to containers each and every time, a developer can directly use the images stored in the registry. - This image registry can either be public or private and Docker hub is the most popular and famous public registry available. How many Docker components are there? There are three docker components, they are - Docker Client, Docker Host, and Docker Registry. - Docker Client: This component performs \u201cbuild\u201d and \u201crun\u201d operations for the purpose of opening communication with the docker host. - Docker Host: This component has the main docker daemon and hosts containers and their associated images. The daemon establishes a connection with the docker registry. - Docker Registry: This component stores the docker images. There can be a public registry or a private one. The most famous public registries are Docker Hub and Docker Cloud. What is a Docker Hub? - It is a public cloud-based registry provided by Docker for storing public images of the containers along with the provision of finding and sharing them. - The images can be pushed to Docker Hub through the docker push command. What command can you run to export a docker image as an archive? This can be done using the docker save command and the syntax is: # docker save -o <exported_name>.tar <container-name> What command can be run to import a pre-exported Docker image into another Docker host? This can be done using the docker load command and the syntax is: # docker load -i <export_image_name>.tar Can a paused container be removed from Docker? No, it is not possible! A container MUST be in the stopped state before we can remove it. What command is used to check for the version of docker client and server? - The command used to get all version information of the client and server is the \"docker version\". - To get only the server version details, we can run: # docker version --format '{{.Server.Version}}' (adsbygoogle = window.adsbygoogle || []).push({}); Docker Intermediate Interview Questions \uf0c1 Differentiate between virtualization and containerization. The question indirectly translates to explaining the difference between virtual machines and Docker containers. Virtualization Containerization This helps developers to run and host multiple OS on the hardware of a single physical server. This helps developers to deploy multiple applications using the same operating system on a single virtual machine or server. Hypervisors provide overall virtual machines to the guest operating systems. Containers ensure isolated environment/ user spaces are provided for running the applications. Any changes done within the container do not reflect on the host or other containers of the same host. These virtual machines form an abstraction of the system hardware layer this means that each virtual machine on the host acts like a physical machine. Containers form abstraction of the application layer which means that each container constitutes a different application. Differentiate between COPY and ADD commands that are used in a Dockerfile? Both the commands have similar functionality, but 'COPY' is more preferred because of its higher transparency level than that of 'ADD'. 'COPY' provides just the basic support of copying local files into the container whereas 'ADD' provides additional features like remote URL and tar extraction support. Can a container restart by itself? - Yes, it is possible only while using certain docker-defined policies while using the docker run command. Following are the available policies: 1. Off: In this, the container won\u2019t be restarted in case it is stopped or it fails. 2. On-failure: Here, the container restarts by itself only when it experiences failures not associated with the user. 3. Unless-stopped: Using this policy, ensures that a container can restart only when the command is executed to stop it by the user. 4. Always: Irrespective of the failure or stopping, the container always gets restarted in this type of policy. These policies can be used as: # docker run -dit \u2014 restart [restart-policy-value] [container_name] Can you tell the differences between a docker Image and Layer? Image: This is built up from a series of read-only layers of instructions. An image corresponds to the docker container and is used for speedy operation due to the caching mechanism of each step. Layer: Each layer corresponds to an instruction of the image\u2019s Dockerfile. In simple words, the layer is also an image but it is the image of the instructions run. Consider the example Dockerfile below. FROM ubuntu:18.04 COPY . /myapp RUN make /myapp CMD python /myapp/app.py Importantly, each layer is only a set of differences from the layer before it. - The result of building this docker file is an image. Whereas the instructions present in this file add the layers to the image. The layers can be thought of as intermediate images. In the example above, there are 4 instructions, hence 4 layers are added to the resultant image. What is the purpose of the volume parameter in a docker run command? - The syntax of docker run when using the volumes is: # docker run -v host_path:docker_path <container_name> - The volume parameter is used for syncing a directory of a container with any of the host directories. Consider the below command as an example: # docker run -v /data/app:usr/src/app myapp The above command mounts the directory /data/app in the host to the usr/src/app directory. We can sync the container with the data files from the host without having the need to restart it. - This also ensures data security in cases of container deletion. This ensures that even if the container is deleted, the data of the container exists in the volume mapped host location making it the easiest way to store the container data. Where are docker volumes stored in docker? Volumes are created and managed by Docker and cannot be accessed by non-docker entities. They are stored in Docker host filesystem at '/var/lib/docker/volumes/'. What does the docker info command do? The command gets detailed information about Docker installed on the host system. The information can be like what is the number of containers or images and in what state they are running and hardware specifications like total memory allocated, speed of the processor, kernel version, etc. Can you tell the what are the purposes of up, run, and start commands of docker compose? - Using the up command for keeping a docker-compose up (ideally at all times), we can start or restart all the networks, services, and drivers associated with the app that are specified in the docker-compose.yml file. Now if we are running the docker-compose up in the \u201cattached\u201d mode then all the logs from the containers would be accessible to us. In case the docker-compose is run in the \u201cdetached\u201d mode, then once the containers are started, it just exits and shows no logs. - Using the run command, the docker-compose can run one-off or ad-hoc tasks based on the business requirements. Here, the service name has to be provided and the docker starts only that specific service and also the other services to which the target service is dependent (if any). - This command is helpful for testing the containers and also performing tasks such as adding or removing data to the container volumes etc. - Using the start command, only those containers can be restarted which were already created and then stopped. This is not useful for creating new containers on its own. What are the basic requirements for the docker to run on any system? Docker can run on both Windows and Linux platforms. - For the Windows platform, docker atleast needs Windows 10 64bit with 2GB RAM space. For the lower versions, docker can be installed by taking help of the toolbox. Docker can be downloaded from https://docs.docker.com/docker-forwindows/ website. - For Linux platforms, Docker can run on various Linux flavors such as Ubuntu >=12.04, Fedora >=19, RHEL >=6.5, CentOS >=6 etc. Can you tell the approach to login to the docker registry? Using the docker login command credentials to log in to their own cloud repositories can be entered and accessed. List the most commonly used instructions in Dockerfile? - FROM: This is used to set the base image for upcoming instructions. A docker file is considered to be valid if it starts with the FROM instruction. - LABEL: This is used for the image organization based on projects, modules, or licensing. It also helps in automation as we specify a key-value pair while defining a label that can be later accessed and handled programmatically. - RUN: This command is used to execute instructions following it on the top of the current image in a new layer. Note that with each RUN command execution, we add layers on top of the image and then use that in subsequent steps. - CMD: This command is used to provide default values of an executing container. In cases of multiple CMD commands the last instruction would be considered. Can you differentiate between Daemon Logging and Container Logging? - In docker, logging is supported at 2 levels and they are logging at the Daemon level or logging at the Container level. - Daemon Level: This kind of logging has four levels- Debug, Info, Error, and Fatal. => Debug has all the data that happened during the execution of the daemon process. => Info carries all the information along with the error information during the execution of the daemon process. => Errors have those errors that occurred during the execution of the daemon process. => Fatal has the fatal errors that occurred during the execution. - Container Level: => Container level logging can be done using the command: # sudo docker run \u2013it <container_name> /bin/bash => In order to check for the container level logs, we can run the command: # sudo docker logs <container_id> What is the way to establish communication between docker host and Linux host? This can be done using networking by identifying the \u201cipconfig\u201d on the docker host. This command ensures that an ethernet adapter is created as long as the docker is present in the host. What is the best way of deleting a container? We need to follow the following two steps for deleting a container: # docker stop <container_id> # docker rm <container_id> Can you tell the difference between CMD and ENTRYPOINT? - CMD command provides executable defaults for an executing container. In case the executable has to be omitted then the usage of ENTRYPOINT instruction along with the JSON array format has to be incorporated. - ENTRYPOINT specifies that the instruction within it will always be run when the container starts. This command provides an option to configure the parameters and the executables. If the DockerFile does not have this command, then it would still get inherited from the base image mentioned in the FROM instruction. => The most commonly used ENTRYPOINT is /bin/sh or /bin/bash for most of the base images. - As part of good practices, every DockerFile should have at least one of these two commands. (adsbygoogle = window.adsbygoogle || []).push({}); Docker Advanced Interview Questions \uf0c1 Can we use JSON instead of YAML while developing dockercompose file in Docker? Yes! It can be used. In order to run docker-compose with JSON: # docker-compose -f docker-compose.json up can be used. How many containers you can run in docker and what are the factors influencing this limit? There is no clearly defined limit to the number of containers that can be run within docker. But it all depends on the limitations - more specifically hardware restrictions. The size of the app and the CPU resources available are 2 important factors influencing this limit. In case your application is not very big and you have abundant CPU resources, then we can run a huge number of containers. Describe the lifecycle of Docker Container? The different stages of the docker container from the start of creating it to its end are called the docker container life cycle. The most important stages are: - Created: This is the state where the container has just been created new but not started yet. - Running: In this state, the container would be running with all its associated processes. - Paused: This state happens when the running container has been paused. - Stopped: This state happens when the running container has been stopped. - Deleted: In this, the container is in a dead state. How to use docker for multiple application environments? - Docker-compose feature of docker will come to help here. In the dockercompose file, we can define multiple services, networks, and containers along with the volume mapping in a clean manner, and then we can just call the command \u201cdocker-compose up\u201d. - When there are multiple environments involved - it can be either dev, staging, uat, or production servers, we would want to define the server-specific dependencies and processes for running the application. In this case, we can go ahead with creating environment-specific docker-compose files of the name \u201cdocker-compose.{environment}.yml\u201d and then based on the environment, we can set up and run the application. How will you ensure that a container 1 runs before container 2 while using docker compose? Docker-compose does not wait for any container to be \u201cready\u201d before going ahead with the next containers. In order to achieve the order of execution, we can use: - The \u201cdepends_on\u201d which got added in version 2 of docker-compose can be used as shown in a sample docker-compose.yml file below: version: \"2.4\" services: backend: build: . depends_on: - db db: image: postgres The introduction of service dependencies has various causes and effects: - The docker-compose up command starts and runs the services in the dependency order specified. For the above example, the DB container is started before the backend. - docker-compose up SERVICE_NAME by default includes the dependencies associated with the service. In the given example, running docker-compose up backend creates and starts DB (dependency of backend). - Finally, the command docker-compose stop also stops the services in the order of the dependency specified. For the given example, the backend service is stopped before the DB service. What is docker architecture? What is docker lifecycle ? What is dockerfile and docker compose file ? Explain various layers in a dockerfile What is docker newtworking and tell various types of network in docker What is default network in docker How one containetr talks with other container ? How to debug the container ? What is docker swarm ? Tell some commands in docker. What is difference between ADD/COPY , CMD/ENTRYPOINT,RUN/CMD ? Tell the docker file best practices ? How to reduvce a docker file size ? How to store the docker file in jfrog/dockerhub ? How to create a docker image if no internet connectivity is there ? Write a docker file and state various layers and use the depends_on concept. How to save a container as image and then as a zip file ? What are docker volumes ? (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Docker Interview Questions"},{"location":"nightwolf-cotribution/docker_interview_questions/#docker-interview-questions-and-answers","text":"We have consolidated a list of frequently asked Docker interview questions for Freshers and Experienced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Docker interview questions and Answers"},{"location":"nightwolf-cotribution/docker_interview_questions/#docker-basic-interview-questions","text":"Can you tell something about docker container? - In simplest terms, docker containers consist of applications and all their dependencies. - They share the kernel and system resources with other containers and run asisolated systems in the host operating system. - The main aim of docker containers is to get rid of the infrastructure dependency while deploying and running applications. This means that any containerized application can run on any platform irrespective of the infrastructure being used beneath. - Technically, they are just the runtime instances of docker images. What are docker images? They are executable packages(bundled with application code & dependencies, soware packages, etc.) for the purpose of creating containers. Docker images can be deployed to any docker environment and the containers can be spun up there to run the application. What is a DockerFile? It is a text file that has all commands which need to be run for building a given image. Can you tell what is the functionality of a hypervisor? A hypervisor is a so\u2000\u2000ware that makes virtualization happen because of which is sometimes referred to as the Virtual Machine Monitor. This divides the resources of the host system and allocates them to each guest environment installed. - This means that multiple OS can be installed on a single host system. Hypervisors are of 2 types: 1. Native Hypervisor: This type is also called a Bare-metal Hypervisor and runs directly on the underlying host system which also ensures direct access to the host hardware which is why it does not require base OS. 2. Hosted Hypervisor: This type makes use of the underlying host operating system which has the existing OS installed. What can you tell about Docker Compose? It is a YAML file consisting of all the details regarding various services, networks, and volumes that are needed for setting up the Docker-based application. So, dockercompose is used for creating multiple containers, host them and establish communication between them. For the purpose of communication amongst the containers, ports are exposed by each and every container. Can you tell something about docker namespace? A namespace is basically a Linux feature that ensures OS resources partition in a mutually exclusive manner. This forms the core concept behind containerization as namespaces introduce a layer of isolation amongst the containers. In docker, the namespaces ensure that the containers are portable and they don't affect the underlying host. Examples for namespace types that are currently being supported by Docker \u2013 PID, Mount, User, Network, IPC. What is the docker command that lists the status of all docker containers? In order to get the status of all the containers, we run the below command: docker # ps -a On what circumstances will you lose data stored in a container? The data of a container remains in it until and unless you delete the container. What is docker image registry? - A Docker image registry, in simple terms, is an area where the docker images are stored. Instead of converting the applications to containers each and every time, a developer can directly use the images stored in the registry. - This image registry can either be public or private and Docker hub is the most popular and famous public registry available. How many Docker components are there? There are three docker components, they are - Docker Client, Docker Host, and Docker Registry. - Docker Client: This component performs \u201cbuild\u201d and \u201crun\u201d operations for the purpose of opening communication with the docker host. - Docker Host: This component has the main docker daemon and hosts containers and their associated images. The daemon establishes a connection with the docker registry. - Docker Registry: This component stores the docker images. There can be a public registry or a private one. The most famous public registries are Docker Hub and Docker Cloud. What is a Docker Hub? - It is a public cloud-based registry provided by Docker for storing public images of the containers along with the provision of finding and sharing them. - The images can be pushed to Docker Hub through the docker push command. What command can you run to export a docker image as an archive? This can be done using the docker save command and the syntax is: # docker save -o <exported_name>.tar <container-name> What command can be run to import a pre-exported Docker image into another Docker host? This can be done using the docker load command and the syntax is: # docker load -i <export_image_name>.tar Can a paused container be removed from Docker? No, it is not possible! A container MUST be in the stopped state before we can remove it. What command is used to check for the version of docker client and server? - The command used to get all version information of the client and server is the \"docker version\". - To get only the server version details, we can run: # docker version --format '{{.Server.Version}}' (adsbygoogle = window.adsbygoogle || []).push({});","title":"Docker Basic Interview Questions"},{"location":"nightwolf-cotribution/docker_interview_questions/#docker-intermediate-interview-questions","text":"Differentiate between virtualization and containerization. The question indirectly translates to explaining the difference between virtual machines and Docker containers. Virtualization Containerization This helps developers to run and host multiple OS on the hardware of a single physical server. This helps developers to deploy multiple applications using the same operating system on a single virtual machine or server. Hypervisors provide overall virtual machines to the guest operating systems. Containers ensure isolated environment/ user spaces are provided for running the applications. Any changes done within the container do not reflect on the host or other containers of the same host. These virtual machines form an abstraction of the system hardware layer this means that each virtual machine on the host acts like a physical machine. Containers form abstraction of the application layer which means that each container constitutes a different application. Differentiate between COPY and ADD commands that are used in a Dockerfile? Both the commands have similar functionality, but 'COPY' is more preferred because of its higher transparency level than that of 'ADD'. 'COPY' provides just the basic support of copying local files into the container whereas 'ADD' provides additional features like remote URL and tar extraction support. Can a container restart by itself? - Yes, it is possible only while using certain docker-defined policies while using the docker run command. Following are the available policies: 1. Off: In this, the container won\u2019t be restarted in case it is stopped or it fails. 2. On-failure: Here, the container restarts by itself only when it experiences failures not associated with the user. 3. Unless-stopped: Using this policy, ensures that a container can restart only when the command is executed to stop it by the user. 4. Always: Irrespective of the failure or stopping, the container always gets restarted in this type of policy. These policies can be used as: # docker run -dit \u2014 restart [restart-policy-value] [container_name] Can you tell the differences between a docker Image and Layer? Image: This is built up from a series of read-only layers of instructions. An image corresponds to the docker container and is used for speedy operation due to the caching mechanism of each step. Layer: Each layer corresponds to an instruction of the image\u2019s Dockerfile. In simple words, the layer is also an image but it is the image of the instructions run. Consider the example Dockerfile below. FROM ubuntu:18.04 COPY . /myapp RUN make /myapp CMD python /myapp/app.py Importantly, each layer is only a set of differences from the layer before it. - The result of building this docker file is an image. Whereas the instructions present in this file add the layers to the image. The layers can be thought of as intermediate images. In the example above, there are 4 instructions, hence 4 layers are added to the resultant image. What is the purpose of the volume parameter in a docker run command? - The syntax of docker run when using the volumes is: # docker run -v host_path:docker_path <container_name> - The volume parameter is used for syncing a directory of a container with any of the host directories. Consider the below command as an example: # docker run -v /data/app:usr/src/app myapp The above command mounts the directory /data/app in the host to the usr/src/app directory. We can sync the container with the data files from the host without having the need to restart it. - This also ensures data security in cases of container deletion. This ensures that even if the container is deleted, the data of the container exists in the volume mapped host location making it the easiest way to store the container data. Where are docker volumes stored in docker? Volumes are created and managed by Docker and cannot be accessed by non-docker entities. They are stored in Docker host filesystem at '/var/lib/docker/volumes/'. What does the docker info command do? The command gets detailed information about Docker installed on the host system. The information can be like what is the number of containers or images and in what state they are running and hardware specifications like total memory allocated, speed of the processor, kernel version, etc. Can you tell the what are the purposes of up, run, and start commands of docker compose? - Using the up command for keeping a docker-compose up (ideally at all times), we can start or restart all the networks, services, and drivers associated with the app that are specified in the docker-compose.yml file. Now if we are running the docker-compose up in the \u201cattached\u201d mode then all the logs from the containers would be accessible to us. In case the docker-compose is run in the \u201cdetached\u201d mode, then once the containers are started, it just exits and shows no logs. - Using the run command, the docker-compose can run one-off or ad-hoc tasks based on the business requirements. Here, the service name has to be provided and the docker starts only that specific service and also the other services to which the target service is dependent (if any). - This command is helpful for testing the containers and also performing tasks such as adding or removing data to the container volumes etc. - Using the start command, only those containers can be restarted which were already created and then stopped. This is not useful for creating new containers on its own. What are the basic requirements for the docker to run on any system? Docker can run on both Windows and Linux platforms. - For the Windows platform, docker atleast needs Windows 10 64bit with 2GB RAM space. For the lower versions, docker can be installed by taking help of the toolbox. Docker can be downloaded from https://docs.docker.com/docker-forwindows/ website. - For Linux platforms, Docker can run on various Linux flavors such as Ubuntu >=12.04, Fedora >=19, RHEL >=6.5, CentOS >=6 etc. Can you tell the approach to login to the docker registry? Using the docker login command credentials to log in to their own cloud repositories can be entered and accessed. List the most commonly used instructions in Dockerfile? - FROM: This is used to set the base image for upcoming instructions. A docker file is considered to be valid if it starts with the FROM instruction. - LABEL: This is used for the image organization based on projects, modules, or licensing. It also helps in automation as we specify a key-value pair while defining a label that can be later accessed and handled programmatically. - RUN: This command is used to execute instructions following it on the top of the current image in a new layer. Note that with each RUN command execution, we add layers on top of the image and then use that in subsequent steps. - CMD: This command is used to provide default values of an executing container. In cases of multiple CMD commands the last instruction would be considered. Can you differentiate between Daemon Logging and Container Logging? - In docker, logging is supported at 2 levels and they are logging at the Daemon level or logging at the Container level. - Daemon Level: This kind of logging has four levels- Debug, Info, Error, and Fatal. => Debug has all the data that happened during the execution of the daemon process. => Info carries all the information along with the error information during the execution of the daemon process. => Errors have those errors that occurred during the execution of the daemon process. => Fatal has the fatal errors that occurred during the execution. - Container Level: => Container level logging can be done using the command: # sudo docker run \u2013it <container_name> /bin/bash => In order to check for the container level logs, we can run the command: # sudo docker logs <container_id> What is the way to establish communication between docker host and Linux host? This can be done using networking by identifying the \u201cipconfig\u201d on the docker host. This command ensures that an ethernet adapter is created as long as the docker is present in the host. What is the best way of deleting a container? We need to follow the following two steps for deleting a container: # docker stop <container_id> # docker rm <container_id> Can you tell the difference between CMD and ENTRYPOINT? - CMD command provides executable defaults for an executing container. In case the executable has to be omitted then the usage of ENTRYPOINT instruction along with the JSON array format has to be incorporated. - ENTRYPOINT specifies that the instruction within it will always be run when the container starts. This command provides an option to configure the parameters and the executables. If the DockerFile does not have this command, then it would still get inherited from the base image mentioned in the FROM instruction. => The most commonly used ENTRYPOINT is /bin/sh or /bin/bash for most of the base images. - As part of good practices, every DockerFile should have at least one of these two commands. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Docker Intermediate Interview Questions"},{"location":"nightwolf-cotribution/docker_interview_questions/#docker-advanced-interview-questions","text":"Can we use JSON instead of YAML while developing dockercompose file in Docker? Yes! It can be used. In order to run docker-compose with JSON: # docker-compose -f docker-compose.json up can be used. How many containers you can run in docker and what are the factors influencing this limit? There is no clearly defined limit to the number of containers that can be run within docker. But it all depends on the limitations - more specifically hardware restrictions. The size of the app and the CPU resources available are 2 important factors influencing this limit. In case your application is not very big and you have abundant CPU resources, then we can run a huge number of containers. Describe the lifecycle of Docker Container? The different stages of the docker container from the start of creating it to its end are called the docker container life cycle. The most important stages are: - Created: This is the state where the container has just been created new but not started yet. - Running: In this state, the container would be running with all its associated processes. - Paused: This state happens when the running container has been paused. - Stopped: This state happens when the running container has been stopped. - Deleted: In this, the container is in a dead state. How to use docker for multiple application environments? - Docker-compose feature of docker will come to help here. In the dockercompose file, we can define multiple services, networks, and containers along with the volume mapping in a clean manner, and then we can just call the command \u201cdocker-compose up\u201d. - When there are multiple environments involved - it can be either dev, staging, uat, or production servers, we would want to define the server-specific dependencies and processes for running the application. In this case, we can go ahead with creating environment-specific docker-compose files of the name \u201cdocker-compose.{environment}.yml\u201d and then based on the environment, we can set up and run the application. How will you ensure that a container 1 runs before container 2 while using docker compose? Docker-compose does not wait for any container to be \u201cready\u201d before going ahead with the next containers. In order to achieve the order of execution, we can use: - The \u201cdepends_on\u201d which got added in version 2 of docker-compose can be used as shown in a sample docker-compose.yml file below: version: \"2.4\" services: backend: build: . depends_on: - db db: image: postgres The introduction of service dependencies has various causes and effects: - The docker-compose up command starts and runs the services in the dependency order specified. For the above example, the DB container is started before the backend. - docker-compose up SERVICE_NAME by default includes the dependencies associated with the service. In the given example, running docker-compose up backend creates and starts DB (dependency of backend). - Finally, the command docker-compose stop also stops the services in the order of the dependency specified. For the given example, the backend service is stopped before the DB service. What is docker architecture? What is docker lifecycle ? What is dockerfile and docker compose file ? Explain various layers in a dockerfile What is docker newtworking and tell various types of network in docker What is default network in docker How one containetr talks with other container ? How to debug the container ? What is docker swarm ? Tell some commands in docker. What is difference between ADD/COPY , CMD/ENTRYPOINT,RUN/CMD ? Tell the docker file best practices ? How to reduvce a docker file size ? How to store the docker file in jfrog/dockerhub ? How to create a docker image if no internet connectivity is there ? Write a docker file and state various layers and use the depends_on concept. How to save a container as image and then as a zip file ? What are docker volumes ? (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Docker Advanced Interview Questions"},{"location":"nightwolf-cotribution/gcp-ace-1/","text":"Google Cloud Associate Cloud Engineer Practice Exam - 1: \uf0c1 These are top 50 GCP ACE(Google Cloud Platform Associate Cloud Engineer) certification practice questions/cheatsheet (GCP ACE exam dumps) for professionals who are aspired to be GCP ACE certified. You will find these GCP ACE questions and answers very helpful in your GCP-ACE certification and interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); You have a Cloud Datastore database that you would like to backup. You'd like to issue a command and have it return immediately while the backup runs in the background. You want the backup file to be stored in a Cloud Storage bucket named my-datastore-backup. What command would you use? a). gcloud datastore backup gs://my-datastore-backup b). gsutil datastore export gs://my-datastore-backup --async c). gcloud datastore export gs://my-datastore-backup d). gcloud datastore export gs://my-datastore-backup --async Correct: d Explanation: The correct command is gcloud datastore export gs://my-datastore-backup --async. Export, not backup, is the datastore command to save data to a Cloud Storage bucket. Gsutil is used to manage Cloud Storage, not Cloud Datastore. Reference: https://cloud.google.com/datastore/docs/export-import-entities Your organization has created multiple folders, one for each department. In each folder, departments have one or more projects. What would you expect resources within the folder to share? a). Service accounts b). IAM policies c). IAM roles d). Permissions Correct: b Explanation: Folders are used to group resources that share common IAM policies. Service accounts are specific to a set of operating requirements within a project. Permissions are associated with roles but not directly with folders. IAM roles are granted to identities, not folders. Reference: https://cloud.google.com/resource-manager/docs/creating-managing-folders As a consultant to a mid-sized retailer you have been asked to help choose a managed database platform for the company's inventory management application. The retailer's market is limited to the Northeast United States. What service would you recommend? a). Bigtable b). Cloud Dataproc c). Cloud Spanner d). Cloud SQL Correct: d Explanation: Cloud SQL is a managed relational database service suitable for regionally used applications. Cloud Spanner is also a managed relational database but it is designed for multi-region and global applications. BigQuery is not used for transaction processing systems. Cloud Dataproc is a managed Spark/Hadoop service, not a relational database. For more information, see Reference: https://cloud.google.com/sql/docs An auditor is reviewing your GCP use. They have asked for access to any audit logs available in GCP. What audit logs are available for each project, folder, and organization? a). Admin Activity b). User Login c). Performance Metrics d). System Event e). Data Access f). Policy Access Correct: a, d, e Explanation: Cloud Audit Logs maintain three audit logs: Admin Activity logs, Data Access logs, and System Event logs. There is no such thing as a Policy Access log, a User Login log, or a Performance Metric log in GCP Audit Logs. Reference: https://cloud.google.com/logging/docs/audit A developer is trying to upload files from their local device to a Compute Engine VM using the gcloud scp command. The copy command is failing. What would you check to try to correct the problem? a). Grant the identity the roles/compute.admin role b). Add the identity of the developer to the administrator group for the VM. c). Grant the identity compute.admin permission d). Ensure firewall rules allow traffic to port 22 to allow SSH connections. Correct: d Explanation: To copy files to a VM, a firewall rule must be in place to allow traffic on port 22, the default SSH port. Administrator privileges are not needed to upload a file so the other three options are not correct. For more information, see https://cloud.google.com/compute/docs/instances/transferfiles. Reference: https://cloud.google.com/compute/docs/instances/transferfiles A group of data scientists need access to data stored in Cloud Bigtable. You want to follow Google recommended best practices for security. What role would you assign to the data scientist to allow them to read data from Bigtable? a). roles/bigtable.owner b). roles/bigtable.reader c). roles/bigtable.user e). roles/bigtable.admin Correct: b Explanation: The role/bigtable.reader gives the data scientist the ability to read data but not write data or modify the database. This follows the Principle of Least Privilege as recommended by Google. Roles/bigtable.admin gives permissions to administer all instances in a project, which is not needed by a data scientist. Roles/bigtable.user provides read and write permissions but data scientist do not need read permission. There is no predefined role called roles/bigtable.owner. Reference: https://cloud.google.com/bigtable/docs/access-control You want to load balance an application that receives traffic from other resources in the same VPC. All traffic is TCP with IPv4 addresses. What load balancer would you recommend? a). Internal TCP/UDP Load Balancing b). SSL Proxy Load Balancing c). TCP Proxy Load Balancing d). Network TCP/UDP Load Balancing Correct: a Explanation: Internal TCP/UDP Load Balancing is used for internal traffic, that is not from the internet. SSL Proxy, TCP Proxy, and Network TCP/UDP load balancing are used with external traffic. Reference: https://cloud.google.com/load-balancing/docs/choosing-load-balancer You want to deploy an application to a Kubernetes Engine cluster using a manifest file called my-app.yaml. What command would you use? a). gcloud containers deployment apply my-app.yaml b). kubectl apply -f my-app.yaml c). gcloud deployment apply my-app.yaml d). kubectl deployment apply my-app.yaml Correct: b Explanation: The correct answer is to use the \"kubectl apply -f\" with the name of the deployment file. Deployments are Kubernetes abstractions and are managed using kubectl, not gcloud. The other options are not valid commands. Reference: https://kubernetes.io/docs/reference/kubectl/overview/ A photographer wants to share images they have stored in a Cloud Storage bucket called free-photos-on-gcp. What command would you use to allow all users to read these files? a). gsutil ch allUsers:Viewer gs://free-photos-on-gcp b). gsutil iam ch allUsers:objectViewer gs://free-photos-on-gcp c). gcloud ch allUsers:objectViewer gs://free-photos-on-gcp d). gcloud iam ch allUsers:Viewer gs://free-photos-on-gcp Correct: b Explanation: The correct command is gsutil iam ch allUsers:objectViewer gs://free-photos-on-gcp. Gsutil is used with Cloud Storage, not gcloud so the gcloud ch option is incorrect. The term objectViewer is the correct way to grant read access to objects in a bucket. Reference: https://cloud.google.com/storage/docs/gsutil/commands/iam Your department runs a legacy application on an on premises cluster. The nodes in the cluster are heterogeneous. You want to migrate this cluster to Google Cloud. What Compute Engine resource would you use? a). Autoscaler b). Managed instance groups (MIGs) c). Network load balancer d). Unmanaged instance group Correct: d Explanation: Heterogeneous clusters can be run on unmanaged instance groups but not managed instance groups. Network load balancer is used to distribute workload in a cluster but it is not an instance group itself. Autoscaler adds and removes nodes in a managed instance group as needed. Reference: https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-unmanaged-instances You have created a Kubernetes Engine cluster that will run machine learning training processes and machine learning prediction processes. The training processes require more CPU and memory than the prediction processes. How would you configure the cluster to support this? a). Increase the number of replica sets for the machine learning training process. b). Use two node pools, one configured with more CPU and memory than the other. d). Use multiple pods with some configured for more CPU and memory. c). Increase the number of deployments for the machine learning training process. Correct: b Explanation: Node pools are used to configure resources for particular workloads. All nodes in a node pool are configured the same. Replica sets and deployments do not control the number of CPUs or amount of memory. You will be creating a GKE cluster and want to use Cloud Operations for GKE instead of legacy monitoring and logging. If you create the cluster using a gcloud container clusters create command, what parameter would you specify to explicitly enable Cloud Operations for GKE? a). --enable-gke-monitor b). --disable-legacy-monitoring c). --enable-cloud-operations d). --enable-stackdriver-kubernetes Correct: d Explanation: The correct way to enable Cloud Operations for GKE is to use the parameter --enablestackdriver-kubernetes. The other options are not valid parameter names. Reference: https://cloud.google.com/sdk/gcloud/reference/container/clusters/create A client of yours has a Python 3 application that usually has very little load but sometimes experiences sudden and extreme spikes in traffic. They want to run it in GCP but they want to keep costs as low as possible. They also want to minimize management overhead. What service would you recommend? a). Kubernetes Engine b). Compute Engine c). App Engine d). Cloud Functions Correct: c Explanation: App Engine is designed for applications written in supported languages, including Python 3, that need to run at low cost, and need to scale in response to rapid increases in load. App Engine is a managed service and as such minimizes operational overhead . Compute Engine and Kubernetes Engine both require more management overhead. Cloud Functions are used to respond to events in GCP, not to execute a continually running application. Reference: https://cloud.google.com/appengine/docs/standard During an audit, auditors determined that there are insufficient access controls on Cloud Storage buckets. The auditors recommend you use uniform bucket-level access. After applying uniform bucket-level access some users that had access to objects in buckets no longer have access. What could be the cause? a). Applying uniform bucket-level access removes all access privileges. No user will have access until permissions are reset. b). Users do not have permissions through ACLs that allow them access to objects in buckets. Prior to setting uniform bucket-level access, those users had access through IAM. c). Users do not have IAM permissions that allow them access to objects in buckets. Prior to setting uniform bucket-level access, those users had access through ACLs. d). ACLs are removed when uniform bucket-level access is applied. ACLs must be recreated. Correct: c Explanation: Access is granted to Cloud Storage objects using IAM or access control lists (ACLs). When uniform bucket-level access is applied, users only have access through IAM roles and permissions. A users that could access objects before uniform bucket-level access is applied but not after must have had access through ACLs. Reference: https://cloud.google.com/storage/docs/uniform-bucket-level-access A startup has an app that allows users to upload images to Cloud Storage. The images should be analyzed as soon as possible once they are loaded. Processing takes approximately 1 second for each image. There are periods when no images are uploaded and other times when many images are upload in short periods of time. What compute option would you use to process images? a). Compute Engine b). Kubernetes Engine c). App Engine Flexible d). Cloud Functions Correct: d Explanation: Cloud Functions is used to respond to events in GCP, including uploading of files in Cloud Storage. Processing can finish within the time limits Cloud Functions must run. Since there are periods when no images are uploaded, there is no need to have an application running continuously and checking for new image uploads so App Engine Flexible, Cloud Engine, and Kubernetes Engine are more than required. Reference: https://cloud.google.com/functions/docs/how-to A client has asked for your advice about building a data transformation pipeline. The pipeline will read data from Cloud Storage and Cloud Spanner, merge data from the two sources and write the data to a BigQuery data set. The client does not want to manage servers or other infrastructure, if possible. What GCP service would you recommend? a). Cloud Dataprep b). Compute Engine c). Cloud Data Fusion d). Cloud Build Correct: c Explanation: Cloud Data Fusion is a managed service that is designed for building data transformation pipelines. Compute Engine is not a managed service. Cloud Dataprep is used to prepare data for analytics and machine learning. Cloud Build is a service for creating container images. Reference: https://cloud.google.com/data-fusion/docs/how-to You want to clone a persistent disk. What characteristics of the source and cloned disk must be the same? a). Zone b). disk type c). Region d). Size Correct: a, b, c Explanation: The source and cloned disk must be in the same zone and region and must be of the same type. The size of the clone must be at least the size of the source disk but does not need to be the same. Reference: https://cloud.google.com/compute/docs/disks/create-snapshots You have a set of snapshots that you keep as backups of several persistent disks. You want to know the source disk for each snapshot. What command would you use to get that information? a). gcloud compute disk describe b). gcloud compute snapshots list c). gcloud compute snapshots describe d). gcloud snapshots describe Correct: c Explanation: The correct command is gcloud compute snapshots describe which shows information about the snapshot, including source disk, creation time, and size. The other options are not valid gcloud commands. Reference: https://cloud.google.com/sdk/gcloud/reference/compute/snapshots/describe The CFO of you company feels you are spending too much on BigQuery. You determine that a few long running queries are costing more than they should. You would like to experiment with different ways of writing these queries. You'd like to know the estimated cost of running each query without actually running them. How could you do this? a). Use the --estimate-cost option with the bq command. b). Use the --dry-run option with a bq query command. c). Use the --estimate-cost with the gcloud command. d). Use the Pricing Calculator. Correct: b Explanation: The correct answer is to use the --dry-run option with the bq select command. The Pricing Calculator can give you an estimate of aggregate costs based on storage and amount of data queried but it does not provide estimates of a the cost of running a specific query. There is no --estimate-cost option with either the bq or gcloud command. Reference: https://cloud.google.com/bigquery/docs/estimate-costs You are using HTTP(S) Load Balancing for a Web application that has several services. Depending on the URL specified by a user, requests are routed to different backend services. What would you use to specify how those request should be routed? a). URL maps b). Routes c). Traces d). Firewall rules Correct: a Explanation: URL maps specify direct requests to particular services. Routes are used to specify paths to destination IP addresses outside a subnet. Firewall rules control the flow of traffic on a network. Traces are used to understand performance characteristics of services in a distributed system. Reference: https://cloud.google.com/load-balancing/docs/url-map An application running in Compute Engine sometimes gets spikes in load. You want to add instances automatically when load increases significantly and plan to use managed instance groups. What would you need to create in order to automatically scale the cluster? a). Persistent Disk b). Snapshot c). Load balancer d). Instance template Correct: d Explanation: An instance template is needed to enable Compute Engine to automatically add instances to a managed instance group. Snapshots are not required to add instances to a managed instance group. Persistent disks are not needed to control the addition of nodes to a managed instance group. Load balancers are used with managed instance groups but are not the thing that automatically adds nodes to the managed instance group. Reference: https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances A new team member has just created a new project in GCP. What role is automatically granted to them when they create the project? a). roles/viewer b). roles/browser c). roles/editor d). roles/owner Correct: d Explanation: When you create a project, you are automatically granted the roles/owner role. The owner role includes permissions granted by roles/editor, roles/viewer, and roles/browser. Reference: https://cloud.google.com/resource-manager/docs/access-control-proj As a developer using GCP, you will need to set up a local development environment. You will want to authorize the use of gcloud commands to access resources. What commands could you use to authorize access? a). gcloud config login b). gcloud auth login c). gcloud login d). gcloud init Correct: b, d Explanation: Gcloud init will authorize access and perform other common setup steps. Gcloud auth login will authorize access only. Gcloud login and gcloud config login are not valid commands. Reference: https://cloud.google.com/sdk/docs/initializing A data warehouse administrator is trying to load data from Cloud Storage to BigQuery. What permissions will they need? a). bigquery.tables.create b). bigquery.jobs.create c). bigquery.tables.list d). bigquery.jobs.list e). bigquery.tables.updateData Correct: a, b, e Explanation: To load data, an identity must have bigquery.tables.create, bigquery.tables.updateData, and bigquery.jobs.create. bigquery.tables.list is needed to list tables and metadata on tables. Reference: https://cloud.google.com/bigquery/docs/batch-loading-data Reference: https://cloud.google.com/bigquery/docs/access-control A startup has created an IoT application that analyzes data from sensors deployed on vehicles. The application depends on a database that can write large volumes of data at low latency. The startup has used HBase in the past but want to migrate to a managed database service. What service would you recommend? a). Cloud Spanner b). Bigtable c). Cloud Dataproc d). BigQuery Correct: b Explanation: Bigtable is a wide column database with low latency writes that is well suited for IoT data storage. BigQuery is a data warehouse service. Cloud Dataproc is a managed Spark/Hadoop service. Cloud Spanner is a global-scale relational database designed for transaction processing. Reference: https://cloud.google.com/bigtable/docs/schema-design Reference: https://cloud.google.com/bigtable/docs/schema-design-steps You have created a set of firewall rules to control ingress and egress traffic to a network. Traffic that you intended to allow to leave the network appears to be blocked. What could you do to get information to help you diagnose the problem? a). Enable Cloud Monitoring of each firewall rule b). Use Cloud Debugger to debug the firewall rules c). Enable firewall rule logging for each of the firewall rules d). Enable Cloud Trace of each firewall rule Correct: c Explanation: Firewall rule logging can be enabled for each firewall rule. Each time the rule is applied to allow or deny traffic, a connection record is created. Connection records can be viewed in Cloud Logging. Cloud Monitoring is used for collecting and view metrics on resource performance. Cloud Trace is used to understand performance in distributed systems. Cloud Debugger is used by developers to identify and correct errors in code. Reference: https://cloud.google.com/vpc/docs/firewall-rules-logging A Cloud Storage user wants to rename several files in a bucket. What command should they use? a). gsutil mv b). gsutil cp c). gsutil rename d). gsutil rn Correct: a Explanation: To rename a file in cloud storage, use the move command gsutil mv. Gsutil cp will copy files, not rename them. Gsutil rewrite and gsutil rn are not a valid command. Reference: https://cloud.google.com/storage/docs/gsutil/commands/mv Your organization has created multiple projects in several folders. You have been assigned to manage them and want to get descriptive information about each project. What command would you use to get metadata about a project? a). gcloud describe projects <PROJECT_ID> b). gcloud projects describe <PROJECT_ID> c). gcloud describe projects <PROJECT_NAME> d). gcloud projects describe <PROJECT_NAME> Correct: b Explanation: The correct command is 'gcloud projects describe <PROJECT_ID'>. 'gcloud projects describe <PROJECT_NAME>' is incorrect because PROJECT_NAME is not used in this command. 'gcloud describe projects' is wrong because 'describe' and 'projects' are in the wrong order in the command. 'gcloud describe project <PROJECT_NAME>' is incorrect because it uses PROJECT_NAME instead of PROJECT_ID. Reference: https://cloud.google.com/sdk/gcloud/reference/projects/describe A group of developers are creating a multi-tiered application. Each tier is in its own project. The developer would like to work with a common VPC network. What would you use to implement this? a). Create routes between subnets of each project b). Create a VPN between projects c). Create firewall rules to load balance traffic between each project's subnets. d). Create a shared VPC Correct: d Explanation: A shared VPC allows projects to share a common VPC network. VPNs are used to link VPCs to on premises networks. Routes and firewall rules are not sufficient for implementing a common VPC. Firewall rules are not used to load balance, they are used to control the ingress and egress of traffic on a network. Reference: https://cloud.google.com/vpc/docs/shared-vpc Reference: https://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc As a consultant to a new GCP customer, you are asked to help set up billing accounts. What permission must an identity have in order to create a billing account? a). billing.create b). roles/billing.create c). roles/billing.accounts.create d). billing.accounts.create Correct: d Explanation: billing.accounts.create is the permission needed to create a billing account. billing.create is not a valid permission. Roles are sets of permissions but they are not permissions themselves so roles/billing.create and roles/billing.accounts.create are not correct answers. Reference: https://cloud.google.com/billing/docs/how-to/manage-billing-account You have just created a custom mode network using the command: gcloud compute networks create. You want to eventually deploy instances in multiple regions. What is the next thing you should do? a). Create subnets in regions where you plan to deploy instances b). Create firewall rules to load balance traffic c). Create subnets in all regions d). Create a VPN between the custom model network and other networks in the VPC. Correct: a Explanation: After creating a custom mode network, you will need to create subnets in regions where instances will be deployed. You do not have to create subnets in all regions but an instance cannot be deployed to a region without a subnet. Firewalls are used to control the ingress and egress of data, they are not used to load balance. VPNs are used to provide connectivity between Google Cloud and outside networks, such as an on premises network. Reference: https://cloud.google.com/vpc/docs/using-vpc Reference: https://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/create A client of yours wants to deploy a stateless application to Kubernetes cluster. The replication controller is named my-app-rc. The application should scale based on CPU utilization; specifically when CPU utilization exceeds 80%. There should never be fewer than 2 pods or more than 6. What command would you use to implement autoscaling with these parameters? a). kubectl apply rc my-app-rc --min=2 --max=6 --cpu-percent=80 b). kubectl autoscale rc my-app-rc --min=2 --max=6 --cpu-percent=80 c). gcloud containers apply rc my-app-rc --min=2 --max=6 --cpu-percent=80 d). gcloud containers autoscale rc my-app-rc --min=2 --max=6 --cpu-percent=80 Correct: b Explanation: The correct command is to use kubectl autoscale specifying the appropriate min, max, and cpu percent. Specifically: kubectl autoscale rc my-app-rc --min=2 --max=6 --cpu-percent=80. The other options are not valid commands. Reference: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale The contents of the a Cloud Storage bucket called free-photos-gcp are currently stored in multiregional storage class. You want to change the storage class to nearline. What command would you use? a). gsutil rewrite -s nearline gs://free-photos-gcp b). gsutil migrate --from multiregional --to nearline gs://free-photos-gcp c). gsutil migrate -s nearline gs://free-photos-gcp d). gsutil rewrite -from multiregional --to nearline gs://free-photos-gcp Correct: a Explanation: The correct command for changing the storage class is gsutil rewrite with the target storage class and bucket specified. Gsutil migrate is not a valid command. There is no need to specify the parameters -from or -to. Reference: https://cloud.google.com/storage/docs/gsutil/commands/rewrite You have created a process that will run nightly. The process needs read and write access to two Cloud Storage buckets. You do not want to use your identity to ensure the process has sufficient privileges. How would you ensure the process can read and write to the Cloud Storage buckets? a). Create a service account and grant it a role that provides read and write permission. b). Create a federated identity and grant it permissions directly to enable read and write access. c). Create a service account and assign permissions directly to enable read and write access. d). Create a Cloud Identity and grant it a role that provides read and write permissions. Correct: a Explanation: Service accounts are used to provide applications and instances with an identity that can have roles that give the identity sufficient permission to execute operations it needs to perform. You will be running an application that requires high levels of security. You want to ensure the application does not run on a server that has been compromised by a rootkit or other kernel-level malware. What kind of virtual machine would you use? a). Hardened VM b). Shielded VM c). GPU-enabled VM d). Preemptible VM Correct: b Explanation: Shielded VMs are hardened virtual machines that use Secure Boot, virtual trusted platform module enabled Measured Boot, and integrity monitoring. Preemptible VMs can be taken back by Google at any time but cost significantly less than standard prices. Hardened VM is not a valid option in Compute Engine. GPU-enabled VMs can improve the performance of compute intensive applications, such as training machine learning models. Reference: https://cloud.google.com/security/shielded-cloud/shielded-vm Your company is migrating an on premises archive of files to Google Cloud. The archived files are infrequently used but on average about once every 30 days. You would like to minimize the cost of storage. What storage option would you recommend? a). Nearline Storage b). Coldline Storage c). Multi-regional storage d). Persistent Disks Correct: a Explanation: Nearline Storage is a class of Cloud Storage designed for objects that will be accessed at most once every 30 days. Coldline Storage is suitable for objects accessed at most once per year. Multi-regional storage is best suited for objects that should have low latency access from multiple regions. Persistent disks should not be used for archival storage. Reference: https://cloud.google.com/storage/docs/storage-classes You want to use Cloud Identity to create identities. You have received a verification record for your domain. Where would you add that record? a). In the billing account for your organization b). In the domain's DNS setting c). In the metadata of each resource created in your organization d). In IAM settings for each identity Correct: b Explanation: Cloud Identity provides domain verification records, which are added to DNS settings for the domain. IAM is used to control access granted to identities, it is not a place to manage domains. The billing account is used for payment tracking, it is not a place to manage domains. Resources do have metadata, but that metadata is not used to manage domains. Reference: https://cloud.google.com/identity/docs/verify-domain Your company has an on premises Hadoop cluster that is to be migrated to Google Cloud. The CFO wants to minimize operational overhead. What GCP service would you recommend? a). Bigtable b). Cloud Dataproc c). Cloud Pub/Sub d). Cloud Dataflow Correct: b Explanation: Cloud Dataproc is a managed Spark/Hadoop service that can be used to migrate Hadoop clusters GCP. Cloud Pub/Sub is a queuing service that is used to ingest data and store it until it can be processed. Bigtable is a NoSQL database, not a queueing service. Cloud Dataflow is a stream and batch processing service, not a queueing service. Reference: https://cloud.google.com/dataproc/docs/how-to Your company has a complicated billing structure for GCP projects. You would like to set up multiple configurations for use with the command line interface. What command would you use to create those? a). gcloud config configurations create b). gcloud config configurations set c). gcloud configurations set d). gcloud configurations create Correct: a Explanation: The correct command is gcloud config configurations create. Gcloud configurations crae, gcloud config configurations set, and gcloud configurations set are not valid gcloud commands to create configurations. Reference: https://cloud.google.com/sdk/gcloud/reference/config/configurations/create You have deployed a sole tenant node in Compute Engine. How will this restrict what VMs run on that node. a). Only VMs using the same operating system will run on that node. b). Only VMs from the same organization will run on that node. c). Only one VM will run on that node. d). Only VMs from the same project will run on the node. Correct: d Explanation: On a sole tenant node, only VMs from the same project will run on that node. They do not need to use the same operating system. Sole tenant nodes are not restricted to a single VM. VMs from the same organization but different projects will not run on the same sole tenant instance. Reference: https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes You want to run a Kubernetes cluster for a high availability set of applications. What type of cluster would you use? a). Multi-regional b). Single zone c). Regional d). Multi-zonal Correct: c Explanation: Regional clusters have replicas of the control plane while single zone and multi-zonal clusters have only one control plane. There is no such thing as multi-regional cluster. Reference: https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available-gke-cluster A large enterprise has created multiple organizations in GCP. They would like to connect the VPC networks across organizations. What should they do? a). Implement a Shared VPC b). Implement VPC Network Peering between VPCs c). Implement a VPN between VPCs d). Define firewall rules to allow egress traffic to other VPC networks Correct: b Explanation: Since the connected networks are in different organizations, they must use VPC Network Peering. VPC sharing is only available within a single organization. Firewall rule changes may be needed, but that is not sufficient. VPNs are used to connect GCP networks with on premises networks. Reference: https://cloud.google.com/vpc/docs/vpc-peering You are creating a set of virtual machines in Compute Engine. GCP will automatically assign an IP address to each. What type of IP address will be assigned? a). Regional external address b). Global internal address c). Regional internal address d). Global external address Correct: c Explanation: GCP assigns regional internal IP addresses for VM instances, including GKE pods, nodes, and services. They are also used for Internal TCP/UDP Load Balancing and Internal HTTP(S) Load Balancing. Reference: https://cloud.google.com/compute/docs/ip-addresses You have created a target pool with instances in two zones which are in the same region. The target pool is not functioning correctly. What could be the cause of the problem? a). The target pool is not sending logs to Cloud Logging. b). The target pool is missing a health check. c). The target pool nodes are configured with different memory specifications d). The target pool is not sending metrics to Cloud Monitoring. Correct: b Explanation: Target pools must have a health check to function properly. Nodes can be in different zones but must be in the same region. Cloud Monitoring and Cloud Logging are useful but they are not required for the target pool to function properly. Nodes in a pool have the same configuration. Reference: https://cloud.google.com/load-balancing/docs/target-pools A manager in your company is having trouble tracking the use and cost of resources across several projects. In particular, they do not know which resources are created by different teams they manage. What would you suggest the manager use to help better understand which resources are used by which team? a). IAM policies b). Trace logs c). Audit logs d). Labels Correct: d Explanation: Labels are key-value pairs attached to resources and used to manage them. The manager could use a key-value pair with the key 'team-name' and the value the name of the team that created the resource. Audit logs do not necessarily have the names of teams that own a resource. Traces are used for performance monitoring and analysis. IAM policies are used to control access to resources, not to track which team created them. Reference: https://cloud.google.com/resource-manager/docs/creating-managing-labels The CFO of your company wants to improve an existing data warehouse by migrating it to Google Cloud. They want to minimize operational overhead while ensuring existing SQL tools can be used with the migrated data warehouse. What GCP service would you recommend? a). Cloud SQL b). Cloud Spanner c). Bigtable d). BigQuery Correct: d Explanation: BigQuery is a managed, petabyte scale data warehouse, which uses SQL. Bigtable does not support SQL. Cloud SQL and Cloud Spanner support SQL but are designed for transaction processing, not analytical applications like data warehouses. Reference: https://cloud.google.com/bigquery/docs/how-to A software development team is using Google Container Registry to manage container images. You have recently joined the team and want to view metadata about existing container images. What command would you use? a). gcloud container images list b). gcloud container metadata list c). gcloud images container list d). gcloud container list metadata Correct: a Explanation: The correct command is gcloud container images list. The other options are not valid gcloud commands. For more information, see Reference: https://cloud.google.com/sdk/gcloud/reference/container/images/list Kubernetes Engine collects application logs by default when the log data is written where? a). SYSLOG b). SYSERR c). STDERR d). STDOUT Correct: c, d Explanation: Kubernetes Engine collects log data written to standard output (STDOUT) and standard error (STDERR). Reference: https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine A startup is implementing an IoT application that will ingest data at high speeds. The architect for the startup has decided that data should be ingested in a queue that can store the data until the processing application is able to process it. The architect also wants to use a managed service in Google Cloud. What service would you recommend? a). Bigtable b). Cloud Dataflow c). Cloud Pub/Sub d). Cloud Dataproc Correct: c Explanation: Cloud Pub/Sub is a queuing service that is used to ingest data and store it until it can be processed. Bigtable is a NoSQL database, not a queueing service. Cloud Dataflow is a stream and batch processing service, not a queueing service. Cloud Dataproc is a managed Spark/Hadoop service. Reference: https://cloud.google.com/pubsub/docs/overview To avoid potentially violating a regulation, your company has determined that it will only use GCP resources in North America. How would you ensure no resources are created outside of North America? a). Create a data lifecycle management policy that prevents data from being saved outside of North America. b). Create an Cloud Audit policy that prevents users from creating resources outside of North America. c). Create a policy at the folder level of the resource hierarchy that includes a constraint using a Resource Location Restriction. d). Create a policy at the organization level of the resource hierarchy that includes a constraint using a Resource Location Restriction. Correct: d Explanation: Constraints are the standard way to restrict where resources can be created and applying policies with constraints will enforce those constraints for all resources in the organization. If the policy were applied at the folder level, it would have to be applied for all folders and that is not as efficient as applying at the organization level. There is no such thing as a Cloud Audit policy. Reference: https://cloud.google.com/resource-manager/docs/organizationpolicy/defining-locations Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"GCP ACE Practice Questions-1"},{"location":"nightwolf-cotribution/gcp-ace-1/#google-cloud-associate-cloud-engineer-practice-exam-1","text":"These are top 50 GCP ACE(Google Cloud Platform Associate Cloud Engineer) certification practice questions/cheatsheet (GCP ACE exam dumps) for professionals who are aspired to be GCP ACE certified. You will find these GCP ACE questions and answers very helpful in your GCP-ACE certification and interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); You have a Cloud Datastore database that you would like to backup. You'd like to issue a command and have it return immediately while the backup runs in the background. You want the backup file to be stored in a Cloud Storage bucket named my-datastore-backup. What command would you use? a). gcloud datastore backup gs://my-datastore-backup b). gsutil datastore export gs://my-datastore-backup --async c). gcloud datastore export gs://my-datastore-backup d). gcloud datastore export gs://my-datastore-backup --async Correct: d Explanation: The correct command is gcloud datastore export gs://my-datastore-backup --async. Export, not backup, is the datastore command to save data to a Cloud Storage bucket. Gsutil is used to manage Cloud Storage, not Cloud Datastore. Reference: https://cloud.google.com/datastore/docs/export-import-entities Your organization has created multiple folders, one for each department. In each folder, departments have one or more projects. What would you expect resources within the folder to share? a). Service accounts b). IAM policies c). IAM roles d). Permissions Correct: b Explanation: Folders are used to group resources that share common IAM policies. Service accounts are specific to a set of operating requirements within a project. Permissions are associated with roles but not directly with folders. IAM roles are granted to identities, not folders. Reference: https://cloud.google.com/resource-manager/docs/creating-managing-folders As a consultant to a mid-sized retailer you have been asked to help choose a managed database platform for the company's inventory management application. The retailer's market is limited to the Northeast United States. What service would you recommend? a). Bigtable b). Cloud Dataproc c). Cloud Spanner d). Cloud SQL Correct: d Explanation: Cloud SQL is a managed relational database service suitable for regionally used applications. Cloud Spanner is also a managed relational database but it is designed for multi-region and global applications. BigQuery is not used for transaction processing systems. Cloud Dataproc is a managed Spark/Hadoop service, not a relational database. For more information, see Reference: https://cloud.google.com/sql/docs An auditor is reviewing your GCP use. They have asked for access to any audit logs available in GCP. What audit logs are available for each project, folder, and organization? a). Admin Activity b). User Login c). Performance Metrics d). System Event e). Data Access f). Policy Access Correct: a, d, e Explanation: Cloud Audit Logs maintain three audit logs: Admin Activity logs, Data Access logs, and System Event logs. There is no such thing as a Policy Access log, a User Login log, or a Performance Metric log in GCP Audit Logs. Reference: https://cloud.google.com/logging/docs/audit A developer is trying to upload files from their local device to a Compute Engine VM using the gcloud scp command. The copy command is failing. What would you check to try to correct the problem? a). Grant the identity the roles/compute.admin role b). Add the identity of the developer to the administrator group for the VM. c). Grant the identity compute.admin permission d). Ensure firewall rules allow traffic to port 22 to allow SSH connections. Correct: d Explanation: To copy files to a VM, a firewall rule must be in place to allow traffic on port 22, the default SSH port. Administrator privileges are not needed to upload a file so the other three options are not correct. For more information, see https://cloud.google.com/compute/docs/instances/transferfiles. Reference: https://cloud.google.com/compute/docs/instances/transferfiles A group of data scientists need access to data stored in Cloud Bigtable. You want to follow Google recommended best practices for security. What role would you assign to the data scientist to allow them to read data from Bigtable? a). roles/bigtable.owner b). roles/bigtable.reader c). roles/bigtable.user e). roles/bigtable.admin Correct: b Explanation: The role/bigtable.reader gives the data scientist the ability to read data but not write data or modify the database. This follows the Principle of Least Privilege as recommended by Google. Roles/bigtable.admin gives permissions to administer all instances in a project, which is not needed by a data scientist. Roles/bigtable.user provides read and write permissions but data scientist do not need read permission. There is no predefined role called roles/bigtable.owner. Reference: https://cloud.google.com/bigtable/docs/access-control You want to load balance an application that receives traffic from other resources in the same VPC. All traffic is TCP with IPv4 addresses. What load balancer would you recommend? a). Internal TCP/UDP Load Balancing b). SSL Proxy Load Balancing c). TCP Proxy Load Balancing d). Network TCP/UDP Load Balancing Correct: a Explanation: Internal TCP/UDP Load Balancing is used for internal traffic, that is not from the internet. SSL Proxy, TCP Proxy, and Network TCP/UDP load balancing are used with external traffic. Reference: https://cloud.google.com/load-balancing/docs/choosing-load-balancer You want to deploy an application to a Kubernetes Engine cluster using a manifest file called my-app.yaml. What command would you use? a). gcloud containers deployment apply my-app.yaml b). kubectl apply -f my-app.yaml c). gcloud deployment apply my-app.yaml d). kubectl deployment apply my-app.yaml Correct: b Explanation: The correct answer is to use the \"kubectl apply -f\" with the name of the deployment file. Deployments are Kubernetes abstractions and are managed using kubectl, not gcloud. The other options are not valid commands. Reference: https://kubernetes.io/docs/reference/kubectl/overview/ A photographer wants to share images they have stored in a Cloud Storage bucket called free-photos-on-gcp. What command would you use to allow all users to read these files? a). gsutil ch allUsers:Viewer gs://free-photos-on-gcp b). gsutil iam ch allUsers:objectViewer gs://free-photos-on-gcp c). gcloud ch allUsers:objectViewer gs://free-photos-on-gcp d). gcloud iam ch allUsers:Viewer gs://free-photos-on-gcp Correct: b Explanation: The correct command is gsutil iam ch allUsers:objectViewer gs://free-photos-on-gcp. Gsutil is used with Cloud Storage, not gcloud so the gcloud ch option is incorrect. The term objectViewer is the correct way to grant read access to objects in a bucket. Reference: https://cloud.google.com/storage/docs/gsutil/commands/iam Your department runs a legacy application on an on premises cluster. The nodes in the cluster are heterogeneous. You want to migrate this cluster to Google Cloud. What Compute Engine resource would you use? a). Autoscaler b). Managed instance groups (MIGs) c). Network load balancer d). Unmanaged instance group Correct: d Explanation: Heterogeneous clusters can be run on unmanaged instance groups but not managed instance groups. Network load balancer is used to distribute workload in a cluster but it is not an instance group itself. Autoscaler adds and removes nodes in a managed instance group as needed. Reference: https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-unmanaged-instances You have created a Kubernetes Engine cluster that will run machine learning training processes and machine learning prediction processes. The training processes require more CPU and memory than the prediction processes. How would you configure the cluster to support this? a). Increase the number of replica sets for the machine learning training process. b). Use two node pools, one configured with more CPU and memory than the other. d). Use multiple pods with some configured for more CPU and memory. c). Increase the number of deployments for the machine learning training process. Correct: b Explanation: Node pools are used to configure resources for particular workloads. All nodes in a node pool are configured the same. Replica sets and deployments do not control the number of CPUs or amount of memory. You will be creating a GKE cluster and want to use Cloud Operations for GKE instead of legacy monitoring and logging. If you create the cluster using a gcloud container clusters create command, what parameter would you specify to explicitly enable Cloud Operations for GKE? a). --enable-gke-monitor b). --disable-legacy-monitoring c). --enable-cloud-operations d). --enable-stackdriver-kubernetes Correct: d Explanation: The correct way to enable Cloud Operations for GKE is to use the parameter --enablestackdriver-kubernetes. The other options are not valid parameter names. Reference: https://cloud.google.com/sdk/gcloud/reference/container/clusters/create A client of yours has a Python 3 application that usually has very little load but sometimes experiences sudden and extreme spikes in traffic. They want to run it in GCP but they want to keep costs as low as possible. They also want to minimize management overhead. What service would you recommend? a). Kubernetes Engine b). Compute Engine c). App Engine d). Cloud Functions Correct: c Explanation: App Engine is designed for applications written in supported languages, including Python 3, that need to run at low cost, and need to scale in response to rapid increases in load. App Engine is a managed service and as such minimizes operational overhead . Compute Engine and Kubernetes Engine both require more management overhead. Cloud Functions are used to respond to events in GCP, not to execute a continually running application. Reference: https://cloud.google.com/appengine/docs/standard During an audit, auditors determined that there are insufficient access controls on Cloud Storage buckets. The auditors recommend you use uniform bucket-level access. After applying uniform bucket-level access some users that had access to objects in buckets no longer have access. What could be the cause? a). Applying uniform bucket-level access removes all access privileges. No user will have access until permissions are reset. b). Users do not have permissions through ACLs that allow them access to objects in buckets. Prior to setting uniform bucket-level access, those users had access through IAM. c). Users do not have IAM permissions that allow them access to objects in buckets. Prior to setting uniform bucket-level access, those users had access through ACLs. d). ACLs are removed when uniform bucket-level access is applied. ACLs must be recreated. Correct: c Explanation: Access is granted to Cloud Storage objects using IAM or access control lists (ACLs). When uniform bucket-level access is applied, users only have access through IAM roles and permissions. A users that could access objects before uniform bucket-level access is applied but not after must have had access through ACLs. Reference: https://cloud.google.com/storage/docs/uniform-bucket-level-access A startup has an app that allows users to upload images to Cloud Storage. The images should be analyzed as soon as possible once they are loaded. Processing takes approximately 1 second for each image. There are periods when no images are uploaded and other times when many images are upload in short periods of time. What compute option would you use to process images? a). Compute Engine b). Kubernetes Engine c). App Engine Flexible d). Cloud Functions Correct: d Explanation: Cloud Functions is used to respond to events in GCP, including uploading of files in Cloud Storage. Processing can finish within the time limits Cloud Functions must run. Since there are periods when no images are uploaded, there is no need to have an application running continuously and checking for new image uploads so App Engine Flexible, Cloud Engine, and Kubernetes Engine are more than required. Reference: https://cloud.google.com/functions/docs/how-to A client has asked for your advice about building a data transformation pipeline. The pipeline will read data from Cloud Storage and Cloud Spanner, merge data from the two sources and write the data to a BigQuery data set. The client does not want to manage servers or other infrastructure, if possible. What GCP service would you recommend? a). Cloud Dataprep b). Compute Engine c). Cloud Data Fusion d). Cloud Build Correct: c Explanation: Cloud Data Fusion is a managed service that is designed for building data transformation pipelines. Compute Engine is not a managed service. Cloud Dataprep is used to prepare data for analytics and machine learning. Cloud Build is a service for creating container images. Reference: https://cloud.google.com/data-fusion/docs/how-to You want to clone a persistent disk. What characteristics of the source and cloned disk must be the same? a). Zone b). disk type c). Region d). Size Correct: a, b, c Explanation: The source and cloned disk must be in the same zone and region and must be of the same type. The size of the clone must be at least the size of the source disk but does not need to be the same. Reference: https://cloud.google.com/compute/docs/disks/create-snapshots You have a set of snapshots that you keep as backups of several persistent disks. You want to know the source disk for each snapshot. What command would you use to get that information? a). gcloud compute disk describe b). gcloud compute snapshots list c). gcloud compute snapshots describe d). gcloud snapshots describe Correct: c Explanation: The correct command is gcloud compute snapshots describe which shows information about the snapshot, including source disk, creation time, and size. The other options are not valid gcloud commands. Reference: https://cloud.google.com/sdk/gcloud/reference/compute/snapshots/describe The CFO of you company feels you are spending too much on BigQuery. You determine that a few long running queries are costing more than they should. You would like to experiment with different ways of writing these queries. You'd like to know the estimated cost of running each query without actually running them. How could you do this? a). Use the --estimate-cost option with the bq command. b). Use the --dry-run option with a bq query command. c). Use the --estimate-cost with the gcloud command. d). Use the Pricing Calculator. Correct: b Explanation: The correct answer is to use the --dry-run option with the bq select command. The Pricing Calculator can give you an estimate of aggregate costs based on storage and amount of data queried but it does not provide estimates of a the cost of running a specific query. There is no --estimate-cost option with either the bq or gcloud command. Reference: https://cloud.google.com/bigquery/docs/estimate-costs You are using HTTP(S) Load Balancing for a Web application that has several services. Depending on the URL specified by a user, requests are routed to different backend services. What would you use to specify how those request should be routed? a). URL maps b). Routes c). Traces d). Firewall rules Correct: a Explanation: URL maps specify direct requests to particular services. Routes are used to specify paths to destination IP addresses outside a subnet. Firewall rules control the flow of traffic on a network. Traces are used to understand performance characteristics of services in a distributed system. Reference: https://cloud.google.com/load-balancing/docs/url-map An application running in Compute Engine sometimes gets spikes in load. You want to add instances automatically when load increases significantly and plan to use managed instance groups. What would you need to create in order to automatically scale the cluster? a). Persistent Disk b). Snapshot c). Load balancer d). Instance template Correct: d Explanation: An instance template is needed to enable Compute Engine to automatically add instances to a managed instance group. Snapshots are not required to add instances to a managed instance group. Persistent disks are not needed to control the addition of nodes to a managed instance group. Load balancers are used with managed instance groups but are not the thing that automatically adds nodes to the managed instance group. Reference: https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances A new team member has just created a new project in GCP. What role is automatically granted to them when they create the project? a). roles/viewer b). roles/browser c). roles/editor d). roles/owner Correct: d Explanation: When you create a project, you are automatically granted the roles/owner role. The owner role includes permissions granted by roles/editor, roles/viewer, and roles/browser. Reference: https://cloud.google.com/resource-manager/docs/access-control-proj As a developer using GCP, you will need to set up a local development environment. You will want to authorize the use of gcloud commands to access resources. What commands could you use to authorize access? a). gcloud config login b). gcloud auth login c). gcloud login d). gcloud init Correct: b, d Explanation: Gcloud init will authorize access and perform other common setup steps. Gcloud auth login will authorize access only. Gcloud login and gcloud config login are not valid commands. Reference: https://cloud.google.com/sdk/docs/initializing A data warehouse administrator is trying to load data from Cloud Storage to BigQuery. What permissions will they need? a). bigquery.tables.create b). bigquery.jobs.create c). bigquery.tables.list d). bigquery.jobs.list e). bigquery.tables.updateData Correct: a, b, e Explanation: To load data, an identity must have bigquery.tables.create, bigquery.tables.updateData, and bigquery.jobs.create. bigquery.tables.list is needed to list tables and metadata on tables. Reference: https://cloud.google.com/bigquery/docs/batch-loading-data Reference: https://cloud.google.com/bigquery/docs/access-control A startup has created an IoT application that analyzes data from sensors deployed on vehicles. The application depends on a database that can write large volumes of data at low latency. The startup has used HBase in the past but want to migrate to a managed database service. What service would you recommend? a). Cloud Spanner b). Bigtable c). Cloud Dataproc d). BigQuery Correct: b Explanation: Bigtable is a wide column database with low latency writes that is well suited for IoT data storage. BigQuery is a data warehouse service. Cloud Dataproc is a managed Spark/Hadoop service. Cloud Spanner is a global-scale relational database designed for transaction processing. Reference: https://cloud.google.com/bigtable/docs/schema-design Reference: https://cloud.google.com/bigtable/docs/schema-design-steps You have created a set of firewall rules to control ingress and egress traffic to a network. Traffic that you intended to allow to leave the network appears to be blocked. What could you do to get information to help you diagnose the problem? a). Enable Cloud Monitoring of each firewall rule b). Use Cloud Debugger to debug the firewall rules c). Enable firewall rule logging for each of the firewall rules d). Enable Cloud Trace of each firewall rule Correct: c Explanation: Firewall rule logging can be enabled for each firewall rule. Each time the rule is applied to allow or deny traffic, a connection record is created. Connection records can be viewed in Cloud Logging. Cloud Monitoring is used for collecting and view metrics on resource performance. Cloud Trace is used to understand performance in distributed systems. Cloud Debugger is used by developers to identify and correct errors in code. Reference: https://cloud.google.com/vpc/docs/firewall-rules-logging A Cloud Storage user wants to rename several files in a bucket. What command should they use? a). gsutil mv b). gsutil cp c). gsutil rename d). gsutil rn Correct: a Explanation: To rename a file in cloud storage, use the move command gsutil mv. Gsutil cp will copy files, not rename them. Gsutil rewrite and gsutil rn are not a valid command. Reference: https://cloud.google.com/storage/docs/gsutil/commands/mv Your organization has created multiple projects in several folders. You have been assigned to manage them and want to get descriptive information about each project. What command would you use to get metadata about a project? a). gcloud describe projects <PROJECT_ID> b). gcloud projects describe <PROJECT_ID> c). gcloud describe projects <PROJECT_NAME> d). gcloud projects describe <PROJECT_NAME> Correct: b Explanation: The correct command is 'gcloud projects describe <PROJECT_ID'>. 'gcloud projects describe <PROJECT_NAME>' is incorrect because PROJECT_NAME is not used in this command. 'gcloud describe projects' is wrong because 'describe' and 'projects' are in the wrong order in the command. 'gcloud describe project <PROJECT_NAME>' is incorrect because it uses PROJECT_NAME instead of PROJECT_ID. Reference: https://cloud.google.com/sdk/gcloud/reference/projects/describe A group of developers are creating a multi-tiered application. Each tier is in its own project. The developer would like to work with a common VPC network. What would you use to implement this? a). Create routes between subnets of each project b). Create a VPN between projects c). Create firewall rules to load balance traffic between each project's subnets. d). Create a shared VPC Correct: d Explanation: A shared VPC allows projects to share a common VPC network. VPNs are used to link VPCs to on premises networks. Routes and firewall rules are not sufficient for implementing a common VPC. Firewall rules are not used to load balance, they are used to control the ingress and egress of traffic on a network. Reference: https://cloud.google.com/vpc/docs/shared-vpc Reference: https://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc As a consultant to a new GCP customer, you are asked to help set up billing accounts. What permission must an identity have in order to create a billing account? a). billing.create b). roles/billing.create c). roles/billing.accounts.create d). billing.accounts.create Correct: d Explanation: billing.accounts.create is the permission needed to create a billing account. billing.create is not a valid permission. Roles are sets of permissions but they are not permissions themselves so roles/billing.create and roles/billing.accounts.create are not correct answers. Reference: https://cloud.google.com/billing/docs/how-to/manage-billing-account You have just created a custom mode network using the command: gcloud compute networks create. You want to eventually deploy instances in multiple regions. What is the next thing you should do? a). Create subnets in regions where you plan to deploy instances b). Create firewall rules to load balance traffic c). Create subnets in all regions d). Create a VPN between the custom model network and other networks in the VPC. Correct: a Explanation: After creating a custom mode network, you will need to create subnets in regions where instances will be deployed. You do not have to create subnets in all regions but an instance cannot be deployed to a region without a subnet. Firewalls are used to control the ingress and egress of data, they are not used to load balance. VPNs are used to provide connectivity between Google Cloud and outside networks, such as an on premises network. Reference: https://cloud.google.com/vpc/docs/using-vpc Reference: https://cloud.google.com/sdk/gcloud/reference/compute/networks/subnets/create A client of yours wants to deploy a stateless application to Kubernetes cluster. The replication controller is named my-app-rc. The application should scale based on CPU utilization; specifically when CPU utilization exceeds 80%. There should never be fewer than 2 pods or more than 6. What command would you use to implement autoscaling with these parameters? a). kubectl apply rc my-app-rc --min=2 --max=6 --cpu-percent=80 b). kubectl autoscale rc my-app-rc --min=2 --max=6 --cpu-percent=80 c). gcloud containers apply rc my-app-rc --min=2 --max=6 --cpu-percent=80 d). gcloud containers autoscale rc my-app-rc --min=2 --max=6 --cpu-percent=80 Correct: b Explanation: The correct command is to use kubectl autoscale specifying the appropriate min, max, and cpu percent. Specifically: kubectl autoscale rc my-app-rc --min=2 --max=6 --cpu-percent=80. The other options are not valid commands. Reference: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale The contents of the a Cloud Storage bucket called free-photos-gcp are currently stored in multiregional storage class. You want to change the storage class to nearline. What command would you use? a). gsutil rewrite -s nearline gs://free-photos-gcp b). gsutil migrate --from multiregional --to nearline gs://free-photos-gcp c). gsutil migrate -s nearline gs://free-photos-gcp d). gsutil rewrite -from multiregional --to nearline gs://free-photos-gcp Correct: a Explanation: The correct command for changing the storage class is gsutil rewrite with the target storage class and bucket specified. Gsutil migrate is not a valid command. There is no need to specify the parameters -from or -to. Reference: https://cloud.google.com/storage/docs/gsutil/commands/rewrite You have created a process that will run nightly. The process needs read and write access to two Cloud Storage buckets. You do not want to use your identity to ensure the process has sufficient privileges. How would you ensure the process can read and write to the Cloud Storage buckets? a). Create a service account and grant it a role that provides read and write permission. b). Create a federated identity and grant it permissions directly to enable read and write access. c). Create a service account and assign permissions directly to enable read and write access. d). Create a Cloud Identity and grant it a role that provides read and write permissions. Correct: a Explanation: Service accounts are used to provide applications and instances with an identity that can have roles that give the identity sufficient permission to execute operations it needs to perform. You will be running an application that requires high levels of security. You want to ensure the application does not run on a server that has been compromised by a rootkit or other kernel-level malware. What kind of virtual machine would you use? a). Hardened VM b). Shielded VM c). GPU-enabled VM d). Preemptible VM Correct: b Explanation: Shielded VMs are hardened virtual machines that use Secure Boot, virtual trusted platform module enabled Measured Boot, and integrity monitoring. Preemptible VMs can be taken back by Google at any time but cost significantly less than standard prices. Hardened VM is not a valid option in Compute Engine. GPU-enabled VMs can improve the performance of compute intensive applications, such as training machine learning models. Reference: https://cloud.google.com/security/shielded-cloud/shielded-vm Your company is migrating an on premises archive of files to Google Cloud. The archived files are infrequently used but on average about once every 30 days. You would like to minimize the cost of storage. What storage option would you recommend? a). Nearline Storage b). Coldline Storage c). Multi-regional storage d). Persistent Disks Correct: a Explanation: Nearline Storage is a class of Cloud Storage designed for objects that will be accessed at most once every 30 days. Coldline Storage is suitable for objects accessed at most once per year. Multi-regional storage is best suited for objects that should have low latency access from multiple regions. Persistent disks should not be used for archival storage. Reference: https://cloud.google.com/storage/docs/storage-classes You want to use Cloud Identity to create identities. You have received a verification record for your domain. Where would you add that record? a). In the billing account for your organization b). In the domain's DNS setting c). In the metadata of each resource created in your organization d). In IAM settings for each identity Correct: b Explanation: Cloud Identity provides domain verification records, which are added to DNS settings for the domain. IAM is used to control access granted to identities, it is not a place to manage domains. The billing account is used for payment tracking, it is not a place to manage domains. Resources do have metadata, but that metadata is not used to manage domains. Reference: https://cloud.google.com/identity/docs/verify-domain Your company has an on premises Hadoop cluster that is to be migrated to Google Cloud. The CFO wants to minimize operational overhead. What GCP service would you recommend? a). Bigtable b). Cloud Dataproc c). Cloud Pub/Sub d). Cloud Dataflow Correct: b Explanation: Cloud Dataproc is a managed Spark/Hadoop service that can be used to migrate Hadoop clusters GCP. Cloud Pub/Sub is a queuing service that is used to ingest data and store it until it can be processed. Bigtable is a NoSQL database, not a queueing service. Cloud Dataflow is a stream and batch processing service, not a queueing service. Reference: https://cloud.google.com/dataproc/docs/how-to Your company has a complicated billing structure for GCP projects. You would like to set up multiple configurations for use with the command line interface. What command would you use to create those? a). gcloud config configurations create b). gcloud config configurations set c). gcloud configurations set d). gcloud configurations create Correct: a Explanation: The correct command is gcloud config configurations create. Gcloud configurations crae, gcloud config configurations set, and gcloud configurations set are not valid gcloud commands to create configurations. Reference: https://cloud.google.com/sdk/gcloud/reference/config/configurations/create You have deployed a sole tenant node in Compute Engine. How will this restrict what VMs run on that node. a). Only VMs using the same operating system will run on that node. b). Only VMs from the same organization will run on that node. c). Only one VM will run on that node. d). Only VMs from the same project will run on the node. Correct: d Explanation: On a sole tenant node, only VMs from the same project will run on that node. They do not need to use the same operating system. Sole tenant nodes are not restricted to a single VM. VMs from the same organization but different projects will not run on the same sole tenant instance. Reference: https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes You want to run a Kubernetes cluster for a high availability set of applications. What type of cluster would you use? a). Multi-regional b). Single zone c). Regional d). Multi-zonal Correct: c Explanation: Regional clusters have replicas of the control plane while single zone and multi-zonal clusters have only one control plane. There is no such thing as multi-regional cluster. Reference: https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-creating-a-highly-available-gke-cluster A large enterprise has created multiple organizations in GCP. They would like to connect the VPC networks across organizations. What should they do? a). Implement a Shared VPC b). Implement VPC Network Peering between VPCs c). Implement a VPN between VPCs d). Define firewall rules to allow egress traffic to other VPC networks Correct: b Explanation: Since the connected networks are in different organizations, they must use VPC Network Peering. VPC sharing is only available within a single organization. Firewall rule changes may be needed, but that is not sufficient. VPNs are used to connect GCP networks with on premises networks. Reference: https://cloud.google.com/vpc/docs/vpc-peering You are creating a set of virtual machines in Compute Engine. GCP will automatically assign an IP address to each. What type of IP address will be assigned? a). Regional external address b). Global internal address c). Regional internal address d). Global external address Correct: c Explanation: GCP assigns regional internal IP addresses for VM instances, including GKE pods, nodes, and services. They are also used for Internal TCP/UDP Load Balancing and Internal HTTP(S) Load Balancing. Reference: https://cloud.google.com/compute/docs/ip-addresses You have created a target pool with instances in two zones which are in the same region. The target pool is not functioning correctly. What could be the cause of the problem? a). The target pool is not sending logs to Cloud Logging. b). The target pool is missing a health check. c). The target pool nodes are configured with different memory specifications d). The target pool is not sending metrics to Cloud Monitoring. Correct: b Explanation: Target pools must have a health check to function properly. Nodes can be in different zones but must be in the same region. Cloud Monitoring and Cloud Logging are useful but they are not required for the target pool to function properly. Nodes in a pool have the same configuration. Reference: https://cloud.google.com/load-balancing/docs/target-pools A manager in your company is having trouble tracking the use and cost of resources across several projects. In particular, they do not know which resources are created by different teams they manage. What would you suggest the manager use to help better understand which resources are used by which team? a). IAM policies b). Trace logs c). Audit logs d). Labels Correct: d Explanation: Labels are key-value pairs attached to resources and used to manage them. The manager could use a key-value pair with the key 'team-name' and the value the name of the team that created the resource. Audit logs do not necessarily have the names of teams that own a resource. Traces are used for performance monitoring and analysis. IAM policies are used to control access to resources, not to track which team created them. Reference: https://cloud.google.com/resource-manager/docs/creating-managing-labels The CFO of your company wants to improve an existing data warehouse by migrating it to Google Cloud. They want to minimize operational overhead while ensuring existing SQL tools can be used with the migrated data warehouse. What GCP service would you recommend? a). Cloud SQL b). Cloud Spanner c). Bigtable d). BigQuery Correct: d Explanation: BigQuery is a managed, petabyte scale data warehouse, which uses SQL. Bigtable does not support SQL. Cloud SQL and Cloud Spanner support SQL but are designed for transaction processing, not analytical applications like data warehouses. Reference: https://cloud.google.com/bigquery/docs/how-to A software development team is using Google Container Registry to manage container images. You have recently joined the team and want to view metadata about existing container images. What command would you use? a). gcloud container images list b). gcloud container metadata list c). gcloud images container list d). gcloud container list metadata Correct: a Explanation: The correct command is gcloud container images list. The other options are not valid gcloud commands. For more information, see Reference: https://cloud.google.com/sdk/gcloud/reference/container/images/list Kubernetes Engine collects application logs by default when the log data is written where? a). SYSLOG b). SYSERR c). STDERR d). STDOUT Correct: c, d Explanation: Kubernetes Engine collects log data written to standard output (STDOUT) and standard error (STDERR). Reference: https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine A startup is implementing an IoT application that will ingest data at high speeds. The architect for the startup has decided that data should be ingested in a queue that can store the data until the processing application is able to process it. The architect also wants to use a managed service in Google Cloud. What service would you recommend? a). Bigtable b). Cloud Dataflow c). Cloud Pub/Sub d). Cloud Dataproc Correct: c Explanation: Cloud Pub/Sub is a queuing service that is used to ingest data and store it until it can be processed. Bigtable is a NoSQL database, not a queueing service. Cloud Dataflow is a stream and batch processing service, not a queueing service. Cloud Dataproc is a managed Spark/Hadoop service. Reference: https://cloud.google.com/pubsub/docs/overview To avoid potentially violating a regulation, your company has determined that it will only use GCP resources in North America. How would you ensure no resources are created outside of North America? a). Create a data lifecycle management policy that prevents data from being saved outside of North America. b). Create an Cloud Audit policy that prevents users from creating resources outside of North America. c). Create a policy at the folder level of the resource hierarchy that includes a constraint using a Resource Location Restriction. d). Create a policy at the organization level of the resource hierarchy that includes a constraint using a Resource Location Restriction. Correct: d Explanation: Constraints are the standard way to restrict where resources can be created and applying policies with constraints will enforce those constraints for all resources in the organization. If the policy were applied at the folder level, it would have to be applied for all folders and that is not as efficient as applying at the organization level. There is no such thing as a Cloud Audit policy. Reference: https://cloud.google.com/resource-manager/docs/organizationpolicy/defining-locations","title":"Google Cloud Associate Cloud Engineer Practice Exam - 1:"},{"location":"nightwolf-cotribution/gcp-ace-2/","text":"Google Cloud Associate Cloud Engineer Practice Exam - 2: \uf0c1 These are top 100 GCP ACE(Google Cloud Platform Associate Cloud Engineer) certification practice questions/cheatsheet (GCP ACE exam dumps) for professionals who are aspired to be GCP ACE certified. You will find these GCP ACE questions and answers very helpful in your GCP-ACE certification and interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); A team of developers would like to have a standard work environment in Compute Engine. You would like to be able to create multiple instances with identical configurations. The configurations should be easy to change. What feature of Compute Engine would you use to specify the configuration? a). snapshot b). managed instance groups c). instance template d). unmanaged instance groups Correct: c Explanation: Instance templates specify the configuration of virtual machines and managed instance groups. Unmanaged instance groups may be heterogeneous and do not use instance templates. Snapshots are copies of disks at a point in time. They are useful for backups and copying disks but are not used for specifying configurations. Reference: https://cloud.google.com/compute/docs/images/image-families-best-practices An IoT startup is uploading 2 TB of historical data to Cloud Storage. The data is in files with one file per day per sensor. There are thousands of files. The file names are the date followed by sensor ID, such as 20210101sensorABCD for a file with data from a sensor called ABCD on January 1, 2021. The loads are not proceeding as quickly as expected. What might be the cause of the slower than expected upload? a). The files are in gzip format instead of Avro format. b). The file names use the date as the first part of the filename so they may be creating hotspots when writing data to Cloud Storage. c). The files are in CSV instead of Avro format. d). The data in files is being encrypted before being persisted to storage. Correct: b Explanation: Files are written to servers within the Cloud Storage system based on filenames. Lexically close file names are written to the same server. This can lead to hotspotting, which is an imbalance in workload that has a small number of servers doing more work than other servers. Data is always encrypted before being persisted to storage so there should be no unexpected overhead for encrypting data. The size of the file not the format is the primary determiner of how long a file will take to load. tspots in which a few servers have heavy loads while other servers have little or no load. Load times should not vary based on the file format, such as CSV vs. Avro vs gzip. Reference: https://cloud.google.com/storage/docs/best-practices Your use of GCP has grown significantly and you now have many resources to manage. Some resources are in different environments and some are for different teams. You would like to easily group resources so you can manage them more effectively. What feature of GCP would you use? a). Managed instance groups b). Snapshots c). Images d). Labels Correct: d Explanation: Labels allow you to specify key-value pairs for grouping resources, for example, a VM cloud be labeled with the key value pair 'envior:production.' Images are copies of operating systems, boot loaders, and related data that can be used to create boot disks. Snapshots are copies of persistent volumes made at a specific point in time. Managed instance groups are used to create sets of identically configured VMs that can autoscale. Reference: https://cloud.google.com/compute/docs/labeling-resources You would like to display information about a dataset named primarydata in a project called analytics1. What command would you use? a). bq ls --format=prettyjson analytics1:primarydata b). bq show --format=prettyjson analytics1:primarydata c). bq ls --format=prettyjson analytics1//primarydata d). bq metadata --format=prettyjson analytics1:primarydata Correct: b Explanation: The correct answer is bq show --format=prettyjson analytics1:primarydata. The bq command is used with BigQuery and the show command displays information about a data set. Projects and datasets are specified as [project name]:[dataset name]. The option bq ls --format=prettyjson analytics1:primarydata is incorrect because it specifies ls instead of show. The option bq metadata --format=prettyjson analytics1:primarydata is incorrect because it specifies metadata instead of show. The option bq ls --format=prettyjson analytics1//primarydata is incorrect because it specifies ls instead of show and has the wrong syntax for specifying the project and dataset names. For more information, see https://cloud.google.com/bigquery/docs/reference/bqcli-reference Reference: https://cloud.google.com/bigquery/docs/reference/bqcli-reference A DevOps engineer has just joined your team and needs to review audit logs. Which of the following roles could you use to grant the needed permissions without granting more permissions than needed to read logs? a). roles/logging.configWriter b). roles/logging.bucketWriter c). roles/logging.admin d). roles/logging.privateLogsViewer Correct: d Explanation: roles/logging.privateLogsViewer provides provides read-only access to log entries in logs, including private logs. roles/logging.admin would give permission to review logs but would also grant other permissions that are not needed. roles/logging.configWriter grants permissions to read and write the configurations of logs-based metrics and sinks for exporting logs. roles/logging.bucketWriter provides permission to write to a Cloud Storage bucket. Reference: https://cloud.google.com/logging/docs/audit You have been given the responsibility to manage projects in GCP. What set of permissions will you need to manage projects? a). resourcemanager.projects.getIamPolicy and resourcemanager.projects.setIamPolicy only b). resourcemanager.projects.get and resourcemanager.projects.setIamPolicy only c). resourcemanager.projects.get only d). resourcemanager.projects.get, resourcemanager.projects.getIamPolicy, and resourcemanager.projects.setIamPolicy only Correct: d Explanation: The permissions, resourcemanager.projects.get, for retrieving projects identified by a project ID, resourcemanager.projects.getIamPolicy, for getting IAM access control policy for the specified Project, and resourcemanager.projects.setIamPolicy, for setting IAM access control policy for the specified project, are all required. Reference: https://cloud.google.com/resource-manager/docs/access-control-proj Your team has created a set of Docker images. You want to be able to better manage groups of containers. What could you do to images to enable grouping by environment, installed component, etc? a). Set the billing account parameter when creating the image. b). Add tags with the gcloud container images add-tag command. c). Set the account parameter when creating the images. d). Add configurations with the gcloud container images add-configuration command. Correct: b Explanation: Tags are used to group resources in GCP. They are added to images using the gcloud container images add-tag command. add-tag is not a valid command. Setting the billing account when creating an image is not a valid option. The add-configuration command is not a valid option. Reference: https://cloud.google.com/sdk/gcloud/reference/container/images A game developer is using App Engine to run several services. One of the services queries a Datastore database for user information. Queries over a single property work correctly but queries that reference two or more properties are not returning any data. You have been asked to help diagnose the problem. Which file in the application would you look to first to diagnose the problem? a). dispatch.yaml b). index.yaml c). cron.yaml d). app.yaml Correct: b Explanation: Index.yaml files contain indexes for complex queries that reference more than one attribute. Datastore automatically creates indexes for single attributes. All queries must have a supporting index. App.yaml is for application specifications. Cron.yaml is for scheduled jobs. Dispatch.yaml is for overriding routing rules. Reference: https://cloud.google.com/appengine/docs/flexible/go/configuring-datastore-indexes-with-indexyaml You would like to grant a role to an identity so that it can access a resource. Which of the resource's subcommands would you use to grant a role on a resource? a). add-iam-policy-binding with --member and --role flags b). add-iam-policy-binding with no flags c). set-iam-policy with no flags d). set-iam-policy with --member and --role flags Correct: a Explanation: The correct subcommand is add-iam-policy-binding with --member and --role flags. The subcommand is available for several resource types, including: disk, images, instances, and snapshots among others. A developer has to work with several clients all using Google Cloud. They would like to easily switch between clients when working on the command line. What would you recommend they do? a). Create a configuration for each client and use the gcloud auth command to activate the appropriate configuration for each client. b). Create a policy for each client and use the gcloud policy command to activate the appropriate policy for each client. c). Create a configuration for each client and use the gcloud config configurations activate command to activate the appropriate configuration for each client. d). Create a policy for each client and use the gcloud auth-policy command to activate the appropriate policy for each client. Correct: c Explanation: The gcloud config configurations activate command allows for rapid changes to the configuration for the Cloud SDK and could be used for that purpose in this use case. gcloud auth is not used to activate a configuration. Policies are not used to specify command line configurations. Reference: https://cloud.google.com/sdk/gcloud/reference/config You have created a new set of service accounts and want them to be used by existing VMs running in Compute Engine. What command would you use to assign one of the service accounts to a VM instance? a). gcloud compute instances assign b). gcloud compute instances set-service-account c). gcloud instances set-service-account d). gcloud compute service-accounts assign Correct: b Explanation: The command gcloud compute instances set-service-account assigns a service account to a VM instance. The other options are not valid gcloud commands. Reference: https://cloud.google.com/sdk/gcloud/reference/compute/instances You have been given the responsibility to manage several projects in GCP. You want to list roles defined in a particular project. What Cloud SDK command would you use? (Note: [PROJECT-ID] is a placeholder for a project identifier.) a). gcloud iam roles list --project=[PROJECT-ID] b). gcloud iam list roles --project=[PROJECT-ID] c). gsutil list iam roles [PROJECT-ID] d). gsutil iam list roles --project=[PROJECT-ID] Correct: a Explanation: gcloud iam roles list --project=[PROJECT-ID] is the correct command. Gsutil is the command line for managing Cloud Storage, not IAM so the two options that use gsutil are incorrect. Reference: https://cloud.google.com/sdk/gcloud/reference/iam/roles/list As an administrator of a Kubernetes Engine cluster with many users from several teams and departments, you would like to easily analyze resource usage by team and department. What mechanisms could you use to differentiate resource usage? a). instance attributes b). Namespaces c). key-value pairs d). Guest attributes e). Labels Correct: e Explanation: Labels are key/value pairs that are attached to objects to specify identifying attributes of objects that are useful for users and admins. Namespaces are virtual clusters designed for environments like the one described, with many users, team, or other organization group structures A developer would like to create an image from a snapshot. What command should they use to create an image named devimage1 from a snapshot called devsnapshot1? a). gcloud compute images create devimage1 --source-snapshot devsnapshot1 b). gcloud disk images create devimage1 --source-snapshot devsnapshot1 c). gcloud disk create devimage1 --source-snapshot devsnapshot1 d). gcloud images create devimage1 --source-snapshot devsnapshot1 Correct: a Explanation: Images are Compute Engine resources so they use gcloud compute commands. Images are the resource that is being created so the command is gcloud compute images create followed by the image name and the source-snapshot parameter. gcloud disk and gcloud image are not valid gcloud commands. Reference: https://cloud.google.com/sdk/gcloud/reference/compute/disks/create You have determined that an application using a service account is not functioning correctly. You think it may be an access control problem. You would like to view audit logs to determine if this is the case. What audit log would you review? a). Policy Denied Audit Log b). Identity Access Audit Log c). Admin Activity Log d). Data Access Audit Log Correct: a Explanation: The Policy Denied Audit Log captures details when a user or service account is denied access because of a security policy violation. The Admin Activity Log tracks administrative actions including changes to configurations and metadata. The Data Access Audit Log tracks changes or reads of resource data or metadata. There is no Identity Access Audit Log in GCP. Reference: https://cloud.google.com/logging/docs/audit You have several buckets using Nearline storage class that you would like to change to Coldline storage. What command would you use? a). gsutil rewrite -s STORAGE_CLASS gs://PATH_TO_OBJECT b). gcloud storage rewrite -s STORAGE_CLASS gs://PATH_TO_OBJECT c). gsutil newclass -s STORAGE_CLASS gs://PATH_TO_OBJECT d). gcloud storage newclass -s STORAGE_CLASS gs://PATH_TO_OBJECT Correct: a Explanation: Gsutil is the command line utility for Cloud Storage. The rewrite command with the -s parameter is used to change storage classes. gcloud storage rewrite -s STORAGE_CLASS gs://PATH_TO_OBJECT and gcloud storage newclass -s STORAGE_CLASS gs://PATH_TO_OBJECT are incorrect options because they use gcloud instead of gsutil. gsutil newclass -s STORAGE_CLASS gs://PATH_TO_OBJECT is incorrect, gsutil newclass is not a valid Cloud Storage command. Reference: https://cloud.google.com/storage/docs/gsutil/commands/rewrite As a consultant to a global logistics company, you have been asked to advise on the migration from an on premises inventory system to Google Cloud. The inventory system uses a relational database. Your client wants to use a managed database service that supports users in multiple regions. Inventory data needs to be consistent at all times. What service would you recommend? a). Cloud Bigtable b). Cloud BigQuery c). Cloud Spanner d). Cloud SQL Correct: c Explanation: Cloud Spanner is a globally scalable, managed relational database that supports consistency. Cloud Bigtable is a NoSQL database. BigQuery is an analytical database. Cloud SQL is a relational database but it is designed for regional use cases. Reference: https://cloud.google.com/spanner/docs/concepts A client has asked you to help implement a cluster of servers that can scale up and down as needed and will be resilient to a failure in a zone. What feature of Compute Engine would you use? a). snapshot b). unmanaged instance groups c). managed instance groups d). instance templates Correct: c Explanation: Managed instance groups are used to create sets of identically configured VMs that can autoscale an can be configure for regional deployment, which would address the need to be resilient to a failure in a single zone. A distributed application uses Cloud Pub/Sub to send messages to a service that analyzes the data. Each message should only be sent once but for some reason, messages are sent repeatedly. What configuration parameter would you investigate in order to correct this problem? a). --max-retry-delay b). --ack-deadline c). --dead-letter-topic d). min-retry-delay Correct: b Explanation: The problem may be caused by the consuming application not acknowledging the message has been successfully processed within the time required. The --ack-deadline may be set too low and the consuming application may not send the acknowledgement within that time. Increasing the --ack-deadline would give the consuming application more time to acknowledge successful processing of the message. Dead-letter-topics are specified to receive messages that cannot be acknowledged. --max-retry-delay is the maximum delay between consecutive deliveries of a given message. The min-retry-delay is the minimum delay between consecutive deliveries of a given message. Changing either the max or min retry delay will not affect acknowledging a message, which is the root cause of the problem. Reference: https://cloud.google.com/pubsub/docs/publisher Reference: https://cloud.google.com/pubsub/docs/subscriber Reference: https://cloud.google.com/sdk/gcloud/reference/pubsub/subscriptions/create Reference: https://cloud.google.com/pubsub/docs/dead-letter-topics You are working in Cloud Shell to diagnose a problem with a Cloud SQL server running MySQL. You would like to connect to the MySQL instance from the command line. What command would you use to connect to the database as root using the built-in client? [INSTANCE-ID] is the database instance identifier. a). gcloud sql connect [INSTANCE-ID] --user=root b). gcloud mysql connect [INSTANCE-IP] --user=root c). gcloud mysql --host=[INSTANCE -D] --user=root d). gsutil sql connect [INSTANCE-ID] --user=root Correct: a Explanation: To use the client built into Cloud Shell, the correct command is gcloud sql connect followed by the instance ID of the database you want to connect to along with the --user parameter to specify the username. The option mysql --host=[INSTANCE -D] --user=root. Both of the options, gcloud mysql --host=[INSTANCE -D] --user=root and gcloud mysql connect [INSTANCE-IP] -- user=root, use gcloud mysql instead of gcloud sql and so are incorrect. Also gcloud mysql connect [INSTANCE-IP] --user=root specifies an IP address instead of an instance identifier. gsutil sql connect [INSTANCE-ID] --user=root is incorrect because gcloud is used with Cloud SQL and gsutil is command line utility used with Cloud Storage. Reference: https://cloud.google.com/sdk/gcloud/reference/sql/connect You need to deploy a load balancer that will support internal TCP traffic within a single region. What load balancer would you deploy? a). TCP Proxy b). Internal HTTP(S) Load Balancing c). SSL Proxy d). Network TCP/UDP Load Balancing Correct: d Explanation: The correct answer is Network TCP/UDP Load Balancing, which is used for internal TCP traffic. SSL Proxy is used for external TCP trafficand SSL processing is offloaded to the the proxy. Network TCP/UDP Load Balancing is used for internal TCP traffic. TCP Proxy Load Balancing is a reverse proxy load balancer that distributes TCP traffic coming from the internet to virtual machine (VM) instances in GCP. Reference: https://cloud.google.com/loadbalancing/docs/load-balancing-overview Reference: https://cloud.google.com/loadbalancing/docs/choosing-load-balancer A group of epidemiologists is running a large number of simulations. They are using several high CPU virtual machines. Each simulation takes approximately 10 minutes to complete. If a simulation fails before completing, it is restarted on another VM. The epidemiologists would like to minimize the GCP costs without increasing the time need to complete a simulation. What would you recommend? a). Use preemptible virtual machine instances. b). Use more virtual machine instances. c). Use sole-tenant VMS. d). Use Shielded VM instances. Correct: a Explanation: Preemptible VMs cost significantly less then standard priced VMs. Preemptible VMs run up to 24 hours before being shut down by Google. The epidemiologist can tolerate failures in some simulations since failed simulation are re-run. A VM is mistakenly started in the wrong region and zone. You suspect the default region and zone setting for your project is set incorrectly. What command would you use on the command line to show the default region and zone? [PROJECT-ID] refers to the project identifier to be described. a). gcloud project-info describe --project [PROJECT-ID] b). gcloud project info describe c). gcloud project info list d). gcloud compute project-info describe --project [PROJECT-ID] Correct: d Explanation: This is a compute service setting so you would use gcloud compute and the resource is a project so project-info is required. The correct command is gcloud compute project-info describe -- project [PROJECT ID]. The options gcloud project info describe, gcloud project-info describe -- project [PROJECT ID], and gcloud project info list are not valid gcloud commands, they do not specify compute, which is the name of the service the gcloud command applies to. Reference: https://cloud.google.com/sdk/gcloud/reference/compute/project-info An app development team wants to develop a service written in C++ that can process a file after it is uploaded to Cloud Storage. The processing time varies based on the size and complexity of the content of the file. Almost all files can be processed in less than 1 minute but some files can take up to 20 minutes to process. The developers are already using containers and Cloud Pub/Sub for other services. They plan to use Cloud Storage's trigger mechanism to send a message to Pub/Sub on upload. You want to minimize operational overhead while ensuring all files are processed correctly. What GCP compute service would you use? a). Anthos b). Compute Engine c). App Engine Standard d). Cloud Run Correct: d Explanation: Cloud Run can execute container images with custom applications written in any programming language. It is a managed service so operational overhead is minimized. Cloud Pub/Sub configured with a push subscription could invoke a service running in Cloud Run. Compute Engine is not a managed service and would have more operational overhead. App Engine Standard does not support C++ applications. Anthos is a platform for managing multiple Kubernetes clusters in multiple environments and while it could be used to run the service, it is more than required by the specifications. A startup is deploying analytical services for Internet of Things (IoT) applications. The service will need to ingest large volumes of time series data in short periods of time. Users will query the data is a few different ways but those ways are known and fixed. What managed GCP database service would you recommend? a). Bigtable b). Cloud Spanner c). Cloud SQL d). BigQuery Correct: a Explanation: Bigtable is a NoSQL wide-column database well suited to low latency writes. Since only a few, well defined query patterns are needed, you can design the data model to support those patterns. BigQuery does not support large volumes of low latency writes. Cloud SQL and Cloud Spanner are relational databases designed for transaction processing systems but this use case does not require a transaction processing system. Reference: https://cloud.google.com/bigtable/docs/overview A colleague has asked you to help them better managed multiple files in Cloud Storage. They would like to automate as much of the management as possible. You recommend using lifecycle management policies. What operations can be automatically performed using lifecycle policy management? a). Move files b). Delete c). Set storage class d). Enable versioning e). Copy files Correct: b, c Explanation: The two operations that can be performed using Cloud Storage lifecycle management are deleting objects and setting the storage class. Moving files and enabling versioning are not available in lifecycle management policies. Reference: https://cloud.google.com/storage/docs/lifecycle The CFO of your company is concerned that BigQuery costs are growing too large. They ask for recommendations on how to reduce the cost of querying without reducing the utility of the service. Most of the queries are based on the time the data arrived. What would you recommend as one way to reduce the amount of data scanned? a). Use the LIMIT option in SELECT statements to prevent more than a fixed number of rows from being returned. b). Use covering indexes to respond to queries using only the index without needing to seek additional blocks of data. c). Use ingestion time partitioned tables and specify _PARTITIONTIME filters when querying. d). Use read replicas and query only the read replica not the primary, which is where data is written. Correct: c Explanation: Ingestion time partitioned tables creates partitions by ingestion time. The pseudo column _PARTITIONTIME is added to the table and can be used to restrict the amount of data scanned. BigQuery does not provide for read replicas like some databases. BigQuery does not use indexes. The LIMIT option does not reduce the amount of data scanned, it only limits the number of rows returned. Your most recent GCP bill is higher than expected because of a significant increase in storage charges. Your department recently enabled an audit log that is off by default for most services. You think that audit logging may be responsible for the increased storage charges. Which type of audit log would you think could be responsible? a). Policy Denied Audit Log b). System Event Audit Log c). Data Access Audit Log d). Admin Activity audit logs Correct: c Explanation: Data Access Audit logs generate large amounts of data and are disabled by default for most services; it is enabled by default for BigQuery. The other audit logs are not likely to generate the same volume of data as the Data Access Audit logs. Reference: https://cloud.google.com/logging/docs/audit You are setting up a service in Kubernetes Engine. You would like to have all internal clients send request to a stable internal IP address. What type of service would you create? a). LoadBalancer b). ClusterIP c). Headless d). NodePort Correct: b Explanation: ClusterIP service is the default type of service and is used to enable internal clients to send requests to a stable internal IP address. Headless service type is used when a stable IP address is not needed. NodePort is used to enable clients to send requests to the IP address of a node on one or more nodePort values specified by the service. LoadBalancer clients send request to the IP address of a network load balancer. Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/service A data scientist is running several large queries against BigQuery. They would like to know how much the query will cost before running the query. How would recommend they do that? a). Use the bq query command with the --dry_run flag b). Use the bq query command with the --cost flag c). Use the bq query command with the --limit parameter and the --scan parameter d). Use the bq query command with the --estimate-cost flag Correct: a Explanation: The --dry_run flag is used with the bq query command to estimate the cost of running a query. bq does not have --estimate-cost or --cost flags. There is --scan parameter in bq. Limit can be used with SQL queries to limit the number of rows returned but it does not limit the amount of data scanned. Since it is the amount of data scanned, not returned, that determines cost, using a limit statement will not help reduce cost. Reference: https://cloud.google.com/bigquery/docs/best-practices-costs You are administering a project that uses BigQuery. You would like to list all the datasets in the project. What command would you use? a). bq ls b). gsutil ls c). gsutil dir d). bq dir Correct: a Explanation: BigQuery uses the bq command line and the command to list datasets is bq ls. bq dir is not a valid bq command. Gsutil is the command line utility used with Cloud Storage not BigQuery. Reference: https://cloud.google.com/bigquery/docs/reference/bq-cli-reference You believe you may have over provisioned several VMs. You would like to get data on the CPU load at 15 minute intervals from all Linux servers. How would you do this with the least amount of work? a). Install the Prometheus agent on each server and monitor the load_15m metric. b). Install a bash script that uses the sar -u command to get CPU utilization and write the value to sysout. c). Install the Cloud Monitoring agent on each server and monitor the load_15m metric. d). Install a bash script that uses the sar -u command to get CPU utilization and write the value to syslog. Correct: c Explanation: The Cloud Monitoring agent collects data on CPU utilization and other metrics. Installing the agent and then view the collected data with Cloud Monitoring requires the least amount of work. Installing a bash script that uses the sar -u command to get CPU utilization and write the value to syslog or sysout would work but would require you to write and maintain the script. Prometheus is an open source monitoring tool that could be used but you would need to install, configure, and maintain it. Reference: https://cloud.google.com/monitoring/agent/installation You have deployed a service using App Engine that requires a batch job run every hour. You notice that the batch job is running every two hours instead of every hour. You'd like to change the job specification to correct the problem. What file would you edit to correct the problem? a). app.yaml b). batch.yaml c). job.yaml d). cron.yaml Correct: d Explanation: Cron.yaml files contain specifications for running scheduled jobs in App Engine. App.yaml has overall application specifications. Batch.yaml and job.yaml are not specified as part of App Engine services. Reference: https://cloud.google.com/appengine/docs/flexible/python/scheduling-jobs-with-cron-yaml You want to create an autoscaling Kubernetes cluster in Kubernetes Engine. Which of the following commands would you specify? a). kubectl containers create with --enable-autoscaling flag and --min-nodes and max-nodes parameters. b). gcloud container clusters create with --enable-autoscaling flag and --min-nodes and maxnodes parameters. c). gcloud kubernetes clusters create with --enable-autoscaling flag and --min-nodes and maxnodes parameters. d). kubectl clusters create with --enable-autoscaling flag and --min-nodes and max-nodes parameters. Correct: b Explanation: The correct command is gcloud container clusters create with --enable-autoscaling flag and --min-nodes and max-nodes parameters. Kubectl is a command line for managing resources, such as pods, within a Kubernetes cluster so the options kubectl clusters create with --enableautoscaling flag and --min-nodes and max-nodes parameters and kubectl containers create with --enable-autoscaling flag and --min-nodes and max-nodes parameters are incorrect. The option gcloud kubernetes clusters create with --enable-autoscaling flag and --min-nodes and max-nodes parameters is incorrect because it specifies gcloud kubernetes instead of gcloud containers. (Kubernetes Engine was originally named Container Engine. When the service name was changed, the gcloud command was not changed to reflect the new service name). Reference: https://cloud.google.com/sdk/gcloud/reference/container/clusters/create You need to deploy a load balancer that will support external clients using TCP traffic, including SSL. You want to offload SSL processing. What load balancer would you deploy? a). SSL Proxy b). The explanation was pasted here. Was \"Internal HTTP(S) Load Balancing\" supposed to go here, like the question below? c). TCP Proxy d). Network TCP/UDP Load Balancing Correct: a Explanation: The correct answer is the SSL Proxy, which should be used for external TCP traffic with the SSL processing offloaded to the the proxy. Network TCP/UDP Load Balancing is used for internal TCP traffic. A network load balancer distributes TCP or UDP traffic among virtual machine (VM) instances in the same region. Reference: https://cloud.google.com/loadbalancing/docs/load-balancing-overview Reference: https://cloud.google.com/loadbalancing/docs/choosing-load-balancer You have created a virtual private cloud (VPC) in auto mode, which automatically creates a subnet in each region. How is the CIDR block determined for each region? a). Each region will automatically be assigned a set of predefined IP ranges that fit within the 10.128.0.0/9 CIDR block. b). You will specify non-overlapping CIDR blocks for each region. c). You will specify non-overlapping CIDR ranges for the set of regions you want a subnet created for. d). Each region will be automatically assigned a range of external IP addresses. Correct: a Explanation: When using auto mode VPC creation, a subnet is created in each region and each subnet is assigned a range of IP addresses that fit within the 10.128.0.0/9 CIDR block. You do not have to specify CIDR blocks in auto mode so both of the options, You will specify non-overlapping CIDR blocks for each region and You will specify non-overlapping CIDR ranges for the set of regions you want a subnet created for, are incorrect. External addresses are not assigned in auto mode, so the option each region will be automatically assigned a range of external IP addresses is incorrect. Reference: https://cloud.google.com/vpc/docs/using-vpc A colleague who is new to GCP has asked for your help with understanding predefined roles. They would like to know details about several predefined roles. What command would you suggest they use? [ROLE-ID] indicates where to specify the role identifier in the command. a). gcloud iam roles describe [ROLE-ID] b). gcloud roles predefined describe [ROLE-ID] c). gcloud iam roles list [ROLE-ID] d). gcloud roles predefined describe [ROLE-ID] Correct: a Explanation: gcloud iam roles describe [ROLE-ID] is the correct command for displaying information about a role. The options gcloud roles predefined describe [ROLE-ID] and gcloud roles predefined describe [ROLE-ID] are missing the term iam, which is needed to indicate the gcloud command is for the Identity and Access Management service. The option gcloud iam roles list [ROLE-ID] uses list instead of describe. List is typically used to see short descriptions of multiple resources while describe is typically used to see more detailed information about a resource. Reference: https://cloud.google.com/sdk/gcloud/reference/iam/roles/describe Your team uses a number of custom images. You want to be able to release new versions of each image while still maintaining the ability to rollback to a previous version if needed. What feature of Compute Engine would you use for this purpose? a). community supported images b). managed instance groups c). image families d). unmanaged instance groups Correct: c Explanation: Image families are used to group related images together so you can roll forward or back between specific images versions. Image families always point to the latest version of an image that is not deprecated. Managed and unmanaged instance groups are types of clusters, not an image management feature. Community supported images not directly supported by Compute Engine and are maintained by community members. You are hosting a large amount of image and video content for an educational service. Learners from around the world use the service. You currently host content on servers in your own data center in North America. Learners in Asia, Africa, and Europe experience long latencies loading content. What GCP service could you use to ensure that all learners experience the same level of latency and the latency is kept low, especially for frequently accessed content? a). Persistent disks b). Cloud CDN c). Cloud Storage Nearline Storage Class d). Cloud VPN Correct: b Explanation: Cloud CDN is a content distribution network for global content delivery. It caches data at distributed points around the world so users requests for content are routed to the closest content location. Cloud Storage Nearline Storage Class is appropriate for content that is only accessed once or less per month. Cloud VPNs are used to link Google Cloud network to external networks, such as an on premises data center network. Persistent disks are used to store data on virtual machines and could be used if you were to deploy and manage VMs across multiple regions but that would be less efficient than using Cloud CDN. Reference: https://cloud.google.com/cdn/docs/overview Due to security concerns, you want to ensure all data written to your Cloud Storage buckets are encrypted. What do you need to do to ensure data is encrypted when stored in Cloud Storage? a). Set up customer managed encryption keys and use those keys to encrypt data before saving to Cloud Storage. b). Set a lifecycle policy that specifies encryption is on. c). Use the --encrypt flag with gsutil d). Nothing, this is the default behavior. Correct: d Explanation: All data stored in Google Cloud is encrypted before it is persisted. You do not have to do anything to ensure your data is encrypted at rest in GCP. You may set up customer managed encryption keys if you want to control how keys are managed but it is not necessary. There is no --encrypt flag with gsutil. There is no encryption option with lifecycle management policies. Reference: https://cloud.google.com/storage/docs/encryption A startup is developing an Internet of Things (IoT) service. When data is first ingested, some basic data quality checks are performed that ensure the format is correct. The checks are simple Python functions that apply regular expression checks. The data will be ingested using Pub/Sub. When new data arrives, it should be automatically have the quality checks applied. The checks will always run for less than one second. What compute service would you use to apply the data quality checks? a). App Engine Standard b). Compute Engine c). Cloud Functions d). App Engine Flexible Correct: c Explanation: Cloud Functions are the best option since it is a managed service that can be invoked on events, such as when a message is ingested by Cloud Pub/Sub. Cloud Functions support Python. Also the code to execute runs for short periods of time. Compute Engine could be used but you would be charged for time the VM is running even if it were not processing data and would require more management than Cloud Functions, which is serverless. App Engine Standard and App Engine Flexible could be used but they provide more functionality than is needed to run simple Python functions applying regular expression checks and require more configuration than Cloud Functions. Reference: https://cloud.google.com/functions/docs/concepts/overview Reference: https://cloud.google.com/functions/docs/tutorials/pubsub A team providing business intelligence solutions to your company is migrating to GCP. They make extensive use of SQL and want to continue to use SQL. They currently use relational databases to store data. Data is loaded every night. When data is older than 3 years, it is no longer needed. They expect the database to grow to 100 GB within six months. Most of the operations on the database query a few columns but scan many rows. They also want to minimize database management overhead. What GCP service would you recommend they use? a). Bigtable b). BigQuery c). Cloud Firestore d). Cloud SQL Correct: b Explanation: BigQuery is the best option. BigQuery is a managed analytical database that supports SQL. It is optimized for write once/read many operations. It can easily store 100 GB or more of data. It is a managed service designed to support data warehouses. Bigtable and Cloud Firestore are NoSQL databases and do not support SQL. Cloud SQL does support SQL but it is designed for transaction processing and does not support up to 100 GB in a single database. Reference: https://cloud.google.com/solutions/migration/dw2bq/dw-bq-migrationoverview To improve security of your applications, you want to create several custom roles with limited permissions. What role would give you sufficient permission to create custom roles? a). roles/iam.roles.create.custom b). roles/iam.serviceAccountUser c). roles/iam.roles.create d). roles/iam.serviceAccountUser.create Correct: c Explanation: The role roles/iam.roles.create will provide the permissions required to create custom roles. roles/iam.roles.create.custom is not a valid option. roles/iam.serviceAccountUser is used to grant an identity access to a service account. roles/iam.serviceAccountUser.create is not a valid option. Reference: https://cloud.google.com/iam/docs/creating-custom-roles Reference: https://cloud.google.com/iam/docs/understanding-custom-roles A system admin needs to be able to create an instance that runs as a service account, attaches a persistent disk to an instance that runs as a service account, and set instance metadata on an instance that runs as a service account. Which of the following roles are required to meet those requirements? a). roles/compute.instanceAdmin.v1 b). roles/compute.imageUser c). roles/compute.storageAdmin d). roles/iam.serviceAccountCreator e). roles/iam.serviceAccountUser Correct: a, e Explanation: 2 roles are required: roles/compute.instanceAdmin.v1 and roles/iam.serviceAccountUser. The roles/compute.instanceAdmin.v1 role gives full control of Compute Engine instances, instance groups, disks, snapshots, and images. The roles/iam.serviceAccountUser role gives permission to run operations as the service account. roles/compute.storageAdmin gives only permissions to create, modify, and delete disks, images, and snapshots, which are available by roles/compute.instanceAdmin.v1. roles/iam.serviceAccountCreator gives permission to create service accounts. Reference: https://cloud.google.com/compute/docs/access/iam You are considering running a Windows server in GCP. You would like to review a list of Windows Server images available. What command would you use? a). gcloud compute list windows-cloud b). gcloud compute images list --project windows-cloud --no-standard-images c). gcloud compute images describe --project windows-cloud --no-standard-images d). gcloud compute instances describe windows-cloud Correct: b Explanation: The command gcloud compute images list --project windows-cloud --no-standard-images will list Windows Server images. List provides a list of images with minimal information. Describe is used to show more detailed information. The options gcloud compute list windows-cloud, gcloud compute images describe --project windows-cloud --no-standard-images, and gcloud compute instances describe windows-cloud are not a valid gcloud command, the term windows-cloud is not used in gcloud compute commands. Reference: https://cloud.google.com/sdk/gcloud/reference/compute/images/list You have just taken over responsibility to manage a large number of objects in Cloud Storage. You are reviewing a random sample of objects and want to know the creation time and content type for those objects. You plan to write a shell script to display this data. What command would you use in your script to retrieve that metadata? a). gsutil stat gs://BUCKET_NAME/OBJECT_NAME b). gsutil list gs://BUCKET_NAME/OBJECT_NAME c). gsutil metadata gs://BUCKET_NAME/OBJECT_NAME d). gsutil describe gs://BUCKET_NAME/OBJECT_NAME Correct: a Explanation: The correct command to list that metadata is gsutil stat gs://BUCKET_NAME/OBJECT_NAME. gsutil metadata is not a valid gsutil command. gsutil list is not a valid command, instead gsutil ls is used to list bucket information. gsutil describe is not a valid command. Describe is often used with gcloud commands, but not gsutil. Reference: https://cloud.google.com/storage/docs/gsutil/commands/stat A client of yours needs to be sure that only applications from the same project run on a physical server. For example, two applications from Project A may run on VMs deployed to a single physical server but an application from Project A and Project B will not be deployed to VMs that are running the same physical server. What feature of Compute Engine would you recommend? a). Preemptible VMs b). Sole-tenant nodes c). Managed instance groups d). Shielded VMs Correct: b Explanation: Sole-tenant nodes provide for exclusive access to a physical server. Shielded VMs provide additional security protections but do not guarantee sole-tenancy. Managed instance groups is a set of identically configured VMs. Preemptible VMs are low cost VMs that may be shutdown at any time by Google. Reference: https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes A team of data scientists wants to migrate an on premises Spark cluster to Google Cloud. They would like to use a managed service. What GCP service would you recommend? a). Cloud Dataproc b). Cloud Dataflow c). Cloud Bigtable d). Cloud Data Studio Correct: a Explanation: Cloud Dataproc is a managed Spark/Hadoop cluster service. Cloud Data Studio is a reporting and analytics tool. Cloud Dataflow is a batch and stream processing platform. Cloud Bigtable is a NoSQL database. Reference: https://cloud.google.com/dataproc/docs/concepts/overview You would like to use SSH host keys on your VMs to improve security. What feature of Compute Engine VMs do you need to enable to store SSH host keys? a). Shielded VMs b). labels c). Sole-tenant nodes d). guest attributes Correct: d Explanation: The correct answer is guest attributes. When guest attributes are enabled, Compute Engine will store your generated host keys as guest attributes. Sole-tenant nodes provides for exclusive access to a physical server. Shielded VMs provide additional security protections but do not guarantee sole-tenancy. Labels allow you to specify key-value pairs for grouping resources, for example, a VM cloud be labeled with the key value pair 'envior:production.' Reference: https://cloud.google.com/solutions/connecting-securely You are running several services in Cloud Run. You will need to programmatically determine the name of the configuration that created the container. Where would you find this? a). In the container startup script b). In the service startup script c). In the K_Configuration environment variable d). In the VM instance metadata Correct: c Explanation: When Cloud Run starts a container, it creates environment variables: K_Configuration, K_Revision, K_Service, and Port. K_Configuration specifies the configuration that created the container. When using Docker containers, you specify a command to execute on startup using the CMD command in a Dockerfile configuration. Container startup or service startup scripts do not necessarily set a standard environment variable for storing metadata, such as the name of the configuration used with Cloud Run. VM instance data is incorrect because Cloud Run is serverless and we do not have access to underlying VMs. Reference: https://cloud.google.com/run/docs/reference/container-contract Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"GCP ACE Practice Questions-2"},{"location":"nightwolf-cotribution/gcp-ace-2/#google-cloud-associate-cloud-engineer-practice-exam-2","text":"These are top 100 GCP ACE(Google Cloud Platform Associate Cloud Engineer) certification practice questions/cheatsheet (GCP ACE exam dumps) for professionals who are aspired to be GCP ACE certified. You will find these GCP ACE questions and answers very helpful in your GCP-ACE certification and interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); A team of developers would like to have a standard work environment in Compute Engine. You would like to be able to create multiple instances with identical configurations. The configurations should be easy to change. What feature of Compute Engine would you use to specify the configuration? a). snapshot b). managed instance groups c). instance template d). unmanaged instance groups Correct: c Explanation: Instance templates specify the configuration of virtual machines and managed instance groups. Unmanaged instance groups may be heterogeneous and do not use instance templates. Snapshots are copies of disks at a point in time. They are useful for backups and copying disks but are not used for specifying configurations. Reference: https://cloud.google.com/compute/docs/images/image-families-best-practices An IoT startup is uploading 2 TB of historical data to Cloud Storage. The data is in files with one file per day per sensor. There are thousands of files. The file names are the date followed by sensor ID, such as 20210101sensorABCD for a file with data from a sensor called ABCD on January 1, 2021. The loads are not proceeding as quickly as expected. What might be the cause of the slower than expected upload? a). The files are in gzip format instead of Avro format. b). The file names use the date as the first part of the filename so they may be creating hotspots when writing data to Cloud Storage. c). The files are in CSV instead of Avro format. d). The data in files is being encrypted before being persisted to storage. Correct: b Explanation: Files are written to servers within the Cloud Storage system based on filenames. Lexically close file names are written to the same server. This can lead to hotspotting, which is an imbalance in workload that has a small number of servers doing more work than other servers. Data is always encrypted before being persisted to storage so there should be no unexpected overhead for encrypting data. The size of the file not the format is the primary determiner of how long a file will take to load. tspots in which a few servers have heavy loads while other servers have little or no load. Load times should not vary based on the file format, such as CSV vs. Avro vs gzip. Reference: https://cloud.google.com/storage/docs/best-practices Your use of GCP has grown significantly and you now have many resources to manage. Some resources are in different environments and some are for different teams. You would like to easily group resources so you can manage them more effectively. What feature of GCP would you use? a). Managed instance groups b). Snapshots c). Images d). Labels Correct: d Explanation: Labels allow you to specify key-value pairs for grouping resources, for example, a VM cloud be labeled with the key value pair 'envior:production.' Images are copies of operating systems, boot loaders, and related data that can be used to create boot disks. Snapshots are copies of persistent volumes made at a specific point in time. Managed instance groups are used to create sets of identically configured VMs that can autoscale. Reference: https://cloud.google.com/compute/docs/labeling-resources You would like to display information about a dataset named primarydata in a project called analytics1. What command would you use? a). bq ls --format=prettyjson analytics1:primarydata b). bq show --format=prettyjson analytics1:primarydata c). bq ls --format=prettyjson analytics1//primarydata d). bq metadata --format=prettyjson analytics1:primarydata Correct: b Explanation: The correct answer is bq show --format=prettyjson analytics1:primarydata. The bq command is used with BigQuery and the show command displays information about a data set. Projects and datasets are specified as [project name]:[dataset name]. The option bq ls --format=prettyjson analytics1:primarydata is incorrect because it specifies ls instead of show. The option bq metadata --format=prettyjson analytics1:primarydata is incorrect because it specifies metadata instead of show. The option bq ls --format=prettyjson analytics1//primarydata is incorrect because it specifies ls instead of show and has the wrong syntax for specifying the project and dataset names. For more information, see https://cloud.google.com/bigquery/docs/reference/bqcli-reference Reference: https://cloud.google.com/bigquery/docs/reference/bqcli-reference A DevOps engineer has just joined your team and needs to review audit logs. Which of the following roles could you use to grant the needed permissions without granting more permissions than needed to read logs? a). roles/logging.configWriter b). roles/logging.bucketWriter c). roles/logging.admin d). roles/logging.privateLogsViewer Correct: d Explanation: roles/logging.privateLogsViewer provides provides read-only access to log entries in logs, including private logs. roles/logging.admin would give permission to review logs but would also grant other permissions that are not needed. roles/logging.configWriter grants permissions to read and write the configurations of logs-based metrics and sinks for exporting logs. roles/logging.bucketWriter provides permission to write to a Cloud Storage bucket. Reference: https://cloud.google.com/logging/docs/audit You have been given the responsibility to manage projects in GCP. What set of permissions will you need to manage projects? a). resourcemanager.projects.getIamPolicy and resourcemanager.projects.setIamPolicy only b). resourcemanager.projects.get and resourcemanager.projects.setIamPolicy only c). resourcemanager.projects.get only d). resourcemanager.projects.get, resourcemanager.projects.getIamPolicy, and resourcemanager.projects.setIamPolicy only Correct: d Explanation: The permissions, resourcemanager.projects.get, for retrieving projects identified by a project ID, resourcemanager.projects.getIamPolicy, for getting IAM access control policy for the specified Project, and resourcemanager.projects.setIamPolicy, for setting IAM access control policy for the specified project, are all required. Reference: https://cloud.google.com/resource-manager/docs/access-control-proj Your team has created a set of Docker images. You want to be able to better manage groups of containers. What could you do to images to enable grouping by environment, installed component, etc? a). Set the billing account parameter when creating the image. b). Add tags with the gcloud container images add-tag command. c). Set the account parameter when creating the images. d). Add configurations with the gcloud container images add-configuration command. Correct: b Explanation: Tags are used to group resources in GCP. They are added to images using the gcloud container images add-tag command. add-tag is not a valid command. Setting the billing account when creating an image is not a valid option. The add-configuration command is not a valid option. Reference: https://cloud.google.com/sdk/gcloud/reference/container/images A game developer is using App Engine to run several services. One of the services queries a Datastore database for user information. Queries over a single property work correctly but queries that reference two or more properties are not returning any data. You have been asked to help diagnose the problem. Which file in the application would you look to first to diagnose the problem? a). dispatch.yaml b). index.yaml c). cron.yaml d). app.yaml Correct: b Explanation: Index.yaml files contain indexes for complex queries that reference more than one attribute. Datastore automatically creates indexes for single attributes. All queries must have a supporting index. App.yaml is for application specifications. Cron.yaml is for scheduled jobs. Dispatch.yaml is for overriding routing rules. Reference: https://cloud.google.com/appengine/docs/flexible/go/configuring-datastore-indexes-with-indexyaml You would like to grant a role to an identity so that it can access a resource. Which of the resource's subcommands would you use to grant a role on a resource? a). add-iam-policy-binding with --member and --role flags b). add-iam-policy-binding with no flags c). set-iam-policy with no flags d). set-iam-policy with --member and --role flags Correct: a Explanation: The correct subcommand is add-iam-policy-binding with --member and --role flags. The subcommand is available for several resource types, including: disk, images, instances, and snapshots among others. A developer has to work with several clients all using Google Cloud. They would like to easily switch between clients when working on the command line. What would you recommend they do? a). Create a configuration for each client and use the gcloud auth command to activate the appropriate configuration for each client. b). Create a policy for each client and use the gcloud policy command to activate the appropriate policy for each client. c). Create a configuration for each client and use the gcloud config configurations activate command to activate the appropriate configuration for each client. d). Create a policy for each client and use the gcloud auth-policy command to activate the appropriate policy for each client. Correct: c Explanation: The gcloud config configurations activate command allows for rapid changes to the configuration for the Cloud SDK and could be used for that purpose in this use case. gcloud auth is not used to activate a configuration. Policies are not used to specify command line configurations. Reference: https://cloud.google.com/sdk/gcloud/reference/config You have created a new set of service accounts and want them to be used by existing VMs running in Compute Engine. What command would you use to assign one of the service accounts to a VM instance? a). gcloud compute instances assign b). gcloud compute instances set-service-account c). gcloud instances set-service-account d). gcloud compute service-accounts assign Correct: b Explanation: The command gcloud compute instances set-service-account assigns a service account to a VM instance. The other options are not valid gcloud commands. Reference: https://cloud.google.com/sdk/gcloud/reference/compute/instances You have been given the responsibility to manage several projects in GCP. You want to list roles defined in a particular project. What Cloud SDK command would you use? (Note: [PROJECT-ID] is a placeholder for a project identifier.) a). gcloud iam roles list --project=[PROJECT-ID] b). gcloud iam list roles --project=[PROJECT-ID] c). gsutil list iam roles [PROJECT-ID] d). gsutil iam list roles --project=[PROJECT-ID] Correct: a Explanation: gcloud iam roles list --project=[PROJECT-ID] is the correct command. Gsutil is the command line for managing Cloud Storage, not IAM so the two options that use gsutil are incorrect. Reference: https://cloud.google.com/sdk/gcloud/reference/iam/roles/list As an administrator of a Kubernetes Engine cluster with many users from several teams and departments, you would like to easily analyze resource usage by team and department. What mechanisms could you use to differentiate resource usage? a). instance attributes b). Namespaces c). key-value pairs d). Guest attributes e). Labels Correct: e Explanation: Labels are key/value pairs that are attached to objects to specify identifying attributes of objects that are useful for users and admins. Namespaces are virtual clusters designed for environments like the one described, with many users, team, or other organization group structures A developer would like to create an image from a snapshot. What command should they use to create an image named devimage1 from a snapshot called devsnapshot1? a). gcloud compute images create devimage1 --source-snapshot devsnapshot1 b). gcloud disk images create devimage1 --source-snapshot devsnapshot1 c). gcloud disk create devimage1 --source-snapshot devsnapshot1 d). gcloud images create devimage1 --source-snapshot devsnapshot1 Correct: a Explanation: Images are Compute Engine resources so they use gcloud compute commands. Images are the resource that is being created so the command is gcloud compute images create followed by the image name and the source-snapshot parameter. gcloud disk and gcloud image are not valid gcloud commands. Reference: https://cloud.google.com/sdk/gcloud/reference/compute/disks/create You have determined that an application using a service account is not functioning correctly. You think it may be an access control problem. You would like to view audit logs to determine if this is the case. What audit log would you review? a). Policy Denied Audit Log b). Identity Access Audit Log c). Admin Activity Log d). Data Access Audit Log Correct: a Explanation: The Policy Denied Audit Log captures details when a user or service account is denied access because of a security policy violation. The Admin Activity Log tracks administrative actions including changes to configurations and metadata. The Data Access Audit Log tracks changes or reads of resource data or metadata. There is no Identity Access Audit Log in GCP. Reference: https://cloud.google.com/logging/docs/audit You have several buckets using Nearline storage class that you would like to change to Coldline storage. What command would you use? a). gsutil rewrite -s STORAGE_CLASS gs://PATH_TO_OBJECT b). gcloud storage rewrite -s STORAGE_CLASS gs://PATH_TO_OBJECT c). gsutil newclass -s STORAGE_CLASS gs://PATH_TO_OBJECT d). gcloud storage newclass -s STORAGE_CLASS gs://PATH_TO_OBJECT Correct: a Explanation: Gsutil is the command line utility for Cloud Storage. The rewrite command with the -s parameter is used to change storage classes. gcloud storage rewrite -s STORAGE_CLASS gs://PATH_TO_OBJECT and gcloud storage newclass -s STORAGE_CLASS gs://PATH_TO_OBJECT are incorrect options because they use gcloud instead of gsutil. gsutil newclass -s STORAGE_CLASS gs://PATH_TO_OBJECT is incorrect, gsutil newclass is not a valid Cloud Storage command. Reference: https://cloud.google.com/storage/docs/gsutil/commands/rewrite As a consultant to a global logistics company, you have been asked to advise on the migration from an on premises inventory system to Google Cloud. The inventory system uses a relational database. Your client wants to use a managed database service that supports users in multiple regions. Inventory data needs to be consistent at all times. What service would you recommend? a). Cloud Bigtable b). Cloud BigQuery c). Cloud Spanner d). Cloud SQL Correct: c Explanation: Cloud Spanner is a globally scalable, managed relational database that supports consistency. Cloud Bigtable is a NoSQL database. BigQuery is an analytical database. Cloud SQL is a relational database but it is designed for regional use cases. Reference: https://cloud.google.com/spanner/docs/concepts A client has asked you to help implement a cluster of servers that can scale up and down as needed and will be resilient to a failure in a zone. What feature of Compute Engine would you use? a). snapshot b). unmanaged instance groups c). managed instance groups d). instance templates Correct: c Explanation: Managed instance groups are used to create sets of identically configured VMs that can autoscale an can be configure for regional deployment, which would address the need to be resilient to a failure in a single zone. A distributed application uses Cloud Pub/Sub to send messages to a service that analyzes the data. Each message should only be sent once but for some reason, messages are sent repeatedly. What configuration parameter would you investigate in order to correct this problem? a). --max-retry-delay b). --ack-deadline c). --dead-letter-topic d). min-retry-delay Correct: b Explanation: The problem may be caused by the consuming application not acknowledging the message has been successfully processed within the time required. The --ack-deadline may be set too low and the consuming application may not send the acknowledgement within that time. Increasing the --ack-deadline would give the consuming application more time to acknowledge successful processing of the message. Dead-letter-topics are specified to receive messages that cannot be acknowledged. --max-retry-delay is the maximum delay between consecutive deliveries of a given message. The min-retry-delay is the minimum delay between consecutive deliveries of a given message. Changing either the max or min retry delay will not affect acknowledging a message, which is the root cause of the problem. Reference: https://cloud.google.com/pubsub/docs/publisher Reference: https://cloud.google.com/pubsub/docs/subscriber Reference: https://cloud.google.com/sdk/gcloud/reference/pubsub/subscriptions/create Reference: https://cloud.google.com/pubsub/docs/dead-letter-topics You are working in Cloud Shell to diagnose a problem with a Cloud SQL server running MySQL. You would like to connect to the MySQL instance from the command line. What command would you use to connect to the database as root using the built-in client? [INSTANCE-ID] is the database instance identifier. a). gcloud sql connect [INSTANCE-ID] --user=root b). gcloud mysql connect [INSTANCE-IP] --user=root c). gcloud mysql --host=[INSTANCE -D] --user=root d). gsutil sql connect [INSTANCE-ID] --user=root Correct: a Explanation: To use the client built into Cloud Shell, the correct command is gcloud sql connect followed by the instance ID of the database you want to connect to along with the --user parameter to specify the username. The option mysql --host=[INSTANCE -D] --user=root. Both of the options, gcloud mysql --host=[INSTANCE -D] --user=root and gcloud mysql connect [INSTANCE-IP] -- user=root, use gcloud mysql instead of gcloud sql and so are incorrect. Also gcloud mysql connect [INSTANCE-IP] --user=root specifies an IP address instead of an instance identifier. gsutil sql connect [INSTANCE-ID] --user=root is incorrect because gcloud is used with Cloud SQL and gsutil is command line utility used with Cloud Storage. Reference: https://cloud.google.com/sdk/gcloud/reference/sql/connect You need to deploy a load balancer that will support internal TCP traffic within a single region. What load balancer would you deploy? a). TCP Proxy b). Internal HTTP(S) Load Balancing c). SSL Proxy d). Network TCP/UDP Load Balancing Correct: d Explanation: The correct answer is Network TCP/UDP Load Balancing, which is used for internal TCP traffic. SSL Proxy is used for external TCP trafficand SSL processing is offloaded to the the proxy. Network TCP/UDP Load Balancing is used for internal TCP traffic. TCP Proxy Load Balancing is a reverse proxy load balancer that distributes TCP traffic coming from the internet to virtual machine (VM) instances in GCP. Reference: https://cloud.google.com/loadbalancing/docs/load-balancing-overview Reference: https://cloud.google.com/loadbalancing/docs/choosing-load-balancer A group of epidemiologists is running a large number of simulations. They are using several high CPU virtual machines. Each simulation takes approximately 10 minutes to complete. If a simulation fails before completing, it is restarted on another VM. The epidemiologists would like to minimize the GCP costs without increasing the time need to complete a simulation. What would you recommend? a). Use preemptible virtual machine instances. b). Use more virtual machine instances. c). Use sole-tenant VMS. d). Use Shielded VM instances. Correct: a Explanation: Preemptible VMs cost significantly less then standard priced VMs. Preemptible VMs run up to 24 hours before being shut down by Google. The epidemiologist can tolerate failures in some simulations since failed simulation are re-run. A VM is mistakenly started in the wrong region and zone. You suspect the default region and zone setting for your project is set incorrectly. What command would you use on the command line to show the default region and zone? [PROJECT-ID] refers to the project identifier to be described. a). gcloud project-info describe --project [PROJECT-ID] b). gcloud project info describe c). gcloud project info list d). gcloud compute project-info describe --project [PROJECT-ID] Correct: d Explanation: This is a compute service setting so you would use gcloud compute and the resource is a project so project-info is required. The correct command is gcloud compute project-info describe -- project [PROJECT ID]. The options gcloud project info describe, gcloud project-info describe -- project [PROJECT ID], and gcloud project info list are not valid gcloud commands, they do not specify compute, which is the name of the service the gcloud command applies to. Reference: https://cloud.google.com/sdk/gcloud/reference/compute/project-info An app development team wants to develop a service written in C++ that can process a file after it is uploaded to Cloud Storage. The processing time varies based on the size and complexity of the content of the file. Almost all files can be processed in less than 1 minute but some files can take up to 20 minutes to process. The developers are already using containers and Cloud Pub/Sub for other services. They plan to use Cloud Storage's trigger mechanism to send a message to Pub/Sub on upload. You want to minimize operational overhead while ensuring all files are processed correctly. What GCP compute service would you use? a). Anthos b). Compute Engine c). App Engine Standard d). Cloud Run Correct: d Explanation: Cloud Run can execute container images with custom applications written in any programming language. It is a managed service so operational overhead is minimized. Cloud Pub/Sub configured with a push subscription could invoke a service running in Cloud Run. Compute Engine is not a managed service and would have more operational overhead. App Engine Standard does not support C++ applications. Anthos is a platform for managing multiple Kubernetes clusters in multiple environments and while it could be used to run the service, it is more than required by the specifications. A startup is deploying analytical services for Internet of Things (IoT) applications. The service will need to ingest large volumes of time series data in short periods of time. Users will query the data is a few different ways but those ways are known and fixed. What managed GCP database service would you recommend? a). Bigtable b). Cloud Spanner c). Cloud SQL d). BigQuery Correct: a Explanation: Bigtable is a NoSQL wide-column database well suited to low latency writes. Since only a few, well defined query patterns are needed, you can design the data model to support those patterns. BigQuery does not support large volumes of low latency writes. Cloud SQL and Cloud Spanner are relational databases designed for transaction processing systems but this use case does not require a transaction processing system. Reference: https://cloud.google.com/bigtable/docs/overview A colleague has asked you to help them better managed multiple files in Cloud Storage. They would like to automate as much of the management as possible. You recommend using lifecycle management policies. What operations can be automatically performed using lifecycle policy management? a). Move files b). Delete c). Set storage class d). Enable versioning e). Copy files Correct: b, c Explanation: The two operations that can be performed using Cloud Storage lifecycle management are deleting objects and setting the storage class. Moving files and enabling versioning are not available in lifecycle management policies. Reference: https://cloud.google.com/storage/docs/lifecycle The CFO of your company is concerned that BigQuery costs are growing too large. They ask for recommendations on how to reduce the cost of querying without reducing the utility of the service. Most of the queries are based on the time the data arrived. What would you recommend as one way to reduce the amount of data scanned? a). Use the LIMIT option in SELECT statements to prevent more than a fixed number of rows from being returned. b). Use covering indexes to respond to queries using only the index without needing to seek additional blocks of data. c). Use ingestion time partitioned tables and specify _PARTITIONTIME filters when querying. d). Use read replicas and query only the read replica not the primary, which is where data is written. Correct: c Explanation: Ingestion time partitioned tables creates partitions by ingestion time. The pseudo column _PARTITIONTIME is added to the table and can be used to restrict the amount of data scanned. BigQuery does not provide for read replicas like some databases. BigQuery does not use indexes. The LIMIT option does not reduce the amount of data scanned, it only limits the number of rows returned. Your most recent GCP bill is higher than expected because of a significant increase in storage charges. Your department recently enabled an audit log that is off by default for most services. You think that audit logging may be responsible for the increased storage charges. Which type of audit log would you think could be responsible? a). Policy Denied Audit Log b). System Event Audit Log c). Data Access Audit Log d). Admin Activity audit logs Correct: c Explanation: Data Access Audit logs generate large amounts of data and are disabled by default for most services; it is enabled by default for BigQuery. The other audit logs are not likely to generate the same volume of data as the Data Access Audit logs. Reference: https://cloud.google.com/logging/docs/audit You are setting up a service in Kubernetes Engine. You would like to have all internal clients send request to a stable internal IP address. What type of service would you create? a). LoadBalancer b). ClusterIP c). Headless d). NodePort Correct: b Explanation: ClusterIP service is the default type of service and is used to enable internal clients to send requests to a stable internal IP address. Headless service type is used when a stable IP address is not needed. NodePort is used to enable clients to send requests to the IP address of a node on one or more nodePort values specified by the service. LoadBalancer clients send request to the IP address of a network load balancer. Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/service A data scientist is running several large queries against BigQuery. They would like to know how much the query will cost before running the query. How would recommend they do that? a). Use the bq query command with the --dry_run flag b). Use the bq query command with the --cost flag c). Use the bq query command with the --limit parameter and the --scan parameter d). Use the bq query command with the --estimate-cost flag Correct: a Explanation: The --dry_run flag is used with the bq query command to estimate the cost of running a query. bq does not have --estimate-cost or --cost flags. There is --scan parameter in bq. Limit can be used with SQL queries to limit the number of rows returned but it does not limit the amount of data scanned. Since it is the amount of data scanned, not returned, that determines cost, using a limit statement will not help reduce cost. Reference: https://cloud.google.com/bigquery/docs/best-practices-costs You are administering a project that uses BigQuery. You would like to list all the datasets in the project. What command would you use? a). bq ls b). gsutil ls c). gsutil dir d). bq dir Correct: a Explanation: BigQuery uses the bq command line and the command to list datasets is bq ls. bq dir is not a valid bq command. Gsutil is the command line utility used with Cloud Storage not BigQuery. Reference: https://cloud.google.com/bigquery/docs/reference/bq-cli-reference You believe you may have over provisioned several VMs. You would like to get data on the CPU load at 15 minute intervals from all Linux servers. How would you do this with the least amount of work? a). Install the Prometheus agent on each server and monitor the load_15m metric. b). Install a bash script that uses the sar -u command to get CPU utilization and write the value to sysout. c). Install the Cloud Monitoring agent on each server and monitor the load_15m metric. d). Install a bash script that uses the sar -u command to get CPU utilization and write the value to syslog. Correct: c Explanation: The Cloud Monitoring agent collects data on CPU utilization and other metrics. Installing the agent and then view the collected data with Cloud Monitoring requires the least amount of work. Installing a bash script that uses the sar -u command to get CPU utilization and write the value to syslog or sysout would work but would require you to write and maintain the script. Prometheus is an open source monitoring tool that could be used but you would need to install, configure, and maintain it. Reference: https://cloud.google.com/monitoring/agent/installation You have deployed a service using App Engine that requires a batch job run every hour. You notice that the batch job is running every two hours instead of every hour. You'd like to change the job specification to correct the problem. What file would you edit to correct the problem? a). app.yaml b). batch.yaml c). job.yaml d). cron.yaml Correct: d Explanation: Cron.yaml files contain specifications for running scheduled jobs in App Engine. App.yaml has overall application specifications. Batch.yaml and job.yaml are not specified as part of App Engine services. Reference: https://cloud.google.com/appengine/docs/flexible/python/scheduling-jobs-with-cron-yaml You want to create an autoscaling Kubernetes cluster in Kubernetes Engine. Which of the following commands would you specify? a). kubectl containers create with --enable-autoscaling flag and --min-nodes and max-nodes parameters. b). gcloud container clusters create with --enable-autoscaling flag and --min-nodes and maxnodes parameters. c). gcloud kubernetes clusters create with --enable-autoscaling flag and --min-nodes and maxnodes parameters. d). kubectl clusters create with --enable-autoscaling flag and --min-nodes and max-nodes parameters. Correct: b Explanation: The correct command is gcloud container clusters create with --enable-autoscaling flag and --min-nodes and max-nodes parameters. Kubectl is a command line for managing resources, such as pods, within a Kubernetes cluster so the options kubectl clusters create with --enableautoscaling flag and --min-nodes and max-nodes parameters and kubectl containers create with --enable-autoscaling flag and --min-nodes and max-nodes parameters are incorrect. The option gcloud kubernetes clusters create with --enable-autoscaling flag and --min-nodes and max-nodes parameters is incorrect because it specifies gcloud kubernetes instead of gcloud containers. (Kubernetes Engine was originally named Container Engine. When the service name was changed, the gcloud command was not changed to reflect the new service name). Reference: https://cloud.google.com/sdk/gcloud/reference/container/clusters/create You need to deploy a load balancer that will support external clients using TCP traffic, including SSL. You want to offload SSL processing. What load balancer would you deploy? a). SSL Proxy b). The explanation was pasted here. Was \"Internal HTTP(S) Load Balancing\" supposed to go here, like the question below? c). TCP Proxy d). Network TCP/UDP Load Balancing Correct: a Explanation: The correct answer is the SSL Proxy, which should be used for external TCP traffic with the SSL processing offloaded to the the proxy. Network TCP/UDP Load Balancing is used for internal TCP traffic. A network load balancer distributes TCP or UDP traffic among virtual machine (VM) instances in the same region. Reference: https://cloud.google.com/loadbalancing/docs/load-balancing-overview Reference: https://cloud.google.com/loadbalancing/docs/choosing-load-balancer You have created a virtual private cloud (VPC) in auto mode, which automatically creates a subnet in each region. How is the CIDR block determined for each region? a). Each region will automatically be assigned a set of predefined IP ranges that fit within the 10.128.0.0/9 CIDR block. b). You will specify non-overlapping CIDR blocks for each region. c). You will specify non-overlapping CIDR ranges for the set of regions you want a subnet created for. d). Each region will be automatically assigned a range of external IP addresses. Correct: a Explanation: When using auto mode VPC creation, a subnet is created in each region and each subnet is assigned a range of IP addresses that fit within the 10.128.0.0/9 CIDR block. You do not have to specify CIDR blocks in auto mode so both of the options, You will specify non-overlapping CIDR blocks for each region and You will specify non-overlapping CIDR ranges for the set of regions you want a subnet created for, are incorrect. External addresses are not assigned in auto mode, so the option each region will be automatically assigned a range of external IP addresses is incorrect. Reference: https://cloud.google.com/vpc/docs/using-vpc A colleague who is new to GCP has asked for your help with understanding predefined roles. They would like to know details about several predefined roles. What command would you suggest they use? [ROLE-ID] indicates where to specify the role identifier in the command. a). gcloud iam roles describe [ROLE-ID] b). gcloud roles predefined describe [ROLE-ID] c). gcloud iam roles list [ROLE-ID] d). gcloud roles predefined describe [ROLE-ID] Correct: a Explanation: gcloud iam roles describe [ROLE-ID] is the correct command for displaying information about a role. The options gcloud roles predefined describe [ROLE-ID] and gcloud roles predefined describe [ROLE-ID] are missing the term iam, which is needed to indicate the gcloud command is for the Identity and Access Management service. The option gcloud iam roles list [ROLE-ID] uses list instead of describe. List is typically used to see short descriptions of multiple resources while describe is typically used to see more detailed information about a resource. Reference: https://cloud.google.com/sdk/gcloud/reference/iam/roles/describe Your team uses a number of custom images. You want to be able to release new versions of each image while still maintaining the ability to rollback to a previous version if needed. What feature of Compute Engine would you use for this purpose? a). community supported images b). managed instance groups c). image families d). unmanaged instance groups Correct: c Explanation: Image families are used to group related images together so you can roll forward or back between specific images versions. Image families always point to the latest version of an image that is not deprecated. Managed and unmanaged instance groups are types of clusters, not an image management feature. Community supported images not directly supported by Compute Engine and are maintained by community members. You are hosting a large amount of image and video content for an educational service. Learners from around the world use the service. You currently host content on servers in your own data center in North America. Learners in Asia, Africa, and Europe experience long latencies loading content. What GCP service could you use to ensure that all learners experience the same level of latency and the latency is kept low, especially for frequently accessed content? a). Persistent disks b). Cloud CDN c). Cloud Storage Nearline Storage Class d). Cloud VPN Correct: b Explanation: Cloud CDN is a content distribution network for global content delivery. It caches data at distributed points around the world so users requests for content are routed to the closest content location. Cloud Storage Nearline Storage Class is appropriate for content that is only accessed once or less per month. Cloud VPNs are used to link Google Cloud network to external networks, such as an on premises data center network. Persistent disks are used to store data on virtual machines and could be used if you were to deploy and manage VMs across multiple regions but that would be less efficient than using Cloud CDN. Reference: https://cloud.google.com/cdn/docs/overview Due to security concerns, you want to ensure all data written to your Cloud Storage buckets are encrypted. What do you need to do to ensure data is encrypted when stored in Cloud Storage? a). Set up customer managed encryption keys and use those keys to encrypt data before saving to Cloud Storage. b). Set a lifecycle policy that specifies encryption is on. c). Use the --encrypt flag with gsutil d). Nothing, this is the default behavior. Correct: d Explanation: All data stored in Google Cloud is encrypted before it is persisted. You do not have to do anything to ensure your data is encrypted at rest in GCP. You may set up customer managed encryption keys if you want to control how keys are managed but it is not necessary. There is no --encrypt flag with gsutil. There is no encryption option with lifecycle management policies. Reference: https://cloud.google.com/storage/docs/encryption A startup is developing an Internet of Things (IoT) service. When data is first ingested, some basic data quality checks are performed that ensure the format is correct. The checks are simple Python functions that apply regular expression checks. The data will be ingested using Pub/Sub. When new data arrives, it should be automatically have the quality checks applied. The checks will always run for less than one second. What compute service would you use to apply the data quality checks? a). App Engine Standard b). Compute Engine c). Cloud Functions d). App Engine Flexible Correct: c Explanation: Cloud Functions are the best option since it is a managed service that can be invoked on events, such as when a message is ingested by Cloud Pub/Sub. Cloud Functions support Python. Also the code to execute runs for short periods of time. Compute Engine could be used but you would be charged for time the VM is running even if it were not processing data and would require more management than Cloud Functions, which is serverless. App Engine Standard and App Engine Flexible could be used but they provide more functionality than is needed to run simple Python functions applying regular expression checks and require more configuration than Cloud Functions. Reference: https://cloud.google.com/functions/docs/concepts/overview Reference: https://cloud.google.com/functions/docs/tutorials/pubsub A team providing business intelligence solutions to your company is migrating to GCP. They make extensive use of SQL and want to continue to use SQL. They currently use relational databases to store data. Data is loaded every night. When data is older than 3 years, it is no longer needed. They expect the database to grow to 100 GB within six months. Most of the operations on the database query a few columns but scan many rows. They also want to minimize database management overhead. What GCP service would you recommend they use? a). Bigtable b). BigQuery c). Cloud Firestore d). Cloud SQL Correct: b Explanation: BigQuery is the best option. BigQuery is a managed analytical database that supports SQL. It is optimized for write once/read many operations. It can easily store 100 GB or more of data. It is a managed service designed to support data warehouses. Bigtable and Cloud Firestore are NoSQL databases and do not support SQL. Cloud SQL does support SQL but it is designed for transaction processing and does not support up to 100 GB in a single database. Reference: https://cloud.google.com/solutions/migration/dw2bq/dw-bq-migrationoverview To improve security of your applications, you want to create several custom roles with limited permissions. What role would give you sufficient permission to create custom roles? a). roles/iam.roles.create.custom b). roles/iam.serviceAccountUser c). roles/iam.roles.create d). roles/iam.serviceAccountUser.create Correct: c Explanation: The role roles/iam.roles.create will provide the permissions required to create custom roles. roles/iam.roles.create.custom is not a valid option. roles/iam.serviceAccountUser is used to grant an identity access to a service account. roles/iam.serviceAccountUser.create is not a valid option. Reference: https://cloud.google.com/iam/docs/creating-custom-roles Reference: https://cloud.google.com/iam/docs/understanding-custom-roles A system admin needs to be able to create an instance that runs as a service account, attaches a persistent disk to an instance that runs as a service account, and set instance metadata on an instance that runs as a service account. Which of the following roles are required to meet those requirements? a). roles/compute.instanceAdmin.v1 b). roles/compute.imageUser c). roles/compute.storageAdmin d). roles/iam.serviceAccountCreator e). roles/iam.serviceAccountUser Correct: a, e Explanation: 2 roles are required: roles/compute.instanceAdmin.v1 and roles/iam.serviceAccountUser. The roles/compute.instanceAdmin.v1 role gives full control of Compute Engine instances, instance groups, disks, snapshots, and images. The roles/iam.serviceAccountUser role gives permission to run operations as the service account. roles/compute.storageAdmin gives only permissions to create, modify, and delete disks, images, and snapshots, which are available by roles/compute.instanceAdmin.v1. roles/iam.serviceAccountCreator gives permission to create service accounts. Reference: https://cloud.google.com/compute/docs/access/iam You are considering running a Windows server in GCP. You would like to review a list of Windows Server images available. What command would you use? a). gcloud compute list windows-cloud b). gcloud compute images list --project windows-cloud --no-standard-images c). gcloud compute images describe --project windows-cloud --no-standard-images d). gcloud compute instances describe windows-cloud Correct: b Explanation: The command gcloud compute images list --project windows-cloud --no-standard-images will list Windows Server images. List provides a list of images with minimal information. Describe is used to show more detailed information. The options gcloud compute list windows-cloud, gcloud compute images describe --project windows-cloud --no-standard-images, and gcloud compute instances describe windows-cloud are not a valid gcloud command, the term windows-cloud is not used in gcloud compute commands. Reference: https://cloud.google.com/sdk/gcloud/reference/compute/images/list You have just taken over responsibility to manage a large number of objects in Cloud Storage. You are reviewing a random sample of objects and want to know the creation time and content type for those objects. You plan to write a shell script to display this data. What command would you use in your script to retrieve that metadata? a). gsutil stat gs://BUCKET_NAME/OBJECT_NAME b). gsutil list gs://BUCKET_NAME/OBJECT_NAME c). gsutil metadata gs://BUCKET_NAME/OBJECT_NAME d). gsutil describe gs://BUCKET_NAME/OBJECT_NAME Correct: a Explanation: The correct command to list that metadata is gsutil stat gs://BUCKET_NAME/OBJECT_NAME. gsutil metadata is not a valid gsutil command. gsutil list is not a valid command, instead gsutil ls is used to list bucket information. gsutil describe is not a valid command. Describe is often used with gcloud commands, but not gsutil. Reference: https://cloud.google.com/storage/docs/gsutil/commands/stat A client of yours needs to be sure that only applications from the same project run on a physical server. For example, two applications from Project A may run on VMs deployed to a single physical server but an application from Project A and Project B will not be deployed to VMs that are running the same physical server. What feature of Compute Engine would you recommend? a). Preemptible VMs b). Sole-tenant nodes c). Managed instance groups d). Shielded VMs Correct: b Explanation: Sole-tenant nodes provide for exclusive access to a physical server. Shielded VMs provide additional security protections but do not guarantee sole-tenancy. Managed instance groups is a set of identically configured VMs. Preemptible VMs are low cost VMs that may be shutdown at any time by Google. Reference: https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes A team of data scientists wants to migrate an on premises Spark cluster to Google Cloud. They would like to use a managed service. What GCP service would you recommend? a). Cloud Dataproc b). Cloud Dataflow c). Cloud Bigtable d). Cloud Data Studio Correct: a Explanation: Cloud Dataproc is a managed Spark/Hadoop cluster service. Cloud Data Studio is a reporting and analytics tool. Cloud Dataflow is a batch and stream processing platform. Cloud Bigtable is a NoSQL database. Reference: https://cloud.google.com/dataproc/docs/concepts/overview You would like to use SSH host keys on your VMs to improve security. What feature of Compute Engine VMs do you need to enable to store SSH host keys? a). Shielded VMs b). labels c). Sole-tenant nodes d). guest attributes Correct: d Explanation: The correct answer is guest attributes. When guest attributes are enabled, Compute Engine will store your generated host keys as guest attributes. Sole-tenant nodes provides for exclusive access to a physical server. Shielded VMs provide additional security protections but do not guarantee sole-tenancy. Labels allow you to specify key-value pairs for grouping resources, for example, a VM cloud be labeled with the key value pair 'envior:production.' Reference: https://cloud.google.com/solutions/connecting-securely You are running several services in Cloud Run. You will need to programmatically determine the name of the configuration that created the container. Where would you find this? a). In the container startup script b). In the service startup script c). In the K_Configuration environment variable d). In the VM instance metadata Correct: c Explanation: When Cloud Run starts a container, it creates environment variables: K_Configuration, K_Revision, K_Service, and Port. K_Configuration specifies the configuration that created the container. When using Docker containers, you specify a command to execute on startup using the CMD command in a Dockerfile configuration. Container startup or service startup scripts do not necessarily set a standard environment variable for storing metadata, such as the name of the configuration used with Cloud Run. VM instance data is incorrect because Cloud Run is serverless and we do not have access to underlying VMs. Reference: https://cloud.google.com/run/docs/reference/container-contract","title":"Google Cloud Associate Cloud Engineer Practice Exam - 2:"},{"location":"nightwolf-cotribution/gcp-ace-3/","text":"Google Cloud Associate Cloud Engineer Practice Exam - 3: \uf0c1 These are top 50 GCP ACE(Google Cloud Platform Associate Cloud Engineer) certification practice questions/cheatsheet (GCP ACE exam dumps) for professionals who are aspired to be GCP ACE certified. You will find these GCP ACE questions and answers very helpful in your GCP-ACE certification and interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); You deployed a Python application to GCP App Engine Standard service in the us-central region. Most of your customers are based in Japan and are experiencing slowness due to the latency. You want to transfer the application from us-central region to asia-northeast1 region to minimize latency. What should you do? a). Create a new GCP project. Create a new App Engine Application in the new GCP project and set its region to asia-northeast-1. Delete the old App Engine application. b). Update the region property to asia-northeast1 on the App Engine application. c). Update the default region property to asia-northeast1 on the App Engine Service. d). Deploy a new app engine application in the same GCP project and set the region to asianortheast1. Delete the old App Engine application. Correct: a Explanation: App Engine is regional, and you cannot change an app's region after you set it. You can deploy additional services in the App Engine, but they will all be targeted to the same region. Reference: https://cloud.google.com/appengine/docs/locations You deployed a java application in a single Google Cloud Compute Engine VM. During peak usage, the application CPU is maxed out and results in stuck threads which ultimately make the system unresponsive, and requires a reboot. Your operations team want to receive an email alert when the CPU utilization is greater than 95% for more than 10 minutes so they can manually change the instance type to another instance that offers more CPU. What should you do? a). In Cloud Logging, create logs based metric for CPU usage and store it as a custom metric in Cloud Monitoring. Create an Alerting policy based on CPU utilization in Cloud Monitoring and trigger an email notification when the utilization exceeds the threshold. b). Link the GCP project to a Cloud Monitoring workspace. Configure an Alerting policy based on CPU utilization in Cloud Monitoring and trigger an email notification when the utilization exceeds the threshold. c). Write a custom script to monitor CPU usage and send an email notification when the usage exceeds the threshold. d). Link the project to a Cloud Monitoring workspace. Write a custom script that captures CPU utilization every minute and sends to Cloud Monitoring as a custom metric. Add an uptime check based on the CPU utilization Correct: b Explanation: A Workspace is a tool for monitoring resources contained in one or more Google Cloud projects or AWS accounts. In our case, we create a Stackdriver workspace and link our project to this workspace. Reference: https://cloud.google.com/monitoring/workspaces Your company installs and manages several types of IoT devices all over the world. Events range from 50,000 to 500,000 messages a second. You want to identify the best solution for ingesting, transforming, storing and analyzing this data in GCP platform. What GCP services should you use? a). Cloud Pub/Sub for ingesting, Cloud Dataflow for transforming, Cloud Datastore for storing and BigQuery for analyzing the time-series data. b). Firebase Messages for ingesting, Cloud Pub/Sub for transforming, Cloud Spanner for storing and BigQuery for analyzing the time-series data. c). Cloud Pub/Sub for ingesting, Cloud Dataflow for transforming, Cloud Bigtable for storing and BigQuery for analyzing the time-series data. d). Cloud Pub/Sub for ingesting, Cloud Storage for transforming, BigQuery for storing and Cloud Bigtable for analyzing the time-series data. Correct: c Explanation: For ingesting time series data, your best bet is Cloud Pub/Sub. For processing the data in pipelines, your best bet is Cloud Dataflow. That leaves us with two remaining options; both have BigQuery for analyzing the data. For storage, it is a choice between Bigtable and Datastore. Bigtable provides out of the box support for time series data. So using Bigtable for Storage is the right answer. Reference: https://cloud.google.com/bigtable/docs/schema-design-time-series You migrated an internal HR system from an on-premises database to Google Cloud Compute Engine Managed Instance Group (MIG). The networks team at your company has asked you to associate the internal DNS records of the VMs with a custom DNS zone. You want to follow Google recommended practices. What should you do? a). 1. Provision the VMs with custom hostnames. b). 1. Create a new Cloud DNS zone and set its visibility to private. 2. When provisioning the VMs, associate the DNS records with the new DNS zone. c). 1. Install a new BIND DNS server on Google Compute Engine, using the BIND name server software (BIND9). 2. Configure a Cloud DNS forwarding zone to direct all requests to the Internal BIND DNS server. 3. When provisioning the VMs, associate the DNS records with the Internal BIND DNS server. d). 1. Create a new Cloud DNS zone and a new VPC and associate the DNS zone with the VPC. 2. When provisioning the VMs, associate the DNS records with the new DNS zone. 3. Configure firewall rules to block all external (public) traffic. 4. Finally, configure the DNS zone associated with the default VPC to direct all requests to the new DNS zone. Correct: b Explanation: Our requirements here are Internal and Custom Zone. You should do when you want internal DNS records in a custom zone. Cloud DNS gives you the option of private zones and internal DNS names. Reference: https://cloud.google.com/dns/docs/overview#concepts An application that you are migrating to Google Cloud relies on overnight batch jobs that take between 2 to 3 hours to complete. You want to do this at a minimal cost. Where should you run these batch jobs? a). Run the batch jobs in a GKE cluster on a node pool with four instances of type f1-micro. b). Run the batch jobs in a GKE cluster on a node pool with a single instance of type e2-small. c). Run the batch jobs in a preemptible compute engine instance of appropriate machine type. d). Run the batch jobs in a non-preemptible shared core compute engine instance that supports short periods of bursting. Correct: c Explanation: Requirements - achieve end goal while minimizing service costs. We minimize the cost by selecting a preemptible instance of the appropriate type. If the preemptible instance is terminated, the next nightly run picks up the unprocessed volume. Your company produces documentary videos for a reputed television channel and stores its videos in Google Cloud Storage for long term archival. Videos older than 90 days are accessed only in exceptional circumstances and videos older than one year are no longer needed. How should you optimise the storage to reduce costs? a). Configure a lifecycle rule to transition objects older than 90 days to Coldline Storage Class. Configure an other lifecycle rule to delete objects older than 365 days from Coldline Storage Class. b). Use a Cloud Function to rewrite the storage class to Coldline for objects older than 90 days. Use another Cloud Function to delete objects older than 275 days from Coldline Storage Class. c). Use a Cloud Function to rewrite the storage class to Coldline for objects older than 90 days. Use another Cloud Function to delete objects older than 365 days from Coldline Storage Class. d). Configure a lifecycle rule to transition objects older than 90 days to Coldline Storage Class. Configure another lifecycle rule to delete objects older than 275 days from Coldline Storage Class. Correct: a Explanation: Object Lifecycle Management does not rewrite an object when changing its storage class. When an object is transitioned to Nearline Storage, Coldline Storage, or Archive Storage using the SetStorageClass feature, any subsequent early deletion and associated charges are based on the original creation time of the object, regardless of when the storage class changed. Reference: https://cloud.google.com/storage/docs/lifecycle Your company is migrating an application from its on-premises data centre to Google Cloud. One of the applications uses a custom Linux distribution that is not available on Google Cloud. Your solution architect has suggested using VMWare tools to exporting the image and store it in a Cloud Storage bucket. The VM Image is a single compressed 64 GB tar file. You started copying this file using gsutil over a dedicated 1Gbps network, but the transfer is taking a very long time to complete. Your solution architect has suggested using all of the 1Gbps Network to transfer the file quickly. What should you do? a). Increase the transfer speed by decreasing the TCP window size. b). Restart the transfer from GCP console. c). Upload the file Multi-Regional instead and move the file to Nearline Storage Class. d). Use parallel composite uploads to speed up the transfer. Correct: d Explanation: With cloud storage, Object composition can be used for uploading an object in parallel: you can divide your data into multiple chunks, upload each chunk to a distinct object in parallel, compose your final object, and delete any temporary source objects. This option helps maximize your bandwidth usage and ensures the file is uploaded as fast as possible. Reference: https://cloud.google.com/storage/docs/composite-objects#uploads Your team is responsible for the migration of all legacy on-premises applications to Google Cloud. Your team is a big admirer of serverless and has chosen App Engine Standard as the preferred choice for compute workloads. Your manager asked you to migrate a legacy accounting application built in C++, but you realized App Engine Standard doesn\u2019t support C++. What GCP compute services should you use instead to maintain the serverless aspect? (Choose two answers) a). Deploy the containerized version of the application in Cloud Run. b). Convert the application into a set of functions and deploy them in Google Cloud Functions. c). Deploy the containerized version of the application in Google Kubernetes Engine (GKE). d). Deploy the containerized version of the application in Cloud Run on GKE. e). Deploy the containerized version of the application in App Engine Flex. Correct: a, d Explanation: App engine standard currently supports Python, Java, Node.js, PHP, Ruby and Go. The question already states App Engine doesn't support C#. We are required to ensure we maintain the serverless aspect of our application. Cloud Run is a fully managed compute platform that automatically scales your stateless containers. Cloud Run is serverless: it abstracts away all infrastructure management, so you can focus on what matters most\u2014building great applications. Run your containers in fully managed Cloud Run or on Anthos, which supports both Google Cloud and on\u2010premises environments. Cloud Run is built upon an open standard, Knative, enabling the portability of your applications. Cloud Run implements the Knative serving API, an open-source project to run serverless workloads on top of Kubernetes. That means you can deploy Cloud Run services anywhere Kubernetes runs. And suppose you need more control over your services (like access to GPU or more memory). In that case, you can also deploy these serverless containers in your GKE cluster instead of using the fully managed environment. When using the fully managed environment, Cloud Run on GKE is serverless. Reference: https://cloud.google.com/appengine/docs/standard/ Reference: https://cloud.google.com/run Reference: https://github.com/knative/serving/blob/master/docs/spec/spec.md Reference: https://cloud.google.com/blog/products/serverless/cloud-run-bringing-serverless-tocontainers You've deployed a microservice that uses sha1 algorithm with a salt value to has usernames. You deployed this to GKE cluster using deployment file: apiVersion: apps/v1 kind: Deployment metadata: name: sha1_hash_app-deployment spec: selector: matchLabels: app: sha1_hash_app replicas: 3 template: metadata: labels: app: sha1_hash_app spec: containers: - name: hash-me image: gcr.io/hash-repo/sha1_hash_app:2.17 env: - name: SALT_VALUE value: \"z0rtkty12$!\" ports: - containerPort: 8080 You need to make changes to prevent the salt value from being stored in plain text. You want to follow Google-recommended practices. What should you do? a). Save the salt value in a Kubernetes ConfigMap object. Modify the YAML configuration file to reference the ConfigMap object. b). Bake the salt value into the container image. c). Save the salt value in a Kubernetes Persistent Volume. Modify the YAML configuration file to include a Persistent Volume Claim to mount the volume and reference the password from the file. d). Save the salt value in a Kubernetes secret object. Modify the YAML configuration file to reference the secret object. Correct: d Explanation: In GKE, you can create a secret to hold the password; and then use the secret as an environment variable in the YAML file. You can create a secret using: kubectl create secret generic passwords --from-literal sha1_hash_app_SALT_VALUE=z0rtkty12$! And you can then modify the YAML file to reference this secret as shown below. apiVersion: apps/v1 kind: Deployment metadata: name: sha1_hash_app-deployment spec: selector: matchLabels: app: sha1_hash_app replicas: 3 template: metadata: labels: app: sha1_hash_app spec: containers: - name: hash-me image: gcr.io/hash-repo/sha1_hash_app:2.17 env: - name: SALT_VALUE valueFrom: secretKeyRef: name: passwords key: sha1_hash_app_SALT_VALUE ports: - containerPort: 8080 Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/secret You developed an enhancement to a production application deployed in App Engine Standard service. Unit testing and user acceptance testing has succeeded, and you deployed the new version to production. Users have started complaining of slow performance after the recent update, and you need to revert to the previous version immediately. How can you do this? a). Deploy the previous version as a new App Engine Application and use traffic splitting feature to send all traffic to the new application. b). In the App Engine Console, identify the App Engine application versions and make the previous version the default to route all traffic to it. c). In the App Engine Console, identify the App Engine application and select Revert. d). Execute gcloud app restore to rollback to the previous version. Correct: b Explanation: You can roll back to a previous version in the app engine GCP console. Go back to the list of versions and check the box next to the version that you want to receive all traffic and click the MAKE DEFAULT button located above the list. Traffic immediately switches over to the selected version. Reference: https://cloud.google.com/community/tutorials/how-to-roll-your-app-engine-managed-vms-app-back-to-a-previous-version-part-1 You want to migrate a public NodeJS application, which serves requests over HTTPS, from your on-premises data centre to Google Cloud Platform. You plan to host it on a fleet of instances behind Managed Instances Group (MIG) in Google Compute Engine. You need to configure a GCP load balancer to terminate SSL session before passing traffic to the VMs. Which GCP Load balancer should you use? a). Use HTTP(S) load balancer. b). Use External TCP proxy load balancer. c). Use Internal TCP load balancer. d). Use External SSL proxy load balancer. Correct: a Explanation: This option fits all requirements. It can serve public traffic, can terminate SSL at the load balancer and follows google recommended practices. \"The backends of a backend service can be either instance groups or network endpoint groups (NEGs), but not a combination of both.\" \"An external HTTP(S) load balancer distributes traffic from the internet.\" \"The client SSL session terminates at the load balancer.\" \"For HTTP traffic, use HTTP Load Balancing instead.\" Reference: https://cloud.google.com/load-balancing/docs/https The application development team at your company wants to use the biggest CIDR range possible for a VPC and has asked for your suggestion. Your operations team is averse to using any beta features. What should you suggest? a). Use 10.0.0.0/8 CIDR range. b). Use 0.0.0.0/0 CIDR range. c). Use 172.16.0.0/12 CIDR range. d). Use 192.168.0.0/16 CIDR range. Correct: a Explanation: The private network range is defined by IETF (Ref: https://tools.ietf.org/html/rfc1918) and adhered to by all cloud providers. The supported internal IP Address ranges are 24-bit block 10.0.0.0/8 (16777216 IP Addresses) 20-bit block 172.16.0.0/12 (1048576 IP Addresses) 16-bit block 192.168.0.0/16 (65536 IP Addresses) 10.0.0.0/8 gives you the most extensive range - 16777216 IP Addresses. Your company is migrating a mission-critical application from the on-premises data centre to Google Cloud Platform. The application requires 12 Compute Engine VMs to handle traffic at peak usage times. Your operations team have asked you to ensure the VMs restart automatically (i.e. without manual intervention) if/when they crash, and the processing capacity of the application does not reduce down during system maintenance. What should you do? a). Deploy the application on a Managed Instance Group (MIG) that disables the creation retry mode by setting the --nocreation-retries flag. b). Create an instance template with availability policy that turns off the automatic restart behaviour and sets on-host maintenance to terminate instances during maintenance events. Deploy the application on a Managed Instance Group (MIG) based on this template. c). Create an instance template with availability policy that turns on the automatic restart behaviour and sets on-host maintenance to live migrate instances during maintenance events. Deploy the application on a Managed Instance Group (MIG) based on this template. d). Deploy the application on a Managed Instance Group (MIG) with autohealing health check set to healthy (HTTP). Correct: c Explanation: Requirements 12 instances - indicates we need to look for MIG (Managed Instances Group) where we can configure healing/scaling settings. Highly available during system maintenance - indicates we need to look for Live Migration. Automatically restart on crash - indicates we need to look for options that enable automatic restarts. Enabling automatic restart ensures that compute engine instances are automatically restarted when they crash. And Enabling \"Migrate VM Instance\" enables live migrates, i.e. compute instances are migrated during system maintenance and remain running during the migration. Automatic Restart - If your instance is set to terminate when there is a maintenance event, or if your instance crashes because of an underlying hardware issue, you can set up Compute Engine to automatically restart the instance by setting the automaticRestart field to true. This setting does not apply if the instance is taken offline through a user action, such as calling sudo shutdown, or during a zone outage. Enabling the Migrate VM Instance option migrates your instance away from an infrastructure maintenance event, and your instance remains running during the migration. Your instance might experience a short period of decreased performance, although generally, most instances should not notice any difference. Live migration is ideal for instances that require constant uptime and can tolerate a short period of decreased performance. Reference: https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options#autorestart Reference: https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options#live_migrate A recent reorganization in your company has seen the creation of a new data custodian team \u2013 responsible for managing data in all storage locations. Your production GCP project uses buckets in Cloud Storage, and you need to delegate control to the new team to manage objects and buckets in your GCP project. What role should you grant them? a). Grant the data custodian team Storage Object Admin IAM role. b). Grant the data custodian team Storage Object Creator IAM role. c). Grant the data custodian team Storage Admin IAM role. d). Grant the data custodian team Project Editor IAM role. Correct: c Explanation: This role grants full control of buckets and objects. When applied to an individual bucket, control applies only to the specified bucket and objects within the bucket. Reference: https://cloud.google.com/iam/docs/understanding-roles#storage-roles You are in the process of migrating a mission-critical application from your on-premises data centre to Google Kubernetes Engine (GKE). Your operations team do not want to take on the overhead for upgrading the GKE cluster and have asked you to ensure the Kubernetes version is always stable and supported. What should you do? a). When provisioning the GKE cluster, ensure you use the latest stable and supported version. b). Update your GKE cluster to turn on GKE\u2019s node auto-upgrade feature. c). Update your GKE cluster to turn on GKE\u2019s node auto-repair feature. d). When provisioning the GKE cluster, use Container Optimized OS node images. Correct: b Explanation: Node auto-upgrades help you keep the nodes in your cluster up to date with the cluster master version when your master is updated on your behalf. When you create a new cluster or node pool with Google Cloud Console or the gcloud command, node auto-upgrade is enabled by default. Reference: https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-upgrades Your company stores sensitive user information (PII) in three multi-regional buckets in US, Europe and Asia. All three buckets have data access logging enabled on them. The compliance team has received reports of fraudulent activity and has begun investigating a customer care representative. It believes the specific individual may have accessed some objects they are not authorized to and may have added labels to some files in the buckets to enable favourable discounts for their friends. The compliance team has asked you to provide them with a report of activities for this customer service representative on all three buckets. How can you do this efficiently? a). Enable a Cloud Trace on the bucket and wait for the user to access objects/set metadata to capture their activities. b). Retrieve this information from the Cloud Storage bucket page in GCP Console. c). Apply the necessary filters in Cloud Logging Console to retrieve this information. d). Retrieve this information from Activity logs in GCP Console. Correct: c Explanation: Our requirements are - sensitive data, verify access, fewest possible steps. Data access logs are already enabled, so we already record all API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data. Data Access audit logs do not record the data-access operations on resources that are publicly shared (available to All Users or All Authenticated Users), or that can be accessed without logging into Google Cloud. Since we are dealing with sensitive data, it is safe to assume that these buckets are not publicly shared and therefore enabling Data access logging logs all data-access operations on resources. These logs are sent to Stackdriver where they can be viewed by applying a suitable filter. Unlike activity logs, retrieving the required information to verify is quicker through Stackdriver as you can apply filters such as: resource.type=\"gcs_bucket\" (resource.labels.bucket_name=\"gcp-ace-lab-255520\" OR resource.labels.bucket_name=\"gcp-ace-lab-255521\" OR resource.labels.bucket_name=\"gcp-ace-lab-255522\") (protoPayload.methodName=\"storage.objects.get\" OR protoPayload.methodName=\"storage.objects.update\") protoPayload.authenticationInfo.principalEmail=\"XXXXXXXXXXX@gXXXX.com\" and query just the gets and updates, for specific buckets for a specific user. This option involves fewer steps and is more efficient. Data access logging is not enabled by default and needs to be enabled explicitly. Your finance department wants you to create a new billing account and link all development and test Google Cloud Projects to the new billing account. What should you do? a). Ask your security administrator to grant you the Billing Account Administrator role on the existing Billing Account. Link all development and test projects to the existing Billing Account. b). Ask your security administrator to grant you the Billing Account Creator role on the GCP organization and Project Billing Manager role on all the development and test projects. Link all the development and test projects to an existing Billing Account. c). Ask your security administrator to grant you the Billing Account Creator role on the GCP organization and Project Billing Manager role on all the development and test projects. Create a new Billing Account and link all the development and test projects to the new Billing Account. d). Ask your security administrator to grant you the Billing Account Administrator role on the existing Billing Account. Create new development and test projects and link them to the existing Billing Account. Correct: c Explanation: We are required to link an existing google cloud project with a new billing account. The purpose of the Project Billing Manager is to Link/unlink the project to/from a billing account. It is granted at the organization or project level. Project Billing Manager role allows a user to attach the project to the billing account, but does not grant any rights over resources. Project Owners can use this role to allow someone else to manage the billing for the project without granting them resource access. Billing Account Creator - Use this role for initial billing setup or to allow the creation of additional billing accounts. Reference: https://cloud.google.com/billing/docs/how-to/billing-access You are migrating your on-premises workloads to GCP VPC, and you want to use Compute Engine virtual machines. You want to separate the Finance team VMs and the Procurement team VMs into separate subnets. You need all VMs to communicate with each other over their internal IP addresses without adding routes. What should you do? a). Use Deployment Manager to create two VPCs, each with a subnet in the same region. Ensure the subnets use overlapping IP range. b). Use Deployment Manager to create a new VPC with 2 subnets in the same region. Ensure the subnets use the same IP range. c). Use Deployment Manager to create two VPCs, each with a subnet in a different region. Ensure the subnets use non-overlapping IP range. d). Use Deployment Manager to create a new VPC with 2 subnets in 2 different regions. Ensure the subnets use non-overlapping IP range. Correct: d Explanation: When we create subnets in the same VPC with different CIDR ranges, they can communicate automatically within VPC. \"Resources within a VPC network can communicate with one another by using internal (private) IPv4 addresses, subject to applicable network firewall rules.\" Reference: https://cloud.google.com/vpc/docs/vpc You deployed a Java application on four Google Cloud Compute Engine VMs in two zones behind a network load balancer. During peak usage, the application has stuck threads. This issue ultimately takes down the whole system and requires a reboot of all VMs. Your operations team have recently heard about self-healing mechanisms in Google Cloud and have asked you to identify if it is possible to automatically recreate the VMs if they remain unresponsive for 3 attempts 10 seconds apart. What should you do? a). Use a global HTTP(s) Load Balancer instead and set the load balancer health check to healthy (HTTP). b). Enable autohealing and set the autohealing health check to healthy (HTTP). c). Use a global HTTP(s) Load Balancer instead and limit Requests Per Second (RPS) to 10. d). Enable autoscaling on the Managed Instance Group (MIG). Correct: b Explanation: To enable auto-healing, you need to group the instances into a managed instance group. Managed instance groups (MIGs) maintain the high availability of your applications by proactively keeping your virtual machine (VM) instances available. An auto-healing policy on the MIG relies on an application-based health check to verify that an application is responding as expected. If the auto-healer determines that an application isn't responding, the managed instance group automatically recreates that instance. It is essential to use separate health checks for load balancing and auto-healing. Health checks for load balancing can and should be more aggressive because these health checks determine whether an instance receives user traffic. You want to catch non-responsive instances quickly, so you can redirect traffic if necessary. In contrast, health checking for auto-healing causes Compute Engine to replace failing instances proactively, so this health check should be more conservative than a load balancing health check. You want to deploy a cost-sensitive application to Google Cloud Compute Engine. You want the application to be up at all times, but because of the cost-sensitive nature of the application, you only want to run the application in a single VM instance. How should you configure the managed instance group? a). Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum instances to 2. b). Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum instances to 1. c). Disable autoscaling on the Managed Instance Group (MIG) and set mininum instances to 1 and maximum instances to 2. d). Disable autoscaling on the Managed Instance Group (MIG) and set mininum instances to 1 and maximum instances to 1. Correct: b Explanation: Requirements: 1. Since we need the application running at all times, we need a minimum 1 instance. 2. Only a single instance of the VM should run, we need a maximum 1 instance. We want the application running at all times. If the VM crashes due to any underlying hardware failure, we want another instance to be added to MIG so that application can continue to serve requests. We can achieve this by enabling autoscaling. The only option that satisfies these three is Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum instances to 1. Reference: https://cloud.google.com/compute/docs/autoscaler You are developing a simple application in App Engine Standard service. Unit testing and user acceptance testing has succeeded, and you want to build a new App Engine application to serve as your performance testing environment. What should you do? a). Configure a Deployment Manager YAML template to copy the application from the development GCP project into the performance testing GCP project. b). Create a new GCP project for the performance testing environment using gcloud and copy the application from the development GCP project into the performance testing GCP project. c). Create a new GCP project for the performance testing environment using gcloud and deploy your App Engine application to the new GCP project. d). Use gcloud to deploy the application to a new performance testing GCP project by specifying the --project parameter. Select Yes when prompted for confirmation on creating a new project. Correct: c Explanation: You can deploy to a different project by using --project flag. By default, the service is deployed the current project configured via: $ gcloud config set core/project PROJECT To override this value for a single deployment, use the --project flag: $ gcloud app deploy ~/my_app/app.yaml --project=PROJECT Reference: https://cloud.google.com/sdk/gcloud/reference/app/deploy Reference: Reference: Reference: Reference: Reference: Reference: Reference: Reference: Reference: Reference: Reference: Reference: (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"GCP ACE Practice Questions-3"},{"location":"nightwolf-cotribution/gcp-ace-3/#google-cloud-associate-cloud-engineer-practice-exam-3","text":"These are top 50 GCP ACE(Google Cloud Platform Associate Cloud Engineer) certification practice questions/cheatsheet (GCP ACE exam dumps) for professionals who are aspired to be GCP ACE certified. You will find these GCP ACE questions and answers very helpful in your GCP-ACE certification and interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); You deployed a Python application to GCP App Engine Standard service in the us-central region. Most of your customers are based in Japan and are experiencing slowness due to the latency. You want to transfer the application from us-central region to asia-northeast1 region to minimize latency. What should you do? a). Create a new GCP project. Create a new App Engine Application in the new GCP project and set its region to asia-northeast-1. Delete the old App Engine application. b). Update the region property to asia-northeast1 on the App Engine application. c). Update the default region property to asia-northeast1 on the App Engine Service. d). Deploy a new app engine application in the same GCP project and set the region to asianortheast1. Delete the old App Engine application. Correct: a Explanation: App Engine is regional, and you cannot change an app's region after you set it. You can deploy additional services in the App Engine, but they will all be targeted to the same region. Reference: https://cloud.google.com/appengine/docs/locations You deployed a java application in a single Google Cloud Compute Engine VM. During peak usage, the application CPU is maxed out and results in stuck threads which ultimately make the system unresponsive, and requires a reboot. Your operations team want to receive an email alert when the CPU utilization is greater than 95% for more than 10 minutes so they can manually change the instance type to another instance that offers more CPU. What should you do? a). In Cloud Logging, create logs based metric for CPU usage and store it as a custom metric in Cloud Monitoring. Create an Alerting policy based on CPU utilization in Cloud Monitoring and trigger an email notification when the utilization exceeds the threshold. b). Link the GCP project to a Cloud Monitoring workspace. Configure an Alerting policy based on CPU utilization in Cloud Monitoring and trigger an email notification when the utilization exceeds the threshold. c). Write a custom script to monitor CPU usage and send an email notification when the usage exceeds the threshold. d). Link the project to a Cloud Monitoring workspace. Write a custom script that captures CPU utilization every minute and sends to Cloud Monitoring as a custom metric. Add an uptime check based on the CPU utilization Correct: b Explanation: A Workspace is a tool for monitoring resources contained in one or more Google Cloud projects or AWS accounts. In our case, we create a Stackdriver workspace and link our project to this workspace. Reference: https://cloud.google.com/monitoring/workspaces Your company installs and manages several types of IoT devices all over the world. Events range from 50,000 to 500,000 messages a second. You want to identify the best solution for ingesting, transforming, storing and analyzing this data in GCP platform. What GCP services should you use? a). Cloud Pub/Sub for ingesting, Cloud Dataflow for transforming, Cloud Datastore for storing and BigQuery for analyzing the time-series data. b). Firebase Messages for ingesting, Cloud Pub/Sub for transforming, Cloud Spanner for storing and BigQuery for analyzing the time-series data. c). Cloud Pub/Sub for ingesting, Cloud Dataflow for transforming, Cloud Bigtable for storing and BigQuery for analyzing the time-series data. d). Cloud Pub/Sub for ingesting, Cloud Storage for transforming, BigQuery for storing and Cloud Bigtable for analyzing the time-series data. Correct: c Explanation: For ingesting time series data, your best bet is Cloud Pub/Sub. For processing the data in pipelines, your best bet is Cloud Dataflow. That leaves us with two remaining options; both have BigQuery for analyzing the data. For storage, it is a choice between Bigtable and Datastore. Bigtable provides out of the box support for time series data. So using Bigtable for Storage is the right answer. Reference: https://cloud.google.com/bigtable/docs/schema-design-time-series You migrated an internal HR system from an on-premises database to Google Cloud Compute Engine Managed Instance Group (MIG). The networks team at your company has asked you to associate the internal DNS records of the VMs with a custom DNS zone. You want to follow Google recommended practices. What should you do? a). 1. Provision the VMs with custom hostnames. b). 1. Create a new Cloud DNS zone and set its visibility to private. 2. When provisioning the VMs, associate the DNS records with the new DNS zone. c). 1. Install a new BIND DNS server on Google Compute Engine, using the BIND name server software (BIND9). 2. Configure a Cloud DNS forwarding zone to direct all requests to the Internal BIND DNS server. 3. When provisioning the VMs, associate the DNS records with the Internal BIND DNS server. d). 1. Create a new Cloud DNS zone and a new VPC and associate the DNS zone with the VPC. 2. When provisioning the VMs, associate the DNS records with the new DNS zone. 3. Configure firewall rules to block all external (public) traffic. 4. Finally, configure the DNS zone associated with the default VPC to direct all requests to the new DNS zone. Correct: b Explanation: Our requirements here are Internal and Custom Zone. You should do when you want internal DNS records in a custom zone. Cloud DNS gives you the option of private zones and internal DNS names. Reference: https://cloud.google.com/dns/docs/overview#concepts An application that you are migrating to Google Cloud relies on overnight batch jobs that take between 2 to 3 hours to complete. You want to do this at a minimal cost. Where should you run these batch jobs? a). Run the batch jobs in a GKE cluster on a node pool with four instances of type f1-micro. b). Run the batch jobs in a GKE cluster on a node pool with a single instance of type e2-small. c). Run the batch jobs in a preemptible compute engine instance of appropriate machine type. d). Run the batch jobs in a non-preemptible shared core compute engine instance that supports short periods of bursting. Correct: c Explanation: Requirements - achieve end goal while minimizing service costs. We minimize the cost by selecting a preemptible instance of the appropriate type. If the preemptible instance is terminated, the next nightly run picks up the unprocessed volume. Your company produces documentary videos for a reputed television channel and stores its videos in Google Cloud Storage for long term archival. Videos older than 90 days are accessed only in exceptional circumstances and videos older than one year are no longer needed. How should you optimise the storage to reduce costs? a). Configure a lifecycle rule to transition objects older than 90 days to Coldline Storage Class. Configure an other lifecycle rule to delete objects older than 365 days from Coldline Storage Class. b). Use a Cloud Function to rewrite the storage class to Coldline for objects older than 90 days. Use another Cloud Function to delete objects older than 275 days from Coldline Storage Class. c). Use a Cloud Function to rewrite the storage class to Coldline for objects older than 90 days. Use another Cloud Function to delete objects older than 365 days from Coldline Storage Class. d). Configure a lifecycle rule to transition objects older than 90 days to Coldline Storage Class. Configure another lifecycle rule to delete objects older than 275 days from Coldline Storage Class. Correct: a Explanation: Object Lifecycle Management does not rewrite an object when changing its storage class. When an object is transitioned to Nearline Storage, Coldline Storage, or Archive Storage using the SetStorageClass feature, any subsequent early deletion and associated charges are based on the original creation time of the object, regardless of when the storage class changed. Reference: https://cloud.google.com/storage/docs/lifecycle Your company is migrating an application from its on-premises data centre to Google Cloud. One of the applications uses a custom Linux distribution that is not available on Google Cloud. Your solution architect has suggested using VMWare tools to exporting the image and store it in a Cloud Storage bucket. The VM Image is a single compressed 64 GB tar file. You started copying this file using gsutil over a dedicated 1Gbps network, but the transfer is taking a very long time to complete. Your solution architect has suggested using all of the 1Gbps Network to transfer the file quickly. What should you do? a). Increase the transfer speed by decreasing the TCP window size. b). Restart the transfer from GCP console. c). Upload the file Multi-Regional instead and move the file to Nearline Storage Class. d). Use parallel composite uploads to speed up the transfer. Correct: d Explanation: With cloud storage, Object composition can be used for uploading an object in parallel: you can divide your data into multiple chunks, upload each chunk to a distinct object in parallel, compose your final object, and delete any temporary source objects. This option helps maximize your bandwidth usage and ensures the file is uploaded as fast as possible. Reference: https://cloud.google.com/storage/docs/composite-objects#uploads Your team is responsible for the migration of all legacy on-premises applications to Google Cloud. Your team is a big admirer of serverless and has chosen App Engine Standard as the preferred choice for compute workloads. Your manager asked you to migrate a legacy accounting application built in C++, but you realized App Engine Standard doesn\u2019t support C++. What GCP compute services should you use instead to maintain the serverless aspect? (Choose two answers) a). Deploy the containerized version of the application in Cloud Run. b). Convert the application into a set of functions and deploy them in Google Cloud Functions. c). Deploy the containerized version of the application in Google Kubernetes Engine (GKE). d). Deploy the containerized version of the application in Cloud Run on GKE. e). Deploy the containerized version of the application in App Engine Flex. Correct: a, d Explanation: App engine standard currently supports Python, Java, Node.js, PHP, Ruby and Go. The question already states App Engine doesn't support C#. We are required to ensure we maintain the serverless aspect of our application. Cloud Run is a fully managed compute platform that automatically scales your stateless containers. Cloud Run is serverless: it abstracts away all infrastructure management, so you can focus on what matters most\u2014building great applications. Run your containers in fully managed Cloud Run or on Anthos, which supports both Google Cloud and on\u2010premises environments. Cloud Run is built upon an open standard, Knative, enabling the portability of your applications. Cloud Run implements the Knative serving API, an open-source project to run serverless workloads on top of Kubernetes. That means you can deploy Cloud Run services anywhere Kubernetes runs. And suppose you need more control over your services (like access to GPU or more memory). In that case, you can also deploy these serverless containers in your GKE cluster instead of using the fully managed environment. When using the fully managed environment, Cloud Run on GKE is serverless. Reference: https://cloud.google.com/appengine/docs/standard/ Reference: https://cloud.google.com/run Reference: https://github.com/knative/serving/blob/master/docs/spec/spec.md Reference: https://cloud.google.com/blog/products/serverless/cloud-run-bringing-serverless-tocontainers You've deployed a microservice that uses sha1 algorithm with a salt value to has usernames. You deployed this to GKE cluster using deployment file: apiVersion: apps/v1 kind: Deployment metadata: name: sha1_hash_app-deployment spec: selector: matchLabels: app: sha1_hash_app replicas: 3 template: metadata: labels: app: sha1_hash_app spec: containers: - name: hash-me image: gcr.io/hash-repo/sha1_hash_app:2.17 env: - name: SALT_VALUE value: \"z0rtkty12$!\" ports: - containerPort: 8080 You need to make changes to prevent the salt value from being stored in plain text. You want to follow Google-recommended practices. What should you do? a). Save the salt value in a Kubernetes ConfigMap object. Modify the YAML configuration file to reference the ConfigMap object. b). Bake the salt value into the container image. c). Save the salt value in a Kubernetes Persistent Volume. Modify the YAML configuration file to include a Persistent Volume Claim to mount the volume and reference the password from the file. d). Save the salt value in a Kubernetes secret object. Modify the YAML configuration file to reference the secret object. Correct: d Explanation: In GKE, you can create a secret to hold the password; and then use the secret as an environment variable in the YAML file. You can create a secret using: kubectl create secret generic passwords --from-literal sha1_hash_app_SALT_VALUE=z0rtkty12$! And you can then modify the YAML file to reference this secret as shown below. apiVersion: apps/v1 kind: Deployment metadata: name: sha1_hash_app-deployment spec: selector: matchLabels: app: sha1_hash_app replicas: 3 template: metadata: labels: app: sha1_hash_app spec: containers: - name: hash-me image: gcr.io/hash-repo/sha1_hash_app:2.17 env: - name: SALT_VALUE valueFrom: secretKeyRef: name: passwords key: sha1_hash_app_SALT_VALUE ports: - containerPort: 8080 Reference: https://cloud.google.com/kubernetes-engine/docs/concepts/secret You developed an enhancement to a production application deployed in App Engine Standard service. Unit testing and user acceptance testing has succeeded, and you deployed the new version to production. Users have started complaining of slow performance after the recent update, and you need to revert to the previous version immediately. How can you do this? a). Deploy the previous version as a new App Engine Application and use traffic splitting feature to send all traffic to the new application. b). In the App Engine Console, identify the App Engine application versions and make the previous version the default to route all traffic to it. c). In the App Engine Console, identify the App Engine application and select Revert. d). Execute gcloud app restore to rollback to the previous version. Correct: b Explanation: You can roll back to a previous version in the app engine GCP console. Go back to the list of versions and check the box next to the version that you want to receive all traffic and click the MAKE DEFAULT button located above the list. Traffic immediately switches over to the selected version. Reference: https://cloud.google.com/community/tutorials/how-to-roll-your-app-engine-managed-vms-app-back-to-a-previous-version-part-1 You want to migrate a public NodeJS application, which serves requests over HTTPS, from your on-premises data centre to Google Cloud Platform. You plan to host it on a fleet of instances behind Managed Instances Group (MIG) in Google Compute Engine. You need to configure a GCP load balancer to terminate SSL session before passing traffic to the VMs. Which GCP Load balancer should you use? a). Use HTTP(S) load balancer. b). Use External TCP proxy load balancer. c). Use Internal TCP load balancer. d). Use External SSL proxy load balancer. Correct: a Explanation: This option fits all requirements. It can serve public traffic, can terminate SSL at the load balancer and follows google recommended practices. \"The backends of a backend service can be either instance groups or network endpoint groups (NEGs), but not a combination of both.\" \"An external HTTP(S) load balancer distributes traffic from the internet.\" \"The client SSL session terminates at the load balancer.\" \"For HTTP traffic, use HTTP Load Balancing instead.\" Reference: https://cloud.google.com/load-balancing/docs/https The application development team at your company wants to use the biggest CIDR range possible for a VPC and has asked for your suggestion. Your operations team is averse to using any beta features. What should you suggest? a). Use 10.0.0.0/8 CIDR range. b). Use 0.0.0.0/0 CIDR range. c). Use 172.16.0.0/12 CIDR range. d). Use 192.168.0.0/16 CIDR range. Correct: a Explanation: The private network range is defined by IETF (Ref: https://tools.ietf.org/html/rfc1918) and adhered to by all cloud providers. The supported internal IP Address ranges are 24-bit block 10.0.0.0/8 (16777216 IP Addresses) 20-bit block 172.16.0.0/12 (1048576 IP Addresses) 16-bit block 192.168.0.0/16 (65536 IP Addresses) 10.0.0.0/8 gives you the most extensive range - 16777216 IP Addresses. Your company is migrating a mission-critical application from the on-premises data centre to Google Cloud Platform. The application requires 12 Compute Engine VMs to handle traffic at peak usage times. Your operations team have asked you to ensure the VMs restart automatically (i.e. without manual intervention) if/when they crash, and the processing capacity of the application does not reduce down during system maintenance. What should you do? a). Deploy the application on a Managed Instance Group (MIG) that disables the creation retry mode by setting the --nocreation-retries flag. b). Create an instance template with availability policy that turns off the automatic restart behaviour and sets on-host maintenance to terminate instances during maintenance events. Deploy the application on a Managed Instance Group (MIG) based on this template. c). Create an instance template with availability policy that turns on the automatic restart behaviour and sets on-host maintenance to live migrate instances during maintenance events. Deploy the application on a Managed Instance Group (MIG) based on this template. d). Deploy the application on a Managed Instance Group (MIG) with autohealing health check set to healthy (HTTP). Correct: c Explanation: Requirements 12 instances - indicates we need to look for MIG (Managed Instances Group) where we can configure healing/scaling settings. Highly available during system maintenance - indicates we need to look for Live Migration. Automatically restart on crash - indicates we need to look for options that enable automatic restarts. Enabling automatic restart ensures that compute engine instances are automatically restarted when they crash. And Enabling \"Migrate VM Instance\" enables live migrates, i.e. compute instances are migrated during system maintenance and remain running during the migration. Automatic Restart - If your instance is set to terminate when there is a maintenance event, or if your instance crashes because of an underlying hardware issue, you can set up Compute Engine to automatically restart the instance by setting the automaticRestart field to true. This setting does not apply if the instance is taken offline through a user action, such as calling sudo shutdown, or during a zone outage. Enabling the Migrate VM Instance option migrates your instance away from an infrastructure maintenance event, and your instance remains running during the migration. Your instance might experience a short period of decreased performance, although generally, most instances should not notice any difference. Live migration is ideal for instances that require constant uptime and can tolerate a short period of decreased performance. Reference: https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options#autorestart Reference: https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options#live_migrate A recent reorganization in your company has seen the creation of a new data custodian team \u2013 responsible for managing data in all storage locations. Your production GCP project uses buckets in Cloud Storage, and you need to delegate control to the new team to manage objects and buckets in your GCP project. What role should you grant them? a). Grant the data custodian team Storage Object Admin IAM role. b). Grant the data custodian team Storage Object Creator IAM role. c). Grant the data custodian team Storage Admin IAM role. d). Grant the data custodian team Project Editor IAM role. Correct: c Explanation: This role grants full control of buckets and objects. When applied to an individual bucket, control applies only to the specified bucket and objects within the bucket. Reference: https://cloud.google.com/iam/docs/understanding-roles#storage-roles You are in the process of migrating a mission-critical application from your on-premises data centre to Google Kubernetes Engine (GKE). Your operations team do not want to take on the overhead for upgrading the GKE cluster and have asked you to ensure the Kubernetes version is always stable and supported. What should you do? a). When provisioning the GKE cluster, ensure you use the latest stable and supported version. b). Update your GKE cluster to turn on GKE\u2019s node auto-upgrade feature. c). Update your GKE cluster to turn on GKE\u2019s node auto-repair feature. d). When provisioning the GKE cluster, use Container Optimized OS node images. Correct: b Explanation: Node auto-upgrades help you keep the nodes in your cluster up to date with the cluster master version when your master is updated on your behalf. When you create a new cluster or node pool with Google Cloud Console or the gcloud command, node auto-upgrade is enabled by default. Reference: https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-upgrades Your company stores sensitive user information (PII) in three multi-regional buckets in US, Europe and Asia. All three buckets have data access logging enabled on them. The compliance team has received reports of fraudulent activity and has begun investigating a customer care representative. It believes the specific individual may have accessed some objects they are not authorized to and may have added labels to some files in the buckets to enable favourable discounts for their friends. The compliance team has asked you to provide them with a report of activities for this customer service representative on all three buckets. How can you do this efficiently? a). Enable a Cloud Trace on the bucket and wait for the user to access objects/set metadata to capture their activities. b). Retrieve this information from the Cloud Storage bucket page in GCP Console. c). Apply the necessary filters in Cloud Logging Console to retrieve this information. d). Retrieve this information from Activity logs in GCP Console. Correct: c Explanation: Our requirements are - sensitive data, verify access, fewest possible steps. Data access logs are already enabled, so we already record all API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data. Data Access audit logs do not record the data-access operations on resources that are publicly shared (available to All Users or All Authenticated Users), or that can be accessed without logging into Google Cloud. Since we are dealing with sensitive data, it is safe to assume that these buckets are not publicly shared and therefore enabling Data access logging logs all data-access operations on resources. These logs are sent to Stackdriver where they can be viewed by applying a suitable filter. Unlike activity logs, retrieving the required information to verify is quicker through Stackdriver as you can apply filters such as: resource.type=\"gcs_bucket\" (resource.labels.bucket_name=\"gcp-ace-lab-255520\" OR resource.labels.bucket_name=\"gcp-ace-lab-255521\" OR resource.labels.bucket_name=\"gcp-ace-lab-255522\") (protoPayload.methodName=\"storage.objects.get\" OR protoPayload.methodName=\"storage.objects.update\") protoPayload.authenticationInfo.principalEmail=\"XXXXXXXXXXX@gXXXX.com\" and query just the gets and updates, for specific buckets for a specific user. This option involves fewer steps and is more efficient. Data access logging is not enabled by default and needs to be enabled explicitly. Your finance department wants you to create a new billing account and link all development and test Google Cloud Projects to the new billing account. What should you do? a). Ask your security administrator to grant you the Billing Account Administrator role on the existing Billing Account. Link all development and test projects to the existing Billing Account. b). Ask your security administrator to grant you the Billing Account Creator role on the GCP organization and Project Billing Manager role on all the development and test projects. Link all the development and test projects to an existing Billing Account. c). Ask your security administrator to grant you the Billing Account Creator role on the GCP organization and Project Billing Manager role on all the development and test projects. Create a new Billing Account and link all the development and test projects to the new Billing Account. d). Ask your security administrator to grant you the Billing Account Administrator role on the existing Billing Account. Create new development and test projects and link them to the existing Billing Account. Correct: c Explanation: We are required to link an existing google cloud project with a new billing account. The purpose of the Project Billing Manager is to Link/unlink the project to/from a billing account. It is granted at the organization or project level. Project Billing Manager role allows a user to attach the project to the billing account, but does not grant any rights over resources. Project Owners can use this role to allow someone else to manage the billing for the project without granting them resource access. Billing Account Creator - Use this role for initial billing setup or to allow the creation of additional billing accounts. Reference: https://cloud.google.com/billing/docs/how-to/billing-access You are migrating your on-premises workloads to GCP VPC, and you want to use Compute Engine virtual machines. You want to separate the Finance team VMs and the Procurement team VMs into separate subnets. You need all VMs to communicate with each other over their internal IP addresses without adding routes. What should you do? a). Use Deployment Manager to create two VPCs, each with a subnet in the same region. Ensure the subnets use overlapping IP range. b). Use Deployment Manager to create a new VPC with 2 subnets in the same region. Ensure the subnets use the same IP range. c). Use Deployment Manager to create two VPCs, each with a subnet in a different region. Ensure the subnets use non-overlapping IP range. d). Use Deployment Manager to create a new VPC with 2 subnets in 2 different regions. Ensure the subnets use non-overlapping IP range. Correct: d Explanation: When we create subnets in the same VPC with different CIDR ranges, they can communicate automatically within VPC. \"Resources within a VPC network can communicate with one another by using internal (private) IPv4 addresses, subject to applicable network firewall rules.\" Reference: https://cloud.google.com/vpc/docs/vpc You deployed a Java application on four Google Cloud Compute Engine VMs in two zones behind a network load balancer. During peak usage, the application has stuck threads. This issue ultimately takes down the whole system and requires a reboot of all VMs. Your operations team have recently heard about self-healing mechanisms in Google Cloud and have asked you to identify if it is possible to automatically recreate the VMs if they remain unresponsive for 3 attempts 10 seconds apart. What should you do? a). Use a global HTTP(s) Load Balancer instead and set the load balancer health check to healthy (HTTP). b). Enable autohealing and set the autohealing health check to healthy (HTTP). c). Use a global HTTP(s) Load Balancer instead and limit Requests Per Second (RPS) to 10. d). Enable autoscaling on the Managed Instance Group (MIG). Correct: b Explanation: To enable auto-healing, you need to group the instances into a managed instance group. Managed instance groups (MIGs) maintain the high availability of your applications by proactively keeping your virtual machine (VM) instances available. An auto-healing policy on the MIG relies on an application-based health check to verify that an application is responding as expected. If the auto-healer determines that an application isn't responding, the managed instance group automatically recreates that instance. It is essential to use separate health checks for load balancing and auto-healing. Health checks for load balancing can and should be more aggressive because these health checks determine whether an instance receives user traffic. You want to catch non-responsive instances quickly, so you can redirect traffic if necessary. In contrast, health checking for auto-healing causes Compute Engine to replace failing instances proactively, so this health check should be more conservative than a load balancing health check. You want to deploy a cost-sensitive application to Google Cloud Compute Engine. You want the application to be up at all times, but because of the cost-sensitive nature of the application, you only want to run the application in a single VM instance. How should you configure the managed instance group? a). Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum instances to 2. b). Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum instances to 1. c). Disable autoscaling on the Managed Instance Group (MIG) and set mininum instances to 1 and maximum instances to 2. d). Disable autoscaling on the Managed Instance Group (MIG) and set mininum instances to 1 and maximum instances to 1. Correct: b Explanation: Requirements: 1. Since we need the application running at all times, we need a minimum 1 instance. 2. Only a single instance of the VM should run, we need a maximum 1 instance. We want the application running at all times. If the VM crashes due to any underlying hardware failure, we want another instance to be added to MIG so that application can continue to serve requests. We can achieve this by enabling autoscaling. The only option that satisfies these three is Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum instances to 1. Reference: https://cloud.google.com/compute/docs/autoscaler You are developing a simple application in App Engine Standard service. Unit testing and user acceptance testing has succeeded, and you want to build a new App Engine application to serve as your performance testing environment. What should you do? a). Configure a Deployment Manager YAML template to copy the application from the development GCP project into the performance testing GCP project. b). Create a new GCP project for the performance testing environment using gcloud and copy the application from the development GCP project into the performance testing GCP project. c). Create a new GCP project for the performance testing environment using gcloud and deploy your App Engine application to the new GCP project. d). Use gcloud to deploy the application to a new performance testing GCP project by specifying the --project parameter. Select Yes when prompted for confirmation on creating a new project. Correct: c Explanation: You can deploy to a different project by using --project flag. By default, the service is deployed the current project configured via: $ gcloud config set core/project PROJECT To override this value for a single deployment, use the --project flag: $ gcloud app deploy ~/my_app/app.yaml --project=PROJECT Reference: https://cloud.google.com/sdk/gcloud/reference/app/deploy Reference: Reference: Reference: Reference: Reference: Reference: Reference: Reference: Reference: Reference: Reference: Reference: (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Google Cloud Associate Cloud Engineer Practice Exam - 3:"},{"location":"nightwolf-cotribution/git/","text":"Top 50 GIT Interview Questions and Answers for DevOps Roles - Solved \uf0c1 We have consolidated a list of frequently asked GIT interview questions for Freshers and Experienced DevOps Engineer. You will find these questions very helpful in your interviews for DevOps Engineer. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Q1) What is a version control system (VCS)? Version control systems are software tools that help a team manage changes to source code/documents over time. Q2) Why are version control systems (VCS) necessary? They allow you to 1. Keep track of code changes. 2. Can help team memembers to synchronize the code to the latest version easily. 3. It helps teams to develop products faster. 4. Helps teams to collaborate with each other easily. 5. It acts as a backup for your code base. Q3) What are distributed version control systems (DVCS)? Distributed revision control synchronizes repositories by transferring patches from peer to peer. There is no single central version of the codebase instead, each user can download a working copy and full change history. Hence not requiring to be connected to the server all the time. Example : GIT Q4) What are centralized version control systems (CVCS)? It is a version control where there is a single central repository hosted on a server. This server is expected to have the latest code and expects all its clients to contribute by being connected to the server always. Q5) How does git maintain the data internally. Git mainly uses three different types of objects to hold information about a repository BLOB binary form of the actual data TREE It contains pointers to the objects COMMIT Commit object contains information about the author, date, ha Q6) Can git be used locally without using GIT server or any SAAS providers like git, bitbucket &etc Yes, if a person is willing to work on the project alone he can use git to maintain the state of the project. However the full potential of git is unutilized. Q7) How do you find a list of files impacted with a particular commit hash? git diff-tree -r {hash} Q8) What is a conflict in git. Is it necessary? A conflict in git arises when branches are merged with new commits which has changes on same file(s). In cases like this git cannot take precendence of changes hen Q9) What is the difference between git pull and git fetch Git fetch will download new commits from a remote repository to a local repository git pull does the same as git fetch and also merges the same into your local working files Q10) How are conflicts solved? The files in conflict must be edited and fixed. Then add the resolved files by running 1. git add . 2. git commit Q11) What is the command that defines the author email to be used for all the commits performed by the current user? # git config global user.email <email> Q12) GIT belongs to which generation of version control tools 3rd Q13) What is GIT stash? When changes are made to the working directory but you don t want to comattempting to recreate a clean working directory. GIT stash is used where all changes in working directory and Index are are pushed into a stack. Q14) How do you create a new branches # git branch <branch name> Q15) How do you checkout to a particular branch # git checkout Q17) What languages were used to build the git? C was the major language althought few parts were also written using shell, perl, tcl and python Q18) What is git config? git config allows you to configure git installation. example includes setting user name , password, email etc Q19) What is git clone? Git clone lets users to copy an exising git repository that resides in the server. It is the easiest way that a new developer can start using and contributing to the project Q20) What is branching? Branching is analogus to a storyline on which changes are made. The changes can resides in different branches so that each branch can make changes independent of each other. Q21) What is the command line environment used to perform git operations? # git bash Q22) What is git cherry pick? cherry pick allows you to pick a particular commit from a branch and insert to another branch. This is different from git merge in that, git merge will bring in all commits from branches while cherry pick picks a specific commit only. Q23) How to return a commit that has been pushed and made open? # git revert HEAD~2.HEAD Q24) What is gitflow workflow? It is a workflow that can be used to maintain large projects and it mainly consists of The master branch is always ready for live release with everything production-ready. The Hotflix branches help in quick patching of production releases. The Develop branch helps in merging of all feature branches and also performs all the tests. The Feature branch implies a unique branch for every new feature. The feature branch could be pushed to the development branch just like their parent branch. (adsbygoogle = window.adsbygoogle || []).push({}); Q25) What is the syntax for rebasing ? # git rebase [new-commit] Q26) What are git webhooks? When plannning to cascade/notify new activities on git server to a different tool like jenkins, git webhook is used. the webhook contains information about the activity (ex: push on the server). Q27) What is git instaweb Its a command that helps in directing a web browser and running a web server with an interface to the local repository. Q28) What hash is used in git ? sha1 Q29) sha1 is now considered unsecured. does that mean git is under threat. No. since git uses sha1 to only hash data to compare files/maintain state agit Q30) What is the max number of heads can be used in git? unlimited Q31) How many characters are used in sha1 name 40 chars Q32)What is a commit message. It is the message assigned to a commit made. When a commit is made a hash is assigned automatically but for humans it becomes difficult to make anything out of this hash at later point of time. Hence a commit message is assigned along for his future reference Q33)Name few graphical git clients git cola, git gui Q34)What is git diff? Git diff represents the changes between the commits and changes between working tree and commits. Q35)What is git pull origin? The command git pull origin master tells git to perform a pull operation (download a copy of repository) where the origin represents the server url (alias) and master is the name of the branch Q36) What is gitlog? It is basically a command that can be executed when it comes to finding the history of a project according to the date, changes made, the developer who handled it and usefulness of the same. Q37) How can a developer update his changes to the git server. Perform git push from the branch he is currently checked out to. Q38) How do you make git not to consider few files/directories track changes. need to add them in .gitignore file Q39)What is the difference between git add . git add -all They are same Q40) What is git bisect? Its the command that uses a binary search algorithm to find which commit \u00fe\u00ffa bug.Before the bug is introduced into the commit, the commit is referred Q41) What is git stash drop? Its the command that helps you remove the last entry made to the stash list or it can also help to eliminate any stash entry. Q42) How do you delete a branch locally? git branch -d <branch name> Q43) What is git clean? It cleans a repository by removing the files that are not currently tracked by git recursively. Q44) What is git pop? It is the command that helps you retrieve the changes pushed on the stack. Q45) What is git fork? A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project. Generally open source projects follow fork workflow to allow users to contribute to the project. GIT Cheat Sheet (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"GIT Interview Questions"},{"location":"nightwolf-cotribution/git/#top-50-git-interview-questions-and-answers-for-devops-roles-solved","text":"We have consolidated a list of frequently asked GIT interview questions for Freshers and Experienced DevOps Engineer. You will find these questions very helpful in your interviews for DevOps Engineer. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Q1) What is a version control system (VCS)? Version control systems are software tools that help a team manage changes to source code/documents over time. Q2) Why are version control systems (VCS) necessary? They allow you to 1. Keep track of code changes. 2. Can help team memembers to synchronize the code to the latest version easily. 3. It helps teams to develop products faster. 4. Helps teams to collaborate with each other easily. 5. It acts as a backup for your code base. Q3) What are distributed version control systems (DVCS)? Distributed revision control synchronizes repositories by transferring patches from peer to peer. There is no single central version of the codebase instead, each user can download a working copy and full change history. Hence not requiring to be connected to the server all the time. Example : GIT Q4) What are centralized version control systems (CVCS)? It is a version control where there is a single central repository hosted on a server. This server is expected to have the latest code and expects all its clients to contribute by being connected to the server always. Q5) How does git maintain the data internally. Git mainly uses three different types of objects to hold information about a repository BLOB binary form of the actual data TREE It contains pointers to the objects COMMIT Commit object contains information about the author, date, ha Q6) Can git be used locally without using GIT server or any SAAS providers like git, bitbucket &etc Yes, if a person is willing to work on the project alone he can use git to maintain the state of the project. However the full potential of git is unutilized. Q7) How do you find a list of files impacted with a particular commit hash? git diff-tree -r {hash} Q8) What is a conflict in git. Is it necessary? A conflict in git arises when branches are merged with new commits which has changes on same file(s). In cases like this git cannot take precendence of changes hen Q9) What is the difference between git pull and git fetch Git fetch will download new commits from a remote repository to a local repository git pull does the same as git fetch and also merges the same into your local working files Q10) How are conflicts solved? The files in conflict must be edited and fixed. Then add the resolved files by running 1. git add . 2. git commit Q11) What is the command that defines the author email to be used for all the commits performed by the current user? # git config global user.email <email> Q12) GIT belongs to which generation of version control tools 3rd Q13) What is GIT stash? When changes are made to the working directory but you don t want to comattempting to recreate a clean working directory. GIT stash is used where all changes in working directory and Index are are pushed into a stack. Q14) How do you create a new branches # git branch <branch name> Q15) How do you checkout to a particular branch # git checkout Q17) What languages were used to build the git? C was the major language althought few parts were also written using shell, perl, tcl and python Q18) What is git config? git config allows you to configure git installation. example includes setting user name , password, email etc Q19) What is git clone? Git clone lets users to copy an exising git repository that resides in the server. It is the easiest way that a new developer can start using and contributing to the project Q20) What is branching? Branching is analogus to a storyline on which changes are made. The changes can resides in different branches so that each branch can make changes independent of each other. Q21) What is the command line environment used to perform git operations? # git bash Q22) What is git cherry pick? cherry pick allows you to pick a particular commit from a branch and insert to another branch. This is different from git merge in that, git merge will bring in all commits from branches while cherry pick picks a specific commit only. Q23) How to return a commit that has been pushed and made open? # git revert HEAD~2.HEAD Q24) What is gitflow workflow? It is a workflow that can be used to maintain large projects and it mainly consists of The master branch is always ready for live release with everything production-ready. The Hotflix branches help in quick patching of production releases. The Develop branch helps in merging of all feature branches and also performs all the tests. The Feature branch implies a unique branch for every new feature. The feature branch could be pushed to the development branch just like their parent branch. (adsbygoogle = window.adsbygoogle || []).push({}); Q25) What is the syntax for rebasing ? # git rebase [new-commit] Q26) What are git webhooks? When plannning to cascade/notify new activities on git server to a different tool like jenkins, git webhook is used. the webhook contains information about the activity (ex: push on the server). Q27) What is git instaweb Its a command that helps in directing a web browser and running a web server with an interface to the local repository. Q28) What hash is used in git ? sha1 Q29) sha1 is now considered unsecured. does that mean git is under threat. No. since git uses sha1 to only hash data to compare files/maintain state agit Q30) What is the max number of heads can be used in git? unlimited Q31) How many characters are used in sha1 name 40 chars Q32)What is a commit message. It is the message assigned to a commit made. When a commit is made a hash is assigned automatically but for humans it becomes difficult to make anything out of this hash at later point of time. Hence a commit message is assigned along for his future reference Q33)Name few graphical git clients git cola, git gui Q34)What is git diff? Git diff represents the changes between the commits and changes between working tree and commits. Q35)What is git pull origin? The command git pull origin master tells git to perform a pull operation (download a copy of repository) where the origin represents the server url (alias) and master is the name of the branch Q36) What is gitlog? It is basically a command that can be executed when it comes to finding the history of a project according to the date, changes made, the developer who handled it and usefulness of the same. Q37) How can a developer update his changes to the git server. Perform git push from the branch he is currently checked out to. Q38) How do you make git not to consider few files/directories track changes. need to add them in .gitignore file Q39)What is the difference between git add . git add -all They are same Q40) What is git bisect? Its the command that uses a binary search algorithm to find which commit \u00fe\u00ffa bug.Before the bug is introduced into the commit, the commit is referred Q41) What is git stash drop? Its the command that helps you remove the last entry made to the stash list or it can also help to eliminate any stash entry. Q42) How do you delete a branch locally? git branch -d <branch name> Q43) What is git clean? It cleans a repository by removing the files that are not currently tracked by git recursively. Q44) What is git pop? It is the command that helps you retrieve the changes pushed on the stack. Q45) What is git fork? A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project. Generally open source projects follow fork workflow to allow users to contribute to the project.","title":"Top 50 GIT Interview Questions and Answers for DevOps Roles - Solved"},{"location":"nightwolf-cotribution/git_cheatsheet/","text":"GIT Cheat Sheet This page consists of most useful git commands which can be helful for your day to day job. (adsbygoogle = window.adsbygoogle || []).push({}); What is Git ? \uf0c1 Git is the most commonly used Version Control System. Git tracks the changes you make to files, so you have a record of what has been done, and you can revert to specific versions should you ever need to. Git also makes the collaborations easier, allowing changes by multiple people to all be merge into one source. SETUP \uf0c1 Configuring user information used across all local repositories: S.N. Command Description 1 git config --global user.name \u201cFirstname Lastname\u201d Configuring the name 2 git config --global user.email \"User's Email id\u201d Configuring email GIT BASICS \uf0c1 Configuring user information, initializing and cloning repositories S.N. Command Description 1 git init Initialize an existing directory as a Git repository 2 git clone [url] Retrieve an entire repository from a hosted location via URL STAGE & SNAPSHOT \uf0c1 Working with snapshots and the Git staging area S.N. Command Description 1 git status Show the modified files in the working directory,staged for your next commit 2 git add [file] Add a file as it looks now to your next commit (stage) 3 git reset [file] Unstage a file while retaining the changes in working directory 4 git diff Diff of what is changed but not staged 5 git diff --staged Diff of what is staged but not yet committed 6 git commit -m \u201c[descriptive message]\u201d Commit your staged content as a new commit snapshot BRANCH & MERGE \uf0c1 Isolating work in branches, changing context, and integrating changes S.N. Command Description 1 git branch List your branches. a * will appear next to the currently active branch 2 git branch [branch-name] Create a new branch at the current commit 3 git checkout Switch to another branch and check it out into your working directory 4 git merge [branch] Merge the specified branch\u2019s history into the current one 5 git log Show all commits in the current branch\u2019s history (adsbygoogle = window.adsbygoogle || []).push({}); INSPECT & COMPARE \uf0c1 Examining logs, diffs and object information S.N. Command Description 1 git log Show the commit history for the currently active branch 2 git log branch-B..branch-A Show the commits on branchA that are not on branchB 3 git log --follow [file] Show the commits that changed file, even across renames 4 git show [SHA] Show SHA object in Git in human-readable format 5 git diff branchB...branchA Show the diff of what is in branchA that is not in branchB SHARE & UPDATE \uf0c1 Retrieving updates from another repository and updating local repos S.N. Command Description 1 git remote add [alias] [url] Add a git URL as an alias 2 git fetch [alias] Fetch down all the branches from that Git remote 3 git merge [alias]/[branch] Merge a remote branch into your current branch to bring it up to date 4 git push [alias] [branch] Transmit local branch commits to the remote repository branch 5 git pull Fetch and merge any commits from the tracking remote branch TRACKING PATH CHANGES \uf0c1 Versioning file removes and path changes S.N. Command Description 1 git rm [file] Delete the file from project and stage the removal for commit 2 git mv [existing-path] [new-path] Change an existing file path and stage the move 3 git log --stat -M Show all commit logs with indication of any paths that moved REWRITE HISTORY \uf0c1 Rewriting branches, updating commits and clearing history S.N. Command Description 1 git rebase [branch] Apply any commits of current branch ahead of specified one 2 git reset --hard [commit] Clear staging area, rewrite working tree from specific commit TEMPORARY COMMITS \uf0c1 Temporarily store modified, tracked files in order to change branches S.N. Command Description 1 git stash Save modified and staged changes 2 git stash list List stack-order of stashed file changes 3 git stash pop Write working from top of stash stack 4 git stash drop Discard the changes from top of stash stack (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"GIT CheatSheet"},{"location":"nightwolf-cotribution/git_cheatsheet/#what-is-git","text":"Git is the most commonly used Version Control System. Git tracks the changes you make to files, so you have a record of what has been done, and you can revert to specific versions should you ever need to. Git also makes the collaborations easier, allowing changes by multiple people to all be merge into one source.","title":"What is Git ?"},{"location":"nightwolf-cotribution/git_cheatsheet/#setup","text":"Configuring user information used across all local repositories: S.N. Command Description 1 git config --global user.name \u201cFirstname Lastname\u201d Configuring the name 2 git config --global user.email \"User's Email id\u201d Configuring email","title":"SETUP"},{"location":"nightwolf-cotribution/git_cheatsheet/#git-basics","text":"Configuring user information, initializing and cloning repositories S.N. Command Description 1 git init Initialize an existing directory as a Git repository 2 git clone [url] Retrieve an entire repository from a hosted location via URL","title":"GIT BASICS"},{"location":"nightwolf-cotribution/git_cheatsheet/#stage-snapshot","text":"Working with snapshots and the Git staging area S.N. Command Description 1 git status Show the modified files in the working directory,staged for your next commit 2 git add [file] Add a file as it looks now to your next commit (stage) 3 git reset [file] Unstage a file while retaining the changes in working directory 4 git diff Diff of what is changed but not staged 5 git diff --staged Diff of what is staged but not yet committed 6 git commit -m \u201c[descriptive message]\u201d Commit your staged content as a new commit snapshot","title":"STAGE &amp; SNAPSHOT"},{"location":"nightwolf-cotribution/git_cheatsheet/#branch-merge","text":"Isolating work in branches, changing context, and integrating changes S.N. Command Description 1 git branch List your branches. a * will appear next to the currently active branch 2 git branch [branch-name] Create a new branch at the current commit 3 git checkout Switch to another branch and check it out into your working directory 4 git merge [branch] Merge the specified branch\u2019s history into the current one 5 git log Show all commits in the current branch\u2019s history (adsbygoogle = window.adsbygoogle || []).push({});","title":"BRANCH &amp; MERGE"},{"location":"nightwolf-cotribution/git_cheatsheet/#inspect-compare","text":"Examining logs, diffs and object information S.N. Command Description 1 git log Show the commit history for the currently active branch 2 git log branch-B..branch-A Show the commits on branchA that are not on branchB 3 git log --follow [file] Show the commits that changed file, even across renames 4 git show [SHA] Show SHA object in Git in human-readable format 5 git diff branchB...branchA Show the diff of what is in branchA that is not in branchB","title":"INSPECT &amp; COMPARE"},{"location":"nightwolf-cotribution/git_cheatsheet/#share-update","text":"Retrieving updates from another repository and updating local repos S.N. Command Description 1 git remote add [alias] [url] Add a git URL as an alias 2 git fetch [alias] Fetch down all the branches from that Git remote 3 git merge [alias]/[branch] Merge a remote branch into your current branch to bring it up to date 4 git push [alias] [branch] Transmit local branch commits to the remote repository branch 5 git pull Fetch and merge any commits from the tracking remote branch","title":"SHARE &amp; UPDATE"},{"location":"nightwolf-cotribution/git_cheatsheet/#tracking-path-changes","text":"Versioning file removes and path changes S.N. Command Description 1 git rm [file] Delete the file from project and stage the removal for commit 2 git mv [existing-path] [new-path] Change an existing file path and stage the move 3 git log --stat -M Show all commit logs with indication of any paths that moved","title":"TRACKING PATH CHANGES"},{"location":"nightwolf-cotribution/git_cheatsheet/#rewrite-history","text":"Rewriting branches, updating commits and clearing history S.N. Command Description 1 git rebase [branch] Apply any commits of current branch ahead of specified one 2 git reset --hard [commit] Clear staging area, rewrite working tree from specific commit","title":"REWRITE HISTORY"},{"location":"nightwolf-cotribution/git_cheatsheet/#temporary-commits","text":"Temporarily store modified, tracked files in order to change branches S.N. Command Description 1 git stash Save modified and staged changes 2 git stash list List stack-order of stashed file changes 3 git stash pop Write working from top of stash stack 4 git stash drop Discard the changes from top of stash stack (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"TEMPORARY COMMITS"},{"location":"nightwolf-cotribution/http_protocols/","text":"HTTP Protocols and major version differences \uf0c1 In the earliest phase (HTTP/0.9), the HTTP protocol did not use headers and only transmitted plain HTML files. It was a one-line protocol only supporting the GET method. Key Features of HTTP/1.0: \u2022 The concept of headers both for requests (from the client machine) as well as responses (from servers) was introduced. The use of headers such as GET, POST, HEAD added extended flexibility, none of which was possible with the earlier version. \u2022 Version information was now included. \u2022 It allowed a single request/response for every TCP connection. \u2022 Status codes were used to indicate successful requests and to indicate transmission errors. \u2022 The content-type header made it possible to send files other than plain HTML, including scripts and media. /// Key Features of HTTP/1.1: \u2022 It was no longer required for each connection to be terminated immediately after every request was served with a response; instead, with the keep-alive header, it was possible to have persistent connections. It allowed multiple requests/responses per TCP connection. \u2022 The Upgrade header was used to indicate a preference from the client that made it possible to switch to a more preferred protocol if found appropriate by the server. \u2022 HTTP/1.1 provided support for chunk transfers that allowed streaming of content dynamically as chunks and for additional headers to be sent after the message body. This enhancement was particularly useful in cases where values of a field remained unknown until the content had been produced. For example, when the content had to be digitally signed, it was not possible to do so before the entire content gets generated. \u2022 Other features that reinforced its stability were introduced such as: \u2022 pipelining (the second request is sent before the response to the first is adequately served) \u2022 content negotiation (an exchange between client and server to determine the media type, it also provides the provision to serve different versions of a resource at the same URI) \u2022 cache control (used to specify caching policies in both requests and responses) /// Key Features of HTTP/2: - It introduces the concept of a server push where the server anticipates the resources that will be required by the client and pushes them prior to the client making requests. The client retains the authority to deny the server push; however, in most cases, this feature adds a lot of efficiency to the process. - Introduces the concept of multiplexing that interleaves the requests and responses without head-of-line blocking and does so over a single TCP connection. - It is a binary protocol i.e. only binary commands in the form of 0s and 1s are transmitted over the wire. The binary framing layer divides the message into frames that are segregated based on their type \u2013 Data or Header. This feature greatly increases efficiency in terms of security, compression and multiplexing. - HTTP/2 uses HPACK header compression algorithm that is resilient to attacks like CRIME and utilizes static Huffman encoding. /// What are the other differences between HTTP/2 and HTTP/1.1 that impact performance? Multiplexing: HTTP/1.1 loads resources one after the other, so if one resource cannot be loaded, it blocks all the other resources behind it. In contrast, HTTP/2 is able to use a single TCP connection to send multiple streams of data at once so that no one resource blocks any other resource. HTTP/2 does this by splitting data into binary-code messages and numbering these messages so that the client knows which stream each binary message belongs to. Server push: Typically, a server only serves content to a client device if the client asks for it. However, this approach is not always practical for modern webpages, which often involve several dozen separate resources that the client must request. HTTP/2 solves this problem by allowing a server to \"push\" content to a client before the client asks for it. The server also sends a message letting the client know what pushed content to expect \u2013 like if Bob had sent Alice a Table of Contents of his novel before sending the whole thing. Header compression: Small files load more quickly than large ones. To speed up web performance, both HTTP/1.1 and HTTP/2 compress HTTP messages to make them smaller. However, HTTP/2 uses a more advanced compression method called HPACK that eliminates redundant information in HTTP header packets. This eliminates a few bytes from every HTTP packet. Given the volume of HTTP packets involved in loading even a single webpage, those bytes add up quickly, resulting in faster loading. /// How does prioritization in HTTP/2 affect performance? In HTTP/2, developers have hands-on, detailed control over prioritization. This allows them to maximize perceived and actual page load speed to a degree that was not possible in HTTP/1.1. HTTP/2 offers a feature called weighted prioritization. This allows developers to decide which page resources will load first, every time. In HTTP/2, when a client makes a request for a webpage, the server sends several streams of data to the client at once, instead of sending one thing after another. This method of data delivery is known as multiplexing. Developers can assign each of these data streams a different weighted value, and the value tells the client which data stream to render first. Imagine that Alice wants to read a novel that her friend Bob wrote, but both Alice and Bob only communicate through the regular mail. Alice sends a letter to Bob and asks Bob to send her his novel. Bob decides to send the novel HTTP/1.1-style: He mails one chapter at a time, and he only mails the next chapter after receiving a reply letter from Alice confirming that she received the previous chapter. Using this method of content delivery, it takes Alice many weeks to read Bob's novel. Now imagine that Bob decides to send Alice his novel HTTP/2-style: In this case, he sends each chapter of the novel separately (to stay within the postal service's size limits) but all at the same time. He also numbers each chapter: Chapter 1, Chapter 2, etc. Now, Alice receives the novel all at once and can assemble it in the correct order on her own time. If a chapter is missing, she may send a quick reply asking for that specific chapter, but otherwise the process is complete, and Alice can read the novel in just a few days.","title":"HTTP Protocols and major version differences"},{"location":"nightwolf-cotribution/http_protocols/#http-protocols-and-major-version-differences","text":"In the earliest phase (HTTP/0.9), the HTTP protocol did not use headers and only transmitted plain HTML files. It was a one-line protocol only supporting the GET method. Key Features of HTTP/1.0: \u2022 The concept of headers both for requests (from the client machine) as well as responses (from servers) was introduced. The use of headers such as GET, POST, HEAD added extended flexibility, none of which was possible with the earlier version. \u2022 Version information was now included. \u2022 It allowed a single request/response for every TCP connection. \u2022 Status codes were used to indicate successful requests and to indicate transmission errors. \u2022 The content-type header made it possible to send files other than plain HTML, including scripts and media. /// Key Features of HTTP/1.1: \u2022 It was no longer required for each connection to be terminated immediately after every request was served with a response; instead, with the keep-alive header, it was possible to have persistent connections. It allowed multiple requests/responses per TCP connection. \u2022 The Upgrade header was used to indicate a preference from the client that made it possible to switch to a more preferred protocol if found appropriate by the server. \u2022 HTTP/1.1 provided support for chunk transfers that allowed streaming of content dynamically as chunks and for additional headers to be sent after the message body. This enhancement was particularly useful in cases where values of a field remained unknown until the content had been produced. For example, when the content had to be digitally signed, it was not possible to do so before the entire content gets generated. \u2022 Other features that reinforced its stability were introduced such as: \u2022 pipelining (the second request is sent before the response to the first is adequately served) \u2022 content negotiation (an exchange between client and server to determine the media type, it also provides the provision to serve different versions of a resource at the same URI) \u2022 cache control (used to specify caching policies in both requests and responses) /// Key Features of HTTP/2: - It introduces the concept of a server push where the server anticipates the resources that will be required by the client and pushes them prior to the client making requests. The client retains the authority to deny the server push; however, in most cases, this feature adds a lot of efficiency to the process. - Introduces the concept of multiplexing that interleaves the requests and responses without head-of-line blocking and does so over a single TCP connection. - It is a binary protocol i.e. only binary commands in the form of 0s and 1s are transmitted over the wire. The binary framing layer divides the message into frames that are segregated based on their type \u2013 Data or Header. This feature greatly increases efficiency in terms of security, compression and multiplexing. - HTTP/2 uses HPACK header compression algorithm that is resilient to attacks like CRIME and utilizes static Huffman encoding. /// What are the other differences between HTTP/2 and HTTP/1.1 that impact performance? Multiplexing: HTTP/1.1 loads resources one after the other, so if one resource cannot be loaded, it blocks all the other resources behind it. In contrast, HTTP/2 is able to use a single TCP connection to send multiple streams of data at once so that no one resource blocks any other resource. HTTP/2 does this by splitting data into binary-code messages and numbering these messages so that the client knows which stream each binary message belongs to. Server push: Typically, a server only serves content to a client device if the client asks for it. However, this approach is not always practical for modern webpages, which often involve several dozen separate resources that the client must request. HTTP/2 solves this problem by allowing a server to \"push\" content to a client before the client asks for it. The server also sends a message letting the client know what pushed content to expect \u2013 like if Bob had sent Alice a Table of Contents of his novel before sending the whole thing. Header compression: Small files load more quickly than large ones. To speed up web performance, both HTTP/1.1 and HTTP/2 compress HTTP messages to make them smaller. However, HTTP/2 uses a more advanced compression method called HPACK that eliminates redundant information in HTTP header packets. This eliminates a few bytes from every HTTP packet. Given the volume of HTTP packets involved in loading even a single webpage, those bytes add up quickly, resulting in faster loading. /// How does prioritization in HTTP/2 affect performance? In HTTP/2, developers have hands-on, detailed control over prioritization. This allows them to maximize perceived and actual page load speed to a degree that was not possible in HTTP/1.1. HTTP/2 offers a feature called weighted prioritization. This allows developers to decide which page resources will load first, every time. In HTTP/2, when a client makes a request for a webpage, the server sends several streams of data to the client at once, instead of sending one thing after another. This method of data delivery is known as multiplexing. Developers can assign each of these data streams a different weighted value, and the value tells the client which data stream to render first. Imagine that Alice wants to read a novel that her friend Bob wrote, but both Alice and Bob only communicate through the regular mail. Alice sends a letter to Bob and asks Bob to send her his novel. Bob decides to send the novel HTTP/1.1-style: He mails one chapter at a time, and he only mails the next chapter after receiving a reply letter from Alice confirming that she received the previous chapter. Using this method of content delivery, it takes Alice many weeks to read Bob's novel. Now imagine that Bob decides to send Alice his novel HTTP/2-style: In this case, he sends each chapter of the novel separately (to stay within the postal service's size limits) but all at the same time. He also numbers each chapter: Chapter 1, Chapter 2, etc. Now, Alice receives the novel all at once and can assemble it in the correct order on her own time. If a chapter is missing, she may send a quick reply asking for that specific chapter, but otherwise the process is complete, and Alice can read the novel in just a few days.","title":"HTTP Protocols and major version differences"},{"location":"nightwolf-cotribution/java_interview_questions-2/","text":"Top 100 Java Interview Questions - 2 \uf0c1 We have prepared a set of frequently asked Core Java interview questions to help Freshers and Experienced Java developers in their preparations for Interview. You will find these questions very helpful in your Java/OOPs interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Give an example of use of Pointers in Java class. Ans: There are no pointers in Java. So we can't use concept of pointers in Java. How can we restrict inheritance for a class so that no class can be inherited from it? Ans: If we want a class not to be extended further by any class, we can use the keyword Final with the class name. In the following example, Stone class is Final and can not be extended: public Final Class Stone { // Class methods and Variables } What's the access scope of Protected Access specifier? Ans: When a method or a variable is declared with Protected access specifier, it becomes accessible in the same class,any other class of the same package as well as a sub-class. * * * * Access Levels * * * * ** Modifier Class Package SubClass World public Y Y Y Y protected Y Y Y N no modifier Y Y N N private Y N N N What's difference between Stack and Queue? Ans: Stack and Queue both are used as placeholder for a collection of data. The primary difference between a stack and a queue is that stack is based on Last in First out (LIFO) principle while a queue is based on FIFO (First In First Out) principle. In java, how we can disallow serialization of variables? Ans: If we want certain variables of a class not to be serialized, we can use the keyword transient while declaring them. For example, the variable trans_var below is a transient variable and can not be serialized: public class transientExample { private transient trans_var; // rest of the code } How can we use primitive data types as objects? Ans: Primitive data types like int can be handled as objects by the use of their respective wrapper classes. For example, Integer is a wrapper class for primitive data type int. We can apply different methods to a wrapper class, just like any other object. Which types of exceptions are caught at compile time? Ans: Checked exceptions can be caught at the time of program compilation. Checked exceptions must be handled by using try catch block in the code in order to successfully compile the code. Describe different states of a thread. Ans: A thread in Java can be in either of the following states: Ready: When a thread is created, it is in Ready state. Running: A thread currently being executed is in running state. Waiting: A thread waiting for another thread to free certain resources is in waiting state. Dead: A thread which has gone dead after execution is in dead state. Can we use a default constructor of a class even if an explicit constructor is defined? Ans: Java provides a default no argument constructor if no explicit constructor is defined in a Java class. But if an explicit constructor has been defined, default constructor can not be invoked and developer can use only those constructors which are defined in the class. Can we override a method by using same method name and arguments but different return types? Ans: The basic condition of method overriding is that method name, arguments as well as return type must be exactly same as is that of the method being overridden. Hence usinga different return type does not override a method. What will be the output of following piece of code? public class operatorExample { public static void main(String args[]) { int x=4; system.out.println(x++); } } Ans: In this case postfix ++ operator is used which first returns the value and then increments. Hence the output will be 4. A person says that he compiled a java class successfully without even having a main method in it? Is it possible? Ans: main method is an entry point of Java class and is required for execution of the program. However a class gets compiled successfully even if it does not have a main method. It can not be run though. Can we call a non-static method from inside a static method? Ans: Non-Static methods are owned by objects of a class and have object level scope and in order to call the non-Static methods from a static block (like froma static main method), an object of the class needs to be created first. Then using object reference, these methods can be invoked. What are the two environment variables that must be set in order to run any Java programs? Ans: Java programs can be executed in a machine only once following two environment variables have been properly set: 1. PATH variable 2. CLASSPATH variable Can variables be used in Java without initialization? Ans: In Java, if a variable is used ina code without prior initialization by a valid value, program does not compile and gives an error as no default value is assigned to variables in Java. Can a class in Java be inherited from more than one class? Ans: In Java, a class can be derived from only one class and not from multiple classes. Multiple inheritances is not supported by Java. Can a constructor have different name than a Class name in Java? Ans: Constructor in Java must have same name as the class name and if the name is different, it does not act as a constructor and compiler thinks of it as a normal method. What will be the output of Round(3.7) and Ceill3.7)? Ans: Round(3.7) returns 4 and Ceil(3.7) returns 4. Can we use goto in Java to go to a particular line? Ans: In Java, there is not goto keyword and java doesn't support this feature of going to a particular labeled line. Can a dead thread be started again? Ans: In java, a thread which is in dead state can't be started again. There is no way to restart a dead thread. Is the following class declaration correct? public abstract final class testClass{ // Class methods and variables } Ans: The above class declaration is incorrect as an abstract class can not be declared as Final. Is JDK required on each machine to run a Java program? Ans: JDK is development Kit of Java and is required for development only and to run a Java program on a machine, JDK isn't required. Only JRE is required. What's the difference between comparison done by equals method and == operator? Ans: In Java, equals() method is used to compare the contents of two string objects and returns true if the two have same value while == operator compares the references of two string objects. In the following example, equals() returns true as the two string objects have same values. However \"==\" operator returns false as both string objects are referencing to different objects: public class equalsTest { public static void main(String args[]) { String str1 = new String(\"Hello World\"); String str2 new String(\"Hello World\"); if (str1.equals(str2)) { // this condition is true System.out.printin(\"str1 and str2 are equal in terms of values\"); } if (str1== str2) { //This condition is true System.out.println(\"Both strings are referencing same object\"); } else { //This condition is NOT true System.out.println(\"Both strings are referencing different objects\"); } } } Is it possible to define a method in Java class but provide it's implementation in the code of another language like C? Ans: Yes, we can do this by use of native methods. In case of native method based development, we define public static methods in our Java class without its implementation and then implementation is done in another language like C separately. How are destructors defined in Java? Ans: In Java, there are no destructors defined in the class as there is no need to do so. Java has its own garbage collection mechanism which does the job automatically by destroying the objects when no longer referenced. Can a variable be local and static at the same time? Ans: No a variable can't be static as well as local at the same time. Defining a local variable as static gives compilation error. Can we have static methods in an Interface? Ans: Static methods can not be overridden in any class while any methods in an interface are by default abstract and are supposed to be implemented in the classes being implementing the interface. So it makes no sense to have static methods in an interface in Java. In a class implementing an interface, can we change the value of any variable defined in the interface? Ans: No, we can not change the value of any variable of an interface in the implementing class as all variables defined in the interface are by default public, static and Final and final variables are like constants which can not be changed later. Is it correct to say that due to garbage collection feature in Java, a java program never goes out of memory? Ans: Even though automatic garbage collection is provided by Java, it does not ensure that a Java program will not go out of memory as there is a possibility that creation of Java objects is being done at a faster pace compared to garbage collection resulting in filling of all the available memory resources. So, garbage collection helps in reducing the chances of a program going out of memory but it does not ensure that. Can we have any other return type than void for main method? Ans: No, Java class main method can have only void return type for the program to get successfully executed. Nonetheless, if you absolutely must return a value to at the completion of main method, you can use System.exit(int status). I want to re-reach and use an object once it has been garbage collected. How it's possible? Ans: Once an object has been destroyed by garbage collector, it no longer exists on the heap and it can not be accessed again. There is no way to reference it again. In Java thread programming, which method is a must implemetation for all threads? Ans: Run() is a method of Runnable interface that must be implemented by all threads. I want to control database connections in my program and want that only one thread should be able to make database connection at a time. How can I implement this logic? Ans: This can be implemented by use of the concept of synchronization. Database related code can be placed in a method which hs synchronized keyword so that only one thread can access it at a time. How can an exception be thrown manually by a programmer? Ans: In order to throw an exception in a block of code manually, throw keyword is used. Then this exception is caught and handled in the catch block. public void topMethod() { try { excMethod(); }catch(ManualException e){} } public void excMethod{ String name=null; if(name == null}{ throw (new ManualException(\"Exception thrown manually \"); } } I want my class to be developed in sucha way that no other class (even derived class) can create its objects. How can I do so? Ans: If we declare the constructor of a class as private, it will not be accessible by any other class and hence, no other class will be able to instantiate it and formation of its object will be limited to itself only. How objects are stored in Java? Ans: In java, each object when created gets a memory space from a heap. When an object is destroyed by a garbage collector, the space allocated to it from the heap is re-allocated to the heap and becomes available for any new objects. How can we find the actual size of an object on the heap? Ans: In java, there is no way to find out the exact size of an object on the heap. Which of the following classes will have more memory allocated? Class A: Three methods, four variables, no object Class B: Five methods, three variables, no object Ans: Memory is not allocated before creation of objects. Since for both classes, there are no objects created so no memory is allocated on heap for any class. What happens if an exception is not handled in a program? Ans: If an exception is not handled in a program using try catch blocks, program gets aborted and no statement executes after the statement which caused exception throwing. I have multiple constructors defined in a class. Is it possible to call a constructor from another constructor's body? Ans: If a class has multiple constructors, it's possible to call one constructor from the body of another one using this(). What's meant by anonymous class? Ans: An anonymous class is a class defined without any name in a single line of code using new keyword. For example, in below code we have defined an anonymous class in one line of code: public java.util.Enumeration testMethod() { return new java.util.Enumeration() @Override public boolean hasMoreElements() { // TODO Auto-generated method stub return false; } @Override public Object nextElement() { // TODO Auto-generated method stub return null; } } Is there a way to increase the size of an array after its declaration? Ans: Arrays are static and once we have specified its size, we can not change it. If we want to use such collections where we may require a change of size ( no of items), we should prefer vector Over array. If an application has multiple classes in it, is it okay to have a main method in more than one class? Ans: If there is main method in more than one classes in a java application, it will not cause any issue as entry point for any application will be a specific class and code will start from the main method of that particular class only. I want to persist data of objects for later use. What's the best approach to do so? Ans: The best way to persist data for future use is to use the concept of serialization. What is a Local class in Java? Ans: In Java, if we define a new class inside a particular block, it is called a local class. Such a class has local scope and is not usable outside the block where its defined. String and StringBuffer both represent String objects. Can we compare String and StringBuffer in Java? Ans: Although String and StringBuffer both represent String objects, we can not compare them with each other and if we try to compare them, we get an error. Which APl is provided by Java for operations on set of objects? Ans: Java provides a Collection APl which provides many useful methods which can be applied on a set of objects. Some of the important classes provided by Collection API include ArrayList, Hash Map, TreeSet and TreeMap. Can we cast any other type to Boolean Type with type casting? Ans: No, we can neither cast any other primitive type to Boolean data type nor can cast Boolean data type to any other primitive data type. Can we use different return types for methods when overridden? Ans: The basic requirement of method overriding in Java is that the overridden method should have same name, and parameters.But a method can be overridden with a different return type as long as the new return type extends the original. For example, method is returning a reference type. Class B extends A { A method(int x){ // original method } B method(int x){ //overridden method } } What's the base class of all exception classes? Ans: In Java, Java.lang.Throwable is the super class of all exception classes and all exception classes are derived from this base class. What's the order of call of constructors in inheritiance? Ans: In case of inheritance, whena new object of a derived class is created, first the constructor of the super class is invoked and then the constructor of the derived class is invoked. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Java/OOPs Interview Questions-2"},{"location":"nightwolf-cotribution/java_interview_questions-2/#top-100-java-interview-questions-2","text":"We have prepared a set of frequently asked Core Java interview questions to help Freshers and Experienced Java developers in their preparations for Interview. You will find these questions very helpful in your Java/OOPs interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Give an example of use of Pointers in Java class. Ans: There are no pointers in Java. So we can't use concept of pointers in Java. How can we restrict inheritance for a class so that no class can be inherited from it? Ans: If we want a class not to be extended further by any class, we can use the keyword Final with the class name. In the following example, Stone class is Final and can not be extended: public Final Class Stone { // Class methods and Variables } What's the access scope of Protected Access specifier? Ans: When a method or a variable is declared with Protected access specifier, it becomes accessible in the same class,any other class of the same package as well as a sub-class. * * * * Access Levels * * * * ** Modifier Class Package SubClass World public Y Y Y Y protected Y Y Y N no modifier Y Y N N private Y N N N What's difference between Stack and Queue? Ans: Stack and Queue both are used as placeholder for a collection of data. The primary difference between a stack and a queue is that stack is based on Last in First out (LIFO) principle while a queue is based on FIFO (First In First Out) principle. In java, how we can disallow serialization of variables? Ans: If we want certain variables of a class not to be serialized, we can use the keyword transient while declaring them. For example, the variable trans_var below is a transient variable and can not be serialized: public class transientExample { private transient trans_var; // rest of the code } How can we use primitive data types as objects? Ans: Primitive data types like int can be handled as objects by the use of their respective wrapper classes. For example, Integer is a wrapper class for primitive data type int. We can apply different methods to a wrapper class, just like any other object. Which types of exceptions are caught at compile time? Ans: Checked exceptions can be caught at the time of program compilation. Checked exceptions must be handled by using try catch block in the code in order to successfully compile the code. Describe different states of a thread. Ans: A thread in Java can be in either of the following states: Ready: When a thread is created, it is in Ready state. Running: A thread currently being executed is in running state. Waiting: A thread waiting for another thread to free certain resources is in waiting state. Dead: A thread which has gone dead after execution is in dead state. Can we use a default constructor of a class even if an explicit constructor is defined? Ans: Java provides a default no argument constructor if no explicit constructor is defined in a Java class. But if an explicit constructor has been defined, default constructor can not be invoked and developer can use only those constructors which are defined in the class. Can we override a method by using same method name and arguments but different return types? Ans: The basic condition of method overriding is that method name, arguments as well as return type must be exactly same as is that of the method being overridden. Hence usinga different return type does not override a method. What will be the output of following piece of code? public class operatorExample { public static void main(String args[]) { int x=4; system.out.println(x++); } } Ans: In this case postfix ++ operator is used which first returns the value and then increments. Hence the output will be 4. A person says that he compiled a java class successfully without even having a main method in it? Is it possible? Ans: main method is an entry point of Java class and is required for execution of the program. However a class gets compiled successfully even if it does not have a main method. It can not be run though. Can we call a non-static method from inside a static method? Ans: Non-Static methods are owned by objects of a class and have object level scope and in order to call the non-Static methods from a static block (like froma static main method), an object of the class needs to be created first. Then using object reference, these methods can be invoked. What are the two environment variables that must be set in order to run any Java programs? Ans: Java programs can be executed in a machine only once following two environment variables have been properly set: 1. PATH variable 2. CLASSPATH variable Can variables be used in Java without initialization? Ans: In Java, if a variable is used ina code without prior initialization by a valid value, program does not compile and gives an error as no default value is assigned to variables in Java. Can a class in Java be inherited from more than one class? Ans: In Java, a class can be derived from only one class and not from multiple classes. Multiple inheritances is not supported by Java. Can a constructor have different name than a Class name in Java? Ans: Constructor in Java must have same name as the class name and if the name is different, it does not act as a constructor and compiler thinks of it as a normal method. What will be the output of Round(3.7) and Ceill3.7)? Ans: Round(3.7) returns 4 and Ceil(3.7) returns 4. Can we use goto in Java to go to a particular line? Ans: In Java, there is not goto keyword and java doesn't support this feature of going to a particular labeled line. Can a dead thread be started again? Ans: In java, a thread which is in dead state can't be started again. There is no way to restart a dead thread. Is the following class declaration correct? public abstract final class testClass{ // Class methods and variables } Ans: The above class declaration is incorrect as an abstract class can not be declared as Final. Is JDK required on each machine to run a Java program? Ans: JDK is development Kit of Java and is required for development only and to run a Java program on a machine, JDK isn't required. Only JRE is required. What's the difference between comparison done by equals method and == operator? Ans: In Java, equals() method is used to compare the contents of two string objects and returns true if the two have same value while == operator compares the references of two string objects. In the following example, equals() returns true as the two string objects have same values. However \"==\" operator returns false as both string objects are referencing to different objects: public class equalsTest { public static void main(String args[]) { String str1 = new String(\"Hello World\"); String str2 new String(\"Hello World\"); if (str1.equals(str2)) { // this condition is true System.out.printin(\"str1 and str2 are equal in terms of values\"); } if (str1== str2) { //This condition is true System.out.println(\"Both strings are referencing same object\"); } else { //This condition is NOT true System.out.println(\"Both strings are referencing different objects\"); } } } Is it possible to define a method in Java class but provide it's implementation in the code of another language like C? Ans: Yes, we can do this by use of native methods. In case of native method based development, we define public static methods in our Java class without its implementation and then implementation is done in another language like C separately. How are destructors defined in Java? Ans: In Java, there are no destructors defined in the class as there is no need to do so. Java has its own garbage collection mechanism which does the job automatically by destroying the objects when no longer referenced. Can a variable be local and static at the same time? Ans: No a variable can't be static as well as local at the same time. Defining a local variable as static gives compilation error. Can we have static methods in an Interface? Ans: Static methods can not be overridden in any class while any methods in an interface are by default abstract and are supposed to be implemented in the classes being implementing the interface. So it makes no sense to have static methods in an interface in Java. In a class implementing an interface, can we change the value of any variable defined in the interface? Ans: No, we can not change the value of any variable of an interface in the implementing class as all variables defined in the interface are by default public, static and Final and final variables are like constants which can not be changed later. Is it correct to say that due to garbage collection feature in Java, a java program never goes out of memory? Ans: Even though automatic garbage collection is provided by Java, it does not ensure that a Java program will not go out of memory as there is a possibility that creation of Java objects is being done at a faster pace compared to garbage collection resulting in filling of all the available memory resources. So, garbage collection helps in reducing the chances of a program going out of memory but it does not ensure that. Can we have any other return type than void for main method? Ans: No, Java class main method can have only void return type for the program to get successfully executed. Nonetheless, if you absolutely must return a value to at the completion of main method, you can use System.exit(int status). I want to re-reach and use an object once it has been garbage collected. How it's possible? Ans: Once an object has been destroyed by garbage collector, it no longer exists on the heap and it can not be accessed again. There is no way to reference it again. In Java thread programming, which method is a must implemetation for all threads? Ans: Run() is a method of Runnable interface that must be implemented by all threads. I want to control database connections in my program and want that only one thread should be able to make database connection at a time. How can I implement this logic? Ans: This can be implemented by use of the concept of synchronization. Database related code can be placed in a method which hs synchronized keyword so that only one thread can access it at a time. How can an exception be thrown manually by a programmer? Ans: In order to throw an exception in a block of code manually, throw keyword is used. Then this exception is caught and handled in the catch block. public void topMethod() { try { excMethod(); }catch(ManualException e){} } public void excMethod{ String name=null; if(name == null}{ throw (new ManualException(\"Exception thrown manually \"); } } I want my class to be developed in sucha way that no other class (even derived class) can create its objects. How can I do so? Ans: If we declare the constructor of a class as private, it will not be accessible by any other class and hence, no other class will be able to instantiate it and formation of its object will be limited to itself only. How objects are stored in Java? Ans: In java, each object when created gets a memory space from a heap. When an object is destroyed by a garbage collector, the space allocated to it from the heap is re-allocated to the heap and becomes available for any new objects. How can we find the actual size of an object on the heap? Ans: In java, there is no way to find out the exact size of an object on the heap. Which of the following classes will have more memory allocated? Class A: Three methods, four variables, no object Class B: Five methods, three variables, no object Ans: Memory is not allocated before creation of objects. Since for both classes, there are no objects created so no memory is allocated on heap for any class. What happens if an exception is not handled in a program? Ans: If an exception is not handled in a program using try catch blocks, program gets aborted and no statement executes after the statement which caused exception throwing. I have multiple constructors defined in a class. Is it possible to call a constructor from another constructor's body? Ans: If a class has multiple constructors, it's possible to call one constructor from the body of another one using this(). What's meant by anonymous class? Ans: An anonymous class is a class defined without any name in a single line of code using new keyword. For example, in below code we have defined an anonymous class in one line of code: public java.util.Enumeration testMethod() { return new java.util.Enumeration() @Override public boolean hasMoreElements() { // TODO Auto-generated method stub return false; } @Override public Object nextElement() { // TODO Auto-generated method stub return null; } } Is there a way to increase the size of an array after its declaration? Ans: Arrays are static and once we have specified its size, we can not change it. If we want to use such collections where we may require a change of size ( no of items), we should prefer vector Over array. If an application has multiple classes in it, is it okay to have a main method in more than one class? Ans: If there is main method in more than one classes in a java application, it will not cause any issue as entry point for any application will be a specific class and code will start from the main method of that particular class only. I want to persist data of objects for later use. What's the best approach to do so? Ans: The best way to persist data for future use is to use the concept of serialization. What is a Local class in Java? Ans: In Java, if we define a new class inside a particular block, it is called a local class. Such a class has local scope and is not usable outside the block where its defined. String and StringBuffer both represent String objects. Can we compare String and StringBuffer in Java? Ans: Although String and StringBuffer both represent String objects, we can not compare them with each other and if we try to compare them, we get an error. Which APl is provided by Java for operations on set of objects? Ans: Java provides a Collection APl which provides many useful methods which can be applied on a set of objects. Some of the important classes provided by Collection API include ArrayList, Hash Map, TreeSet and TreeMap. Can we cast any other type to Boolean Type with type casting? Ans: No, we can neither cast any other primitive type to Boolean data type nor can cast Boolean data type to any other primitive data type. Can we use different return types for methods when overridden? Ans: The basic requirement of method overriding in Java is that the overridden method should have same name, and parameters.But a method can be overridden with a different return type as long as the new return type extends the original. For example, method is returning a reference type. Class B extends A { A method(int x){ // original method } B method(int x){ //overridden method } } What's the base class of all exception classes? Ans: In Java, Java.lang.Throwable is the super class of all exception classes and all exception classes are derived from this base class. What's the order of call of constructors in inheritiance? Ans: In case of inheritance, whena new object of a derived class is created, first the constructor of the super class is invoked and then the constructor of the derived class is invoked. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Top 100 Java Interview Questions - 2"},{"location":"nightwolf-cotribution/java_interview_questions/","text":"Top 100 Java Interview Questions & Answers \uf0c1 We have prepared a set of frequently asked Core Java interview questions to help Freshers and Experienced Java developers in their preparations for Interview. You will find these questions very helpful in your Java/OOPs interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is the difference between an Inner Class and a Sub-Class? Ans: An Inner class isa class which is nested within another class. An Inner class has access rights for the class which is nesting it and it can access all variables and methods defined in the outer class. A sub-class is a class which inherits from another class called super class. Sub-class can access all public and protected methods and fields of its super class. What are the various access specifiers for Java classes? Ans: In Java, access specifiers are the keywords used before a class name which defines the access scope. The types of access specifiers for classes are: 1. Public: Class, Method, Field is accessible from anywhere. 2. Protected:Method, Field can be accessed from the same class to which they belong or from the sub-classes,and from the class of same package, but not from outside 3. Default: Method, Field,class can be accessed only from the same package and not from outside of it's native package. 4. Private: Method, Field can be accessed from the same class to which they belong8. What's the purpose of Static methods and static variables? Ans: When there is a requirement to share a method or a variable between multiple objects of a class instead of creating separate copies for each object, we use static keyword to make a method or variable shared for all objects. What is data encapsulation and what's its significance? Ans: Encapsulation is a concept in Object Oriented Programming for combining properties and methods in a single unit. Encapsulation helps programmers to follow a modular approach for software development as each object has its own set of methods and variables and serves its functions independent of other objects. Encapsulation also serves data hiding purpose. What is a singleton class? Give a practical example of its usage. A singleton class in java can have only one instance and hence all its methods and variables belong to just one instance. Singleton class concept is useful for the situations when there is a need to limit the number of objects for a class. The best example of singleton usage scenario is when there is a limit of having only one connection to a database due to some driver limitations or because of any licensing issues. What are Loops in Java? What are three types of loops? Ans: Looping is used in programming to execute a statement or a block of statement repeatedly. There are three types of loops in Java: 1) For Loops : For loops are used in java to execute statements repeatedly for a given number of times. For loops are used when number of times to execute the statements is known to programmer. 2) While Loops: While loop is used when certain statements need to be executed repeatedly until a condition is fulfilled. In while loops, condition is checked first before execution of statements. 3) Do While Loops: Do While Loop is same as While loop with only difference that condition is checked after execution of block of statements. Hence in case of do while loop, statements are executed at least once. What is an infinite Loop? How infinite loop is declared? Ans: An infinite loop runs without any condition and runs infinitely. An infinite loop can be broken by definingg any breaking logic in the body of the statement blocks. Infinite loop is declared as follows: for (;;) { //Statements to execute // Add any loop breaking logic } What is the difference between continue and break statement? Ans: break and continue are two important keywords used in Loops. When a break keyword is used in a loop, loop is broken instantly while when continue keyword is used, current iteration is broken and loop continues with next iteration. In below example, Loop is broken when counter reaches 4. for (counter-0;counter<10;counter+){ system.out.println(counter); if (counter == 4){ break;} } In the below example when counter reaches 4, loop jumps to next iteration and any statements after the continue keyword are skipped for current iteration. for (counter=0;counter<10;counter++) { system.out.printin(counter); if (counter==4){ continue; } system.out.println(\"This will not get printed when counter is 4\"); } What is the difference between double and float variables in Java? Ans: In java, float takes 4 bytes in memory while Double takes 8 bytes in memory. Float is single precision floating point decimal number while Double is double precision decimal number. What is Final Keyword in Java? Give an example. Ans: In java, a constant is declared using the keyword Final. Value can be assigned only once and after assignment, value of a constant can't be changed. In below example, a constant with the name const_val is declared and assigned avalue: private final int const_val=100 When a method is declared as final,it can NOT be overridden by the subclasses.This method are faster than any other method,because they are resolved at complied time. When a class is declares as final,it cannot be subclassed. Example String,Integer and other wrapper classes. What is ternary operator? Give an example. Ans: Ternary operator, also called conditional operator is used to decide which value to assign to a variable based on a Boolean value evaluation. It's denoted as? In the below example, if rank is 1, status is assigned a value of \"Done\" else \"Pending\". public class conditionTest { public static void main(String args[]) { string status; int rank 3; status = (rank == 1)? \"Done\": \"Pending\": System.out.println(status); } } How can you generate random numbers in Java? Ans: - Using Math.random() you can generate random numbers in the range greater than or equal to 0.1 and less than 1.0 - Using Random class in package java.util What is default switch case? Give example. Ans: In a switch statement, default case is executed when no other switch condition matches. Default case is an optional case. It can be declared only once all other switch cases have been coded. In the below example, when score is not 1 or 2, default case is used. public class switchExample { int score=4; public static void main(String args[]) { switch (score) { case 1: system.out.println(\"Score is 1\"); break; case 2: system.out.println(\"Score is 2\"); break; default: system.out.println(\"Default Case\") } } } What is the base class in Java from which all classes are derived? Ans: java.lang.object Can main() method in Java can return any data? Ans: In java, main() method can't return any data and hence, ite's always declared with a void return type. What are Java Packages? What's the significance of packages? Ans: In Java, package is a collection of classes and interfaces which are bundled together as they are related to each other. Use of packages helps developers to modularize the code and group the code for proper re-use. Once code has been packaged in Packages, it can be imported in other classes and used. Can we declare a class as Abstract without having any abstract method? Ans: Yes we can create an abstract class by using abstract keyword before class name even if it doesn't have any abstract method. However, if a class has even one abstract method, it must be declared as abstract otherwise it will give an error. What's the difference between an Abstract Class and Interface in Java? Ans: The primary difference between an abstract class and interface is that an interface can only possess declaration of public static methods with no concrete implementation while an abstract class can have members with any access specifiers (public, private etc) with or without concrete implementation. Another key difference in the use of abstract classes and interfaces is that a class which implements an interface must implement all the methods of the interface while a class which inherits from an abstract class doesn't require implementation of all the methods of its super class. A class can implement multiple interfaces but it can extend only one abstract class. What are the performance implications of Interfaces over abstract classes? Ans: Interfaces are slower in performance as compared to abstract classes as extra indirections are required for interfaces. Another key factor for developers to take into consideration is that any class can extend only one abstract class while a class can implement many interfaces. Use of interfaces also puts an extra burden on the developers as any time an interface is implemented in a class; developer is forced to implement each and every method of interface. Does Importing a package imports its sub-packages as well in Java? Ans: In java, when a package is imported, its sub-packages are not imported and developer needs to import therm separately if required. For example, if a developer imports a package university.*, all classes in the package named university are loaded but no classes from the sub-package are loaded. To load the classes from its sub-package ( say department), developer has to import it explicitly as follows: Import university.department.* Can we declare the main method of our class as private? Ans: In java, main method must be public static in order to run any application correctly. If main method is declared as private, developer will not get any compilation error however, it will not get executed and will give a runtime error. How can we pass argument to a function by reference instead of pass by value? Ans: In java, we can pass argument to a function only by value and not by reference. How an object is serialized in java? Ans: In java, to convert an object into byte stream by serialization, an interface with the name Serializable is implemented by the class. All objects of a class implementing serializable interface get serialized and their state is saved in byte stream. When we should use serialization? Ans: Serialization is used when data needs to be transmitted over the network. Using serialization, object's state is saved and converted into byte stream. The byte stream is transferred over the network and the object is re-created at destination. Is it compulsory for a Try Block to be followed by a Catch Block in Java for Exception handling? Ans: Try block needs to be followed by either Catch block or Finally block or both. Any exception thrown from try block needs to be either caught in the catch block or else any specific tasks to be performed before code abortion are put in the Finally block. Is there any way to skip Finally block of exception even if some exception occurs in the exception block? Ans: If an exception is raised in Try block, control passes to catch block if it exists otherwise to finally block. Finally block is always executed when an exception occurs and the only way to avoid execution of any statements in Finally block is by aborting the code forcibly by writing following line of code at the end of try block: System.exit(0); When the constructor of a class is invoked? Ans: The constructor of a class is invoked every time an object is created with new keyword. For example, in the following class two objects are created using new keyword and hence, constructor is invoked two times. public class const_example { const_example() { system.out.println(\"Inside constructor\"); } public static void main(String args[]){ const_example cl=new const_example(); const example c2=new const_example(); } } Can a class have multiple constructors? Ans: Yes, a class can have multiple constructors with different parameters. Which constructor gets used for object creation depends on the arguments passed while creating the objects. Can we override static methods of a class? Ans: We cannot override static methods. Static methods belong to a class and not to individual objects and are resolved at the time of compilation (not at runtime). Even if we try to override static method, we will not get an complitaion error,nor the impact of overriding when running the code. In the below example, what will be the output? public class superclass { public void displayResult(){ system.out.printin(\"Printing from superclass\"); } } public class subclass extends superclass{ public void displayResult() { system.out.println(\"Displaying from subClass\"); super.displayResult(); } public static void main(String args[]){ subclass obj=new subclass); obj.displayResuit(); } } Ans: Output will be: Displaying from subclass Displaying from superclass Is String a data type in java? Ans: String is not a primitive data type in java. When a string is created in java, it is actually an object of Java. lang.String class that gets created. After creation of this string object, all built-in methods of String class can be used on the string object. In the below example, how many String Objects are created? String s1=\"l am Java Expert\"; String s2-\"l am C Expert\"; String s3-\"l am Java Expert\" Ans: In the above example, two objects of Java.Lang.String class are created. s1 and s3 are references to same object. Why Strings in Java are called as Immutable? Ans: In java, string objects are called immutable as once value has been assigned to a string, it can not be changed and if changed, a new object is created. In below example, reference str refers to a string object having value \"Value one\". String str=\"Value One\" When a new value is assigned to it, a new String object gets created and the reference is moved to the new object. str=\"New Value\"; What's the difference between an array and Vector? Ans: An array groups data of same primitive type and is static in nature while vectors are dynamic in nature and can hold data of different data types. What is multi-threading? Ans: Multi threading is a programming concept to run multiple tasks in a concurrent manner within a single program. Threads share same process stack and running in parallel. It helps in performance improvement of any program. Why Runnable Interface is used in Java? Ans: Runnable interface is used in java for implementing multi threaded applications. Java.Lang.Runnable interface is implemented by a class to support multi threading. What are the two ways of implementing multi-threading in Java? Ans: Multi threaded applications can be developed in Java by using any of the following two methodologies: 1. By using Java.Lang.Runnable Interface. Classes implement this interface to enable multi threading. There is a Run() method in this interface which is implemented. 2. By writing a class that extend Java.Lang.Thread class. When a lot of changes are required in data, which one should be a preference to be used? String or StringBuffer? Ans: Since StringBuffers are dynamic in nature and we can change the values of StringBuffer objects unlike String which is immutable, it is always a good choice to use StringBuffer when data is being changed too much. If we use String in such a case, for every data change a new String object will be created which will be an extra overhead. What's the purpose of using Break in each case of Switch Statement? Ans: Break is used after each case (except the last one) in a switch so that code breaks after the valid case and does not flow in the proceeding cases too. If break is not used after each case, all cases after the valid case also get executed resulting in wrong results. How garbage collection is done in Java? Ans: In java, when an object is not referenced any more, garbage collection takes place and the object is destroyed automatically. For automatic garbage collection java calls either System.gc) method or Runtime.gc() method. How we can execute any code even before main method? Ans: If we want to execute any statements before even creation of objects at load time of class, we can use a static block of code in the class. Any statements inside this static block of code will get executed once at the time of loading the class even before creation of objects in the main method. Can a class be a super class and a sub-class at the same time? Give example. Ans: If there is a hierarchy of inheritance used, a class can be a super class for another class and a sub-class for another one at the same time. In the example below, continent class is sub-class of world class and it's super class of country class. public class world { ------------ } public class continenet extends world { ------------ } public class country extends continent{ ------------ } How objects of a class are created if no constructor is defined in the class? Ans: Even if no explicit constructor is defined in a java class, objects get created successfully as a default constructor is implicitly used for object creation. This constructor has no parameters. In multi-threading how can we ensure that a resource isn't used by multiple threads simultaneously? Ans: In multi-threading, access to the resources which are shared among multiple threads can be controlled by using the concept of synchronization. Using synchronized keyword, we can ensure that only one thread can use shared resource at a time and others can get control of the resource only once it has become free from the other one using it. Can we call the constructor of a class more than once for an object? Ans: Constructor is called automatically when we create an object using new keyword. It is called only once for an object at the time of object creation and hence, we can not invoke the constructor again for an object after its creation. There are two classes named classA and classB. Both classes are in the same package. Can a private member of classA can be accessed by an object of classB? Ans: Private members of a class are not accessible outside the scope of that class and any other class even in the same package can not access them. Can we have two methods in a class with the same name? Ans: We can define two methods in a class with the same name but with different number/type of parameters. Which method is to get invoked will depend upon the parameters passed. For example in the class below we have two print methods with same name but different parameters. Depending upon the parameters, appropriate one will be called: public class methodExample { public void print() { system.out.println(\"Print method without parameters.\"); } public void print(String name){ system.out.printin(\"Print method with parameter\"); } public static void main(String args[]){ methodExample objl= new method Example(); obj1.print(); objl.print(\"xx\"); } } How can we make copy of a java object? Ans: We can use the concept of cloning to create copy of an object. Using clone, we create copies with the actual state of an object. Clone() is a method of Cloneable interface and hence, Cloneable interface needs to be implemented for making object copies. What's the benefit of using inheritance? Ans: Key benefit of using inheritance is reusability of code as inheritance enables sub-classes to reuse the code of its super class. Polymorphism (Extensibility ) is another great benefit which allow new functionality to be introduced without effecting existing derived classes. What's the default access specifier for variables and methods of a class? Ans: Default access specifier for variables and method is package protected i.e variables and class is available to any other class but in the same package,not outside the package. Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Java/OOPs Interview Questions"},{"location":"nightwolf-cotribution/java_interview_questions/#top-100-java-interview-questions-answers","text":"We have prepared a set of frequently asked Core Java interview questions to help Freshers and Experienced Java developers in their preparations for Interview. You will find these questions very helpful in your Java/OOPs interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is the difference between an Inner Class and a Sub-Class? Ans: An Inner class isa class which is nested within another class. An Inner class has access rights for the class which is nesting it and it can access all variables and methods defined in the outer class. A sub-class is a class which inherits from another class called super class. Sub-class can access all public and protected methods and fields of its super class. What are the various access specifiers for Java classes? Ans: In Java, access specifiers are the keywords used before a class name which defines the access scope. The types of access specifiers for classes are: 1. Public: Class, Method, Field is accessible from anywhere. 2. Protected:Method, Field can be accessed from the same class to which they belong or from the sub-classes,and from the class of same package, but not from outside 3. Default: Method, Field,class can be accessed only from the same package and not from outside of it's native package. 4. Private: Method, Field can be accessed from the same class to which they belong8. What's the purpose of Static methods and static variables? Ans: When there is a requirement to share a method or a variable between multiple objects of a class instead of creating separate copies for each object, we use static keyword to make a method or variable shared for all objects. What is data encapsulation and what's its significance? Ans: Encapsulation is a concept in Object Oriented Programming for combining properties and methods in a single unit. Encapsulation helps programmers to follow a modular approach for software development as each object has its own set of methods and variables and serves its functions independent of other objects. Encapsulation also serves data hiding purpose. What is a singleton class? Give a practical example of its usage. A singleton class in java can have only one instance and hence all its methods and variables belong to just one instance. Singleton class concept is useful for the situations when there is a need to limit the number of objects for a class. The best example of singleton usage scenario is when there is a limit of having only one connection to a database due to some driver limitations or because of any licensing issues. What are Loops in Java? What are three types of loops? Ans: Looping is used in programming to execute a statement or a block of statement repeatedly. There are three types of loops in Java: 1) For Loops : For loops are used in java to execute statements repeatedly for a given number of times. For loops are used when number of times to execute the statements is known to programmer. 2) While Loops: While loop is used when certain statements need to be executed repeatedly until a condition is fulfilled. In while loops, condition is checked first before execution of statements. 3) Do While Loops: Do While Loop is same as While loop with only difference that condition is checked after execution of block of statements. Hence in case of do while loop, statements are executed at least once. What is an infinite Loop? How infinite loop is declared? Ans: An infinite loop runs without any condition and runs infinitely. An infinite loop can be broken by definingg any breaking logic in the body of the statement blocks. Infinite loop is declared as follows: for (;;) { //Statements to execute // Add any loop breaking logic } What is the difference between continue and break statement? Ans: break and continue are two important keywords used in Loops. When a break keyword is used in a loop, loop is broken instantly while when continue keyword is used, current iteration is broken and loop continues with next iteration. In below example, Loop is broken when counter reaches 4. for (counter-0;counter<10;counter+){ system.out.println(counter); if (counter == 4){ break;} } In the below example when counter reaches 4, loop jumps to next iteration and any statements after the continue keyword are skipped for current iteration. for (counter=0;counter<10;counter++) { system.out.printin(counter); if (counter==4){ continue; } system.out.println(\"This will not get printed when counter is 4\"); } What is the difference between double and float variables in Java? Ans: In java, float takes 4 bytes in memory while Double takes 8 bytes in memory. Float is single precision floating point decimal number while Double is double precision decimal number. What is Final Keyword in Java? Give an example. Ans: In java, a constant is declared using the keyword Final. Value can be assigned only once and after assignment, value of a constant can't be changed. In below example, a constant with the name const_val is declared and assigned avalue: private final int const_val=100 When a method is declared as final,it can NOT be overridden by the subclasses.This method are faster than any other method,because they are resolved at complied time. When a class is declares as final,it cannot be subclassed. Example String,Integer and other wrapper classes. What is ternary operator? Give an example. Ans: Ternary operator, also called conditional operator is used to decide which value to assign to a variable based on a Boolean value evaluation. It's denoted as? In the below example, if rank is 1, status is assigned a value of \"Done\" else \"Pending\". public class conditionTest { public static void main(String args[]) { string status; int rank 3; status = (rank == 1)? \"Done\": \"Pending\": System.out.println(status); } } How can you generate random numbers in Java? Ans: - Using Math.random() you can generate random numbers in the range greater than or equal to 0.1 and less than 1.0 - Using Random class in package java.util What is default switch case? Give example. Ans: In a switch statement, default case is executed when no other switch condition matches. Default case is an optional case. It can be declared only once all other switch cases have been coded. In the below example, when score is not 1 or 2, default case is used. public class switchExample { int score=4; public static void main(String args[]) { switch (score) { case 1: system.out.println(\"Score is 1\"); break; case 2: system.out.println(\"Score is 2\"); break; default: system.out.println(\"Default Case\") } } } What is the base class in Java from which all classes are derived? Ans: java.lang.object Can main() method in Java can return any data? Ans: In java, main() method can't return any data and hence, ite's always declared with a void return type. What are Java Packages? What's the significance of packages? Ans: In Java, package is a collection of classes and interfaces which are bundled together as they are related to each other. Use of packages helps developers to modularize the code and group the code for proper re-use. Once code has been packaged in Packages, it can be imported in other classes and used. Can we declare a class as Abstract without having any abstract method? Ans: Yes we can create an abstract class by using abstract keyword before class name even if it doesn't have any abstract method. However, if a class has even one abstract method, it must be declared as abstract otherwise it will give an error. What's the difference between an Abstract Class and Interface in Java? Ans: The primary difference between an abstract class and interface is that an interface can only possess declaration of public static methods with no concrete implementation while an abstract class can have members with any access specifiers (public, private etc) with or without concrete implementation. Another key difference in the use of abstract classes and interfaces is that a class which implements an interface must implement all the methods of the interface while a class which inherits from an abstract class doesn't require implementation of all the methods of its super class. A class can implement multiple interfaces but it can extend only one abstract class. What are the performance implications of Interfaces over abstract classes? Ans: Interfaces are slower in performance as compared to abstract classes as extra indirections are required for interfaces. Another key factor for developers to take into consideration is that any class can extend only one abstract class while a class can implement many interfaces. Use of interfaces also puts an extra burden on the developers as any time an interface is implemented in a class; developer is forced to implement each and every method of interface. Does Importing a package imports its sub-packages as well in Java? Ans: In java, when a package is imported, its sub-packages are not imported and developer needs to import therm separately if required. For example, if a developer imports a package university.*, all classes in the package named university are loaded but no classes from the sub-package are loaded. To load the classes from its sub-package ( say department), developer has to import it explicitly as follows: Import university.department.* Can we declare the main method of our class as private? Ans: In java, main method must be public static in order to run any application correctly. If main method is declared as private, developer will not get any compilation error however, it will not get executed and will give a runtime error. How can we pass argument to a function by reference instead of pass by value? Ans: In java, we can pass argument to a function only by value and not by reference. How an object is serialized in java? Ans: In java, to convert an object into byte stream by serialization, an interface with the name Serializable is implemented by the class. All objects of a class implementing serializable interface get serialized and their state is saved in byte stream. When we should use serialization? Ans: Serialization is used when data needs to be transmitted over the network. Using serialization, object's state is saved and converted into byte stream. The byte stream is transferred over the network and the object is re-created at destination. Is it compulsory for a Try Block to be followed by a Catch Block in Java for Exception handling? Ans: Try block needs to be followed by either Catch block or Finally block or both. Any exception thrown from try block needs to be either caught in the catch block or else any specific tasks to be performed before code abortion are put in the Finally block. Is there any way to skip Finally block of exception even if some exception occurs in the exception block? Ans: If an exception is raised in Try block, control passes to catch block if it exists otherwise to finally block. Finally block is always executed when an exception occurs and the only way to avoid execution of any statements in Finally block is by aborting the code forcibly by writing following line of code at the end of try block: System.exit(0); When the constructor of a class is invoked? Ans: The constructor of a class is invoked every time an object is created with new keyword. For example, in the following class two objects are created using new keyword and hence, constructor is invoked two times. public class const_example { const_example() { system.out.println(\"Inside constructor\"); } public static void main(String args[]){ const_example cl=new const_example(); const example c2=new const_example(); } } Can a class have multiple constructors? Ans: Yes, a class can have multiple constructors with different parameters. Which constructor gets used for object creation depends on the arguments passed while creating the objects. Can we override static methods of a class? Ans: We cannot override static methods. Static methods belong to a class and not to individual objects and are resolved at the time of compilation (not at runtime). Even if we try to override static method, we will not get an complitaion error,nor the impact of overriding when running the code. In the below example, what will be the output? public class superclass { public void displayResult(){ system.out.printin(\"Printing from superclass\"); } } public class subclass extends superclass{ public void displayResult() { system.out.println(\"Displaying from subClass\"); super.displayResult(); } public static void main(String args[]){ subclass obj=new subclass); obj.displayResuit(); } } Ans: Output will be: Displaying from subclass Displaying from superclass Is String a data type in java? Ans: String is not a primitive data type in java. When a string is created in java, it is actually an object of Java. lang.String class that gets created. After creation of this string object, all built-in methods of String class can be used on the string object. In the below example, how many String Objects are created? String s1=\"l am Java Expert\"; String s2-\"l am C Expert\"; String s3-\"l am Java Expert\" Ans: In the above example, two objects of Java.Lang.String class are created. s1 and s3 are references to same object. Why Strings in Java are called as Immutable? Ans: In java, string objects are called immutable as once value has been assigned to a string, it can not be changed and if changed, a new object is created. In below example, reference str refers to a string object having value \"Value one\". String str=\"Value One\" When a new value is assigned to it, a new String object gets created and the reference is moved to the new object. str=\"New Value\"; What's the difference between an array and Vector? Ans: An array groups data of same primitive type and is static in nature while vectors are dynamic in nature and can hold data of different data types. What is multi-threading? Ans: Multi threading is a programming concept to run multiple tasks in a concurrent manner within a single program. Threads share same process stack and running in parallel. It helps in performance improvement of any program. Why Runnable Interface is used in Java? Ans: Runnable interface is used in java for implementing multi threaded applications. Java.Lang.Runnable interface is implemented by a class to support multi threading. What are the two ways of implementing multi-threading in Java? Ans: Multi threaded applications can be developed in Java by using any of the following two methodologies: 1. By using Java.Lang.Runnable Interface. Classes implement this interface to enable multi threading. There is a Run() method in this interface which is implemented. 2. By writing a class that extend Java.Lang.Thread class. When a lot of changes are required in data, which one should be a preference to be used? String or StringBuffer? Ans: Since StringBuffers are dynamic in nature and we can change the values of StringBuffer objects unlike String which is immutable, it is always a good choice to use StringBuffer when data is being changed too much. If we use String in such a case, for every data change a new String object will be created which will be an extra overhead. What's the purpose of using Break in each case of Switch Statement? Ans: Break is used after each case (except the last one) in a switch so that code breaks after the valid case and does not flow in the proceeding cases too. If break is not used after each case, all cases after the valid case also get executed resulting in wrong results. How garbage collection is done in Java? Ans: In java, when an object is not referenced any more, garbage collection takes place and the object is destroyed automatically. For automatic garbage collection java calls either System.gc) method or Runtime.gc() method. How we can execute any code even before main method? Ans: If we want to execute any statements before even creation of objects at load time of class, we can use a static block of code in the class. Any statements inside this static block of code will get executed once at the time of loading the class even before creation of objects in the main method. Can a class be a super class and a sub-class at the same time? Give example. Ans: If there is a hierarchy of inheritance used, a class can be a super class for another class and a sub-class for another one at the same time. In the example below, continent class is sub-class of world class and it's super class of country class. public class world { ------------ } public class continenet extends world { ------------ } public class country extends continent{ ------------ } How objects of a class are created if no constructor is defined in the class? Ans: Even if no explicit constructor is defined in a java class, objects get created successfully as a default constructor is implicitly used for object creation. This constructor has no parameters. In multi-threading how can we ensure that a resource isn't used by multiple threads simultaneously? Ans: In multi-threading, access to the resources which are shared among multiple threads can be controlled by using the concept of synchronization. Using synchronized keyword, we can ensure that only one thread can use shared resource at a time and others can get control of the resource only once it has become free from the other one using it. Can we call the constructor of a class more than once for an object? Ans: Constructor is called automatically when we create an object using new keyword. It is called only once for an object at the time of object creation and hence, we can not invoke the constructor again for an object after its creation. There are two classes named classA and classB. Both classes are in the same package. Can a private member of classA can be accessed by an object of classB? Ans: Private members of a class are not accessible outside the scope of that class and any other class even in the same package can not access them. Can we have two methods in a class with the same name? Ans: We can define two methods in a class with the same name but with different number/type of parameters. Which method is to get invoked will depend upon the parameters passed. For example in the class below we have two print methods with same name but different parameters. Depending upon the parameters, appropriate one will be called: public class methodExample { public void print() { system.out.println(\"Print method without parameters.\"); } public void print(String name){ system.out.printin(\"Print method with parameter\"); } public static void main(String args[]){ methodExample objl= new method Example(); obj1.print(); objl.print(\"xx\"); } } How can we make copy of a java object? Ans: We can use the concept of cloning to create copy of an object. Using clone, we create copies with the actual state of an object. Clone() is a method of Cloneable interface and hence, Cloneable interface needs to be implemented for making object copies. What's the benefit of using inheritance? Ans: Key benefit of using inheritance is reusability of code as inheritance enables sub-classes to reuse the code of its super class. Polymorphism (Extensibility ) is another great benefit which allow new functionality to be introduced without effecting existing derived classes. What's the default access specifier for variables and methods of a class? Ans: Default access specifier for variables and method is package protected i.e variables and class is available to any other class but in the same package,not outside the package.","title":"Top 100 Java Interview Questions &amp; Answers"},{"location":"nightwolf-cotribution/jenkins/","text":"Top Jenkins Interview Questions & answers for Experienced DevOps Engineer - Solved \uf0c1 We have consolidated Jenkins frequently asked interview questions in DevOps interviews. You will find these questions very helpful in your interviews for DevOps roles. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Q#1) What is Jenkins? Answer: Jenkins is a free open source Continuous Integration tool and automation server to monitor continuous integration and delivery. It is written in Java. It is known as an automated Continuous Delivery tool that helps to build and test the software system with easy integration of changes to the system. Jenkins follows Groovy Scripting. Also, it enables developers to continuously check in their code and also analyze the postbuild actions. The automation testers can use to run their tests as soon as the new code is added or code is modified. Q#2) What are the features of Jenkins? Answer: Jenkins comes with the following features: 1. Free open source. 2. Easy installation on various operating systems. 3. Build Pipeline Support. 4. Workflow Plugin. 5. Test harness built around JUnit. 6. Easy upgrades. 7. Rapid release cycle. 8. Easy configuration setup. 9. Extensible with the use of third-party plugins. Q#3) What are the advantages of Jenkins? Why we use Jenkins? Answer: Jenkins is used to continuously monitor the large code base in real-time. It enables developers to find bugs in their code and fix them. Email notifications are made to the developers regarding their check-ins as a post-build action. Advantages of Jenkins are as follows: - Build failures are cached during the integration stage. - Notifies the developers about build report status using LDAP (Lightweight Directory Access Protocol) mail server. - Maven release project is automated with simple steps. - Easy bug tracking. - Automatic changes get updated in the build report with notification. - Supports Continuous Integration in agile development and test-driven development. Q#4) Mention some of the important plugins in Jenkins? Answer: Plugins in Jenkins includes: - Gits - Maven 2 Project - HTML Publisher - Copy Artcraft - Join - Green Balls - Amazon EC2 Q#5) What is Continuous Integration in Jenkins? Answer: Continuous integration is the process of continuously checking-in the developer code into a version control system and triggering the build to check and identify bugs in the written code. This is a very quick process and also gives them a chance to fix the bugs. Jenkins is one such continuous integration tool. In software development, multiple developers work on different software modules. While performing integration testing all the modules are being integrated together. It is considered as the development practice to integrate the code into the source repository Whenever the programmer/developer makes any change to the current code, then it automatically gets integrated with the system running on the tester\u2019s machine and makes the testing task easy and speedy for the system testers. Continuous Integration comprises of: - Development and Compilation - Database Integration - Unit Testing - Production Deployment - Code Labeling - Functional Testing - Generating and Analyzing Reports Q#6) What is the difference between Hudson and Jenkins? Answer: There is no difference between Hudson and Jenkins. Hudson was the former name of Jenkins, after going through several issues the name was changed to Jenkins. Q#7) What is Groovy in Jenkins? Answer: Groovy is the default scripting language that is being used in the development of JMeter Version 3.1. Currently Apache Groovy is the dynamic object-oriented programming language that is used as a scripting language for the Java platform. Apache Groovy comes with some useful features such as Java Compatibility and Development Support. Q#8) Which command is used to start Jenkins? Answer: You can follow the below-mentioned steps to start Jenkins: 1. Open Command Prompt 2. From the Command Prompt browse the directory where Jenkins. war resides 3. Run the command given below: D:\\>Java \u2013jar Jenkins.war Q#9) What is Jenkinsfile? Answer: The text file where all the definitions of pipelines are defined is called Jenkinsfile. It is being checked in the source control repository. Q#10) What is the difference between Continuous Integration, Continuous Delivery, and Continuous Deployment? Answer: The diagrammatic representation given below can elaborate on the differences between Continuous Integration, Continuous Delivery, and Continuous Deployment more precisely. Continuous Integration: (It involves keeping the latest copy of the source code at a commonly shared hub where all the developers can check to fetch out the latest change in order to avoid conflict.) Continuous Delivery: (Manual Deployment to Production. It does not involve every change to be deployed.) Continuous Deployment: (Automated Deployment to Production. Involves every change to be deployed automatically.) Q#11) What is Jenkins Pipeline? What is a CI CD pipeline? Answer: The pipeline can be defined as the suite of plugins supporting the implementation and integration of continuous delivery pipelines in Jenkins. Continuous integration or continuous delivery pipeline consists of build, deploy, test, release pipeline. The pipeline feature saves a lot of time and error in maintaining the builds. Basically, a pipeline is a group of build jobs that are chained and integrated in sequence. Q#12) What are Scripted Pipelines in Jenkins? Answer: Scripted Pipeline follows Groovy Syntax as given below: Node { } In the above syntax, the node is a part of the Jenkins distributed mode architecture, where there are two types of node, Master which handle all the tasks in the development environment and the Agent is being used to handle multiple tasks individually. Q #13) What are Declarative Pipelines in Jenkins? Answer: Declarative Pipelines are the newest additions to Jenkins that simplify the groovy syntax of Jenkins pipelines (top-level pipeline) with some exceptions, such as: No semicolon to be used as a statement separator. The top-level pipeline should be enclosed within block viz; The common syntax is: pipeline { /* Declarative Pipeline */ } Blocks must contain Sections, Directives, steps or assignments. pipeline { agent any stages { stage(\u2018Build\u2019) { steps { // Statements\u2026 } } stage (\u2018Test\u2019) { steps { // Statements\u2026 } } } } The above code has 3 major elements - Pipeline: The block of script contents. - Agent: Defines where the pipeline will start running from. - Stage: The pipelines contain several steps enclosed in the block called Stage. Q#14) What is SCM? Which SCM tools are supported in Jenkins? Answer: - SCM stands for Source Control Management. - SCM module specifies the source code location. - The entry point to SCM is being specified as jenkins_jobs.scm. - The job specified with \u2018scm\u2019 attribute accepts multiple numbers of SCM definitions. The SCM can be defined as: scm: name: eloc \u2013 scm scm: git: url: ssh://Jenkins.org/eloc.git Jenkins supported SCM tools include: - CVS - Git - Perforce - AccuRev - Subversion - Clearcase - RTC - Mercurial Q#15) Which CI Tools are used in Jenkin? Answer: Jenkins supported the following CI tools: 1. Jenkins 2. GitLab CI 3. Travis CI 4. CircleCI 5. Codeship 6. Go CD 7. TeamCity 8. Bamboo Q#16) Which commands can be used to start Jenkins manually? Answer: You can use the following commands to start Jenkins manually: 1. (Jenkins_url)/restart: To force restart without waiting for build completion. 2. (Jenkin_url)/safeRestart: Waits until all the build gets completed before restarting. Q#17) Which Environmental Directives are used in Jenkins? Answer: Environmental Directives is the sequence that specifies pairs of the key-values called Environmental Variables for the steps in the pipeline. Q#18) What are Triggers? Answer: Trigger in Jenkins defines the way in which the pipeline should be executed frequently. PollSCM, Cron, etc are the currently available Triggers. Q#19) What is Agent Directive in Jenkins? Answer: The Agent is the section that specifies the execution point for the entire pipeline or any specific stage in the pipeline. This section is being specified at the top-level inside the pipeline block. Q#20) How to make sure that your project build does not break in Jenkins? Answer: You need to follow the below-mentioned steps to make sure that the Project build does not break: 1. Clean and successful installation of Jenkins on your local machine with all unit tests. 2. All code changes are reflected successfully. 3. Checking for repository synchronization to make sure that all the differences and changes related to config and other settings are saved in the repository. (adsbygoogle = window.adsbygoogle || []).push({}); Q#21) What is the difference between Maven, Ant, and Jenkins? Answer: Maven vs Jenkins: Maven is a build tool like Ant. It consists of a pom.xml file which is specified in Jenkins to run the code. Whereas, Jenkins is used as a continuous integration tool and automates the deployment process. The reports of the builds can be used to set a mark for continuous delivery as well. Q#22) How will you define Post in Jenkins? Answer: Post is a section that contains several additional steps that might execute after the completion of the pipeline. The execution of all the steps within the condition block depends upon the completion status of the pipeline. The condition block includes the following conditions \u2013 changed success, always, failure, unstable and aborted. Q#23) What are Parameters in Jenkins? Answer: Parameters are supported by the Agent section and are used to support various use-cases pipelines. Parameters are defined at the top-level of the pipeline or inside an individual stage directive. Q#24) How you can set up a Jenkins job? Answer: Setting up a new job in Jenkins is elaborated below with snapshots: Step 1: Go to the Jenkins Dashboard and log in with your registered login credentials. Step 2: Click on the New Item that is shown in the left panel of the page. Step 3: Click on the Freestyle Project from the given list on the upcoming page and specify the item name in the text box. Step 4: Add the URL to the Git Repository. Step 5: Go to the Build section and click on the Add build step => Execute Windows batch command. Step 6: Enter the command in the command window as shown below. Step 7: After saving all the settings and changes click on Build Now. Step 8: To see the status of the build click on Console Output. Q#25) What are the two components (pre-requisites) that Jenkins is mainly integrated with? Answer: Jenkins integrates with: 1. Build tools/ Build working script like Maven script. 2. Version control system/Accessible source code repository like Git repository. Q#26) How can You Clone a Git Repository via Jenkins? Answer: To create a clone repository via Jenkins you need to use your login credentials in the Jenkins System. To achieve the same you need to enter the Jenkins job directory and execute the git config command. Q#27) How can you secure Jenkins? Answer: Securing Jenkins is a little lengthy process, and there are two aspects of securing Jenkins: (i) Access Control which includes authenticating users and giving them an appropriate set of permissions, which can be done in 2 ways. - Security Realm determines a user or a group of users with their passwords. - Authorization Strategy defines what should be accessible to which user. In this case, there might be different types of security based on the permissions granted to the user such as Quick and simple security with easy setup, Standard security setup, Apache front-end security, etc. (ii) Protecting Jenkins users from outside threats. Q#28) How to create a backup and copy files in Jenkins? Answer: In Jenkins, all the settings, build logs and configurations are stored in the JENKINS_HOME directory. Whenever you want to create a backup of your Jenkins you can back up JENKINS_HOME directory frequently. It consists of all the job configurations and slave node configurations. Hence, regularly copying this directory allows us to keep a backup of Jenkins. You can maintain a separate backfile and copy it whenever you need the same. If you want to copy the Jenkins job, then you can do so by simply replicating the job directory. Q#29) What is the use of Backup Plugin in Jenkins? How to use it? Answer: Jenkins Backup Plugin is used to back up the critical configurations and settings in order to use them in the future in case of any failure or as per the need of time. The following steps are followed to back up your settings by using the Backup Plugin. Step 1: Go to the Jenkins Dashboard and click on Manage Jenkins. Step 2: Click on Manage Plugins that appears on the next page. Step 3: Go to Available Tab on the next page and search for ThinBackup. Step 4: Once you choose the available option, it will start installing. Step 5: Once it is installed the following screen will appear, from there choose Settings. Step 6: Enter the necessary details like backup directory along with other options as shown on the below screen and save the settings. The backup will be saved to the specified Backup Directory. Step 7: Go to the previous page to test whether the backup is happening or not by clicking on Backup Now as shown in the below image. Step 8: At last, you can check the Backup Directory specified in the ThinBackup Settings. (Step 6) to check the whole backup Q#30) What is Flow Control in Jenkins? Answer: In Jenkins, flow control follows the pipeline structure (scripted pipeline) that are being executed from the top to bottom of the Jenkins file. Q#31) What is the solution if you find a broken build for your project? Answer: To resolve the broken build follow the below-mentioned steps: - Open console output for the build and check if any file change has missed. OR - Clean and update your local workspace to replicate the problem on the local system and try to resolve it (In case you couldn\u2019t find out the issue in the console output). Q#32) What are the basic requirements for installing Jenkins? Answer: For installing Jenkins you need the following system configuration: 1. Java 7 or above. 2. Servlet 3.1 3. RAM ranging from 200 MB to 70+ GB depending on the project build needs. 4. 2 MB or more of memory. Q#33) How can you define a Continuous Delivery Workflow? Answer: The flowchart below shows the Continuous Delivery Workflow. Hope it will be much easier to understand with visuals. Q#34) What are the various ways in which the build can be scheduled in Jenkins? Answer: The build can be triggered in the following ways: 1. After the completion of other builds. 2. By source code management (modifications) commit. 3. At a specific time. 4. By requesting manual builds. Q#35) Why is Jenkins called a Continuous Delivery Tool? Answer: We have seen the Continuous Delivery workflow in the previous question, now let\u2019s see the step by step process of why Jenkins is being called as a Continuous Delivery Tool: 1. Developers work on their local environment for making changes in the source code and push it into the code repository. 2. When a change is detected, Jenkins performs several tests and code standards to check whether the changes are good to deploy or not. 3. Upon a successful build, it is being viewed by the developers. 4. Then the change is deployed manually on a staging environment where the client can have a look at it. 5. When all the changes get approved by the developers, testers, and clients, the final outcome is saved manually on the production server to be used by the end-users of the product. In this way, Jenkins follows a Continuous Delivery approach and is called the Continuous Delivery Tool. Q#36) Give any simple example of Jenkins script. Answer: This is a Jenkins declarative pipeline code for Java: pipeline { agent stages { stage('Building your first asset') { agent steps { echo 'Build asset' } } stage('Test') { agent steps { echo 'Building project 1' } } } } \uf0c1 (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Jenkins Interview Questions"},{"location":"nightwolf-cotribution/jenkins/#top-jenkins-interview-questions-answers-for-experienced-devops-engineer-solved","text":"We have consolidated Jenkins frequently asked interview questions in DevOps interviews. You will find these questions very helpful in your interviews for DevOps roles. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Q#1) What is Jenkins? Answer: Jenkins is a free open source Continuous Integration tool and automation server to monitor continuous integration and delivery. It is written in Java. It is known as an automated Continuous Delivery tool that helps to build and test the software system with easy integration of changes to the system. Jenkins follows Groovy Scripting. Also, it enables developers to continuously check in their code and also analyze the postbuild actions. The automation testers can use to run their tests as soon as the new code is added or code is modified. Q#2) What are the features of Jenkins? Answer: Jenkins comes with the following features: 1. Free open source. 2. Easy installation on various operating systems. 3. Build Pipeline Support. 4. Workflow Plugin. 5. Test harness built around JUnit. 6. Easy upgrades. 7. Rapid release cycle. 8. Easy configuration setup. 9. Extensible with the use of third-party plugins. Q#3) What are the advantages of Jenkins? Why we use Jenkins? Answer: Jenkins is used to continuously monitor the large code base in real-time. It enables developers to find bugs in their code and fix them. Email notifications are made to the developers regarding their check-ins as a post-build action. Advantages of Jenkins are as follows: - Build failures are cached during the integration stage. - Notifies the developers about build report status using LDAP (Lightweight Directory Access Protocol) mail server. - Maven release project is automated with simple steps. - Easy bug tracking. - Automatic changes get updated in the build report with notification. - Supports Continuous Integration in agile development and test-driven development. Q#4) Mention some of the important plugins in Jenkins? Answer: Plugins in Jenkins includes: - Gits - Maven 2 Project - HTML Publisher - Copy Artcraft - Join - Green Balls - Amazon EC2 Q#5) What is Continuous Integration in Jenkins? Answer: Continuous integration is the process of continuously checking-in the developer code into a version control system and triggering the build to check and identify bugs in the written code. This is a very quick process and also gives them a chance to fix the bugs. Jenkins is one such continuous integration tool. In software development, multiple developers work on different software modules. While performing integration testing all the modules are being integrated together. It is considered as the development practice to integrate the code into the source repository Whenever the programmer/developer makes any change to the current code, then it automatically gets integrated with the system running on the tester\u2019s machine and makes the testing task easy and speedy for the system testers. Continuous Integration comprises of: - Development and Compilation - Database Integration - Unit Testing - Production Deployment - Code Labeling - Functional Testing - Generating and Analyzing Reports Q#6) What is the difference between Hudson and Jenkins? Answer: There is no difference between Hudson and Jenkins. Hudson was the former name of Jenkins, after going through several issues the name was changed to Jenkins. Q#7) What is Groovy in Jenkins? Answer: Groovy is the default scripting language that is being used in the development of JMeter Version 3.1. Currently Apache Groovy is the dynamic object-oriented programming language that is used as a scripting language for the Java platform. Apache Groovy comes with some useful features such as Java Compatibility and Development Support. Q#8) Which command is used to start Jenkins? Answer: You can follow the below-mentioned steps to start Jenkins: 1. Open Command Prompt 2. From the Command Prompt browse the directory where Jenkins. war resides 3. Run the command given below: D:\\>Java \u2013jar Jenkins.war Q#9) What is Jenkinsfile? Answer: The text file where all the definitions of pipelines are defined is called Jenkinsfile. It is being checked in the source control repository. Q#10) What is the difference between Continuous Integration, Continuous Delivery, and Continuous Deployment? Answer: The diagrammatic representation given below can elaborate on the differences between Continuous Integration, Continuous Delivery, and Continuous Deployment more precisely. Continuous Integration: (It involves keeping the latest copy of the source code at a commonly shared hub where all the developers can check to fetch out the latest change in order to avoid conflict.) Continuous Delivery: (Manual Deployment to Production. It does not involve every change to be deployed.) Continuous Deployment: (Automated Deployment to Production. Involves every change to be deployed automatically.) Q#11) What is Jenkins Pipeline? What is a CI CD pipeline? Answer: The pipeline can be defined as the suite of plugins supporting the implementation and integration of continuous delivery pipelines in Jenkins. Continuous integration or continuous delivery pipeline consists of build, deploy, test, release pipeline. The pipeline feature saves a lot of time and error in maintaining the builds. Basically, a pipeline is a group of build jobs that are chained and integrated in sequence. Q#12) What are Scripted Pipelines in Jenkins? Answer: Scripted Pipeline follows Groovy Syntax as given below: Node { } In the above syntax, the node is a part of the Jenkins distributed mode architecture, where there are two types of node, Master which handle all the tasks in the development environment and the Agent is being used to handle multiple tasks individually. Q #13) What are Declarative Pipelines in Jenkins? Answer: Declarative Pipelines are the newest additions to Jenkins that simplify the groovy syntax of Jenkins pipelines (top-level pipeline) with some exceptions, such as: No semicolon to be used as a statement separator. The top-level pipeline should be enclosed within block viz; The common syntax is: pipeline { /* Declarative Pipeline */ } Blocks must contain Sections, Directives, steps or assignments. pipeline { agent any stages { stage(\u2018Build\u2019) { steps { // Statements\u2026 } } stage (\u2018Test\u2019) { steps { // Statements\u2026 } } } } The above code has 3 major elements - Pipeline: The block of script contents. - Agent: Defines where the pipeline will start running from. - Stage: The pipelines contain several steps enclosed in the block called Stage. Q#14) What is SCM? Which SCM tools are supported in Jenkins? Answer: - SCM stands for Source Control Management. - SCM module specifies the source code location. - The entry point to SCM is being specified as jenkins_jobs.scm. - The job specified with \u2018scm\u2019 attribute accepts multiple numbers of SCM definitions. The SCM can be defined as: scm: name: eloc \u2013 scm scm: git: url: ssh://Jenkins.org/eloc.git Jenkins supported SCM tools include: - CVS - Git - Perforce - AccuRev - Subversion - Clearcase - RTC - Mercurial Q#15) Which CI Tools are used in Jenkin? Answer: Jenkins supported the following CI tools: 1. Jenkins 2. GitLab CI 3. Travis CI 4. CircleCI 5. Codeship 6. Go CD 7. TeamCity 8. Bamboo Q#16) Which commands can be used to start Jenkins manually? Answer: You can use the following commands to start Jenkins manually: 1. (Jenkins_url)/restart: To force restart without waiting for build completion. 2. (Jenkin_url)/safeRestart: Waits until all the build gets completed before restarting. Q#17) Which Environmental Directives are used in Jenkins? Answer: Environmental Directives is the sequence that specifies pairs of the key-values called Environmental Variables for the steps in the pipeline. Q#18) What are Triggers? Answer: Trigger in Jenkins defines the way in which the pipeline should be executed frequently. PollSCM, Cron, etc are the currently available Triggers. Q#19) What is Agent Directive in Jenkins? Answer: The Agent is the section that specifies the execution point for the entire pipeline or any specific stage in the pipeline. This section is being specified at the top-level inside the pipeline block. Q#20) How to make sure that your project build does not break in Jenkins? Answer: You need to follow the below-mentioned steps to make sure that the Project build does not break: 1. Clean and successful installation of Jenkins on your local machine with all unit tests. 2. All code changes are reflected successfully. 3. Checking for repository synchronization to make sure that all the differences and changes related to config and other settings are saved in the repository. (adsbygoogle = window.adsbygoogle || []).push({}); Q#21) What is the difference between Maven, Ant, and Jenkins? Answer: Maven vs Jenkins: Maven is a build tool like Ant. It consists of a pom.xml file which is specified in Jenkins to run the code. Whereas, Jenkins is used as a continuous integration tool and automates the deployment process. The reports of the builds can be used to set a mark for continuous delivery as well. Q#22) How will you define Post in Jenkins? Answer: Post is a section that contains several additional steps that might execute after the completion of the pipeline. The execution of all the steps within the condition block depends upon the completion status of the pipeline. The condition block includes the following conditions \u2013 changed success, always, failure, unstable and aborted. Q#23) What are Parameters in Jenkins? Answer: Parameters are supported by the Agent section and are used to support various use-cases pipelines. Parameters are defined at the top-level of the pipeline or inside an individual stage directive. Q#24) How you can set up a Jenkins job? Answer: Setting up a new job in Jenkins is elaborated below with snapshots: Step 1: Go to the Jenkins Dashboard and log in with your registered login credentials. Step 2: Click on the New Item that is shown in the left panel of the page. Step 3: Click on the Freestyle Project from the given list on the upcoming page and specify the item name in the text box. Step 4: Add the URL to the Git Repository. Step 5: Go to the Build section and click on the Add build step => Execute Windows batch command. Step 6: Enter the command in the command window as shown below. Step 7: After saving all the settings and changes click on Build Now. Step 8: To see the status of the build click on Console Output. Q#25) What are the two components (pre-requisites) that Jenkins is mainly integrated with? Answer: Jenkins integrates with: 1. Build tools/ Build working script like Maven script. 2. Version control system/Accessible source code repository like Git repository. Q#26) How can You Clone a Git Repository via Jenkins? Answer: To create a clone repository via Jenkins you need to use your login credentials in the Jenkins System. To achieve the same you need to enter the Jenkins job directory and execute the git config command. Q#27) How can you secure Jenkins? Answer: Securing Jenkins is a little lengthy process, and there are two aspects of securing Jenkins: (i) Access Control which includes authenticating users and giving them an appropriate set of permissions, which can be done in 2 ways. - Security Realm determines a user or a group of users with their passwords. - Authorization Strategy defines what should be accessible to which user. In this case, there might be different types of security based on the permissions granted to the user such as Quick and simple security with easy setup, Standard security setup, Apache front-end security, etc. (ii) Protecting Jenkins users from outside threats. Q#28) How to create a backup and copy files in Jenkins? Answer: In Jenkins, all the settings, build logs and configurations are stored in the JENKINS_HOME directory. Whenever you want to create a backup of your Jenkins you can back up JENKINS_HOME directory frequently. It consists of all the job configurations and slave node configurations. Hence, regularly copying this directory allows us to keep a backup of Jenkins. You can maintain a separate backfile and copy it whenever you need the same. If you want to copy the Jenkins job, then you can do so by simply replicating the job directory. Q#29) What is the use of Backup Plugin in Jenkins? How to use it? Answer: Jenkins Backup Plugin is used to back up the critical configurations and settings in order to use them in the future in case of any failure or as per the need of time. The following steps are followed to back up your settings by using the Backup Plugin. Step 1: Go to the Jenkins Dashboard and click on Manage Jenkins. Step 2: Click on Manage Plugins that appears on the next page. Step 3: Go to Available Tab on the next page and search for ThinBackup. Step 4: Once you choose the available option, it will start installing. Step 5: Once it is installed the following screen will appear, from there choose Settings. Step 6: Enter the necessary details like backup directory along with other options as shown on the below screen and save the settings. The backup will be saved to the specified Backup Directory. Step 7: Go to the previous page to test whether the backup is happening or not by clicking on Backup Now as shown in the below image. Step 8: At last, you can check the Backup Directory specified in the ThinBackup Settings. (Step 6) to check the whole backup Q#30) What is Flow Control in Jenkins? Answer: In Jenkins, flow control follows the pipeline structure (scripted pipeline) that are being executed from the top to bottom of the Jenkins file. Q#31) What is the solution if you find a broken build for your project? Answer: To resolve the broken build follow the below-mentioned steps: - Open console output for the build and check if any file change has missed. OR - Clean and update your local workspace to replicate the problem on the local system and try to resolve it (In case you couldn\u2019t find out the issue in the console output). Q#32) What are the basic requirements for installing Jenkins? Answer: For installing Jenkins you need the following system configuration: 1. Java 7 or above. 2. Servlet 3.1 3. RAM ranging from 200 MB to 70+ GB depending on the project build needs. 4. 2 MB or more of memory. Q#33) How can you define a Continuous Delivery Workflow? Answer: The flowchart below shows the Continuous Delivery Workflow. Hope it will be much easier to understand with visuals. Q#34) What are the various ways in which the build can be scheduled in Jenkins? Answer: The build can be triggered in the following ways: 1. After the completion of other builds. 2. By source code management (modifications) commit. 3. At a specific time. 4. By requesting manual builds. Q#35) Why is Jenkins called a Continuous Delivery Tool? Answer: We have seen the Continuous Delivery workflow in the previous question, now let\u2019s see the step by step process of why Jenkins is being called as a Continuous Delivery Tool: 1. Developers work on their local environment for making changes in the source code and push it into the code repository. 2. When a change is detected, Jenkins performs several tests and code standards to check whether the changes are good to deploy or not. 3. Upon a successful build, it is being viewed by the developers. 4. Then the change is deployed manually on a staging environment where the client can have a look at it. 5. When all the changes get approved by the developers, testers, and clients, the final outcome is saved manually on the production server to be used by the end-users of the product. In this way, Jenkins follows a Continuous Delivery approach and is called the Continuous Delivery Tool. Q#36) Give any simple example of Jenkins script. Answer: This is a Jenkins declarative pipeline code for Java: pipeline { agent stages { stage('Building your first asset') { agent steps { echo 'Build asset' } } stage('Test') { agent steps { echo 'Building project 1' } } } }","title":"Top Jenkins Interview Questions &amp; answers for Experienced DevOps Engineer - Solved"},{"location":"nightwolf-cotribution/jenkins/#_1","text":"(adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":""},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/","text":"Kubernetes Cheat Sheet This article consists a list of kubectl commands which are helpful for your day to day job. (adsbygoogle = window.adsbygoogle || []).push({}); Creating Objects \uf0c1 Name Command Create resource kubectl apply -f file_name.yaml Create from multiple files kubectl apply -f file_name_1.yaml -f file_name_2.yaml Create all files in directory kubectl apply -f directory_name Create from url kubectl apply -f https://url/uri Create pod kubectl run pod_name --image image_name Create pod, then expose it as service kubectl run pod_name --image image_name --port port --expose Create pod yaml file kubectl run pod_name --image image_name --dry-run=client -o yaml > file_name.yaml Create deployment kubectl create deployment deployment_name --image image_name Create deployment yaml file kubectl create deployment deployment_name --image image_name --dry-run=client -o yaml > file_name.yaml Create service kubectl create service service-type service_name --tcp=port:target_port Create service yaml file kubectl create service service-type service_name --tcp=port:target_port --dry-run=client -o yaml > file_name.yaml Expose service from pod/deployment kubectl expose deployment {pod/deployment_name} --type=service-type --port port --target-port target_port Create config map from key-value kubectl create configmap configmap_name --from-literal={key:value} --from-literal={key:value} Create config map from file kubectl create configmap configmap_name --from-file=file_name Create config map from env file kubectl create configmap configmap_name --from-env-file=file_name Create secret from key-value kubectl create secret generic secret_name --from-literal={key:value} --from-literal={key:value} Create secret from file kubectl create secret generic secret_name --from-file=file_name Create job kubectl create job job_name --image=image_name Create job from cronjob kubectl create job job_name --from=cronjob/cronjob-name Create cronjob kubectl create cronjob --image=image_name --schedule='cron-syntax' -- {command} Monitoring Usage Commands \uf0c1 Name Command Get node cpu and memory utilization kubectl top node node_name Get pod cpu and memory utilization kubectl top pods pod_name Node Commands \uf0c1 Name Command Describe node kubectl describe node node_name Get node in yaml kubectl get node node_name -o yaml Get node kubectl get node node_name Drain node kubectl drain node node_name Cordon node kubectl cordon node node_name Uncordon node kubectl uncordon node node_name Pod Commands \uf0c1 Name Command Get pod kubectl get pod pod_name Get pod in yaml kubectl get pod pod_name -o yaml Get pod wide information kubectl get pod pod_name -o wide Get pod with watch kubectl get pod pod_name -w Edit pod kubectl edit pod pod_name Describe pod kubectl describe pod pod_name Delete pod kubectl delete pod pod_name Log pod kubectl logs pod pod_name Tail -f pod kubectl logs pod -f pod_name Execute into pod kubectl exec -it pod pod_name /bin/bash Running Temporary Image kubectl run pod_name --image=curlimages/curl --rm -it --restart=Never -- curl {destination Deployment Commands \uf0c1 Name Command Get deployment kubectl get deployment deployment_name Get deployment in yaml kubectl get deployment deployment_name -o yaml Get deployment wide information kubectl get deployment deployment_name -o wide Edit deployment kubectl edit deployment deployment_name Describe deployment kubectl describe deployment deployment_name Delete deployment kubectl delete deployment deployment_name Log deployment kubectl logs deployment/deployment_name -f Update image kubectl set image deployment deployment_name container_name=new_image_name Scale deployment with replicas kubectl scale deployment deployment_name --replicas replicas_name Service Commands \uf0c1 Name Command Get service kubectl get service service_name Get service in yaml Get service wide information kubectl get service service_name -o wide Edit service kubectl edit service service_name Describe service kubectl describe service service_name Delete service kubectl delete service service_name Endpoints Commands \uf0c1 Name Command Get endpoints kubectl get endpoints endpoints_name Ingress Commands \uf0c1 Name Command Get ingress kubectl get ingress Get ingress in yaml kubectl get ingress -o yaml Get ingress wide information kubectl get ingress -o wide Edit ingress kubectl edit ingress ingress_name Describe ingress kubectl describe ingress ingress_name Delete ingress kubectl delete ingress ingress_name DaemonSet Commands \uf0c1 Name Command Get daemonset kubectl get daemonset daemonset_name Get daemonset in yaml kubectl get daemonset daemonset_name -o yaml Edit daemonset kubectl edit daemonset daemonset_name Describe daemonset kubectl describe daemonset daemonset_name Delete daemonset kubectl delete deployment daemonset_name StatefulSet Commands \uf0c1 Name Command Get statefulset kubectl get statefulset statefulset_name Get statefulset in yaml kubectl get statefulset statefulset_name -o yaml Edit statefulset kubectl edit statefulset statefulset_name Describe statefulset kubectl describe statefulset statefulset_name Delete statefuleset kubectl delete statefulset statefulset_name ConfigMaps Commands \uf0c1 Name Command Get configmap kubectl get configmap configmap_name Get configmap in yaml kubectl get configmap configmap_name -o yaml Edit configmap kubectl edit configmap configmap_name Describe configmap kubectl describe configmap configmap_name Delete configmap kubectl delete configmap configmap_name Secret Commands \uf0c1 Name Command Get secret kubectl get secret secret_name Get secret in yaml kubectl get secret secret_name -o yaml Edit secret kubectl edit secret secret_name Describe secret kubectl describe secret secret_name Delete secret kubectl delete secret secret_name Rollout Commands \uf0c1 Name Command Restart deployment kubectl rollout restart deployment deployment_name Undo deployment with the latest revision kubectl rollout undo deployment deployment_name Undo deployment with specified revision kubectl rollout undo deployment deployment_name --to-revision revision_number Get all revisions of deployment kubectl rollout history deployment deployment_name Get specified revision of deployment kubectl rollout history deployment deployment_name --revision= revision_number Job Commands \uf0c1 Name Command Get job kubectl get job job_name Get job in yaml kubectl get job job_name -o yaml Edit job in yaml kubectl edit job job_name Describe job kubectl describe job job_name Delete job kubectl delete job job_name Cronjob Commands \uf0c1 Name Command Get cronjob kubectl get cronjob cronjob_name Get cronjob in yaml kubectl get cronjob cronjob_name -o yaml Edit cronjob kubectl edit cronjob cronjob_name Describe cronjob kubectl describe cronjob cronjob_name Delete cronjob kubectl delete cronjob cronjob_name Network Policy Commands \uf0c1 Name Command Get networkpolicy kubectl get networkpolicy networkpolicy_name Get networkpolicy in yaml kubectl get networkpolicy networkpolicy_name -o yaml Get networkpolicy wide information kubectl get networkpolicy networkpolicy_name -o wide Edit networkpolicy kubectl edit networkpolicy networkpolicy_name Describe networkpolicy kubectl describe networkpolicy networkpolicy_name Delete networkpolicy kubectl delete networkpolicy networkpolicy_name Labels and Selectors Commands \uf0c1 Name Command Show labels of node,pod and deployment kubectl get {node/pod/deployment} --show-labels Attach labels to kubectl label {node/pod/deployment} pod_name {key}={value} Remove labels from kubectl label {node/pod/deployment} {pod_name} {key}- Select node,pod and deployment by using labels kubectl get {node/pod/deployment} -l {key}={value} (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Kubernetes CheatSheet"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#creating-objects","text":"Name Command Create resource kubectl apply -f file_name.yaml Create from multiple files kubectl apply -f file_name_1.yaml -f file_name_2.yaml Create all files in directory kubectl apply -f directory_name Create from url kubectl apply -f https://url/uri Create pod kubectl run pod_name --image image_name Create pod, then expose it as service kubectl run pod_name --image image_name --port port --expose Create pod yaml file kubectl run pod_name --image image_name --dry-run=client -o yaml > file_name.yaml Create deployment kubectl create deployment deployment_name --image image_name Create deployment yaml file kubectl create deployment deployment_name --image image_name --dry-run=client -o yaml > file_name.yaml Create service kubectl create service service-type service_name --tcp=port:target_port Create service yaml file kubectl create service service-type service_name --tcp=port:target_port --dry-run=client -o yaml > file_name.yaml Expose service from pod/deployment kubectl expose deployment {pod/deployment_name} --type=service-type --port port --target-port target_port Create config map from key-value kubectl create configmap configmap_name --from-literal={key:value} --from-literal={key:value} Create config map from file kubectl create configmap configmap_name --from-file=file_name Create config map from env file kubectl create configmap configmap_name --from-env-file=file_name Create secret from key-value kubectl create secret generic secret_name --from-literal={key:value} --from-literal={key:value} Create secret from file kubectl create secret generic secret_name --from-file=file_name Create job kubectl create job job_name --image=image_name Create job from cronjob kubectl create job job_name --from=cronjob/cronjob-name Create cronjob kubectl create cronjob --image=image_name --schedule='cron-syntax' -- {command}","title":"Creating Objects"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#monitoring-usage-commands","text":"Name Command Get node cpu and memory utilization kubectl top node node_name Get pod cpu and memory utilization kubectl top pods pod_name","title":"Monitoring Usage Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#node-commands","text":"Name Command Describe node kubectl describe node node_name Get node in yaml kubectl get node node_name -o yaml Get node kubectl get node node_name Drain node kubectl drain node node_name Cordon node kubectl cordon node node_name Uncordon node kubectl uncordon node node_name","title":"Node Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#pod-commands","text":"Name Command Get pod kubectl get pod pod_name Get pod in yaml kubectl get pod pod_name -o yaml Get pod wide information kubectl get pod pod_name -o wide Get pod with watch kubectl get pod pod_name -w Edit pod kubectl edit pod pod_name Describe pod kubectl describe pod pod_name Delete pod kubectl delete pod pod_name Log pod kubectl logs pod pod_name Tail -f pod kubectl logs pod -f pod_name Execute into pod kubectl exec -it pod pod_name /bin/bash Running Temporary Image kubectl run pod_name --image=curlimages/curl --rm -it --restart=Never -- curl {destination","title":"Pod Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#deployment-commands","text":"Name Command Get deployment kubectl get deployment deployment_name Get deployment in yaml kubectl get deployment deployment_name -o yaml Get deployment wide information kubectl get deployment deployment_name -o wide Edit deployment kubectl edit deployment deployment_name Describe deployment kubectl describe deployment deployment_name Delete deployment kubectl delete deployment deployment_name Log deployment kubectl logs deployment/deployment_name -f Update image kubectl set image deployment deployment_name container_name=new_image_name Scale deployment with replicas kubectl scale deployment deployment_name --replicas replicas_name","title":"Deployment Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#service-commands","text":"Name Command Get service kubectl get service service_name Get service in yaml Get service wide information kubectl get service service_name -o wide Edit service kubectl edit service service_name Describe service kubectl describe service service_name Delete service kubectl delete service service_name","title":"Service Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#endpoints-commands","text":"Name Command Get endpoints kubectl get endpoints endpoints_name","title":"Endpoints Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#ingress-commands","text":"Name Command Get ingress kubectl get ingress Get ingress in yaml kubectl get ingress -o yaml Get ingress wide information kubectl get ingress -o wide Edit ingress kubectl edit ingress ingress_name Describe ingress kubectl describe ingress ingress_name Delete ingress kubectl delete ingress ingress_name","title":"Ingress Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#daemonset-commands","text":"Name Command Get daemonset kubectl get daemonset daemonset_name Get daemonset in yaml kubectl get daemonset daemonset_name -o yaml Edit daemonset kubectl edit daemonset daemonset_name Describe daemonset kubectl describe daemonset daemonset_name Delete daemonset kubectl delete deployment daemonset_name","title":"DaemonSet Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#statefulset-commands","text":"Name Command Get statefulset kubectl get statefulset statefulset_name Get statefulset in yaml kubectl get statefulset statefulset_name -o yaml Edit statefulset kubectl edit statefulset statefulset_name Describe statefulset kubectl describe statefulset statefulset_name Delete statefuleset kubectl delete statefulset statefulset_name","title":"StatefulSet Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#configmaps-commands","text":"Name Command Get configmap kubectl get configmap configmap_name Get configmap in yaml kubectl get configmap configmap_name -o yaml Edit configmap kubectl edit configmap configmap_name Describe configmap kubectl describe configmap configmap_name Delete configmap kubectl delete configmap configmap_name","title":"ConfigMaps Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#secret-commands","text":"Name Command Get secret kubectl get secret secret_name Get secret in yaml kubectl get secret secret_name -o yaml Edit secret kubectl edit secret secret_name Describe secret kubectl describe secret secret_name Delete secret kubectl delete secret secret_name","title":"Secret Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#rollout-commands","text":"Name Command Restart deployment kubectl rollout restart deployment deployment_name Undo deployment with the latest revision kubectl rollout undo deployment deployment_name Undo deployment with specified revision kubectl rollout undo deployment deployment_name --to-revision revision_number Get all revisions of deployment kubectl rollout history deployment deployment_name Get specified revision of deployment kubectl rollout history deployment deployment_name --revision= revision_number","title":"Rollout Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#job-commands","text":"Name Command Get job kubectl get job job_name Get job in yaml kubectl get job job_name -o yaml Edit job in yaml kubectl edit job job_name Describe job kubectl describe job job_name Delete job kubectl delete job job_name","title":"Job Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#cronjob-commands","text":"Name Command Get cronjob kubectl get cronjob cronjob_name Get cronjob in yaml kubectl get cronjob cronjob_name -o yaml Edit cronjob kubectl edit cronjob cronjob_name Describe cronjob kubectl describe cronjob cronjob_name Delete cronjob kubectl delete cronjob cronjob_name","title":"Cronjob Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#network-policy-commands","text":"Name Command Get networkpolicy kubectl get networkpolicy networkpolicy_name Get networkpolicy in yaml kubectl get networkpolicy networkpolicy_name -o yaml Get networkpolicy wide information kubectl get networkpolicy networkpolicy_name -o wide Edit networkpolicy kubectl edit networkpolicy networkpolicy_name Describe networkpolicy kubectl describe networkpolicy networkpolicy_name Delete networkpolicy kubectl delete networkpolicy networkpolicy_name","title":"Network Policy Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#labels-and-selectors-commands","text":"Name Command Show labels of node,pod and deployment kubectl get {node/pod/deployment} --show-labels Attach labels to kubectl label {node/pod/deployment} pod_name {key}={value} Remove labels from kubectl label {node/pod/deployment} {pod_name} {key}- Select node,pod and deployment by using labels kubectl get {node/pod/deployment} -l {key}={value} (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Labels and Selectors Commands"},{"location":"nightwolf-cotribution/kubernetes_interview_questions/","text":"Kubernetes Interview Questions \uf0c1 We have prepared a set of frequently asked Kubernetes questions to help Freshers and Experienced Admins in their preparations for Interview. You will find these questions very helpful in your Linux/Ansible/DevOps/Kubernetes Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is the architecture of kubernetes ? What does control manager, etcd, scheduler, API server do ? What is a manifest file and what are the components of it ? What is node affinity, pod afiinity , taint toleration ? What is node port, cluster ip ? What is persitant volumes and why we use it ? Describe what is pod and what is pod lifecycle. What are the components on master and worked node ? What is ingress controller ? What are types of services in kuberntes ? How one pod talks with other pod ? How the pod healcheck is done(describe rediness, livesness) ? How the monitoring is done(integration on Prometheus and grafana) ? What is deamonset, replicaset, horizontal pod autoscaler ? Write a manifest file of your own choice. What is namespace and why we use it ? What are helm charts and uses ? (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Kubernetes Interview Questions"},{"location":"nightwolf-cotribution/kubernetes_interview_questions/#kubernetes-interview-questions","text":"We have prepared a set of frequently asked Kubernetes questions to help Freshers and Experienced Admins in their preparations for Interview. You will find these questions very helpful in your Linux/Ansible/DevOps/Kubernetes Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is the architecture of kubernetes ? What does control manager, etcd, scheduler, API server do ? What is a manifest file and what are the components of it ? What is node affinity, pod afiinity , taint toleration ? What is node port, cluster ip ? What is persitant volumes and why we use it ? Describe what is pod and what is pod lifecycle. What are the components on master and worked node ? What is ingress controller ? What are types of services in kuberntes ? How one pod talks with other pod ? How the pod healcheck is done(describe rediness, livesness) ? How the monitoring is done(integration on Prometheus and grafana) ? What is deamonset, replicaset, horizontal pod autoscaler ? Write a manifest file of your own choice. What is namespace and why we use it ? What are helm charts and uses ? (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Kubernetes Interview Questions"},{"location":"nightwolf-cotribution/linux_L1/","text":"Linux Admin Interview Questions and Answers for Freshers and Experienced - Solved \uf0c1 We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); 1. What did you learn yesterday/this week? 2. Describe the general file system hierarchy of a Linux system. 3. What is the name and the UID of the administrator user in Linux/Unix? Hint => name of user is root and uid is 1. 4. How to list all files, including hidden ones, in a directory? 5. What is the Unix/Linux command to remove a directory and its contents? Hint => rm -rf DIR_NAME 6. Which command will show you free/used memory? Does free memory exist on Linux? Hint => free -m 7. How to search for the string \"my konfu is the best\" in files of a directory recursively? Hint => grep -rin \"my konfu is the best\" DIR_NAME 8. How to connect to a remote server or what is SSH? Hint => We can use ssh command to connect to remote server. command : ssh USER@10.0.0.1 9. How to get all environment variables and how can you use them? Hint => env command print all the environment variables associated with user. 10. I get \"command not found\" when I run ifconfig -a. What can be wrong? Hint => a). Either ifconfig command binary is not present in /usr/sbin/ or /sbin. b). Or PATH environment for user variable do not have /usr/sbin/ and /sbin paths. 11. What happens if I type TAB-TAB? 12. What command will show the available disk space on the Unix/Linux system? Hint => df -h 13. What commands do you know that can be used to check DNS records? Hint => a) nslookup b) dig c) host 14. What Unix/Linux commands will alter a files ownership, files permissions? Hint => a) To change file ownership: chown b) To change file permissions: chmod 15. What does chmod +x FILENAME do? Hint => It will provide shell executable permissions for FILENAME to everybody. 16. What does the permission 0750 on a file mean? 17. What does the permission 0750 on a directory mean? 18. How to add a new system user without login permissions? Hint => useradd -s /sbin/nologin tony OR useradd -M tony usermod -L tony 19. How to add/remove a group from a user? Hint => # gpasswd -d user_name group_name 20. What is a bash alias? Hint => An alias is a (usually short) name that the shell translates into another (usually longer) name or command. Aliases allow you to define new commands by substituting a string for the first token of a simple command 21. How do you set the mail address of the root/a user? Hint => The system administrator can define email aliases in the file /etc/aliases. This file contains lines like root: cwd@mailhost.example.com, /root/mailbox; the effect is the same as having cwd@mailhost.example.com, root/mailbox in ~root/.forward. You may need to run a program such as newaliases after changing /etc/aliases. 22. What does CTRL-c do? Hint => It sends the SIGINT signal to the process, which is technically just a request\u2014most processes will honor it, but some may ignore it. 23. What does CTRL-d do? Hint => Close the bash shell. This sends an EOF (End-of-file) marker to bash, and bash exits. 24. What does CTRL-z do? Hint => Suspend the current foreground process running in bash. This sends the SIGTSTP signal to the process. To return the process to the foreground later, use the fg process_name command. 25. What is in /etc/services? Hint => /etc/services contains port number mapping with service name. 26. How to redirect STDOUT and STDERR in bash? Hint => a) for all output : &> /dev/null b) To redirect the standard error (stderr): 2> /dev/null c) To redirect the standard output(stdout): 1> /dev/null (adsbygoogle = window.adsbygoogle || []).push({}); 27. What is the difference between UNIX and Linux. 28. What is the difference between Telnet and SSH? 29. Which difference have between public and private SSH keys? 30. Explain the three load averages and what do they indicate. What command can be used to view the load averages? 31. Can you name a lower-case letter that is not a valid option for GNU ls? 32. What is a Linux kernel module? 33. Walk me through the steps in booting into single user mode to troubleshoot a problem. 34. Walk me through the steps you'd take to troubleshoot a 404 error on a web application you administer. 35. What is ICMP protocol? Why do you need to use? 36. What do the following commands do and how would you use them? tee awk tr cut tac curl wget watch head tail less cat touch sar netstat tcpdump lsof 37. What does an & after a command do? 38. What does & disown after a command do? 39. What is a packet filter and how does it work? 40. What is Virtual Memory? 41. What is swap and what is it used for? 42. What is an A record, an NS record, a PTR record, a CNAME record, an MX record? 43. Are there any other RRs and what are they used for? 44. What is a Split-Horizon DNS? 45. What is the sticky bit? 46. What does the immutable bit do to a file? 47. What is the difference between hardlinks and symlinks? What happens when you remove the source to a symlink/hardlink? 48. What is an inode and what fields are stored in an inode? 49. How to force/trigger a file system check on next reboot? 50. What is SNMP and what is it used for? (adsbygoogle = window.adsbygoogle || []).push({}); 51. What is a runlevel and how to get the current runlevel? 52. What is SSH port forwarding? 53. What is the difference between local and remote port forwarding? 54. What are the steps to add a user to a system without using useradd/adduser? 55. What is MAJOR and MINOR numbers of special files? 56. Describe the mknod command and when you would use it. 57. Describe a scenario when you get a \"filesystem is full\" error, but \"df\" shows there is free space. 58. Describe a scenario when deleting a file, but \"df\" not showing the space being freed. 59. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. 60. Different types of file systems? Hint => ext2, ext3, ext4, xfs, vfat etc. 61. Difference between ext4 and xfs? 62. When v create user which files are referred? 63. Differnce between passwd and shadow file? Hint => /etc/passwd contains details abou User like home directory, shell etc. /etc/shadow conatains User password hashes. 64. What are the contents in passwd file? Hint => root:x:0:0:root:/root:/bin/bash \u2022 login name \u2022 optional encrypted password \u2022 numerical user ID \u2022 numerical group ID \u2022 user name or comment field \u2022 user home directory \u2022 optional user command interpreter 65. List all users who are not given passwd? with cmd Hint => cat /etc/shadow| awk -F \":\" '$2 ~ /[a-z]/ {print $1}' 66. How to check what processess are running on which port? Hint => Using \"netstat\" and \"ss\" command we can relate processes with port. 67. How to extend partition in lvm? and indepth questions in lvm ? Hint => 1. First attach extra storage/disk to server. 2. Create a new partition on the disk using \"fdisk\" or \"parted\" command. 3. Create a new Physical Volume(PV) on that new partition. e.g. \"pvcreate /dev/sdb1\" 4. Exetend existing Volume Group(VG) using new PV. e.g. \"vgextend vg_name /dev/sdb1\" 5. Now extend the LV. e.g . \"lvextend -l 100%FREE /dev/mapper/vg_name-lv_name\" 6. now execute \"resize2fs\" or \"xfs_growfs\". 68. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses, 69. What function does DNS play on a network? 70. On which port dns works? Hint => DNS works on port 53. 71. What is HTTP? 72. What is an HTTP proxy and how does it work? 73. Describe briefly how HTTPS works. 74. What is SMTP? Give the basic scenario of how a mail message is delivered via SMTP. 75. What is RAID? What is RAID0, RAID1, RAID5, RAID10? 76. What is a level 0 backup? What is an incremental backup? Next Page (adsbygoogle = window.adsbygoogle || []).push({}); (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Linux Interview Questions for Freshers and Experienced - L1"},{"location":"nightwolf-cotribution/linux_L1/#linux-admin-interview-questions-and-answers-for-freshers-and-experienced-solved","text":"We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); 1. What did you learn yesterday/this week? 2. Describe the general file system hierarchy of a Linux system. 3. What is the name and the UID of the administrator user in Linux/Unix? Hint => name of user is root and uid is 1. 4. How to list all files, including hidden ones, in a directory? 5. What is the Unix/Linux command to remove a directory and its contents? Hint => rm -rf DIR_NAME 6. Which command will show you free/used memory? Does free memory exist on Linux? Hint => free -m 7. How to search for the string \"my konfu is the best\" in files of a directory recursively? Hint => grep -rin \"my konfu is the best\" DIR_NAME 8. How to connect to a remote server or what is SSH? Hint => We can use ssh command to connect to remote server. command : ssh USER@10.0.0.1 9. How to get all environment variables and how can you use them? Hint => env command print all the environment variables associated with user. 10. I get \"command not found\" when I run ifconfig -a. What can be wrong? Hint => a). Either ifconfig command binary is not present in /usr/sbin/ or /sbin. b). Or PATH environment for user variable do not have /usr/sbin/ and /sbin paths. 11. What happens if I type TAB-TAB? 12. What command will show the available disk space on the Unix/Linux system? Hint => df -h 13. What commands do you know that can be used to check DNS records? Hint => a) nslookup b) dig c) host 14. What Unix/Linux commands will alter a files ownership, files permissions? Hint => a) To change file ownership: chown b) To change file permissions: chmod 15. What does chmod +x FILENAME do? Hint => It will provide shell executable permissions for FILENAME to everybody. 16. What does the permission 0750 on a file mean? 17. What does the permission 0750 on a directory mean? 18. How to add a new system user without login permissions? Hint => useradd -s /sbin/nologin tony OR useradd -M tony usermod -L tony 19. How to add/remove a group from a user? Hint => # gpasswd -d user_name group_name 20. What is a bash alias? Hint => An alias is a (usually short) name that the shell translates into another (usually longer) name or command. Aliases allow you to define new commands by substituting a string for the first token of a simple command 21. How do you set the mail address of the root/a user? Hint => The system administrator can define email aliases in the file /etc/aliases. This file contains lines like root: cwd@mailhost.example.com, /root/mailbox; the effect is the same as having cwd@mailhost.example.com, root/mailbox in ~root/.forward. You may need to run a program such as newaliases after changing /etc/aliases. 22. What does CTRL-c do? Hint => It sends the SIGINT signal to the process, which is technically just a request\u2014most processes will honor it, but some may ignore it. 23. What does CTRL-d do? Hint => Close the bash shell. This sends an EOF (End-of-file) marker to bash, and bash exits. 24. What does CTRL-z do? Hint => Suspend the current foreground process running in bash. This sends the SIGTSTP signal to the process. To return the process to the foreground later, use the fg process_name command. 25. What is in /etc/services? Hint => /etc/services contains port number mapping with service name. 26. How to redirect STDOUT and STDERR in bash? Hint => a) for all output : &> /dev/null b) To redirect the standard error (stderr): 2> /dev/null c) To redirect the standard output(stdout): 1> /dev/null (adsbygoogle = window.adsbygoogle || []).push({}); 27. What is the difference between UNIX and Linux. 28. What is the difference between Telnet and SSH? 29. Which difference have between public and private SSH keys? 30. Explain the three load averages and what do they indicate. What command can be used to view the load averages? 31. Can you name a lower-case letter that is not a valid option for GNU ls? 32. What is a Linux kernel module? 33. Walk me through the steps in booting into single user mode to troubleshoot a problem. 34. Walk me through the steps you'd take to troubleshoot a 404 error on a web application you administer. 35. What is ICMP protocol? Why do you need to use? 36. What do the following commands do and how would you use them? tee awk tr cut tac curl wget watch head tail less cat touch sar netstat tcpdump lsof 37. What does an & after a command do? 38. What does & disown after a command do? 39. What is a packet filter and how does it work? 40. What is Virtual Memory? 41. What is swap and what is it used for? 42. What is an A record, an NS record, a PTR record, a CNAME record, an MX record? 43. Are there any other RRs and what are they used for? 44. What is a Split-Horizon DNS? 45. What is the sticky bit? 46. What does the immutable bit do to a file? 47. What is the difference between hardlinks and symlinks? What happens when you remove the source to a symlink/hardlink? 48. What is an inode and what fields are stored in an inode? 49. How to force/trigger a file system check on next reboot? 50. What is SNMP and what is it used for? (adsbygoogle = window.adsbygoogle || []).push({}); 51. What is a runlevel and how to get the current runlevel? 52. What is SSH port forwarding? 53. What is the difference between local and remote port forwarding? 54. What are the steps to add a user to a system without using useradd/adduser? 55. What is MAJOR and MINOR numbers of special files? 56. Describe the mknod command and when you would use it. 57. Describe a scenario when you get a \"filesystem is full\" error, but \"df\" shows there is free space. 58. Describe a scenario when deleting a file, but \"df\" not showing the space being freed. 59. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. 60. Different types of file systems? Hint => ext2, ext3, ext4, xfs, vfat etc. 61. Difference between ext4 and xfs? 62. When v create user which files are referred? 63. Differnce between passwd and shadow file? Hint => /etc/passwd contains details abou User like home directory, shell etc. /etc/shadow conatains User password hashes. 64. What are the contents in passwd file? Hint => root:x:0:0:root:/root:/bin/bash \u2022 login name \u2022 optional encrypted password \u2022 numerical user ID \u2022 numerical group ID \u2022 user name or comment field \u2022 user home directory \u2022 optional user command interpreter 65. List all users who are not given passwd? with cmd Hint => cat /etc/shadow| awk -F \":\" '$2 ~ /[a-z]/ {print $1}' 66. How to check what processess are running on which port? Hint => Using \"netstat\" and \"ss\" command we can relate processes with port. 67. How to extend partition in lvm? and indepth questions in lvm ? Hint => 1. First attach extra storage/disk to server. 2. Create a new partition on the disk using \"fdisk\" or \"parted\" command. 3. Create a new Physical Volume(PV) on that new partition. e.g. \"pvcreate /dev/sdb1\" 4. Exetend existing Volume Group(VG) using new PV. e.g. \"vgextend vg_name /dev/sdb1\" 5. Now extend the LV. e.g . \"lvextend -l 100%FREE /dev/mapper/vg_name-lv_name\" 6. now execute \"resize2fs\" or \"xfs_growfs\". 68. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses, 69. What function does DNS play on a network? 70. On which port dns works? Hint => DNS works on port 53. 71. What is HTTP? 72. What is an HTTP proxy and how does it work? 73. Describe briefly how HTTPS works. 74. What is SMTP? Give the basic scenario of how a mail message is delivered via SMTP. 75. What is RAID? What is RAID0, RAID1, RAID5, RAID10? 76. What is a level 0 backup? What is an incremental backup?","title":"Linux Admin Interview Questions and Answers for Freshers and Experienced - Solved"},{"location":"nightwolf-cotribution/linux_L2/","text":"Linux Interview Questions and Answers for Experienced Linux Admins - Solved \uf0c1 We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. Most of these are scenario based Linux interview questions for Experienced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); 1. Please explain Linux booting processing? 2. Difference between RHEL6 and RHEL7 booting process. 3. Difference between systemd and initd. Hint => \"Systemd\" is an enhanced version of \"init\". \"sysytemd\" was launched with RHEL 7.x has capability to start the services in parallel. Please refer to below table for more information. 4. What are the inodes and how will you free up them? 5. How to check the loaded kernel modules. Hint => using \"lsmod\" command 6. How to load kernel module. any commands ? Hint => We can use \"modprobe\" or \"insmod\" commands to load kernel module. 7. How to blacklist a module. 8. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- 9. Explain few examples of kernel panic. Hint => A kernel panic is one of several Linux boot issues. In basic terms, it is a situation when the kernel can't load properly and therefore the system fails to boot. During the boot process, the kernel does not load directly. Instead, initramfs loads in RAM, then it points to the kernel (vmlinuz), and then the operating system boots. If initramfs gets corrupted or deleted at this stage because of recent OS patching, updates, or other causes, then we face a kernel panic. Causes of Kernel panics: a). If the initramfs file gets corrupted. b). If initramfs is not created properly for the specified kernel. Every kernel version has its own corresponding initramfs. c). If the installed kernel is not supported or not installed correctly. d). If recent patches have some flaws. e). If a module has been installed from online or another source, but the initrd image is not created with the latest installed module. 10. What is an oops in terms of Kernel? Hint => An oops indicates a kernel bug and should always be reported and fixed. When an oops occurs, the system will print out information that is relevent to debugging the problem, like the contents of all the CPU registers, and the location of page descriptor tables. In particular, the contents of the EIP (instruction pointer) is printed. Like this: EIP: 0010:[<00000000>] Call Trace: [<c010b860>] An oops is not a kernel panic. In a panic, the kernel cannot continue; the system grinds to a halt and must be restarted. An oops may cause a panic if a vital part of the system is destroyed. An oops in a device driver, for example, will almost never cause a panic. 11. What is \"nohup\" used for? Hint => nohup is a POSIX command which means \"no hang up\". Its purpose is to execute a command such that it ignores the HUP (hangup) signal and therefore does not stop when the user logs out. Output that would normally go to the terminal goes to a file called nohup. ## nuhup Syntax: nohup command [command-argument ...] 12. How to troubleshoot a issue where a client not able to access a server? 13. What are outputs when you execute the NATCAT (nc) command and explain each ? Hint => - Connected - Connection timeout - Connection refused(if port / server is blocking by the firewall) 14. How can we check the packet flow in our system? Hint => We can use \"tcpdump\" command to check the incoming and outgoing packets. 15. what is the steal value in top command? 16. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses. Files used by tcpwrappr for access control are /etc/hosts.allow and /etc/hosts.deny. 17. Describe how 'ps' works. Hint => On Linux, the ps command works by reading files in the proc filesystem. The directory '/proc/PID' contains various files that provide information about process PID. The content of these files is generated on the fly by the kernel when a process reads them. You can use 'strace' command to actually see how ps works. 18. Which Linux file types do you know? 19. What is \"nohup\" used for? 20. What is the difference between these two commands? myvar=hello & export myvar=hello 21. What happens to a child process that dies and has no parent process to wait for it and what\u2019s bad about this? 22. Explain briefly each one of the process states. 23. How to know which process listens on a specific port? 24. What is a zombie process and what could be the cause of it? 25. You run a bash script and you want to see its output on your terminal and save it to a file at the same time. How can you achieve it? Hint => We can use \"tee\" command to do this. Below is the command expamle: /sbin/ifconfig | tee FILE_NAME 26. Explain what echo \"1\" > /proc/sys/net/ipv4/ip_forward does. 27. What is CPU load ? How to calculate the load average on the system? 28. What the Zombie and Orphan process? How to kill zombie process? 29. What could be the impacts on the system if there are many zombie process are available? 30. Explain the TOP, IOSTAT,VMSTAT,IOTOP commands. 31. You need to upgrade kernel at 100-1000 servers, how you would do this? 32. How can you tell if the httpd package was already installed? 33. How can you list the contents of a package? 34. How can you determine which package is better: openssh-server-5.3p1-118.1.el6_8.x86_64 or openssh-server-6.6p1-1.el6.x86_64 ? (adsbygoogle = window.adsbygoogle || []).push({}); 35. Describe briefly the steps you need to take in order to create and install a valid certificate for the site https://www.nightwolf.in. 36. Can you have several HTTPS virtual hosts sharing the same IP? 37. What is a wildcard certificate? 38. Which Linux file types do you know? 39. How can you get Host, Channel, ID, LUN of SCSI disk? 40. Can you explain to me the difference between block based, and object based storage? 41. What is a tarpipe (or, how would you go about copying everything, including hardlinks and special files, from one server to another)? 42. How many NTP servers would you configure in your local ntp.conf? 43. What does the column 'reach' mean in ntpq -p output? 44. What is bash quick substitution/caret replace(^x^y)? 45. Do you know of any alternative shells? If so, have you used any? 46. How can you limit process memory usage? 47. How do you troubleshoot memory performance issue. Please explain the details. 48. Which tools do you use to troubleshoot high Memory troubleshooting. 49. What are zombie process and how to kill/reclaim them. 50. What are D-State processes and what causes these. 51. If Disk is causing D-state processes, what you can check and can do to fix the issue. 52. How to troubleshoot the system performance if any Linux system is facing slowness? 53. How to troubleshoot high memory usageissue on Linux system. Next Page (adsbygoogle = window.adsbygoogle || []).push({}); (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Linux Interview Questions for Experienced Linux Admins - L2"},{"location":"nightwolf-cotribution/linux_L2/#linux-interview-questions-and-answers-for-experienced-linux-admins-solved","text":"We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. Most of these are scenario based Linux interview questions for Experienced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); 1. Please explain Linux booting processing? 2. Difference between RHEL6 and RHEL7 booting process. 3. Difference between systemd and initd. Hint => \"Systemd\" is an enhanced version of \"init\". \"sysytemd\" was launched with RHEL 7.x has capability to start the services in parallel. Please refer to below table for more information. 4. What are the inodes and how will you free up them? 5. How to check the loaded kernel modules. Hint => using \"lsmod\" command 6. How to load kernel module. any commands ? Hint => We can use \"modprobe\" or \"insmod\" commands to load kernel module. 7. How to blacklist a module. 8. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- 9. Explain few examples of kernel panic. Hint => A kernel panic is one of several Linux boot issues. In basic terms, it is a situation when the kernel can't load properly and therefore the system fails to boot. During the boot process, the kernel does not load directly. Instead, initramfs loads in RAM, then it points to the kernel (vmlinuz), and then the operating system boots. If initramfs gets corrupted or deleted at this stage because of recent OS patching, updates, or other causes, then we face a kernel panic. Causes of Kernel panics: a). If the initramfs file gets corrupted. b). If initramfs is not created properly for the specified kernel. Every kernel version has its own corresponding initramfs. c). If the installed kernel is not supported or not installed correctly. d). If recent patches have some flaws. e). If a module has been installed from online or another source, but the initrd image is not created with the latest installed module. 10. What is an oops in terms of Kernel? Hint => An oops indicates a kernel bug and should always be reported and fixed. When an oops occurs, the system will print out information that is relevent to debugging the problem, like the contents of all the CPU registers, and the location of page descriptor tables. In particular, the contents of the EIP (instruction pointer) is printed. Like this: EIP: 0010:[<00000000>] Call Trace: [<c010b860>] An oops is not a kernel panic. In a panic, the kernel cannot continue; the system grinds to a halt and must be restarted. An oops may cause a panic if a vital part of the system is destroyed. An oops in a device driver, for example, will almost never cause a panic. 11. What is \"nohup\" used for? Hint => nohup is a POSIX command which means \"no hang up\". Its purpose is to execute a command such that it ignores the HUP (hangup) signal and therefore does not stop when the user logs out. Output that would normally go to the terminal goes to a file called nohup. ## nuhup Syntax: nohup command [command-argument ...] 12. How to troubleshoot a issue where a client not able to access a server? 13. What are outputs when you execute the NATCAT (nc) command and explain each ? Hint => - Connected - Connection timeout - Connection refused(if port / server is blocking by the firewall) 14. How can we check the packet flow in our system? Hint => We can use \"tcpdump\" command to check the incoming and outgoing packets. 15. what is the steal value in top command? 16. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses. Files used by tcpwrappr for access control are /etc/hosts.allow and /etc/hosts.deny. 17. Describe how 'ps' works. Hint => On Linux, the ps command works by reading files in the proc filesystem. The directory '/proc/PID' contains various files that provide information about process PID. The content of these files is generated on the fly by the kernel when a process reads them. You can use 'strace' command to actually see how ps works. 18. Which Linux file types do you know? 19. What is \"nohup\" used for? 20. What is the difference between these two commands? myvar=hello & export myvar=hello 21. What happens to a child process that dies and has no parent process to wait for it and what\u2019s bad about this? 22. Explain briefly each one of the process states. 23. How to know which process listens on a specific port? 24. What is a zombie process and what could be the cause of it? 25. You run a bash script and you want to see its output on your terminal and save it to a file at the same time. How can you achieve it? Hint => We can use \"tee\" command to do this. Below is the command expamle: /sbin/ifconfig | tee FILE_NAME 26. Explain what echo \"1\" > /proc/sys/net/ipv4/ip_forward does. 27. What is CPU load ? How to calculate the load average on the system? 28. What the Zombie and Orphan process? How to kill zombie process? 29. What could be the impacts on the system if there are many zombie process are available? 30. Explain the TOP, IOSTAT,VMSTAT,IOTOP commands. 31. You need to upgrade kernel at 100-1000 servers, how you would do this? 32. How can you tell if the httpd package was already installed? 33. How can you list the contents of a package? 34. How can you determine which package is better: openssh-server-5.3p1-118.1.el6_8.x86_64 or openssh-server-6.6p1-1.el6.x86_64 ? (adsbygoogle = window.adsbygoogle || []).push({}); 35. Describe briefly the steps you need to take in order to create and install a valid certificate for the site https://www.nightwolf.in. 36. Can you have several HTTPS virtual hosts sharing the same IP? 37. What is a wildcard certificate? 38. Which Linux file types do you know? 39. How can you get Host, Channel, ID, LUN of SCSI disk? 40. Can you explain to me the difference between block based, and object based storage? 41. What is a tarpipe (or, how would you go about copying everything, including hardlinks and special files, from one server to another)? 42. How many NTP servers would you configure in your local ntp.conf? 43. What does the column 'reach' mean in ntpq -p output? 44. What is bash quick substitution/caret replace(^x^y)? 45. Do you know of any alternative shells? If so, have you used any? 46. How can you limit process memory usage? 47. How do you troubleshoot memory performance issue. Please explain the details. 48. Which tools do you use to troubleshoot high Memory troubleshooting. 49. What are zombie process and how to kill/reclaim them. 50. What are D-State processes and what causes these. 51. If Disk is causing D-state processes, what you can check and can do to fix the issue. 52. How to troubleshoot the system performance if any Linux system is facing slowness? 53. How to troubleshoot high memory usageissue on Linux system.","title":"Linux Interview Questions and Answers for Experienced Linux Admins - Solved"},{"location":"nightwolf-cotribution/linux_L3/","text":"Advanced Linux Interview Questions for Experienced Linux Admins - Solved \uf0c1 We have prepared a set of questions to help Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Infrastructure specialist position, Amazon's Cloud Support Engineer - II position and other reputed firm's interviews as well. Most of these are scenario based Linux interview questions for Experienced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Explain the below error: ------------------------------------------------------------------------------------------- find: page allocation failure. order:5, mode:0x0 Pid: 22938, comm: find Not tainted 2.6.32-279.el6.i686 #1 Call Trace: [<c04f22bc>] ? __alloc_pages_nodemask+0x6bc/0x870 [<c051f1bc>] ? cache_alloc_refill+0x2bc/0x510 [<c083f035>] ? apic_timer_interrupt+0x31/0x38 [<c051f552>] ? __kmalloc+0x142/0x180 [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb2f0>] ? rpc_wait_bit_killable+0x0/0x80 [sunrpc] ------------------------------------------------------------------------------------------- Hint => Above error is kernel dump due to kmalloc buffer exhaustion. The system is out of memory and cannot satisfy the allocation request at all. This is more likely to happen on swap-less systems that are unable to purge application pages to free up memory. The system may have sufficient free pages in total but not enough that are physically contiguous to satisfy the allocation request. This is referred to as memory fragmentation. - The phrase \"page allocation failure\" - this defines the error that should be used for error search. - The process name \"find\" - often page allocation failures are related to code that gets executed by specific threads, try searching KCS with and without this term. If the allocation request was performed by an interrupt service routine then the process name will be irrelevant. - The order of the allocation \"order:5\" and mode \"mode:0x0\" can often be the same for every instance of a particular problem, try searching KCS with and without these terms. - The function requesting the memory allocation \"rpc_malloc()\" is likely to be the same for every instance of this particular problem so include that in the KCS search. - Do not search for the CPU number, the process id or function offsets since they will change and may prevent finding a match. For more details https://access.redhat.com/articles/1360023 What is order:5, mode:0x0 in above output. Difference between kernel panic due to Memory crunch and page allocation failure. Difference between VMalloc and KMalloc Hint => The kmalloc() & vmalloc() functions are a simple interface for obtaining kernel memory in byte-sized chunks. - The kmalloc() function guarantees that the pages are physically contiguous (and virtually contiguous). - The vmalloc() function works in a similar fashion to kmalloc(), except it allocates memory that is only virtually contiguous and not necessarily physically contiguous. How do you troubleshoot memory performance issue. Please explain the details. Which tools do you use to troubleshoot high Memory troubleshooting. What are zombie process and how to kill/reclaim them. What are system calls. What is strace used for. Difference between fork and exec in Linux. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- Explain few examples of kernel panic. How to blacklist a module. What are D-State processes and what causes these. If Disk is causing D-state processes, what you can check and can do to fix the issue. Explain blk_trace. If you are receiving \"Connection reset by peer\" when trying to connect a server, what can be root cause of this and how you can fix this. How will you troubleshoot high load avg. Which tools will you use and why. Explain in details. What can be reason of high load avg in kernel space i.e. %sy in top. What is CPU affinity . How to check it. How to check all cores stat in top. Hint: by pressing 1 or using mpstat What can be the reasons for server to stuck at boot prompt. and how to troubleshoot further ? What is backporting in Linux. Hint: its related to packages. There are two systems and one is taking less booting time and other than taking more than booting time. What can be difference in both ? Please explain few types of kernel errors? How to troubleshoot the system performance if any Linux system is facing slowness? How to troubleshoot high memory usageissue on Linux system. What is CPU load ? How to calculate the load average on the system? What the Zombie and Orphan process? How to kill zombie process? What could be the impacts on the system if there are many zombie process are available? What is affinity in Linux? 1. Binding a process with a CPU core. 2. # taskset < command > What is paging and swapping in Linux? Please explain Page fault ? What is difference between cache and buffer? Explain what is NUMA. You may know this as \"Node Interleaving\u201d. Give an example where NUMA is beneficial, and another where it's detrimental. Hints => a). NUMA: Non Uniform Memory Access (NUMA) is a design where the memory is split into groups by CPU, making the memory access faster for each CPU to their memory group. b). Beneficial: Multiple processes with smaller memory footprints. Good for applications that fork processes, like Apache. Detrimental: Monolithic, memory-heavy, single processes. May be threaded, example MySQL, Redis, Java-everything, Memcached. Follow-Up Q. - what if there's only a single physical proc. Customer complains that their CPUs aren't running as fast as they're rated. How would you Troubleshoot ? Hints => a). Could be heat damage - check /proc/cpuinfo for \"cpu MHz\". b). Could be the wrong CPUs installed. Check the CPU specs in /proc/cpuinfo. OR dmidecode. c). Change system profile to MaximumPerformance with omconfig, then reboot. Can we check System logs on Hardware layer from OS ?How ? Hints => a). Yes, in dmesg logs or kern.log b). By using respective Hardware CLI commands. c). Specific answers hpasmcli -s \"show iml\" ==> For HP servers omreport system [alertlog | esmlog] ==> For Dell Servers What can be done to improve the performance of an I/O-heavy system? Hints => a). Check RAID Level, if server using any. b). Partition aligned, i.e. starting a partition on sector 2048. c). Mount Options like: noatime - (covers norelatime/nodiratime) Can be enabled only via fstab. d). RAID Controller settings - read/write policy, e.g. write-through(good to ensure data has been changed on disk. write-back(relys on cache, battery). e). nobarrier - depends on whether or not the PERC Controller is making use of a battery or not. f). Adjust the filesystem journaling method - the default mount option is in the middle, data=ordered. This indicates the data is written out to the filesystem before the metadata in the journal is updated. The filesystem can be mounted with data=writeback which allows data to be written to the journal and disk at the same time, with the risk of some data possibly being written to the disk after the metadata that cannot be recovered after a crash. Additionally, the journal can be placed on an external device with another mount option, allowing separation of the writes to increase throughput performance for each write operation. g). discard - For SSDs. Also called fs TRIM. Extends life and improves performance of SSD drives. TRIM enables the controller to cache the expected SSD write cycles to collapse them into a \"do only only\" mechanism to prevent burning out the SSD cells, which can only be written to once with voltage. Note: For LVM, set issue_discards=1 in lvm.conf. Note: On Dell, TRIM/discard is only available on RAID controllers >= PERC H710 h). For SSD, you do not want controller caching for performance reasons. By default HPs turn caching off. Dell: Cut-through IO (CTIO) is an IO accelerator for SSD arrays that boosts the throughput of devices connected to the PERC Controller. It is enabled through disabling the write-back cache (enable writethrough cache) and disabling Read Ahead. HP: On servers with SSD drives, write caching appears to be disabled by default for performance reasons (there is no need for write caching on SSD drives which are fast enough). Writes to SSD drives will not be acknowledged until the data is actually written to disk: i). On HP, HP SSD Smart Path - this is enabled by default by the RAID creation process in the HP RAID controller itself, and can be checked with the standard commands using hpssacli to view controller details and RAID logical device details. The theory is this special method of SSD writing replaces traditional caching techniques with a special HP invented method for improving SSD performance and longevity. j). Disk IO Scheduler deadline (recommended for DBs) noop (recommended for VMs) Why would I choose to use RAID-5 instead of RAID-10? Or vice versa? Hints => RAID-5: Pro: Space efficiency - you only sacrifice 1x HDD to redundancy. Pro: Possible with a minimum of 3x drives, as opposed to RAID-10's minimum-4. Con: Slow writes. Note: Reads are still quick though, so you'd be fine on a read-heavy application. Con: If two drives fail, you lose everything. RAID-10: Pro: Great performance with both reads and writes. Good for write-heavy applications. Pro: Less risk of a catastrophic failure, since more than 1 drive can fail safely (Note: Depends which drives). Con: Too much wastage of Space. Only 50% capacity is uasable. How snapshots work ? Can you please explain. In terms of VMWare or LVM(in case someone asks). Hints => As soon as you create a snapshot, LVM creates a pool of blocks. This pool also contains a full copy of the LVM metadata of the volume. When writes happen to the main volume such as updating an inode, the block being overwritten is copied to this new pool and the new block is written to the main volume. This is the \"copy-on-write\". Because of this, the more data that gets changed between when a snapshot was taken and the current state of the main volume, the more space will get consumed by that snapshot pool. When you mount the snapshot, the meta-data written when the snapshot was taken allows the mapping of snapshot pool blocks over changed blocks in the volume (or higher level snapshot). This way when an access comes for a specific block, LVM knows which block access. As far as the filesystem on that volume is concerned, there are no snapshots. In which scenario we need to use vgcfgrestore and vgcfgbackup commands ? When /etc/lvm/archive contents are written (when does backup happen ?) Hints => a). To restore VG configuration. b). Backup is taken whenever we make change to LVM (create/remove LV/VG) and we can use it to restore to the prior state. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. What is deadlock ? How to identify if process is in deadlock in Linux/Unix ? Hint => Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process Difference between insmod and modprobe commands in Linux ? Difference between ext4 and xfs? When we create user which files are referred? What is the difference between a process and a thread? And parent and child processes after a fork system call? What is the difference between exec and fork? How can you limit process memory usage? What is a tunnel and how you can bypass a http proxy? What is the difference between IDS and IPS? What shortcuts do you use on a regular basis? What is the Linux Standard Base? What is an atomic operation? Your freshly configured http server is not running after a restart, what can you do? What kind of keys are in ~/.ssh/authorized_keys and what it is this file used for? I've added my public ssh key into authorized_keys but I'm still getting a password prompt, what can be wrong? Did you ever create RPM's, DEB's or solaris pkg's? What does :(){ :|:& };: do on your system? Hint => :(){ :|:& };: is shell fork bomb, If executed on a server, it will consume all the available resources on your system and eventually crash the system. Elaborated explaination is below: :() { :|:& }; : | | |-- Calling the funcation \":\" | | | |-- Definition of function \":\" in which funcation \":\" is called and its output is passed to same | funcation and process goes in backgrouond. | |-- Decalaring the funcation with name \":\" How do you catch a Linux signal on a script? Can you catch a SIGKILL? What's happening when the Linux kernel is starting the OOM killer and how does it choose which process to kill first? Describe the linux boot process with as much detail as possible, starting from when the system is powered on and ending when you get a prompt. What's a chroot jail? When trying to umount a directory it says it's busy, how to find out which PID holds the directory? What's LD_PRELOAD and when it's used? You ran a binary and nothing happened. How would you debug this? What are cgroups? Can you specify a scenario where you could use them? How can you remove/delete a file with file-name consisting of only non-printable/non-type-able characters? How can you increase or decrease the priority of a process in Linux? A running process gets EAGAIN: Resource temporarily unavailable on reading a socket. How can you close this bad socket/file descriptor without killing the process? What do you control with swapiness? How do you change TCP stack buffers? How do you calculate it? What is Huge Tables? Why isn't it enabled by default? Why and when use it? What is LUKS? How to use it? When we execute below command, which binary is used ? [ -f test ] && echo \"Found the file\" || echo \"create new file\" (adsbygoogle = window.adsbygoogle || []).push({}); Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Advanced Linux Interview Questions for Experienced Admins - L3"},{"location":"nightwolf-cotribution/linux_L3/#advanced-linux-interview-questions-for-experienced-linux-admins-solved","text":"We have prepared a set of questions to help Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Infrastructure specialist position, Amazon's Cloud Support Engineer - II position and other reputed firm's interviews as well. Most of these are scenario based Linux interview questions for Experienced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Explain the below error: ------------------------------------------------------------------------------------------- find: page allocation failure. order:5, mode:0x0 Pid: 22938, comm: find Not tainted 2.6.32-279.el6.i686 #1 Call Trace: [<c04f22bc>] ? __alloc_pages_nodemask+0x6bc/0x870 [<c051f1bc>] ? cache_alloc_refill+0x2bc/0x510 [<c083f035>] ? apic_timer_interrupt+0x31/0x38 [<c051f552>] ? __kmalloc+0x142/0x180 [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb2f0>] ? rpc_wait_bit_killable+0x0/0x80 [sunrpc] ------------------------------------------------------------------------------------------- Hint => Above error is kernel dump due to kmalloc buffer exhaustion. The system is out of memory and cannot satisfy the allocation request at all. This is more likely to happen on swap-less systems that are unable to purge application pages to free up memory. The system may have sufficient free pages in total but not enough that are physically contiguous to satisfy the allocation request. This is referred to as memory fragmentation. - The phrase \"page allocation failure\" - this defines the error that should be used for error search. - The process name \"find\" - often page allocation failures are related to code that gets executed by specific threads, try searching KCS with and without this term. If the allocation request was performed by an interrupt service routine then the process name will be irrelevant. - The order of the allocation \"order:5\" and mode \"mode:0x0\" can often be the same for every instance of a particular problem, try searching KCS with and without these terms. - The function requesting the memory allocation \"rpc_malloc()\" is likely to be the same for every instance of this particular problem so include that in the KCS search. - Do not search for the CPU number, the process id or function offsets since they will change and may prevent finding a match. For more details https://access.redhat.com/articles/1360023 What is order:5, mode:0x0 in above output. Difference between kernel panic due to Memory crunch and page allocation failure. Difference between VMalloc and KMalloc Hint => The kmalloc() & vmalloc() functions are a simple interface for obtaining kernel memory in byte-sized chunks. - The kmalloc() function guarantees that the pages are physically contiguous (and virtually contiguous). - The vmalloc() function works in a similar fashion to kmalloc(), except it allocates memory that is only virtually contiguous and not necessarily physically contiguous. How do you troubleshoot memory performance issue. Please explain the details. Which tools do you use to troubleshoot high Memory troubleshooting. What are zombie process and how to kill/reclaim them. What are system calls. What is strace used for. Difference between fork and exec in Linux. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- Explain few examples of kernel panic. How to blacklist a module. What are D-State processes and what causes these. If Disk is causing D-state processes, what you can check and can do to fix the issue. Explain blk_trace. If you are receiving \"Connection reset by peer\" when trying to connect a server, what can be root cause of this and how you can fix this. How will you troubleshoot high load avg. Which tools will you use and why. Explain in details. What can be reason of high load avg in kernel space i.e. %sy in top. What is CPU affinity . How to check it. How to check all cores stat in top. Hint: by pressing 1 or using mpstat What can be the reasons for server to stuck at boot prompt. and how to troubleshoot further ? What is backporting in Linux. Hint: its related to packages. There are two systems and one is taking less booting time and other than taking more than booting time. What can be difference in both ? Please explain few types of kernel errors? How to troubleshoot the system performance if any Linux system is facing slowness? How to troubleshoot high memory usageissue on Linux system. What is CPU load ? How to calculate the load average on the system? What the Zombie and Orphan process? How to kill zombie process? What could be the impacts on the system if there are many zombie process are available? What is affinity in Linux? 1. Binding a process with a CPU core. 2. # taskset < command > What is paging and swapping in Linux? Please explain Page fault ? What is difference between cache and buffer? Explain what is NUMA. You may know this as \"Node Interleaving\u201d. Give an example where NUMA is beneficial, and another where it's detrimental. Hints => a). NUMA: Non Uniform Memory Access (NUMA) is a design where the memory is split into groups by CPU, making the memory access faster for each CPU to their memory group. b). Beneficial: Multiple processes with smaller memory footprints. Good for applications that fork processes, like Apache. Detrimental: Monolithic, memory-heavy, single processes. May be threaded, example MySQL, Redis, Java-everything, Memcached. Follow-Up Q. - what if there's only a single physical proc. Customer complains that their CPUs aren't running as fast as they're rated. How would you Troubleshoot ? Hints => a). Could be heat damage - check /proc/cpuinfo for \"cpu MHz\". b). Could be the wrong CPUs installed. Check the CPU specs in /proc/cpuinfo. OR dmidecode. c). Change system profile to MaximumPerformance with omconfig, then reboot. Can we check System logs on Hardware layer from OS ?How ? Hints => a). Yes, in dmesg logs or kern.log b). By using respective Hardware CLI commands. c). Specific answers hpasmcli -s \"show iml\" ==> For HP servers omreport system [alertlog | esmlog] ==> For Dell Servers What can be done to improve the performance of an I/O-heavy system? Hints => a). Check RAID Level, if server using any. b). Partition aligned, i.e. starting a partition on sector 2048. c). Mount Options like: noatime - (covers norelatime/nodiratime) Can be enabled only via fstab. d). RAID Controller settings - read/write policy, e.g. write-through(good to ensure data has been changed on disk. write-back(relys on cache, battery). e). nobarrier - depends on whether or not the PERC Controller is making use of a battery or not. f). Adjust the filesystem journaling method - the default mount option is in the middle, data=ordered. This indicates the data is written out to the filesystem before the metadata in the journal is updated. The filesystem can be mounted with data=writeback which allows data to be written to the journal and disk at the same time, with the risk of some data possibly being written to the disk after the metadata that cannot be recovered after a crash. Additionally, the journal can be placed on an external device with another mount option, allowing separation of the writes to increase throughput performance for each write operation. g). discard - For SSDs. Also called fs TRIM. Extends life and improves performance of SSD drives. TRIM enables the controller to cache the expected SSD write cycles to collapse them into a \"do only only\" mechanism to prevent burning out the SSD cells, which can only be written to once with voltage. Note: For LVM, set issue_discards=1 in lvm.conf. Note: On Dell, TRIM/discard is only available on RAID controllers >= PERC H710 h). For SSD, you do not want controller caching for performance reasons. By default HPs turn caching off. Dell: Cut-through IO (CTIO) is an IO accelerator for SSD arrays that boosts the throughput of devices connected to the PERC Controller. It is enabled through disabling the write-back cache (enable writethrough cache) and disabling Read Ahead. HP: On servers with SSD drives, write caching appears to be disabled by default for performance reasons (there is no need for write caching on SSD drives which are fast enough). Writes to SSD drives will not be acknowledged until the data is actually written to disk: i). On HP, HP SSD Smart Path - this is enabled by default by the RAID creation process in the HP RAID controller itself, and can be checked with the standard commands using hpssacli to view controller details and RAID logical device details. The theory is this special method of SSD writing replaces traditional caching techniques with a special HP invented method for improving SSD performance and longevity. j). Disk IO Scheduler deadline (recommended for DBs) noop (recommended for VMs) Why would I choose to use RAID-5 instead of RAID-10? Or vice versa? Hints => RAID-5: Pro: Space efficiency - you only sacrifice 1x HDD to redundancy. Pro: Possible with a minimum of 3x drives, as opposed to RAID-10's minimum-4. Con: Slow writes. Note: Reads are still quick though, so you'd be fine on a read-heavy application. Con: If two drives fail, you lose everything. RAID-10: Pro: Great performance with both reads and writes. Good for write-heavy applications. Pro: Less risk of a catastrophic failure, since more than 1 drive can fail safely (Note: Depends which drives). Con: Too much wastage of Space. Only 50% capacity is uasable. How snapshots work ? Can you please explain. In terms of VMWare or LVM(in case someone asks). Hints => As soon as you create a snapshot, LVM creates a pool of blocks. This pool also contains a full copy of the LVM metadata of the volume. When writes happen to the main volume such as updating an inode, the block being overwritten is copied to this new pool and the new block is written to the main volume. This is the \"copy-on-write\". Because of this, the more data that gets changed between when a snapshot was taken and the current state of the main volume, the more space will get consumed by that snapshot pool. When you mount the snapshot, the meta-data written when the snapshot was taken allows the mapping of snapshot pool blocks over changed blocks in the volume (or higher level snapshot). This way when an access comes for a specific block, LVM knows which block access. As far as the filesystem on that volume is concerned, there are no snapshots. In which scenario we need to use vgcfgrestore and vgcfgbackup commands ? When /etc/lvm/archive contents are written (when does backup happen ?) Hints => a). To restore VG configuration. b). Backup is taken whenever we make change to LVM (create/remove LV/VG) and we can use it to restore to the prior state. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. What is deadlock ? How to identify if process is in deadlock in Linux/Unix ? Hint => Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process Difference between insmod and modprobe commands in Linux ? Difference between ext4 and xfs? When we create user which files are referred? What is the difference between a process and a thread? And parent and child processes after a fork system call? What is the difference between exec and fork? How can you limit process memory usage? What is a tunnel and how you can bypass a http proxy? What is the difference between IDS and IPS? What shortcuts do you use on a regular basis? What is the Linux Standard Base? What is an atomic operation? Your freshly configured http server is not running after a restart, what can you do? What kind of keys are in ~/.ssh/authorized_keys and what it is this file used for? I've added my public ssh key into authorized_keys but I'm still getting a password prompt, what can be wrong? Did you ever create RPM's, DEB's or solaris pkg's? What does :(){ :|:& };: do on your system? Hint => :(){ :|:& };: is shell fork bomb, If executed on a server, it will consume all the available resources on your system and eventually crash the system. Elaborated explaination is below: :() { :|:& }; : | | |-- Calling the funcation \":\" | | | |-- Definition of function \":\" in which funcation \":\" is called and its output is passed to same | funcation and process goes in backgrouond. | |-- Decalaring the funcation with name \":\" How do you catch a Linux signal on a script? Can you catch a SIGKILL? What's happening when the Linux kernel is starting the OOM killer and how does it choose which process to kill first? Describe the linux boot process with as much detail as possible, starting from when the system is powered on and ending when you get a prompt. What's a chroot jail? When trying to umount a directory it says it's busy, how to find out which PID holds the directory? What's LD_PRELOAD and when it's used? You ran a binary and nothing happened. How would you debug this? What are cgroups? Can you specify a scenario where you could use them? How can you remove/delete a file with file-name consisting of only non-printable/non-type-able characters? How can you increase or decrease the priority of a process in Linux? A running process gets EAGAIN: Resource temporarily unavailable on reading a socket. How can you close this bad socket/file descriptor without killing the process? What do you control with swapiness? How do you change TCP stack buffers? How do you calculate it? What is Huge Tables? Why isn't it enabled by default? Why and when use it? What is LUKS? How to use it? When we execute below command, which binary is used ? [ -f test ] && echo \"Found the file\" || echo \"create new file\" (adsbygoogle = window.adsbygoogle || []).push({});","title":"Advanced Linux Interview Questions for Experienced Linux Admins - Solved"},{"location":"nightwolf-cotribution/linux_basic/","text":"Linux Interview Questions and Answers for Freshers - Solved: \uf0c1 We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux Admin, Amazon interview questions for Linux admin and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); 1.What is two types of Linux User Mode ? Command Line GUI 2.What is command for created multiple files at a time? touch 3.What is INODE and How to Identify? Its unique identification code for files and directories, it was generate automatically while creating new file and directories ls -i filename ls -ldi directoryname 4.List of Permissions and Users Read, Write and Execute Owner, Group Owners and Others 5.List of Special Permissions and numerical value. Set User ID = 4 Set Group ID = 2 Stickybit = 1 6. What command to use see Process list in Hierarchical Structure along with PID? pstree -P 7. What is use of \u201ctop\u201d command and how to sort Memory and User wise? Its used to real time monitor hardware utilization of linux machine. Press M to sort Memory wise result Press U to sort User wise result 8. What is command for to force close one particular process kill -9 Processid 9.What is command to refresh NIC ? Service network restart 10.Tell me two types of IP Address configuration Static IP Address Dynamic IP Address 11.How do Enable / Disable Ethernet Device Open and Edit this file #vi /etc/sysconfig/network-scripts/devicename For enable ONBOOT = yes For disable ONBOOT =no 12.What is command to change Hostname without System Restart hostname newhostname hostnamectl 13.What is File Path of Network Configuration ? /etc/sysconfig/network-scripts 14.What is File Path of DNS Configuration ? /etc/resolv.conf 15.How to Update locate DB ? cd/var/lib/mlocate updatedb 16. How to edit and save file using editors? The following commands are used to exit from vi editors. 1. :wq saves the current work and exits the VI. 2. :q! exits the VI without saving current work. 17.What is command for Zip and Unzip files 1. gzip = Compress File 2. gunzip = Uncompress File 18.What is file path of Alias name set by Permanent? /etc/bashrc 19.What is MBR in linux? Its Master Boot Recorder to help booting operating system. 20.What is Two Types of Mount in linux? Temporary Mount Permanent Mount 21.What is command for delete Partition? #umount #palimpsest & OR #parted OR #fdisk 22.What is command for Refresh Partition? mount -a 23.What is SWAP? Linux uses swap space to increase the amount of virtual memory available to a host. It can use one or more dedicated swap partitions or a swap file on a regular filesystem or logical volume. 24.What are types can set SWAP? Temporary set Permanent set 25. What command use for Filesystem Error checking and Error Fixing fsck and e2fsck 26. What is PV, VG, and LVM PV = Physical Volume VG = Volume Group LVM = Logical Volume 27. What is LVM LVM is used to create logical partitions and during run time we can resize particular partition without data loss. Empty partitions only can do LVM creation. 28. What are common commands used for Physical Volume pvcreate pvs pvdisplay 29. What command is used for create Volume Group vgcreate vgs 30. What is Syntax for LVM Create? #lvcreate -L partitionsize -n userdefinename volumegroupname 31. What types of Installation Tools in REDHAT? RPM = Redhat Package Manager YUM = Yellow Dog Updated Modifier 32. Tell me Linux Boot Sequence Floow? BIOS \u2192 MBR \u2192 Boot Loader \u2192 Kernal \u2192 Runlevel 33. Types of Zone in DNS? Forward lookup zone Reverse lookup zone 34. What are inbuilt firwall in Linux ? IP Tables Selinux TCPwrappers 35. What command to Execute disable IPTables permanently? #iptables -F service iptables save 36. What is SELinux? Its one type of firewall in linux To block particular service in a Protocol 37. File to disable SELinux permanently: /etc/selinux/config 38. What is command to check selinux status ? getenforce 39. What is LDAP The Lightweight Directory Access Protocol (LDAP) is a set of open protocols used to access centrally stored information over a network. 40. Which Configuration File Is Required For Ldap Clients? ldap.conf 41. What Is The Name Of Main Configuration File Name For Ldap Server? slapd.conf 42. How Will You Verify Ldap Configuration File? slaptest -u 43. What is command package install using YUM without ask Prompt? yum install packagename -y 44. What is command Uninstall package? yum remove packagename 45. What is command package re-install using YUM without ask Prompt? yum reinstall packagename -y 46. Location of Cron file in linux? /var/spool/cron 47. What is command for to see Particular user Job Schedule ? crontab -lu username 48. What is command for restart cron service? service crond restart 49. What is command for restart postfix service? service postfix restart 50.What is command for FTP service on and restart? chkconfig vsftpd on service vsftpd restart 50. What is Kernel un Unix Operating system? Kernel is the heart of operating system. It interacts with shell and executes the machine level language. (adsbygoogle = window.adsbygoogle || []).push({}); 51. How can I save my input and output commands and see them when required? At the beginning of the session if you use 'script' command then the details of the input and output commands will be saved in a file called typescript and we can view it any time using \u201ccat typescript\u201d command. This is very useful to track what user is doing what. HISTORY command will not work because it shows data only for the current session. 52. How to create a file in Unix? There are multiple way to create files in unix, but the simple way to create a file is using \u201ccat\u201d and \u201cTouch\u201d command Syntax: cat > File name touch file name 53. How can I check which processes are running in my machine? To check process which are running in my machine I can use two commands. (a) TOP and (b) PS 54. What is the difference between TOP and PS command? Top command gives the dynamic view of the processes are running in the server and generally the dynamic change happens in every 3 second. Whereas PS commands gives the static view of the processes. 55. You used TOP command and without aborting the TOP process I need to kill one process. Is it possible to kill? Yes TOP command it self has a command prompt. Type K then it will ask you for the PID of the process to kill. Hit the PID and enter, it will kill the process. 56. What is the difference between creating a file in cat and in touch command? cat command creates a file and we can save some data inside the file but touch command by default will create a blank file. 57. How can I create multiple directories at a time? Say I want to create a directory D1 and inside that D2 and inside that D3. Is it possible? If yes how ? Yes, creating multiple directories is possible. In this scenario the below command works. #mkdir \u2013p D1/D2/D3 58. I want to create D1, under that D2 and D3. Inside D2 I want D4 and inside D3 I want D5 to be created. How is it possible? The below command will work for it. #mkdir \u2013p D1/D2/D4 D1/D3/D5 59. How can I check in which directory I am in ? Use PWD command to check which directory you are in. 60. We are using so many commands and getting output. Have you ever wondered how the commands are executing and getting you the output? Yes, every command in Unix is a C program in the backend. When we type a command and hit enter the program runs in the backend and gives you the output. We can view the C program as well as below. type <Command Name> ->hit enter, it will give you a path where the program the command is located. You can view the program by doing cat and the path name. it will open a C program file in decrypted mode. 61. How can I list the directories and the files ? Using ls command. I can view the directories and files of the system. 62. How can I view hidden files in a system ? Using ls \u2013a command I can view the hidden files of the sytem 63. In real time environment many people use \u201cll\u201d command instead of ls. So is there any command called \u201cll\u201d exits? No, there is no such command called \u201cll\u201d. It\u2019s just the alias of ls command. We can check it by typing alias command. 64. What is a shell ? Description of shell is huge, but yes commonly we explain it as the interpreter between the user and the machine. 65. Describe the usage of rm \u2013r* command in unix and shall we use it in real time environment? rm \u2013r* will remove all the file entries in the current directory. It is not advisable to use this command in real time environment. Specifically in production. Because we have huge files which are necessary to be accessed by other users. 66. What is symbolic link ? The second name of a file is called a link, it\u2019s assigned to create another link to the current file. 67. What is absolute path and relative path in unix ? Absolute path refers to the path starting from the root directory and the path continues with a sequence starting from Root. Whereas relative path is the current path. 68. How can I check the system IP ? type hostname command or else you can use ifconfig as well. 69. How can I check if a server is up and running or not ? you can use ping \u2013t command for this. Ping \u2013t <hostname> or <IP address> 70. How can I append some lines in an existing file ? #cat >> file name and hit enter. You can append lines below the existing lines of the file. And do a ctrl D to save and exit. 71. What is FIFO and LIFO in unix ? FIFO is first in first out and LIFO is last in last out. 72. What is PATH variable ? PATH is an environmental variable which contains the path of the command files and we can change the paths inside the PATH variable. 73. How can I kill a process in unix? first use PS \u2013ef command and get the PID of the process you want to kill. Then use kill -9 <PID_Number> command to kill the process. 74. How can I check the memory size of a linux/unix machine ? Use Free \u2013m or free \u2013G command to check the memory size of a linux machine. 75. How to check disk utilization of a linux server? Use du command to check the disk utilization. 76. How to check the disk free of all the mount points in unix ? use df \u2013h command, it will show the disk free of linux machine. 77. How can I check who are the users logged in my system? use users command. It will how the details of the users logged in to the system. 78. I have a file Mantu.txt which contains multiple lines and few of the lines has a particular pattern as \u201cIndia\u201d. I want to print only those lines. How can I ? I will ue grep command here. And the syntax will be as below. #grep -i \u201cIndia\u201d Mantu.txt grep command is used for pattern earching. 79. What do you mean by a super user ? A admin while giving permission to the users usually give normal access permission but few of the user having special permission then normal user, they are called super user. 80. What is the syntax to move to a super user? sudo su \u2013 <user name> 81. How can I change the permission of a file? Using chmod command I can change the permissions of a file. 82. How can I give all permission to a user? Use the below command to give all read, write and execute permission. chmod 777 <file name> 83. What is a process group in unix ? Collection of more than one process is called as a process group in unix.the function getpgrp returns the process group id. 84. How many numbers are used with kill while killing a process ? There are 64 numbers which can be used with kill command but generaly we use kill -9 85. What are different types of files available in unix? There are multiple type of files available in unix, few of among them are : \u2022 Regular file \u2022 Image file \u2022 Binary file \u2022 Linked file 86. What are cmp and different command in unix? cmp command compares the two files byte by byte and gives the output what is not common in between them. Diff command through the output which is not matching between the two file immediately rather comparing bit by bit. 87. What is pipe command ? why it is used for ? Pipe symbol interlinks two commands. It stores the output of the first command and give it to the second command as input. #cat emp.lst | mantu.txt 88. How can I number the lines of a file in VI editor? Open the file using vi <filename> Then go to command prompt and type set number. The numbers will be set before every line of the file. 89. What is the command to check all the options and detail information of a command in unix ? We can use man <command name>. it will show you all the possible way to use the command. 90. What is head command used for ? head command is used to view the top portions of the file. Say if you want to view top 5 lines of a file then you can use the below command. #cat <filename> | head -5 91. What is tail command used for ? tail command is used to view the bottom of the lines of a file. Say if I want to view bottom 5 lines of a file then I can use the below command. #cat <filename> | tail -5 92. What are the other commands used for pattern searching ? grep, awk and sed are the main command used for pattern searching. 93. How can I search a pattern in vi editor ? Open the file with vi. Use /pattern name , then hit enter, it will show you the matching patterns in VI. 94. How can I delete one line in Vi editor ? Use dd in command mode to delete one line of a file in vi. 95. What is the command used for copying a file? Use cp command while copying a file in unix. #cp <sourcepath of the file> <destination path of the file> 96. What is SCP in unix ? scp stands for secure copy in unix. The files which get copied by using scp command are decrypted so we need not be worry of hacking of the file system. 97. What is mv command in unix? We can move a file or rename a file using this command. General purpose of using mv command is to use it for reaming purpose. 98. What does a touch command do apart from creating a blank file? touch command is used to change the access and modification time of the file. 99. Explain the advantages of executing a process in background We use \u201c&\u201d symbol to execute a job in back ground. When we execute a job or process in unix it starts executing in the prompt itself and we can\u2019t do other stuffs in the command prompt at that time. So until unless the process gets executed we have to seat idle. So for continuous interaction with the command prompt we prefer executing the jobs or processes in back ground. 100. How do you protect file deletion in ext4? You change any attributes of the file to read only. The command is: #chattr +i filename And to disable it: #chattr -i filename (adsbygoogle = window.adsbygoogle || []).push({}); 101. How do you list the kerenel modules which is already loaded ? List Currently Loaded Modules \u2013 lsmod List Available Kernel Modules \u2013 modprobe -l Install New modules into Linux Kernel \u2013 modprobe vmhghs Remove the Currently Loaded Modul \u2013 modprobe -r vmhghs 102. What will happen in chkconfig? issuing the command \u201cchkconfig sendmail on\u201d will create symlinks(softlinks) /etc/rd1.d/K30sendmail /etc/rd2.d/S80sendmail /etc/rd3.d/S80sendmail /etc/rd4.d/S80sendmail /etc/rd5.d/S80sendmail /etc/rd6.d/K30sendmail 103. How To rebuild Corrupted RPM Database ? [root@tecmint]# cd /var/lib [root@tecmint]# rm __db* [root@tecmint]# rpm \u2013rebuilddb [root@tecmint]# rpmdb_verify Packages 104. what resize2fs do at back end? Mounted, Extending The kernel then begins writing additional filesystem metadata on the newly available storage. Unmounted,Shrinking resize2fs makes the filesystem use only the first size bytes of the storage. It does this by moving both filesystem metadata and your data around. After the completes, there will be unused storage at the end of the block device, unused by the filesystem. 105. Special Permissions in linux Sticky bit \u2013 Only created user and root can able to delete the file #chmod o+t nightwolf.txt #chmod +t nightwolf.txt #chmod 1777 nightwolf.txt #ls -ld nightwolf.txt drwxrwxrwt 2 root root 4096 Mar 24 12:19 nightwolf.txt SUID \u2013 Ging permission for all users like root chmod u+s /bin/ls \u2013 ls can be used for all users as like root # chmod 4555 [path_to_file] #ls -l /bin/ls -rwsr-xr-x-x 1 root user 16384 Jan 12 2014 /bin/ls SGID \u2013 SGUID :- chmod g+s /dir \u2013> all subdirectories and files created inside will get same group ownership as the main directory, it doesn\u2019t matter who is creating. #chmod 2555 [dir] #ls -l /usr/bin/write -r-xr-sr-x 1 root tty 11484 Jan 15 17:55 /usr/bin/write 106. Password never expire linux? # chage -M -1 nightwolf \u2013> set the max passwd age to -1 # passwd -x -1 nightwolf # chage -m 0 -M 99999 -I -1 -E -1 nightwolf 107. What files are created/modified when adding a user (useradd) in linux? /etc/passwd and /etc/shadow files from /etc/skel are typically copied into the new user\u2019s home directory 108. How to see and get info about RAM in your system # free # cat /proc/meminfo 109. How will you suspend a running process and put it in the background? Ctrl+z 110. Name the Daemon responsible for tracking System Event on your Linux box? Syslogd 111. To see tar file without extracting? tar -tvf 112. How to check dependencies of RPM Package on before Installing ? # rpm -qpR BitTorrent-5.2.2-1-Python2.4.noarch.rpm /usr/bin/python2.4 python >= 2.3 python(abi) = 2.4 python-crypto >= 2.0 python-psyco python-twisted >= 2.0 python-zopeinterface rpmlib(CompressedFileNames) = 2.6 q : Query a package p : List capabilities this package provides. R: List capabilities on which this package depends. 113. How can we increase disk read performance in single commands? To see the current read performance, # blockdev \u2013getra /dev/sdb 256 # time dd if=/tmp/disk.iso of=/dev/null bs=256k 2549+1 records in 2549+1 records out copied, 6,84256 seconds, 97,7 MB/s real 0m6.845s user 0m0.004s sys 0m0.865s # After test # blockdev \u2013setra 1024 /dev/sdb # time dd if=/tmp/disk.iso of=/dev/null bs=256k 2435+1 records in 2435+1 records out copied, 0,364251 seconds, 1,8 GB/s real 0m0.370s user 0m0.001s sys 0m0.370s 114. How Many Run Levels present in Linux? There are seven run levels, with each having its own properties. \u2022 Halt the system \u2022 Single-user mode \u2022 Multiuser mode without networking(NFS) \u2022 Multi-user mode with text login \u2022 Not used \u2022 Multi-user mode with graphical login \u2022 Reboot 115. How do i check which NFS version ? rpcinfo -p localhost | grep -i nfs rpm -qa | grep nfs rpm -qi nfs nfs-utils 116. Use find command to delete file by inode? Find and remove file using find command follows: # find . -inum 782263 -exec rm -i {} \\; 117. Check if any user is using the file system? Check to the what users are currently using the file system: # fuser -cu /dev/hdc1 /opt/backup: 2337c(root) 118. Explain ntsysv or chkconfig command Both are similar want all services to start in different runlevel # ntsysv \u2013level <level> # chkconfig \u2013list <service name> # chkconfig <service name> on # chkconfig <service name> \u2013level 3 119. Explained BOOT LOADER? The boot loader is then responsible for loading the kernel. A boot loader finds the kernel image on the disk, loads it into memory, starts it. Stage 1 boot loader: First stage the primary boot loader is to find and load the secondary boot loader. It will find by looking through the partition table for an active partition. This is verified methos to the active partition\u2019s boot record is read from the device into RAM and executed. Stage 2 boot loader The second-stage, boot loader called the kernel loader. The first- and second-stage boot loaders combined are calledGRand Unified Bootloader. With stage 2 loaded, GRUB can display a list of available kernels You can select a kernel parameters. 120. Explained about File System Labels? File system labels are useful where you need to address the file system that is on the device. The file system label is set, you can use it when mounting the device. The name replace to device by LABEL=labelname to do this To add a lable on ext3 filesystems # mkfs.ext4 -L mylabel /dev/sda2 To add a lable on exitsting filesystems # tune2fs -L mylabel /dev/sda2 121. To convert ext2 to ext3 filesystem? # tune2fs -j /dev/hda4 => Do it on your own risk. I would recommend to create a new filesystem of ext3 and copy the data from ext2 to ext3. 122. To convert ext3 to ext2 filesystem? # tune2fs -O^has-journal /dev/hda1 => Do it on your own risk. I would recommend to create a new filesystem of ext2 and copy the data from ext3 to ext2. 123. To convert ext2 to ext4 filesystem? # tune2fs -O dir_index,has_journal,uninit_bg /dev/hdXX # e2fsck -pf /dev/hdXX 124. To convert ext3 to ext4 filesystem? umount /dev/sda2 tune2fs -O extents,uninit_bg,dir_index /dev/sda2 e2fsck -pf /dev/sda2 mount /dev/sda2 /home 125. Explained Hash Tables? The bash shell maintains a hash table for each command which has been run. The reason, why it does so is, making the commands run faster. Whenever, a user runs a command on the shell, it first has to search the command executable as to where is it located. whenever the first time bash shell, finds the location of a command executable, it adds it to a hash table. The next time, same command is run, the path is taken from the hash table rather than searched again making the commands run faster. # hash hitscommand 7 /bin/grep 1 /usr/bin/which 1 /usr/bin/touch Reset the hash table # hash -r Delete the corresponding entry # hash -d myprint (adsbygoogle = window.adsbygoogle || []).push({}); 126. Following the program will not affected by this shell /sbin/nologin? FTP clients mail clients sudo many setuid programs telnet/login gdm/kdm/xdm (graphical login) su 127. So how do I find out zombie process? # ps aux | awk \u2018{ print $8 \u201d \u201d $2 }\u2019 | grep -w Z Output: Z 4104 Z 5320 Z 2945 128. How do I kill zombie process? ps axo ppid,stat | grep Z | awk \u2018{print $1}\u2019 | xargs kill -HUP kill -HUP $(ps -A -ostat,ppid | grep -e \u2018[zZ]\u2019| awk \u2018{ print $2 }\u2019) kill -9 $(ps -A -ostat,ppid | grep -e \u2018[zZ]\u2019| awk \u2018{ print $2 }\u2019) 129. Details about Backup? \u2022 full \u2013 as the name implies, this is a backup of everything \u2022 differential \u2013 this is a backup of everything since the last full backup \u2022 incremental \u2013 this is a backup of everything since the last _incremental_ backup This command will create a backup of /home and put that in the file /tmp/home.tar # tar -cvf /tmp/home.tar /home Create a backup of the directories /home /var /root and write that to the file /tmp/system-backup.tar # tar -cvf /tmp/system-backup.tar /home /var /root The following command makes a backup of /home and writes that to the /dev/mt0 device # tar -cvf /dev/mt0 /home 130. To create a compressed archive of the directory /home # tar -zcvf /tmp/home.tar.gz /home # tar -jcvf /tmp/home.tar.bz2 /home 131. Extracts the contents of the compressed file # tar -zxvf /file.tar.gz # tar -jxvf /file.tar.bz2 132. To check the contents of a tar file # tar -tvf file.tgz 133. Making Device Backups Using dd # dd if=/etc/hosts of=/home/somefile # dd if=/etc/passwd of=/home/file1 # dd if=/dev/sda of=/dev/sdb bs=4096 # dd if=backup.tar.gz of=/dev/mt0 134. To save MBR file backup as boot files in tmp directory # dd if=/dev/sda of=/tmp/bootfiles bs=512 count=1 135. Determining Filesystem Usage: To determine how much disk space is being used for a given partition, logical volume, or NFS mount, use the df command.To display the output in \u201chuman readable\u201d format, use the -h argument to df. The du command displays the disk usage totals for each subdirectory and finally the total usage for the current directory.Values are in kilobytes. # du -hs /etc # du -h /vol1/group1/examplefile 136. Reporting Disk Performance: For example, if the access time for a drive suddenly drops, an administrator must quickly start troubleshooting the problem to determine if it is a software or hardware issue or simply due to lack of free space on the disk. 137. Displaying Memory Usage with free # free -m The free command tells you about current memory usage. Two types of system memory exist: physical and virtual. To display the amount of free and used memory, both physical and virtual (swap), use the free command 138. Monitoring and Tuning the Kernel: Using the /proc Directory Instead of executing utilities such as free and top to determine the status of system resources or fdisk to view disk partitions, an administrator can gather system information directly from the kernel through the /proc filesystem. When you view the contents of files in /proc, you are really asking the kernel what the current state is for that particular device or subsystem. To view the contents of a special file in /proc, use the cat, less or more file viewing utilities. 139. Network Information Service (NIS) NIS can have only one authoritative server where the original data files are kept This authoritative server is called the master NIS server. If your organization is large enough, you may need to distribute the load across more than one machine. This can be done by setting up one or more secondary (slave) NIS servers. # echo \u201cNISDOMAIN=nis.nightwolf.in\u201d >> /etc/sysconfig/network # ypserv This daemon runs on the NIS server. It listens for queries from clients and responds with answers to those queries. # ypxfrd This daemon is used for propagating and transferring the NIS databases to slave servers. # ypbind This is the client-side component of NIS. It is responsible for finding an NIS server to be queried for information. The ypbind daemon binds NIS clients to an NIS domain. It must be running on any machines running NIS client programs. 140. Main config file for Yum Server in linux # cat /etc/yum.conf [main] cachedir=/var/cache/yum keepcache=0 debuglevel=2 logfile=/var/log/yum.log pkgpolicy=newest distroverpkg=redhat-release tolerant=1 exactarch=1 obsoletes=1 gpgcheck=1 plugins=1 metadata_expire=1800 141. Change User with noLogin Shell: # useradd -s /sbin/nologin nightwolf 142. Add a User with Home Directory, Custom Shell, Custom Comment and UID/GID # useradd -m -d /var/www/nightwolf -s /bin/zsh -c \u201cnightwolf web user\u201d -u 1000 -g 1000 nightwolf 143. Creating a user along with encrypted password in linux Encrypt your password using below command # openssl passwd -crypt myPa55w0rd 4VU5GpOSRWbeo Now you can use the encrypted the password for your new user # useradd -p 4VU5GpOSRWbeo nightwolf 144. Adding Information to User Account # usermod -c \u201cnightwolf user account\u201d nightwolf 145. Change User Home Directory # usermod -d /var/www/ nightwolf 146. Set User Account Expiry Date # usermod -e 2015-03-15 nightwolf 147. Change User Primary Group # usermod -g group_name nightwolf set the group_name group as a primary group to the user nightwolf 148. Adding Group to an Existing User # usermod -G nightwolf_test nightwolf \u2018nightwolf\u2018 user is added to group called \u2018nightwolf_test\u2018 149. Change User Login Name # usermod -l nightwolf_login_name nightwolf 150. Lock User Account # usermod -L nightwolf nightwolf:!$33341$HEWdPIJ.$qX/RbB.TPGcyerAVDlF4g.:12830:0:99999:7::: you will see a ! added before the encrypted password in /etc/shadow file, means password disabled. (adsbygoogle = window.adsbygoogle || []).push({}); 151. Unlock User Account # usermod -U nightwolf nightwolf:$33341$HEWdPIJ.$qX/RbB.TPGcyerAVDlF4g.:12830:0:99999:7::: 152. Change User Shell # usermod -s /bin/sh nightwolf 153. Change UID and GID of a User # usermod -u 666 -g 777 nightwolf 154. To check on the status of our RAID device # mdadm \u2013query \u2013detail /dev/md0 # cat /proc/mdstat 155. To create RAID disk # mdadm \u2013create /dev/md0 -l 1 -n 3 /dev/sda{5,6,7} # mdadm \u2013create /dev/md0 -l 1 -n 3 /dev/sda /dev/sdb /dev/sdc -x 2 /dev/sdd 156. To create RAID disk with spare disk # mdadm \u2013create /dev/md0 -l 1 -n 2 /dev/sda{5,6,} -x 1 /dev/sda7 # mdadm \u2013manage /dev/md0 \u2013stop # mdadm \u2013create /dev/md0 -l 5 -n 3 /dev/sda{5,6,7} -x 1 /dev/sda8 x\u2014\u2014\u2014> spare-devices 157. To create LVM on RAID 1 disk # pvcreate /dev/md0 # vgcreate datavg /dev/md0 # lvcreate -L +1G -n /dev/datavg/datalv 158. Disk Failure on RAID To simulate a disk failure, we\u2019ll use mdadm to tell the kernel that /dev/sdb1 has failed # mdadm \u2013manage /dev/md0 \u2013fail /dev/sda7 # cat /proc/mdstat sda7 [F] 159. How do remove failed disk from the RAID array # mdadm \u2013manage /dev/md0 \u2013remove /dev/sda7 160. To add raid device # mdadm \u2013manage /dev/md0 \u2013add /dev/sda9 161. To quickly check the state of all your RAID arrays # cat /proc/mdstat 162. userlist_enable vsftpd Will load a list of usernames from the filename specified by the userlist_file directive when this option is enabled. And if a user tries to log in using a name in this file, that user will be denied access before even being prompted for a password. The default value is NO. 163. userlist_deny This option is examined if the userlist_enable option is active. When its value is set to NO, users will be denied login, unless they are explicitly listed in the file specified by userlist_file. When login is denied, the denial is issued before the user is asked for a password; this helps prevent users from sending clear text across the network. The default value is YES. 163. userlist_file This option specifies the name of the file to be loaded when the userlist_enable option is active. The default value is vsftpd.user_list. 164. download_enable If set to NO, all download requests will be denied permission. The default value is YES. 165. write_enable This option controls whether any FTP commands that change the file system are allowed. These commands are used STOR, DELE, RNFR, RNTO, MKD, RMD, APPE, and SITE. The default value is NO. 166. UserDir This directive defines the subdirectory within each user\u2019s home directory, where users can place personal content that they want to make accessible via the web server. This directory is usually named public_html and is usually stored under each user\u2019s home directory. This option is, of course, dependent on the availability of the mod_userdir module in the web server setup. A sample usage of this option in the httpd.conf file is UserDir disable UserDir public_html 167. ErrorDocument The ErrorDocuments directive lets you specify what happens when a client asks for nonexistent document. Specifies a file that the server sends when an error of a specific type occurs. You can also provide a text message for an error. Here are some examples: ErrorDocument 403 \u201cSorry, you cannot access this directory\u201d ErrorDocument 403 /error/noindex.html ErrorDocument 404 /cgi-bin/bad_link.pl ErrorDocument 401 /new_subscriber.htm \u2022 400: Bad Request \u2022 401: Unauthorized \u2022 402: Payment Required \u2022 403: Forbidden \u2022 404: Not Found \u2022 405: Method Not Allowed \u2022 406: Not Acceptable \u2022 407: Proxy Authentication Required \u2022 408: Request Timeout \u2022 409: Conflict \u2022 410: Gone \u2022 411: Length Required \u2022 412: Precondition Failed \u2022 413: Request Entity Too Large \u2022 414: Request-URI Too Long \u2022 415: Unsupported Media Type \u2022 416: Requested Range Not Satisfiable \u2022 417: Expectation Failed \u2022 500: Internal Server Error \u2022 501: Not Implemented \u2022 502: Bad Gateway \u2022 503: Service Unavailable \u2022 504: Gateway Timeout \u2022 505: HTTP Version Not Supported 168. How to connect to a specific share using smbclient, use the following: # smbclient //<servername>/<sharename> -U <username> # smbclient //192.168.10.10/data -U nightwolf # vim /etc/samba/smb.conf workgroup=WORKGROUP hosts allow = <IP addresses> valid users: List of Samba users allowed access to the share. invalid users: List of Samba users denied access the share. If a user is listed in the valid users and the invalid users list, the user is denied access. public: If set to yes, password authentication is not required. Access is granted through the guest user with guest privileges. (default=no) read only: If set to yes, client users can not create, modify, or delete files in the share.(default=yes) printable: If set to yes, client users can open, write to, and submit spool files on the shared directory (default=no) hosts allow: List of clients allowed access to share. Use the command man 5 hosts_access for details on valid IP address formats. browseable: If set to no, the share will not be visible by a net view or a browse list. 169. Find Files Using Name in Current Directory? # find . -name nightwolf.txt 170. Find Files Under Home Directory? # find /home -name nightwolf.txt 171. Find all PHP Files in Directory? # find . -type f -name \u201c*.php\u201d 172. Find Files Without 777 Permissions? # find / -type f ! -perm 777 173. Find SGID Files with 644 Permissions? # find / -perm 2644 174. Find Sticky Bit Files with 551 Permissions? # find / -perm 1551 175. Find SUID Files? # find / -perm /u=s (adsbygoogle = window.adsbygoogle || []).push({}); 176. Find SGID Files? # find / -perm /g=s 177. Find Read Only Files? # find / -perm /u=r 178. Find Executable Files? # find / -perm /a=x 179. Find all Empty Files? # find /tmp -type f -empty 180. Find all Empty Directories? # find /tmp -type d -empty 181. File all Hidden Files? # find /tmp -type f -name \u201c.*\u201d 182. Find Single File Based on User? # find / -user root -name nightwolf.txt 183. Find all Files Based on User? # find /home -user nightwolf 184. Find all Files Based on Group? # find /home -group developer 185. Find Last 50 Days Modified Files? # find / -mtime -50 186. Find Last 50 Days Accessed Files? # find / -atime -50 187. Find Last 50-100 Days Modified Files? # find / -mtime +50 \u2013mtime -100 188. Find Changed Files in Last 1 Hour? # find / -cmin -60 189. Find Modified Files in Last 1 Hour? # find / -mmin -60 190. Find Accessed Files in Last 1 Hour? # find / -amin -60 191. Find 50MB Files? # find / -size 50M 192. Find using inode number? find . -inum 27492358 -exec rm -i {} \\; 193. Main configuration file of Apache server ? /etc/httpd/conf/httpd.conf 194. Main configuration file of Apache server ? /etc/httpd/conf/httpd.conf 195. Specify number of maximum open files in a single login based on the amount of system RAM. # echo \u201c1599383\u201d > /proc/sys/fs/file-max This can also be done by using sysctl sysctl command is used to change Kernel Parameters at run-time # sysctl -w fs.file-max=1599383 Kernel Parameters can also be changed by making changes in the below file:/etc/sysctl.conf Append the below line in the /etc/sysctl.conf file fs.file-max = 1599383 After making the above change run the below command for changes to reflect, loads the sysctl settings # sysctl -p 196. Increase the local port range, by default the port range is small? # echo \u201c1024 65535\u2033 > /proc/sys/net/ipv4/ip_local_port_rangeThis can also be done using sysctl command # sysctl -w net.ipv4.ip_local_port_range=\u201d1024 65535\u201d Append the below line in the /etc/sysctl.conf file net.ipv4.ip_local_port_range = 1024 61000 After making the above change run the below command for changes to reflect, loads the sysctl settings # sysctl \u2013p 197. Disable packet_forwarding(routing)? net.ipv4.ip_forward = 0 # cat /proc/sys/net/ipv4/ip_forward 198. Mount file systems with noatime options? noatime option means it will not update the file and directory access time. Main advantage is I/O performance will increase. 199. Which command is use to extend a logical volume? # lvextend \u2013size +<addsize> /dev/<vgname>/<lvname> # resize2fs /dev/<vgname>/<lvname> # lvextend -L +1G /dev/VolGroup/LogVol1 This will extend the partition size by +1 GB # resize2fs /dev/VolGroup/LogVol1 200. ServerAdmin : Email address This is the e-mail address that the server includes in error messages sent to the client. Defines the e-mail address that is shown when the server generates an error page. The e-mail address that the Web server provides to clients in case any errors occur. (adsbygoogle = window.adsbygoogle || []).push({}); 201. What is the use of SCP command in Linux? SCP command stands for secure copy. It is used to copy/download data from one machine to another machine. 202. What is telnet and what does it do? telnet command is used to check the connectivity to other servers. It helps you to check whether you are able to talk to another server or now. Ex: telnet 192.0.0.1 22 where 22 is the port number. 203. What is a bastion host? A bastion host is also known as a jump server. It is used to connect from one machine to another machine securely. Bastion hosts are used to connecting to private servers securely. 204. What is the command to find the IP address of the host machine in linux? You can use ifconfig/ ipaddr show command to find the IP address of the host machine. 205. Name some of the text editors that are available in Linux? Some of the common text editors that are available in Linux are vi/vim, nano, subl, gedit, atom, emacs. Vi is the default editor that you have in Linux machines. 206. What are the different zip files formats that are available in linux? The different zip formats in Linux are zip, gzip and bzip. 207. What is the difference between cp and mv command? cp command stands for copy and is used to copy data from one location to another. mv stands for the move and is used to move data from one location to another. 208. How can you run a process in the background in Linux? You can run a process in the background by pressing ctrl+z command. 209. What is the use of \u2018chown\u2019 command ? chown stands for \u2018change ownership\u2019 and is used to change the ownership of a file or directory. Eg: chown username.username <filename>. 210. What is the use of \u2018chmod\u2019 command? chmod stands for \u2018change mode\u2019 and is used to change the permissions on files or directories. Eg: # chmod a+w <filename> 211. What is the command to create a zip file in linux? To create a zip file you can use tar command with -cvzf arguments. Eg: tar -cvzf test.tar.gz <file names to be included in the zip> 212. What is the command to unzip the file in linux? To unzip a file you can tar command with -xvzf arguments. Eg: tar -xvzf test.tar.gz 213. What is the command to show the contents of a zip file ? To see the contents of the zip file you can use tar -tvzf arguments. Eg: tar -tvzf test.tar.gz 214. What is a soft link in Linux? A soft link is used to create a shortcut in Linux. This is similar to creating a shortcut in windows systems. 215. What is the command used to create a soft link? To create a soft link you can use ln command with -s arguments. Eg: ln -s /var/www/html html, where /var/www/html is the source file and HTML is the destination of the shortcut. 216. What is the command to remove the soft link in Linux? To remove the soft link in Linux you can use unlink command. Eg: unlink <filename> 217. What is the use of whereis command in linux? Whereis command is used to find the binaries and libraries files of an application in linux. 218. What is the use of man pages? Man pages stand for manual pages. It is the documentation about and helps you to understand the commands and how to use the commands. Eg: man wget. 219. what does \u201c2>\u201d indicate in redirection? This means that output will be shown on the screen and the errors will be written to a file that you specify. Eg: # ls /etc/test 2> error.txt 220. What are the different type of users that you have in Linux? You have 2 types of users in linux. They are \u2022 root user \u2022 standard users. 221. How To check Memory stats and CPU stats ? free & vmstat commands. 222. What is the purpose of runlevels ? Useful for debugging purpose. Basic idea is each runlevel has some services operational and depending on need can enable different runlevels to test which services are running 223. You are able to ping with a numeric IP address, but not by name. How will you debug ? First check /etc/resolv.conf file for DNS Server Entry 224. Generally setting up services in Linux require \u2014\u2014\u2014\u2014\u2014 and \u2014\u2014\u2014\u2014\u2013 Update the associated configuration file and ensure the appropriate daemon is started 225. What is the purpose of iptables command ? Basically allows rule creation to filter packets according to established criteria. Network Address Translation function is also done 226. You are noticing mails are not sent by sendmail. Where can you find the error log to see what happened ? /var/spool/mail/ 227. How can you findout the current runlevel the system is in ? # who -r 228. Linux system has crashed and keeps getting to Debug Prompt. How do you bring it to normal login prompt. Likely due to file system inconsistency, run fsck to check and accept inode repairs. 229. How can you customise startup settings for your login in bash shell You can use file .bashrc for this customisation. 230. What is the first process started by the Schedulerin RHEL 6. init 231. How can you find the current status of Virtual memory in the Linux System ? # cat /proc/meminfo * Note the very useful /proc filesystem 232. Hardware devices are identified as special files in Linux. Name the types. Character,block and Network 233. Name 3 Environment variables SHELL,HOME,PATH 234. Where is my vmlinuz executable loaded from ? /boot 235. As System Administrator you have to apply a patch that is in .tar.gz format. How would you use it ? Get the file and apply tar -zxvf <filename.tar.gz> 236. What are Kernel types and which one is Linux ? Monolithic and Micro. Linux is a Monolithic Kernel. 237. As System Admin, log files are monitored as they grow. How is this achieved ? tail -f 238. Automatic mounting of new file system at boot-time can be done by Adding an entry in /etc/fstab file 239. You recently ran an install, the command for which you need to recall. How do you get this? # history 240. In writing a Bash Shell script, special character\u2019 meaning has to be altered. How is it done? Escape sequence. Precede the special character with a \u2018\\\u2019 241. Linux associates devices with File Descriptors. List them. 0 \u2013 Standard Input 1- Standard Output 2-Standard Error (adsbygoogle = window.adsbygoogle || []).push({}); 242. How do you set a mask to stop certain permissions from being granted by default on file creation? # umask <value> 243. You want to try out a Distribution before installation. Which image would suit? LiveCD 244. System Administrators monitor load averages on System for analysis. How is this done? # top # uptime # w 245. What is the behaviour of the following very useful grep command ? # grep [abc] file1 : Looks for matches in file1 containing either an a or b or c character. 246. You need to make a file with only read permissions for yourself, group and all. How can you ? #chmod 444 file1 247. What does su \u2013 do ? Give an example of when you would use this. Gets the switch to root account. Need to be done before Software installation. 248. Pipe is a very important concept in process communication. Give an example # cat file1 |grep xyz : Pattern matching by grep happens on the displayed file1 249. In Shell scripting command return codes are checked prior to proceeding. Explain These are Exit status codes. Linux follows 0 for success and nonzero codes for failures 250. Signals is one way that process communication happens in Linux. What does ctrl C do ? Generates a SIGINT signal that stops the current process running in the shell. 251. To set a password to the boot loader: # grub grub> md5crypt Password: ************ Encrypted: $1$3yQFp$MEDEglsxOvuTWzWaztRly. grub> quit Next, add this to your grub.conf file like so: default=1 timeout=10 splashimage=(hd0,0)/grub/splash.xpm.gz password \u2013md5 $1$3yQFp$MEDEglsxOvuTWzWaztRly. Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Linux Interview Questions for Freshers"},{"location":"nightwolf-cotribution/linux_basic/#linux-interview-questions-and-answers-for-freshers-solved","text":"We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux Admin, Amazon interview questions for Linux admin and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); 1.What is two types of Linux User Mode ? Command Line GUI 2.What is command for created multiple files at a time? touch 3.What is INODE and How to Identify? Its unique identification code for files and directories, it was generate automatically while creating new file and directories ls -i filename ls -ldi directoryname 4.List of Permissions and Users Read, Write and Execute Owner, Group Owners and Others 5.List of Special Permissions and numerical value. Set User ID = 4 Set Group ID = 2 Stickybit = 1 6. What command to use see Process list in Hierarchical Structure along with PID? pstree -P 7. What is use of \u201ctop\u201d command and how to sort Memory and User wise? Its used to real time monitor hardware utilization of linux machine. Press M to sort Memory wise result Press U to sort User wise result 8. What is command for to force close one particular process kill -9 Processid 9.What is command to refresh NIC ? Service network restart 10.Tell me two types of IP Address configuration Static IP Address Dynamic IP Address 11.How do Enable / Disable Ethernet Device Open and Edit this file #vi /etc/sysconfig/network-scripts/devicename For enable ONBOOT = yes For disable ONBOOT =no 12.What is command to change Hostname without System Restart hostname newhostname hostnamectl 13.What is File Path of Network Configuration ? /etc/sysconfig/network-scripts 14.What is File Path of DNS Configuration ? /etc/resolv.conf 15.How to Update locate DB ? cd/var/lib/mlocate updatedb 16. How to edit and save file using editors? The following commands are used to exit from vi editors. 1. :wq saves the current work and exits the VI. 2. :q! exits the VI without saving current work. 17.What is command for Zip and Unzip files 1. gzip = Compress File 2. gunzip = Uncompress File 18.What is file path of Alias name set by Permanent? /etc/bashrc 19.What is MBR in linux? Its Master Boot Recorder to help booting operating system. 20.What is Two Types of Mount in linux? Temporary Mount Permanent Mount 21.What is command for delete Partition? #umount #palimpsest & OR #parted OR #fdisk 22.What is command for Refresh Partition? mount -a 23.What is SWAP? Linux uses swap space to increase the amount of virtual memory available to a host. It can use one or more dedicated swap partitions or a swap file on a regular filesystem or logical volume. 24.What are types can set SWAP? Temporary set Permanent set 25. What command use for Filesystem Error checking and Error Fixing fsck and e2fsck 26. What is PV, VG, and LVM PV = Physical Volume VG = Volume Group LVM = Logical Volume 27. What is LVM LVM is used to create logical partitions and during run time we can resize particular partition without data loss. Empty partitions only can do LVM creation. 28. What are common commands used for Physical Volume pvcreate pvs pvdisplay 29. What command is used for create Volume Group vgcreate vgs 30. What is Syntax for LVM Create? #lvcreate -L partitionsize -n userdefinename volumegroupname 31. What types of Installation Tools in REDHAT? RPM = Redhat Package Manager YUM = Yellow Dog Updated Modifier 32. Tell me Linux Boot Sequence Floow? BIOS \u2192 MBR \u2192 Boot Loader \u2192 Kernal \u2192 Runlevel 33. Types of Zone in DNS? Forward lookup zone Reverse lookup zone 34. What are inbuilt firwall in Linux ? IP Tables Selinux TCPwrappers 35. What command to Execute disable IPTables permanently? #iptables -F service iptables save 36. What is SELinux? Its one type of firewall in linux To block particular service in a Protocol 37. File to disable SELinux permanently: /etc/selinux/config 38. What is command to check selinux status ? getenforce 39. What is LDAP The Lightweight Directory Access Protocol (LDAP) is a set of open protocols used to access centrally stored information over a network. 40. Which Configuration File Is Required For Ldap Clients? ldap.conf 41. What Is The Name Of Main Configuration File Name For Ldap Server? slapd.conf 42. How Will You Verify Ldap Configuration File? slaptest -u 43. What is command package install using YUM without ask Prompt? yum install packagename -y 44. What is command Uninstall package? yum remove packagename 45. What is command package re-install using YUM without ask Prompt? yum reinstall packagename -y 46. Location of Cron file in linux? /var/spool/cron 47. What is command for to see Particular user Job Schedule ? crontab -lu username 48. What is command for restart cron service? service crond restart 49. What is command for restart postfix service? service postfix restart 50.What is command for FTP service on and restart? chkconfig vsftpd on service vsftpd restart 50. What is Kernel un Unix Operating system? Kernel is the heart of operating system. It interacts with shell and executes the machine level language. (adsbygoogle = window.adsbygoogle || []).push({}); 51. How can I save my input and output commands and see them when required? At the beginning of the session if you use 'script' command then the details of the input and output commands will be saved in a file called typescript and we can view it any time using \u201ccat typescript\u201d command. This is very useful to track what user is doing what. HISTORY command will not work because it shows data only for the current session. 52. How to create a file in Unix? There are multiple way to create files in unix, but the simple way to create a file is using \u201ccat\u201d and \u201cTouch\u201d command Syntax: cat > File name touch file name 53. How can I check which processes are running in my machine? To check process which are running in my machine I can use two commands. (a) TOP and (b) PS 54. What is the difference between TOP and PS command? Top command gives the dynamic view of the processes are running in the server and generally the dynamic change happens in every 3 second. Whereas PS commands gives the static view of the processes. 55. You used TOP command and without aborting the TOP process I need to kill one process. Is it possible to kill? Yes TOP command it self has a command prompt. Type K then it will ask you for the PID of the process to kill. Hit the PID and enter, it will kill the process. 56. What is the difference between creating a file in cat and in touch command? cat command creates a file and we can save some data inside the file but touch command by default will create a blank file. 57. How can I create multiple directories at a time? Say I want to create a directory D1 and inside that D2 and inside that D3. Is it possible? If yes how ? Yes, creating multiple directories is possible. In this scenario the below command works. #mkdir \u2013p D1/D2/D3 58. I want to create D1, under that D2 and D3. Inside D2 I want D4 and inside D3 I want D5 to be created. How is it possible? The below command will work for it. #mkdir \u2013p D1/D2/D4 D1/D3/D5 59. How can I check in which directory I am in ? Use PWD command to check which directory you are in. 60. We are using so many commands and getting output. Have you ever wondered how the commands are executing and getting you the output? Yes, every command in Unix is a C program in the backend. When we type a command and hit enter the program runs in the backend and gives you the output. We can view the C program as well as below. type <Command Name> ->hit enter, it will give you a path where the program the command is located. You can view the program by doing cat and the path name. it will open a C program file in decrypted mode. 61. How can I list the directories and the files ? Using ls command. I can view the directories and files of the system. 62. How can I view hidden files in a system ? Using ls \u2013a command I can view the hidden files of the sytem 63. In real time environment many people use \u201cll\u201d command instead of ls. So is there any command called \u201cll\u201d exits? No, there is no such command called \u201cll\u201d. It\u2019s just the alias of ls command. We can check it by typing alias command. 64. What is a shell ? Description of shell is huge, but yes commonly we explain it as the interpreter between the user and the machine. 65. Describe the usage of rm \u2013r* command in unix and shall we use it in real time environment? rm \u2013r* will remove all the file entries in the current directory. It is not advisable to use this command in real time environment. Specifically in production. Because we have huge files which are necessary to be accessed by other users. 66. What is symbolic link ? The second name of a file is called a link, it\u2019s assigned to create another link to the current file. 67. What is absolute path and relative path in unix ? Absolute path refers to the path starting from the root directory and the path continues with a sequence starting from Root. Whereas relative path is the current path. 68. How can I check the system IP ? type hostname command or else you can use ifconfig as well. 69. How can I check if a server is up and running or not ? you can use ping \u2013t command for this. Ping \u2013t <hostname> or <IP address> 70. How can I append some lines in an existing file ? #cat >> file name and hit enter. You can append lines below the existing lines of the file. And do a ctrl D to save and exit. 71. What is FIFO and LIFO in unix ? FIFO is first in first out and LIFO is last in last out. 72. What is PATH variable ? PATH is an environmental variable which contains the path of the command files and we can change the paths inside the PATH variable. 73. How can I kill a process in unix? first use PS \u2013ef command and get the PID of the process you want to kill. Then use kill -9 <PID_Number> command to kill the process. 74. How can I check the memory size of a linux/unix machine ? Use Free \u2013m or free \u2013G command to check the memory size of a linux machine. 75. How to check disk utilization of a linux server? Use du command to check the disk utilization. 76. How to check the disk free of all the mount points in unix ? use df \u2013h command, it will show the disk free of linux machine. 77. How can I check who are the users logged in my system? use users command. It will how the details of the users logged in to the system. 78. I have a file Mantu.txt which contains multiple lines and few of the lines has a particular pattern as \u201cIndia\u201d. I want to print only those lines. How can I ? I will ue grep command here. And the syntax will be as below. #grep -i \u201cIndia\u201d Mantu.txt grep command is used for pattern earching. 79. What do you mean by a super user ? A admin while giving permission to the users usually give normal access permission but few of the user having special permission then normal user, they are called super user. 80. What is the syntax to move to a super user? sudo su \u2013 <user name> 81. How can I change the permission of a file? Using chmod command I can change the permissions of a file. 82. How can I give all permission to a user? Use the below command to give all read, write and execute permission. chmod 777 <file name> 83. What is a process group in unix ? Collection of more than one process is called as a process group in unix.the function getpgrp returns the process group id. 84. How many numbers are used with kill while killing a process ? There are 64 numbers which can be used with kill command but generaly we use kill -9 85. What are different types of files available in unix? There are multiple type of files available in unix, few of among them are : \u2022 Regular file \u2022 Image file \u2022 Binary file \u2022 Linked file 86. What are cmp and different command in unix? cmp command compares the two files byte by byte and gives the output what is not common in between them. Diff command through the output which is not matching between the two file immediately rather comparing bit by bit. 87. What is pipe command ? why it is used for ? Pipe symbol interlinks two commands. It stores the output of the first command and give it to the second command as input. #cat emp.lst | mantu.txt 88. How can I number the lines of a file in VI editor? Open the file using vi <filename> Then go to command prompt and type set number. The numbers will be set before every line of the file. 89. What is the command to check all the options and detail information of a command in unix ? We can use man <command name>. it will show you all the possible way to use the command. 90. What is head command used for ? head command is used to view the top portions of the file. Say if you want to view top 5 lines of a file then you can use the below command. #cat <filename> | head -5 91. What is tail command used for ? tail command is used to view the bottom of the lines of a file. Say if I want to view bottom 5 lines of a file then I can use the below command. #cat <filename> | tail -5 92. What are the other commands used for pattern searching ? grep, awk and sed are the main command used for pattern searching. 93. How can I search a pattern in vi editor ? Open the file with vi. Use /pattern name , then hit enter, it will show you the matching patterns in VI. 94. How can I delete one line in Vi editor ? Use dd in command mode to delete one line of a file in vi. 95. What is the command used for copying a file? Use cp command while copying a file in unix. #cp <sourcepath of the file> <destination path of the file> 96. What is SCP in unix ? scp stands for secure copy in unix. The files which get copied by using scp command are decrypted so we need not be worry of hacking of the file system. 97. What is mv command in unix? We can move a file or rename a file using this command. General purpose of using mv command is to use it for reaming purpose. 98. What does a touch command do apart from creating a blank file? touch command is used to change the access and modification time of the file. 99. Explain the advantages of executing a process in background We use \u201c&\u201d symbol to execute a job in back ground. When we execute a job or process in unix it starts executing in the prompt itself and we can\u2019t do other stuffs in the command prompt at that time. So until unless the process gets executed we have to seat idle. So for continuous interaction with the command prompt we prefer executing the jobs or processes in back ground. 100. How do you protect file deletion in ext4? You change any attributes of the file to read only. The command is: #chattr +i filename And to disable it: #chattr -i filename (adsbygoogle = window.adsbygoogle || []).push({}); 101. How do you list the kerenel modules which is already loaded ? List Currently Loaded Modules \u2013 lsmod List Available Kernel Modules \u2013 modprobe -l Install New modules into Linux Kernel \u2013 modprobe vmhghs Remove the Currently Loaded Modul \u2013 modprobe -r vmhghs 102. What will happen in chkconfig? issuing the command \u201cchkconfig sendmail on\u201d will create symlinks(softlinks) /etc/rd1.d/K30sendmail /etc/rd2.d/S80sendmail /etc/rd3.d/S80sendmail /etc/rd4.d/S80sendmail /etc/rd5.d/S80sendmail /etc/rd6.d/K30sendmail 103. How To rebuild Corrupted RPM Database ? [root@tecmint]# cd /var/lib [root@tecmint]# rm __db* [root@tecmint]# rpm \u2013rebuilddb [root@tecmint]# rpmdb_verify Packages 104. what resize2fs do at back end? Mounted, Extending The kernel then begins writing additional filesystem metadata on the newly available storage. Unmounted,Shrinking resize2fs makes the filesystem use only the first size bytes of the storage. It does this by moving both filesystem metadata and your data around. After the completes, there will be unused storage at the end of the block device, unused by the filesystem. 105. Special Permissions in linux Sticky bit \u2013 Only created user and root can able to delete the file #chmod o+t nightwolf.txt #chmod +t nightwolf.txt #chmod 1777 nightwolf.txt #ls -ld nightwolf.txt drwxrwxrwt 2 root root 4096 Mar 24 12:19 nightwolf.txt SUID \u2013 Ging permission for all users like root chmod u+s /bin/ls \u2013 ls can be used for all users as like root # chmod 4555 [path_to_file] #ls -l /bin/ls -rwsr-xr-x-x 1 root user 16384 Jan 12 2014 /bin/ls SGID \u2013 SGUID :- chmod g+s /dir \u2013> all subdirectories and files created inside will get same group ownership as the main directory, it doesn\u2019t matter who is creating. #chmod 2555 [dir] #ls -l /usr/bin/write -r-xr-sr-x 1 root tty 11484 Jan 15 17:55 /usr/bin/write 106. Password never expire linux? # chage -M -1 nightwolf \u2013> set the max passwd age to -1 # passwd -x -1 nightwolf # chage -m 0 -M 99999 -I -1 -E -1 nightwolf 107. What files are created/modified when adding a user (useradd) in linux? /etc/passwd and /etc/shadow files from /etc/skel are typically copied into the new user\u2019s home directory 108. How to see and get info about RAM in your system # free # cat /proc/meminfo 109. How will you suspend a running process and put it in the background? Ctrl+z 110. Name the Daemon responsible for tracking System Event on your Linux box? Syslogd 111. To see tar file without extracting? tar -tvf 112. How to check dependencies of RPM Package on before Installing ? # rpm -qpR BitTorrent-5.2.2-1-Python2.4.noarch.rpm /usr/bin/python2.4 python >= 2.3 python(abi) = 2.4 python-crypto >= 2.0 python-psyco python-twisted >= 2.0 python-zopeinterface rpmlib(CompressedFileNames) = 2.6 q : Query a package p : List capabilities this package provides. R: List capabilities on which this package depends. 113. How can we increase disk read performance in single commands? To see the current read performance, # blockdev \u2013getra /dev/sdb 256 # time dd if=/tmp/disk.iso of=/dev/null bs=256k 2549+1 records in 2549+1 records out copied, 6,84256 seconds, 97,7 MB/s real 0m6.845s user 0m0.004s sys 0m0.865s # After test # blockdev \u2013setra 1024 /dev/sdb # time dd if=/tmp/disk.iso of=/dev/null bs=256k 2435+1 records in 2435+1 records out copied, 0,364251 seconds, 1,8 GB/s real 0m0.370s user 0m0.001s sys 0m0.370s 114. How Many Run Levels present in Linux? There are seven run levels, with each having its own properties. \u2022 Halt the system \u2022 Single-user mode \u2022 Multiuser mode without networking(NFS) \u2022 Multi-user mode with text login \u2022 Not used \u2022 Multi-user mode with graphical login \u2022 Reboot 115. How do i check which NFS version ? rpcinfo -p localhost | grep -i nfs rpm -qa | grep nfs rpm -qi nfs nfs-utils 116. Use find command to delete file by inode? Find and remove file using find command follows: # find . -inum 782263 -exec rm -i {} \\; 117. Check if any user is using the file system? Check to the what users are currently using the file system: # fuser -cu /dev/hdc1 /opt/backup: 2337c(root) 118. Explain ntsysv or chkconfig command Both are similar want all services to start in different runlevel # ntsysv \u2013level <level> # chkconfig \u2013list <service name> # chkconfig <service name> on # chkconfig <service name> \u2013level 3 119. Explained BOOT LOADER? The boot loader is then responsible for loading the kernel. A boot loader finds the kernel image on the disk, loads it into memory, starts it. Stage 1 boot loader: First stage the primary boot loader is to find and load the secondary boot loader. It will find by looking through the partition table for an active partition. This is verified methos to the active partition\u2019s boot record is read from the device into RAM and executed. Stage 2 boot loader The second-stage, boot loader called the kernel loader. The first- and second-stage boot loaders combined are calledGRand Unified Bootloader. With stage 2 loaded, GRUB can display a list of available kernels You can select a kernel parameters. 120. Explained about File System Labels? File system labels are useful where you need to address the file system that is on the device. The file system label is set, you can use it when mounting the device. The name replace to device by LABEL=labelname to do this To add a lable on ext3 filesystems # mkfs.ext4 -L mylabel /dev/sda2 To add a lable on exitsting filesystems # tune2fs -L mylabel /dev/sda2 121. To convert ext2 to ext3 filesystem? # tune2fs -j /dev/hda4 => Do it on your own risk. I would recommend to create a new filesystem of ext3 and copy the data from ext2 to ext3. 122. To convert ext3 to ext2 filesystem? # tune2fs -O^has-journal /dev/hda1 => Do it on your own risk. I would recommend to create a new filesystem of ext2 and copy the data from ext3 to ext2. 123. To convert ext2 to ext4 filesystem? # tune2fs -O dir_index,has_journal,uninit_bg /dev/hdXX # e2fsck -pf /dev/hdXX 124. To convert ext3 to ext4 filesystem? umount /dev/sda2 tune2fs -O extents,uninit_bg,dir_index /dev/sda2 e2fsck -pf /dev/sda2 mount /dev/sda2 /home 125. Explained Hash Tables? The bash shell maintains a hash table for each command which has been run. The reason, why it does so is, making the commands run faster. Whenever, a user runs a command on the shell, it first has to search the command executable as to where is it located. whenever the first time bash shell, finds the location of a command executable, it adds it to a hash table. The next time, same command is run, the path is taken from the hash table rather than searched again making the commands run faster. # hash hitscommand 7 /bin/grep 1 /usr/bin/which 1 /usr/bin/touch Reset the hash table # hash -r Delete the corresponding entry # hash -d myprint (adsbygoogle = window.adsbygoogle || []).push({}); 126. Following the program will not affected by this shell /sbin/nologin? FTP clients mail clients sudo many setuid programs telnet/login gdm/kdm/xdm (graphical login) su 127. So how do I find out zombie process? # ps aux | awk \u2018{ print $8 \u201d \u201d $2 }\u2019 | grep -w Z Output: Z 4104 Z 5320 Z 2945 128. How do I kill zombie process? ps axo ppid,stat | grep Z | awk \u2018{print $1}\u2019 | xargs kill -HUP kill -HUP $(ps -A -ostat,ppid | grep -e \u2018[zZ]\u2019| awk \u2018{ print $2 }\u2019) kill -9 $(ps -A -ostat,ppid | grep -e \u2018[zZ]\u2019| awk \u2018{ print $2 }\u2019) 129. Details about Backup? \u2022 full \u2013 as the name implies, this is a backup of everything \u2022 differential \u2013 this is a backup of everything since the last full backup \u2022 incremental \u2013 this is a backup of everything since the last _incremental_ backup This command will create a backup of /home and put that in the file /tmp/home.tar # tar -cvf /tmp/home.tar /home Create a backup of the directories /home /var /root and write that to the file /tmp/system-backup.tar # tar -cvf /tmp/system-backup.tar /home /var /root The following command makes a backup of /home and writes that to the /dev/mt0 device # tar -cvf /dev/mt0 /home 130. To create a compressed archive of the directory /home # tar -zcvf /tmp/home.tar.gz /home # tar -jcvf /tmp/home.tar.bz2 /home 131. Extracts the contents of the compressed file # tar -zxvf /file.tar.gz # tar -jxvf /file.tar.bz2 132. To check the contents of a tar file # tar -tvf file.tgz 133. Making Device Backups Using dd # dd if=/etc/hosts of=/home/somefile # dd if=/etc/passwd of=/home/file1 # dd if=/dev/sda of=/dev/sdb bs=4096 # dd if=backup.tar.gz of=/dev/mt0 134. To save MBR file backup as boot files in tmp directory # dd if=/dev/sda of=/tmp/bootfiles bs=512 count=1 135. Determining Filesystem Usage: To determine how much disk space is being used for a given partition, logical volume, or NFS mount, use the df command.To display the output in \u201chuman readable\u201d format, use the -h argument to df. The du command displays the disk usage totals for each subdirectory and finally the total usage for the current directory.Values are in kilobytes. # du -hs /etc # du -h /vol1/group1/examplefile 136. Reporting Disk Performance: For example, if the access time for a drive suddenly drops, an administrator must quickly start troubleshooting the problem to determine if it is a software or hardware issue or simply due to lack of free space on the disk. 137. Displaying Memory Usage with free # free -m The free command tells you about current memory usage. Two types of system memory exist: physical and virtual. To display the amount of free and used memory, both physical and virtual (swap), use the free command 138. Monitoring and Tuning the Kernel: Using the /proc Directory Instead of executing utilities such as free and top to determine the status of system resources or fdisk to view disk partitions, an administrator can gather system information directly from the kernel through the /proc filesystem. When you view the contents of files in /proc, you are really asking the kernel what the current state is for that particular device or subsystem. To view the contents of a special file in /proc, use the cat, less or more file viewing utilities. 139. Network Information Service (NIS) NIS can have only one authoritative server where the original data files are kept This authoritative server is called the master NIS server. If your organization is large enough, you may need to distribute the load across more than one machine. This can be done by setting up one or more secondary (slave) NIS servers. # echo \u201cNISDOMAIN=nis.nightwolf.in\u201d >> /etc/sysconfig/network # ypserv This daemon runs on the NIS server. It listens for queries from clients and responds with answers to those queries. # ypxfrd This daemon is used for propagating and transferring the NIS databases to slave servers. # ypbind This is the client-side component of NIS. It is responsible for finding an NIS server to be queried for information. The ypbind daemon binds NIS clients to an NIS domain. It must be running on any machines running NIS client programs. 140. Main config file for Yum Server in linux # cat /etc/yum.conf [main] cachedir=/var/cache/yum keepcache=0 debuglevel=2 logfile=/var/log/yum.log pkgpolicy=newest distroverpkg=redhat-release tolerant=1 exactarch=1 obsoletes=1 gpgcheck=1 plugins=1 metadata_expire=1800 141. Change User with noLogin Shell: # useradd -s /sbin/nologin nightwolf 142. Add a User with Home Directory, Custom Shell, Custom Comment and UID/GID # useradd -m -d /var/www/nightwolf -s /bin/zsh -c \u201cnightwolf web user\u201d -u 1000 -g 1000 nightwolf 143. Creating a user along with encrypted password in linux Encrypt your password using below command # openssl passwd -crypt myPa55w0rd 4VU5GpOSRWbeo Now you can use the encrypted the password for your new user # useradd -p 4VU5GpOSRWbeo nightwolf 144. Adding Information to User Account # usermod -c \u201cnightwolf user account\u201d nightwolf 145. Change User Home Directory # usermod -d /var/www/ nightwolf 146. Set User Account Expiry Date # usermod -e 2015-03-15 nightwolf 147. Change User Primary Group # usermod -g group_name nightwolf set the group_name group as a primary group to the user nightwolf 148. Adding Group to an Existing User # usermod -G nightwolf_test nightwolf \u2018nightwolf\u2018 user is added to group called \u2018nightwolf_test\u2018 149. Change User Login Name # usermod -l nightwolf_login_name nightwolf 150. Lock User Account # usermod -L nightwolf nightwolf:!$33341$HEWdPIJ.$qX/RbB.TPGcyerAVDlF4g.:12830:0:99999:7::: you will see a ! added before the encrypted password in /etc/shadow file, means password disabled. (adsbygoogle = window.adsbygoogle || []).push({}); 151. Unlock User Account # usermod -U nightwolf nightwolf:$33341$HEWdPIJ.$qX/RbB.TPGcyerAVDlF4g.:12830:0:99999:7::: 152. Change User Shell # usermod -s /bin/sh nightwolf 153. Change UID and GID of a User # usermod -u 666 -g 777 nightwolf 154. To check on the status of our RAID device # mdadm \u2013query \u2013detail /dev/md0 # cat /proc/mdstat 155. To create RAID disk # mdadm \u2013create /dev/md0 -l 1 -n 3 /dev/sda{5,6,7} # mdadm \u2013create /dev/md0 -l 1 -n 3 /dev/sda /dev/sdb /dev/sdc -x 2 /dev/sdd 156. To create RAID disk with spare disk # mdadm \u2013create /dev/md0 -l 1 -n 2 /dev/sda{5,6,} -x 1 /dev/sda7 # mdadm \u2013manage /dev/md0 \u2013stop # mdadm \u2013create /dev/md0 -l 5 -n 3 /dev/sda{5,6,7} -x 1 /dev/sda8 x\u2014\u2014\u2014> spare-devices 157. To create LVM on RAID 1 disk # pvcreate /dev/md0 # vgcreate datavg /dev/md0 # lvcreate -L +1G -n /dev/datavg/datalv 158. Disk Failure on RAID To simulate a disk failure, we\u2019ll use mdadm to tell the kernel that /dev/sdb1 has failed # mdadm \u2013manage /dev/md0 \u2013fail /dev/sda7 # cat /proc/mdstat sda7 [F] 159. How do remove failed disk from the RAID array # mdadm \u2013manage /dev/md0 \u2013remove /dev/sda7 160. To add raid device # mdadm \u2013manage /dev/md0 \u2013add /dev/sda9 161. To quickly check the state of all your RAID arrays # cat /proc/mdstat 162. userlist_enable vsftpd Will load a list of usernames from the filename specified by the userlist_file directive when this option is enabled. And if a user tries to log in using a name in this file, that user will be denied access before even being prompted for a password. The default value is NO. 163. userlist_deny This option is examined if the userlist_enable option is active. When its value is set to NO, users will be denied login, unless they are explicitly listed in the file specified by userlist_file. When login is denied, the denial is issued before the user is asked for a password; this helps prevent users from sending clear text across the network. The default value is YES. 163. userlist_file This option specifies the name of the file to be loaded when the userlist_enable option is active. The default value is vsftpd.user_list. 164. download_enable If set to NO, all download requests will be denied permission. The default value is YES. 165. write_enable This option controls whether any FTP commands that change the file system are allowed. These commands are used STOR, DELE, RNFR, RNTO, MKD, RMD, APPE, and SITE. The default value is NO. 166. UserDir This directive defines the subdirectory within each user\u2019s home directory, where users can place personal content that they want to make accessible via the web server. This directory is usually named public_html and is usually stored under each user\u2019s home directory. This option is, of course, dependent on the availability of the mod_userdir module in the web server setup. A sample usage of this option in the httpd.conf file is UserDir disable UserDir public_html 167. ErrorDocument The ErrorDocuments directive lets you specify what happens when a client asks for nonexistent document. Specifies a file that the server sends when an error of a specific type occurs. You can also provide a text message for an error. Here are some examples: ErrorDocument 403 \u201cSorry, you cannot access this directory\u201d ErrorDocument 403 /error/noindex.html ErrorDocument 404 /cgi-bin/bad_link.pl ErrorDocument 401 /new_subscriber.htm \u2022 400: Bad Request \u2022 401: Unauthorized \u2022 402: Payment Required \u2022 403: Forbidden \u2022 404: Not Found \u2022 405: Method Not Allowed \u2022 406: Not Acceptable \u2022 407: Proxy Authentication Required \u2022 408: Request Timeout \u2022 409: Conflict \u2022 410: Gone \u2022 411: Length Required \u2022 412: Precondition Failed \u2022 413: Request Entity Too Large \u2022 414: Request-URI Too Long \u2022 415: Unsupported Media Type \u2022 416: Requested Range Not Satisfiable \u2022 417: Expectation Failed \u2022 500: Internal Server Error \u2022 501: Not Implemented \u2022 502: Bad Gateway \u2022 503: Service Unavailable \u2022 504: Gateway Timeout \u2022 505: HTTP Version Not Supported 168. How to connect to a specific share using smbclient, use the following: # smbclient //<servername>/<sharename> -U <username> # smbclient //192.168.10.10/data -U nightwolf # vim /etc/samba/smb.conf workgroup=WORKGROUP hosts allow = <IP addresses> valid users: List of Samba users allowed access to the share. invalid users: List of Samba users denied access the share. If a user is listed in the valid users and the invalid users list, the user is denied access. public: If set to yes, password authentication is not required. Access is granted through the guest user with guest privileges. (default=no) read only: If set to yes, client users can not create, modify, or delete files in the share.(default=yes) printable: If set to yes, client users can open, write to, and submit spool files on the shared directory (default=no) hosts allow: List of clients allowed access to share. Use the command man 5 hosts_access for details on valid IP address formats. browseable: If set to no, the share will not be visible by a net view or a browse list. 169. Find Files Using Name in Current Directory? # find . -name nightwolf.txt 170. Find Files Under Home Directory? # find /home -name nightwolf.txt 171. Find all PHP Files in Directory? # find . -type f -name \u201c*.php\u201d 172. Find Files Without 777 Permissions? # find / -type f ! -perm 777 173. Find SGID Files with 644 Permissions? # find / -perm 2644 174. Find Sticky Bit Files with 551 Permissions? # find / -perm 1551 175. Find SUID Files? # find / -perm /u=s (adsbygoogle = window.adsbygoogle || []).push({}); 176. Find SGID Files? # find / -perm /g=s 177. Find Read Only Files? # find / -perm /u=r 178. Find Executable Files? # find / -perm /a=x 179. Find all Empty Files? # find /tmp -type f -empty 180. Find all Empty Directories? # find /tmp -type d -empty 181. File all Hidden Files? # find /tmp -type f -name \u201c.*\u201d 182. Find Single File Based on User? # find / -user root -name nightwolf.txt 183. Find all Files Based on User? # find /home -user nightwolf 184. Find all Files Based on Group? # find /home -group developer 185. Find Last 50 Days Modified Files? # find / -mtime -50 186. Find Last 50 Days Accessed Files? # find / -atime -50 187. Find Last 50-100 Days Modified Files? # find / -mtime +50 \u2013mtime -100 188. Find Changed Files in Last 1 Hour? # find / -cmin -60 189. Find Modified Files in Last 1 Hour? # find / -mmin -60 190. Find Accessed Files in Last 1 Hour? # find / -amin -60 191. Find 50MB Files? # find / -size 50M 192. Find using inode number? find . -inum 27492358 -exec rm -i {} \\; 193. Main configuration file of Apache server ? /etc/httpd/conf/httpd.conf 194. Main configuration file of Apache server ? /etc/httpd/conf/httpd.conf 195. Specify number of maximum open files in a single login based on the amount of system RAM. # echo \u201c1599383\u201d > /proc/sys/fs/file-max This can also be done by using sysctl sysctl command is used to change Kernel Parameters at run-time # sysctl -w fs.file-max=1599383 Kernel Parameters can also be changed by making changes in the below file:/etc/sysctl.conf Append the below line in the /etc/sysctl.conf file fs.file-max = 1599383 After making the above change run the below command for changes to reflect, loads the sysctl settings # sysctl -p 196. Increase the local port range, by default the port range is small? # echo \u201c1024 65535\u2033 > /proc/sys/net/ipv4/ip_local_port_rangeThis can also be done using sysctl command # sysctl -w net.ipv4.ip_local_port_range=\u201d1024 65535\u201d Append the below line in the /etc/sysctl.conf file net.ipv4.ip_local_port_range = 1024 61000 After making the above change run the below command for changes to reflect, loads the sysctl settings # sysctl \u2013p 197. Disable packet_forwarding(routing)? net.ipv4.ip_forward = 0 # cat /proc/sys/net/ipv4/ip_forward 198. Mount file systems with noatime options? noatime option means it will not update the file and directory access time. Main advantage is I/O performance will increase. 199. Which command is use to extend a logical volume? # lvextend \u2013size +<addsize> /dev/<vgname>/<lvname> # resize2fs /dev/<vgname>/<lvname> # lvextend -L +1G /dev/VolGroup/LogVol1 This will extend the partition size by +1 GB # resize2fs /dev/VolGroup/LogVol1 200. ServerAdmin : Email address This is the e-mail address that the server includes in error messages sent to the client. Defines the e-mail address that is shown when the server generates an error page. The e-mail address that the Web server provides to clients in case any errors occur. (adsbygoogle = window.adsbygoogle || []).push({}); 201. What is the use of SCP command in Linux? SCP command stands for secure copy. It is used to copy/download data from one machine to another machine. 202. What is telnet and what does it do? telnet command is used to check the connectivity to other servers. It helps you to check whether you are able to talk to another server or now. Ex: telnet 192.0.0.1 22 where 22 is the port number. 203. What is a bastion host? A bastion host is also known as a jump server. It is used to connect from one machine to another machine securely. Bastion hosts are used to connecting to private servers securely. 204. What is the command to find the IP address of the host machine in linux? You can use ifconfig/ ipaddr show command to find the IP address of the host machine. 205. Name some of the text editors that are available in Linux? Some of the common text editors that are available in Linux are vi/vim, nano, subl, gedit, atom, emacs. Vi is the default editor that you have in Linux machines. 206. What are the different zip files formats that are available in linux? The different zip formats in Linux are zip, gzip and bzip. 207. What is the difference between cp and mv command? cp command stands for copy and is used to copy data from one location to another. mv stands for the move and is used to move data from one location to another. 208. How can you run a process in the background in Linux? You can run a process in the background by pressing ctrl+z command. 209. What is the use of \u2018chown\u2019 command ? chown stands for \u2018change ownership\u2019 and is used to change the ownership of a file or directory. Eg: chown username.username <filename>. 210. What is the use of \u2018chmod\u2019 command? chmod stands for \u2018change mode\u2019 and is used to change the permissions on files or directories. Eg: # chmod a+w <filename> 211. What is the command to create a zip file in linux? To create a zip file you can use tar command with -cvzf arguments. Eg: tar -cvzf test.tar.gz <file names to be included in the zip> 212. What is the command to unzip the file in linux? To unzip a file you can tar command with -xvzf arguments. Eg: tar -xvzf test.tar.gz 213. What is the command to show the contents of a zip file ? To see the contents of the zip file you can use tar -tvzf arguments. Eg: tar -tvzf test.tar.gz 214. What is a soft link in Linux? A soft link is used to create a shortcut in Linux. This is similar to creating a shortcut in windows systems. 215. What is the command used to create a soft link? To create a soft link you can use ln command with -s arguments. Eg: ln -s /var/www/html html, where /var/www/html is the source file and HTML is the destination of the shortcut. 216. What is the command to remove the soft link in Linux? To remove the soft link in Linux you can use unlink command. Eg: unlink <filename> 217. What is the use of whereis command in linux? Whereis command is used to find the binaries and libraries files of an application in linux. 218. What is the use of man pages? Man pages stand for manual pages. It is the documentation about and helps you to understand the commands and how to use the commands. Eg: man wget. 219. what does \u201c2>\u201d indicate in redirection? This means that output will be shown on the screen and the errors will be written to a file that you specify. Eg: # ls /etc/test 2> error.txt 220. What are the different type of users that you have in Linux? You have 2 types of users in linux. They are \u2022 root user \u2022 standard users. 221. How To check Memory stats and CPU stats ? free & vmstat commands. 222. What is the purpose of runlevels ? Useful for debugging purpose. Basic idea is each runlevel has some services operational and depending on need can enable different runlevels to test which services are running 223. You are able to ping with a numeric IP address, but not by name. How will you debug ? First check /etc/resolv.conf file for DNS Server Entry 224. Generally setting up services in Linux require \u2014\u2014\u2014\u2014\u2014 and \u2014\u2014\u2014\u2014\u2013 Update the associated configuration file and ensure the appropriate daemon is started 225. What is the purpose of iptables command ? Basically allows rule creation to filter packets according to established criteria. Network Address Translation function is also done 226. You are noticing mails are not sent by sendmail. Where can you find the error log to see what happened ? /var/spool/mail/ 227. How can you findout the current runlevel the system is in ? # who -r 228. Linux system has crashed and keeps getting to Debug Prompt. How do you bring it to normal login prompt. Likely due to file system inconsistency, run fsck to check and accept inode repairs. 229. How can you customise startup settings for your login in bash shell You can use file .bashrc for this customisation. 230. What is the first process started by the Schedulerin RHEL 6. init 231. How can you find the current status of Virtual memory in the Linux System ? # cat /proc/meminfo * Note the very useful /proc filesystem 232. Hardware devices are identified as special files in Linux. Name the types. Character,block and Network 233. Name 3 Environment variables SHELL,HOME,PATH 234. Where is my vmlinuz executable loaded from ? /boot 235. As System Administrator you have to apply a patch that is in .tar.gz format. How would you use it ? Get the file and apply tar -zxvf <filename.tar.gz> 236. What are Kernel types and which one is Linux ? Monolithic and Micro. Linux is a Monolithic Kernel. 237. As System Admin, log files are monitored as they grow. How is this achieved ? tail -f 238. Automatic mounting of new file system at boot-time can be done by Adding an entry in /etc/fstab file 239. You recently ran an install, the command for which you need to recall. How do you get this? # history 240. In writing a Bash Shell script, special character\u2019 meaning has to be altered. How is it done? Escape sequence. Precede the special character with a \u2018\\\u2019 241. Linux associates devices with File Descriptors. List them. 0 \u2013 Standard Input 1- Standard Output 2-Standard Error (adsbygoogle = window.adsbygoogle || []).push({}); 242. How do you set a mask to stop certain permissions from being granted by default on file creation? # umask <value> 243. You want to try out a Distribution before installation. Which image would suit? LiveCD 244. System Administrators monitor load averages on System for analysis. How is this done? # top # uptime # w 245. What is the behaviour of the following very useful grep command ? # grep [abc] file1 : Looks for matches in file1 containing either an a or b or c character. 246. You need to make a file with only read permissions for yourself, group and all. How can you ? #chmod 444 file1 247. What does su \u2013 do ? Give an example of when you would use this. Gets the switch to root account. Need to be done before Software installation. 248. Pipe is a very important concept in process communication. Give an example # cat file1 |grep xyz : Pattern matching by grep happens on the displayed file1 249. In Shell scripting command return codes are checked prior to proceeding. Explain These are Exit status codes. Linux follows 0 for success and nonzero codes for failures 250. Signals is one way that process communication happens in Linux. What does ctrl C do ? Generates a SIGINT signal that stops the current process running in the shell. 251. To set a password to the boot loader: # grub grub> md5crypt Password: ************ Encrypted: $1$3yQFp$MEDEglsxOvuTWzWaztRly. grub> quit Next, add this to your grub.conf file like so: default=1 timeout=10 splashimage=(hd0,0)/grub/splash.xpm.gz password \u2013md5 $1$3yQFp$MEDEglsxOvuTWzWaztRly.","title":"Linux Interview Questions and Answers for Freshers - Solved:"},{"location":"nightwolf-cotribution/linux_interview_questions_for_freshers/","text":"Linux 200+ Interview Questions for Freshers - Solved: \uf0c1 We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux Admin, Amazon interview questions for Linux admin and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); When you login you get \u201c$\u201d prompt, what is the prompt for root? Explain the difference between grep and egrep? What is the port for DNS, NTP and NFS? 53,123 and 111/2049 What is the configuration file name of DNS and where is it located? /etc/named.conf How many new directories will be created after running the following command mkdir {a..c}{1..3} 9 Your PC is configured with a DNS server address but not the default gateway. Can the PC access internet? No What is the difference between IP and Gateway? Search online Can you assign one static IP to 2 computers, if not then why? No because it will create IP conflict How to change IPs address to static? ifconfig x.x.x.x You are trying to ping a server by hostname and you get an error message, \u201cping: unknown host \u2026\u201d. What could be the reason and how to solve the problem so you can ping it by hostname? Check for /etc/hosts or DNS to see if it has hostname to IP entry Explain the difference between relative and absolute path? Absolute path starts from / where relative path is your current directory List 3 different methods of adding user? Search online What is the command to change file/directory ownership and group? chown and chgrp List any 3 type of filesystem? ext4,NTFS and FAT When you login you get a message on the screen. What is the name of that file and where is it located? /etc/motd What is /bin directory used for? Search online What are the different types of DNS Server Master and secondary How to change a user password? passwd username What is the version of Redhat Linux you have experience with? 7.4 List any 4 linux distributions? Redhat, CentOS, Ubuntu and SUSE How to logoff from the system? exit Give any 3 examples of operating systems? Windows, Linux and MAC How to create a directory? mkdir Where are the zone files located for DNS service? /var/named/zonefiles How to check kernel version? uname \u2013a Which directory has all the configuration files? /etc How to become root user from a regular user? su \u2013 How many mega bytes in 1 giga bytes? Search online What is the purpose of having different network ports? So the communication of each application goes through a dedicated port How to display first column of a file? cat filename | awk \u2018{print $1}\u2019 What is the name of DNS rpm package? bind What is the difference between nslookup and dig commands? Search online How to check your user id and group id? id How to check a file\u2019s permission? ls \u2013l What is the difference between \u201ckill\u201d and \u201ckill -9\u201d command? Search online What is subnet? Search online You are troubleshooting an issue with Redhat support and they have asked you to send the contents of /etc directory. How and which method you will use to transfer the contents? tar (compress) the entire /etc directory and ftp What is root home directory? /root What is rsyslogd deamon and its purpose? Search online Your company has terminated a server administrator. What is first thing as an administrator you should do to enhance the security? Change root password How to check the computer name or host name in Linux? hostname Which permission allows a user to run an executable with the permissions of the owner of that file? First 3 bits should have x What is the command to untar a tarred file? untar What is /proc directory used for? Search online What is the purpose of nsswitch.conf file It tells the system where to go to resolve hostnames List 3 basic commands to navigate the filesystem? cd, pwd and ls Which service/daemon should be running on the server that allows you to connect remotely? sshd What is the purpose of firewall? Search online List any 3 IT components? Hardware, OS and Applications (adsbygoogle = window.adsbygoogle || []).push({}); Which directory has all the commands we use, e.g. ls, cd etc.? /usr/bin or /bin What is the difference between memory, virtual memory and cache? Search online Which of the following is correct? a. Hardware \u2000 Operating System \u2000 Users b. Operating System \u2000 Users \u2000 Hardware c. Database \u2000 Hardware \u2000 Users Which of the following is a communication command? o grep o mail o touch o cd How to rename a file or directory? mv How to change a hostname in Linux? Search online How to check network interfaces in Linux? ifconfig Why is \u201ctail \u2013f logfilename\u201d command used most often and what does it do? It will output all incoming logs in real time What type of hardware have you worked on? You should get yourself familiar with Dell, HP and UCS hardware by going online and check the vendor websites How to sort a file in reverse order? cat filename | sort \u2013r What is the name of operating system that runs Unix? Solaris, HP-UX etc. List all byte sizes from smallest to largest? Search online How to check the total number of partition in Linux? fdisk -l How to access a linux system from a linux system? ssh Explain the procedure of bonding 2 NICs or interfaces together? Search online What is the exact command syntax to list the 5th column of a file and cut the first 3 letters? cat filename | awk \u2018{print $5}\u2019 | cut \u2013c1-3 What is /etc/hosts file used for? To resolve hostnames with IP address List any 3 options of \u2018df\u2019 command and what they are used for? Search online What is the command to change file/directory permissions? chmod What is the purpose of pipe (|)? To combine multiple commands What is /etc directory used for? For configuration files Which command is used to list files in a directory? ls \u2013l There is a command which gives you information about other commands, please explain that command and what is it used for? man How to delete a file and a directory? rm filename and rmdir dirname What is the difference between \u201ctail\u201d and \u201ctail -10\u201d? None List 4 commands to display or read a file contents? cat, more, less, vi (adsbygoogle = window.adsbygoogle || []).push({}); Which command is used to read the top 5 lines of a file? head -5 filename What are the different commands or methods to write to a file? echo > filename and vi filename What is swap space and how to check swap space? Search online What is inode and how to find an inode of a file? Search online Which file to edit for kernel tuning? Search online What is the latest version of Redhat? Search online Name the command to find specific word from a file? grep word filename You have scheduled a job using crontab but it does not run at the time you specified, what could be the reason and how would you troubleshoot? Check your system time Check your crontab entry Check /var/log/messages How to check system hardware information? dmidecode How to check network interface MAC address? ifconfig If I don\u2019t want others to read my file1, how to do that? Remove r from the last 3 bits of file permission What is the purpose of \u201cuniq\u201d and \u201csed\u201d command? Search online Which command is used to list the contents of a directory in the most recent time and in reverse order, meaning the most updated file should be listed on the bottom? ls \u2013ltr What is the difference between tar, gzip and gunzip? Search online What are the different ways to install and OS? DVD, DVD iso and network boot How to view difference between two files? diff file1 and file2 You noticed that one of the Linux servers has no disk space left, how would you troubleshoot that issue? If running LVM then add more disk and extend LVM If not running LVM then add more disk, create a new partition and link the new partition to an existing filesystem How to check Redhat version release? uname \u2013a or /etc/redhat-release What is the difference between TCP and UDP? Search online What is a zombie process? Search online How do you search for a pattern/word in a file and then replace it in an entire file? sed command Explain the purpose of \u201ctouch\u201d command? To create an empty file If a command hangs, how to stop it and get the prompt back? Ctrl C Which command is used to count words or lines? wc How to check the number of users logged in? who What is the command to view the calendar of 2011? cal 2011 Which command is used to view disk space? df \u2013h How to create a new group in Linux? groupadd What is the command to send a message to everyone who is logged into the system? wall Which command is used to check total number of disks? fdisk \u2013l What is an mail server record in DNS? MX What does the following command line do? ps -ef | awk '{print $1}' | sort | uniq List the first column of all running processes, sort them and remove duplicates You get a call that when a user goes to www.yourwebsite.com it fails and gets an error, how do you troubleshoot? Check for user internet Check to see if user computer has DNS for hostname lookup Check to see if the server is up that is running that website Check to see if the server\u2019s web service is running Check for DNS availability which is resolving that website List 4 different directories in /? /etc, /bin, /tmp, /home What is the output of the following command: $tail -10 filename | head -1 It will show the first line from the last 10 lines of a file What are the different fields in /etc/passwd file? Search online Which command is used to list the processes? ps \u2013ef What is the difference between \u201chostname\u201d and \u201cuname\u201d commands? Hostname will give you system name and uname will give you OS information How to check system load? top and uptime command How to schedule jobs? crontab and at What is the 3rd field when setting up crontab? Day of the month What is the command to create a new user? useradd What is the \u201cinit #\u201d for system reboot? 6 How to restart a service? systemctl restart servicename How to shutdown a system? shutdown or init 0 What is \u201cftp\u201d command used for? To transfer files from one computer to another Explain cron job syntax? First is minute, second is..? Min, house, day of the month, month, day of the week and command How to delete a package in Linux? rpm \u2013e packagename What is the file name where user password information is saved? /etc/shadow Which command you would use to find the location of chmod command? which chmod (adsbygoogle = window.adsbygoogle || []).push({}); Which command is used to check if the other computer is online? ping othercomputer Please explain about LAN, MAN and WAN? Search online How to list hidden files in a directory? ls \u2013la What is the difference between telnet and ssh? ssh is secure where telnet is not How to run a calculator on Linux and exit out of it? bc and quit List any 4 commands to monitor system? top, df \u2013h, iostat, dmesg You are notified that your server is down, list the steps you will take to troubleshoot? Check the system physically Login through system console Ping the system Reboot or boot if possible What is difference between static and DHCP IP? Search online How to write in vi editor mode? i = insert, a = insert in next space, o = insert in new line What is the difference between \u201ccrontab\u201d and \u201cat\u201d jobs? crontab is for repetitive jobs where at is for one time job What is vCenter server in VMWare? Search online What is \u201cdmidecode\u201d command used for? To get system information What is the difference between SAN and NAS? Search online What is the location of system logs? E.g. messages /var/log directory How to setup an alias and what is it used for? alias aliasname=\u201dcommand\u201d It is used to created short-cuts for long commands What is the purpose of \u201cnetstat\u201d command? Search online What are terminal control keys, list any 3? Crtl C, D and Z Which command(s) you would run if you need to find out how many processes are running on your system? ps \u2013ef | wc \u2013l What are the different types of shells? sh, bash, ksh, csh etc. How to delete a line when in vi editor mode? dd Which is the core of the operating system? a) Shell b) Kernel c) Commands d) Script Which among the following interacts directly with system hardware? a) Shell b) Commands c) Kernel d) Applications How to save and quit from vi editor? Shift ZZ or :wq! What is the difference between a process and daemon? Search online What is the process or daemon name for NTP? ntpd (adsbygoogle = window.adsbygoogle || []).push({}); What are a few commands you would run if your system is running slow? top, iostat, df \u2013h, netstat etc. How to install a package in Redhat Linux? yum install packagename What is the difference between \u201cifconfig\u201d and \u201cipconfig\u201d commands? ifconfig for Linux and ipconfig for Windows What is the first line written in a shell script? Define shell e.g. #!/bin/bash Where is the network (Ethernet) file located, please provide exact directory location and file name? /etc/sysconfig/network-scripts/ifcfg-nic Why do we use \u201clast\u201d command? To see who has logged in the system whether active or logged off What is RHEL Linux stands for? Search online To view your command history, which command is used and how to run a specific command? history and history # What is NTP and briefly explain how does it work and where is the config files and related commands of NTP? Search online How to disable firewall in Linux? Search online How to configure mail server relay for sendmail service? Edit /etc/mail/sendmail.mc file and add SMART_HOST entry Where is samba log file located? /var/log/samba What is mkfs command used for? To create a new filesystem If you create a new group, which file does it get created in? /etc/group Which file has DNS server information (e.g. DNS resolution)? /etc/resolv.conf What are the commands you would run if you need to find out the version and build date of a package (e.g. http)? rpm \u2013qi http On the file permissions? What are the first 3 bits for and who is it for? Read, write and execute. They are used for the owner of the file How to create a soft link? ln \u2013s How to write a script to delete messages in a log file older than 30 days automatically? Search online How to quit out of \u201cman\u201d command? q Which command is used to partition disk in Linux? fdisk What is the difference between \u201cshutdown\u201d and \u201chalt\u201d command? Search online What is the exact syntax of mounting NFS share on a client and also how to un-mount? Search online What experience do you have with scripting, explain? if-the, do-while, case, for loop scripts How to get information on all the packages installed on the system? rpm \u2013qa Explain VMWare? Search online You are tasked to examine a log file in order to find out why a particular application keep crashing. Log file is very lengthy, which command can you use to simplify the log search using a search string? grep for error, warning, failure etc. in /var/log/messages file What is /etc/fstab file and explain each column of this file? Search online What the latest version of Windows server? Search online What is the exact command to list only the first 2 lines of history output? history | head -2 How to upgrade Linux from 7.3 to 7.4? yum install update How to tell which shell you are in or running? $0 You have tried to \u201ccd\u201d into a directory but you have been denied. You are not the owner of that directory, what permissions do you need and where? r \u2013 x What is CNAME record in DNS? Entry for hostname to hostname What is the name of VMWare operating system? ESXi What is the client name used to connect to ESXi or vCenter server? vSphere client You get a call from a user saying that I cannot write to a file because it says, permission denied. The file is owned by that user, how do you troubleshoot? Give write permission on the first 3 bits What is the latest version of VMWare? Search online What is the name of firewall daemon in Linux? firewalld Which command syntax you can use to list only the 20th line of a file? Search online What is the difference between run level 3 and 5? 3 = Boot system with networking, 5 = boot system with networking and GUI List a few commands that are used in troubleshooting network related issue? netstat, tcpdump etc. What is the difference between domain and nameserver? Search online You open up a file and it has 3000 lines and it scrolled up really fast, which command you will use to view it one page at a time? more or less How to start a new shell. E.g. start a new ksh shell? Simply type ksh, or bash How to kill a process? kill processID How to check scheduled jobs? crontab \u2013l How to check system memory and CPU usage? free and top Which utility could you use to repair the corrupted file system? fsck What is the command to make a service start at boot? systemctl enable servicename How to combine 2 files into 1? E.g. you 3 lines in file \u201cA\u201d and 5 lines in file \u201cB\u201d, which command syntax to use that will combine into one file of 3+5 = 8 lines cat fileA >> fileB What is echo command used for? To output to a screen What does the following command do? echo This year the summer will be great > file1 It will create a new file \u201cfile1\u201d with the content as \u201cThis year the summer will be great\u201d Which file to modify to allow users to run root commands /etc/sudoers You need to modify httpd.conf file but you cannot find it, Which command line tool you can use to find file? find / -name \u201chttpd.conf\u201d Your system crashed and being restarted, but a message appears, indicating that the operating system cannot be found. What is the most likely cause of the problem? The /boot file is most likely corrupted Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Linux Interview Questions for Freshers - 2"},{"location":"nightwolf-cotribution/linux_interview_questions_for_freshers/#linux-200-interview-questions-for-freshers-solved","text":"We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux Admin, Amazon interview questions for Linux admin and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); When you login you get \u201c$\u201d prompt, what is the prompt for root? Explain the difference between grep and egrep? What is the port for DNS, NTP and NFS? 53,123 and 111/2049 What is the configuration file name of DNS and where is it located? /etc/named.conf How many new directories will be created after running the following command mkdir {a..c}{1..3} 9 Your PC is configured with a DNS server address but not the default gateway. Can the PC access internet? No What is the difference between IP and Gateway? Search online Can you assign one static IP to 2 computers, if not then why? No because it will create IP conflict How to change IPs address to static? ifconfig x.x.x.x You are trying to ping a server by hostname and you get an error message, \u201cping: unknown host \u2026\u201d. What could be the reason and how to solve the problem so you can ping it by hostname? Check for /etc/hosts or DNS to see if it has hostname to IP entry Explain the difference between relative and absolute path? Absolute path starts from / where relative path is your current directory List 3 different methods of adding user? Search online What is the command to change file/directory ownership and group? chown and chgrp List any 3 type of filesystem? ext4,NTFS and FAT When you login you get a message on the screen. What is the name of that file and where is it located? /etc/motd What is /bin directory used for? Search online What are the different types of DNS Server Master and secondary How to change a user password? passwd username What is the version of Redhat Linux you have experience with? 7.4 List any 4 linux distributions? Redhat, CentOS, Ubuntu and SUSE How to logoff from the system? exit Give any 3 examples of operating systems? Windows, Linux and MAC How to create a directory? mkdir Where are the zone files located for DNS service? /var/named/zonefiles How to check kernel version? uname \u2013a Which directory has all the configuration files? /etc How to become root user from a regular user? su \u2013 How many mega bytes in 1 giga bytes? Search online What is the purpose of having different network ports? So the communication of each application goes through a dedicated port How to display first column of a file? cat filename | awk \u2018{print $1}\u2019 What is the name of DNS rpm package? bind What is the difference between nslookup and dig commands? Search online How to check your user id and group id? id How to check a file\u2019s permission? ls \u2013l What is the difference between \u201ckill\u201d and \u201ckill -9\u201d command? Search online What is subnet? Search online You are troubleshooting an issue with Redhat support and they have asked you to send the contents of /etc directory. How and which method you will use to transfer the contents? tar (compress) the entire /etc directory and ftp What is root home directory? /root What is rsyslogd deamon and its purpose? Search online Your company has terminated a server administrator. What is first thing as an administrator you should do to enhance the security? Change root password How to check the computer name or host name in Linux? hostname Which permission allows a user to run an executable with the permissions of the owner of that file? First 3 bits should have x What is the command to untar a tarred file? untar What is /proc directory used for? Search online What is the purpose of nsswitch.conf file It tells the system where to go to resolve hostnames List 3 basic commands to navigate the filesystem? cd, pwd and ls Which service/daemon should be running on the server that allows you to connect remotely? sshd What is the purpose of firewall? Search online List any 3 IT components? Hardware, OS and Applications (adsbygoogle = window.adsbygoogle || []).push({}); Which directory has all the commands we use, e.g. ls, cd etc.? /usr/bin or /bin What is the difference between memory, virtual memory and cache? Search online Which of the following is correct? a. Hardware \u2000 Operating System \u2000 Users b. Operating System \u2000 Users \u2000 Hardware c. Database \u2000 Hardware \u2000 Users Which of the following is a communication command? o grep o mail o touch o cd How to rename a file or directory? mv How to change a hostname in Linux? Search online How to check network interfaces in Linux? ifconfig Why is \u201ctail \u2013f logfilename\u201d command used most often and what does it do? It will output all incoming logs in real time What type of hardware have you worked on? You should get yourself familiar with Dell, HP and UCS hardware by going online and check the vendor websites How to sort a file in reverse order? cat filename | sort \u2013r What is the name of operating system that runs Unix? Solaris, HP-UX etc. List all byte sizes from smallest to largest? Search online How to check the total number of partition in Linux? fdisk -l How to access a linux system from a linux system? ssh Explain the procedure of bonding 2 NICs or interfaces together? Search online What is the exact command syntax to list the 5th column of a file and cut the first 3 letters? cat filename | awk \u2018{print $5}\u2019 | cut \u2013c1-3 What is /etc/hosts file used for? To resolve hostnames with IP address List any 3 options of \u2018df\u2019 command and what they are used for? Search online What is the command to change file/directory permissions? chmod What is the purpose of pipe (|)? To combine multiple commands What is /etc directory used for? For configuration files Which command is used to list files in a directory? ls \u2013l There is a command which gives you information about other commands, please explain that command and what is it used for? man How to delete a file and a directory? rm filename and rmdir dirname What is the difference between \u201ctail\u201d and \u201ctail -10\u201d? None List 4 commands to display or read a file contents? cat, more, less, vi (adsbygoogle = window.adsbygoogle || []).push({}); Which command is used to read the top 5 lines of a file? head -5 filename What are the different commands or methods to write to a file? echo > filename and vi filename What is swap space and how to check swap space? Search online What is inode and how to find an inode of a file? Search online Which file to edit for kernel tuning? Search online What is the latest version of Redhat? Search online Name the command to find specific word from a file? grep word filename You have scheduled a job using crontab but it does not run at the time you specified, what could be the reason and how would you troubleshoot? Check your system time Check your crontab entry Check /var/log/messages How to check system hardware information? dmidecode How to check network interface MAC address? ifconfig If I don\u2019t want others to read my file1, how to do that? Remove r from the last 3 bits of file permission What is the purpose of \u201cuniq\u201d and \u201csed\u201d command? Search online Which command is used to list the contents of a directory in the most recent time and in reverse order, meaning the most updated file should be listed on the bottom? ls \u2013ltr What is the difference between tar, gzip and gunzip? Search online What are the different ways to install and OS? DVD, DVD iso and network boot How to view difference between two files? diff file1 and file2 You noticed that one of the Linux servers has no disk space left, how would you troubleshoot that issue? If running LVM then add more disk and extend LVM If not running LVM then add more disk, create a new partition and link the new partition to an existing filesystem How to check Redhat version release? uname \u2013a or /etc/redhat-release What is the difference between TCP and UDP? Search online What is a zombie process? Search online How do you search for a pattern/word in a file and then replace it in an entire file? sed command Explain the purpose of \u201ctouch\u201d command? To create an empty file If a command hangs, how to stop it and get the prompt back? Ctrl C Which command is used to count words or lines? wc How to check the number of users logged in? who What is the command to view the calendar of 2011? cal 2011 Which command is used to view disk space? df \u2013h How to create a new group in Linux? groupadd What is the command to send a message to everyone who is logged into the system? wall Which command is used to check total number of disks? fdisk \u2013l What is an mail server record in DNS? MX What does the following command line do? ps -ef | awk '{print $1}' | sort | uniq List the first column of all running processes, sort them and remove duplicates You get a call that when a user goes to www.yourwebsite.com it fails and gets an error, how do you troubleshoot? Check for user internet Check to see if user computer has DNS for hostname lookup Check to see if the server is up that is running that website Check to see if the server\u2019s web service is running Check for DNS availability which is resolving that website List 4 different directories in /? /etc, /bin, /tmp, /home What is the output of the following command: $tail -10 filename | head -1 It will show the first line from the last 10 lines of a file What are the different fields in /etc/passwd file? Search online Which command is used to list the processes? ps \u2013ef What is the difference between \u201chostname\u201d and \u201cuname\u201d commands? Hostname will give you system name and uname will give you OS information How to check system load? top and uptime command How to schedule jobs? crontab and at What is the 3rd field when setting up crontab? Day of the month What is the command to create a new user? useradd What is the \u201cinit #\u201d for system reboot? 6 How to restart a service? systemctl restart servicename How to shutdown a system? shutdown or init 0 What is \u201cftp\u201d command used for? To transfer files from one computer to another Explain cron job syntax? First is minute, second is..? Min, house, day of the month, month, day of the week and command How to delete a package in Linux? rpm \u2013e packagename What is the file name where user password information is saved? /etc/shadow Which command you would use to find the location of chmod command? which chmod (adsbygoogle = window.adsbygoogle || []).push({}); Which command is used to check if the other computer is online? ping othercomputer Please explain about LAN, MAN and WAN? Search online How to list hidden files in a directory? ls \u2013la What is the difference between telnet and ssh? ssh is secure where telnet is not How to run a calculator on Linux and exit out of it? bc and quit List any 4 commands to monitor system? top, df \u2013h, iostat, dmesg You are notified that your server is down, list the steps you will take to troubleshoot? Check the system physically Login through system console Ping the system Reboot or boot if possible What is difference between static and DHCP IP? Search online How to write in vi editor mode? i = insert, a = insert in next space, o = insert in new line What is the difference between \u201ccrontab\u201d and \u201cat\u201d jobs? crontab is for repetitive jobs where at is for one time job What is vCenter server in VMWare? Search online What is \u201cdmidecode\u201d command used for? To get system information What is the difference between SAN and NAS? Search online What is the location of system logs? E.g. messages /var/log directory How to setup an alias and what is it used for? alias aliasname=\u201dcommand\u201d It is used to created short-cuts for long commands What is the purpose of \u201cnetstat\u201d command? Search online What are terminal control keys, list any 3? Crtl C, D and Z Which command(s) you would run if you need to find out how many processes are running on your system? ps \u2013ef | wc \u2013l What are the different types of shells? sh, bash, ksh, csh etc. How to delete a line when in vi editor mode? dd Which is the core of the operating system? a) Shell b) Kernel c) Commands d) Script Which among the following interacts directly with system hardware? a) Shell b) Commands c) Kernel d) Applications How to save and quit from vi editor? Shift ZZ or :wq! What is the difference between a process and daemon? Search online What is the process or daemon name for NTP? ntpd (adsbygoogle = window.adsbygoogle || []).push({}); What are a few commands you would run if your system is running slow? top, iostat, df \u2013h, netstat etc. How to install a package in Redhat Linux? yum install packagename What is the difference between \u201cifconfig\u201d and \u201cipconfig\u201d commands? ifconfig for Linux and ipconfig for Windows What is the first line written in a shell script? Define shell e.g. #!/bin/bash Where is the network (Ethernet) file located, please provide exact directory location and file name? /etc/sysconfig/network-scripts/ifcfg-nic Why do we use \u201clast\u201d command? To see who has logged in the system whether active or logged off What is RHEL Linux stands for? Search online To view your command history, which command is used and how to run a specific command? history and history # What is NTP and briefly explain how does it work and where is the config files and related commands of NTP? Search online How to disable firewall in Linux? Search online How to configure mail server relay for sendmail service? Edit /etc/mail/sendmail.mc file and add SMART_HOST entry Where is samba log file located? /var/log/samba What is mkfs command used for? To create a new filesystem If you create a new group, which file does it get created in? /etc/group Which file has DNS server information (e.g. DNS resolution)? /etc/resolv.conf What are the commands you would run if you need to find out the version and build date of a package (e.g. http)? rpm \u2013qi http On the file permissions? What are the first 3 bits for and who is it for? Read, write and execute. They are used for the owner of the file How to create a soft link? ln \u2013s How to write a script to delete messages in a log file older than 30 days automatically? Search online How to quit out of \u201cman\u201d command? q Which command is used to partition disk in Linux? fdisk What is the difference between \u201cshutdown\u201d and \u201chalt\u201d command? Search online What is the exact syntax of mounting NFS share on a client and also how to un-mount? Search online What experience do you have with scripting, explain? if-the, do-while, case, for loop scripts How to get information on all the packages installed on the system? rpm \u2013qa Explain VMWare? Search online You are tasked to examine a log file in order to find out why a particular application keep crashing. Log file is very lengthy, which command can you use to simplify the log search using a search string? grep for error, warning, failure etc. in /var/log/messages file What is /etc/fstab file and explain each column of this file? Search online What the latest version of Windows server? Search online What is the exact command to list only the first 2 lines of history output? history | head -2 How to upgrade Linux from 7.3 to 7.4? yum install update How to tell which shell you are in or running? $0 You have tried to \u201ccd\u201d into a directory but you have been denied. You are not the owner of that directory, what permissions do you need and where? r \u2013 x What is CNAME record in DNS? Entry for hostname to hostname What is the name of VMWare operating system? ESXi What is the client name used to connect to ESXi or vCenter server? vSphere client You get a call from a user saying that I cannot write to a file because it says, permission denied. The file is owned by that user, how do you troubleshoot? Give write permission on the first 3 bits What is the latest version of VMWare? Search online What is the name of firewall daemon in Linux? firewalld Which command syntax you can use to list only the 20th line of a file? Search online What is the difference between run level 3 and 5? 3 = Boot system with networking, 5 = boot system with networking and GUI List a few commands that are used in troubleshooting network related issue? netstat, tcpdump etc. What is the difference between domain and nameserver? Search online You open up a file and it has 3000 lines and it scrolled up really fast, which command you will use to view it one page at a time? more or less How to start a new shell. E.g. start a new ksh shell? Simply type ksh, or bash How to kill a process? kill processID How to check scheduled jobs? crontab \u2013l How to check system memory and CPU usage? free and top Which utility could you use to repair the corrupted file system? fsck What is the command to make a service start at boot? systemctl enable servicename How to combine 2 files into 1? E.g. you 3 lines in file \u201cA\u201d and 5 lines in file \u201cB\u201d, which command syntax to use that will combine into one file of 3+5 = 8 lines cat fileA >> fileB What is echo command used for? To output to a screen What does the following command do? echo This year the summer will be great > file1 It will create a new file \u201cfile1\u201d with the content as \u201cThis year the summer will be great\u201d Which file to modify to allow users to run root commands /etc/sudoers You need to modify httpd.conf file but you cannot find it, Which command line tool you can use to find file? find / -name \u201chttpd.conf\u201d Your system crashed and being restarted, but a message appears, indicating that the operating system cannot be found. What is the most likely cause of the problem? The /boot file is most likely corrupted","title":"Linux 200+ Interview Questions for Freshers - Solved:"},{"location":"nightwolf-cotribution/linux_questionairs/","text":"Top 100 Linux Interview Questions and Answers - Solved: \uf0c1 We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Technical Solutions Specialist - Infrastructure), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. Most of these are scenario based Linux interview questions for Experienced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); 1. Explain the below error: ------------------------------------------------------------------------------------------- find: page allocation failure. order:5, mode:0x0 Pid: 22938, comm: find Not tainted 2.6.32-279.el6.i686 #1 Call Trace: [<c04f22bc>] ? __alloc_pages_nodemask+0x6bc/0x870 [<c051f1bc>] ? cache_alloc_refill+0x2bc/0x510 [<c083f035>] ? apic_timer_interrupt+0x31/0x38 [<c051f552>] ? __kmalloc+0x142/0x180 [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb2f0>] ? rpc_wait_bit_killable+0x0/0x80 [sunrpc] ------------------------------------------------------------------------------------------- Hint => Above error is kernel dump due to kmalloc buffer exhaustion. The system is out of memory and cannot satisfy the allocation request at all. This is more likely to happen on swap-less systems that are unable to purge application pages to free up memory. The system may have sufficient free pages in total but not enough that are physically contiguous to satisfy the allocation request. This is referred to as memory fragmentation. - The phrase \"page allocation failure\" - this defines the error message that should be used in a KCS search. - The process name \"find\" - often page allocation failures are related to code that gets executed by specific threads, try searching KCS with and without this term. If the allocation request was performed by an interrupt service routine then the process name will be irrelevant. - The order of the allocation \"order:5\" and mode \"mode:0x0\" can often be the same for every instance of a particular problem, try searching KCS with and without these terms. - The function requesting the memory allocation \"rpc_malloc()\" is likely to be the same for every instance of this particular problem so include that in the KCS search. - Do not search for the CPU number, the process id or function offsets since they will change and may prevent finding a match. For more details https://access.redhat.com/articles/1360023 2. What is order:5, mode:0x0 in above output. 3. Difference between kernel panic due to Memory crunch and page allocation failure. 4. Difference between VMalloc and KMalloc Hint => The kmalloc() & vmalloc() functions are a simple interface for obtaining kernel memory in byte-sized chunks. - The kmalloc() function guarantees that the pages are physically contiguous (and virtually contiguous). - The vmalloc() function works in a similar fashion to kmalloc(), except it allocates memory that is only virtually contiguous and not necessarily physically contiguous. 5. How do you troubleshoot memory performance issue. Please explain the details. 6. Which tools do you use to troubleshoot high Memory troubleshooting. 7. What are zombie process and how to kill/reclaim them. 8. What are system calls. 9. What is strace used for. 10. Difference between fork and exec in Linux. 11. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- 12. Explain few examples of kernel panic. 13. How to blacklist a module. 14. What are D-State processes and what causes these. 15. If Disk is causing D-state processes, what you can check and can do to fix the issue. 16. Explain blk_trace. 17. If you are receiving \"Connection reset by peer\" when trying to connect a server, what can be root cause of this and how you can fix this. 18. How will you troubleshoot high load avg. Which tools will you use and why. Explain in details. 19. What can be reason of high load avg in kernel space i.e. %sy in top. 20. What is CPU affinity . How to check it. 21. How to check all cores stat in top. Hint: by pressing 1 or using mpstat 22. What can be the reasons for server to stuck at boot prompt. and how to troubleshoot further ? 23. What is backporting in Linux. Hint: its related to packages. 24. There are two systems and one is taking less booting time and other than taking more than booting time. 25. Please explain Linux booting processing? 26. How to check the loaded and unloaded modules 27. Please explain few types of kernel errors? 28. How to troubleshoot the system performance if any Linux system is facing slowness? 29. How to troubleshoot high memory usageissue on Linux system. 30. What is CPU load ? How to calculate the load average on the system? 31. What the Zombie and Orphan process? How to kill zombie process? 32. What could be the impacts on the system if there are many zombie process are available? 33. What is affinity in Linux? 1. Binding a process with a CPU core. 2. # taskset < command > 34. What is paging and swapping in Linux? Please explain Page fault ? 35. What is difference between cache and buffer? 36. Explain the TOP, IOSTAT,VMSTAT,IOTOP commands. 37. Difference between RHEL6 and RHEL7 booting process. 38. Difference between systemd and initd 39. How to troubleshoot a issue where a client not able to access a server? 40. What are outputs when you execute the NATCAT (nc) command and explain each ? Hint => - Connected - Connection timeout - Connection refused(if port / server is blocking by the firewall) 41. What are the inodes and how will you free up them? 42. How can we check the packet flow in our system? 43. what is the steal value in top command? 44. Explain what is NUMA. You may know this as \"Node Interleaving\u201d. Give an example where NUMA is beneficial, and another where it's detrimental. Hints => a). NUMA: Non Uniform Memory Access (NUMA) is a design where the memory is split into groups by CPU, making the memory access faster for each CPU to their memory group. b). Beneficial: Multiple processes with smaller memory footprints. Good for applications that fork processes, like Apache. Detrimental: Monolithic, memory-heavy, single processes. May be threaded, example MySQL, Redis, Java-everything, Memcached. Follow-Up Q. - what if there's only a single physical proc. 45. Customer complains that their CPUs aren't running as fast as they're rated. How would you Troubleshoot ? Hints => a). Could be heat damage - check /proc/cpuinfo for \"cpu MHz\". b). Could be the wrong CPUs installed, for that matter. Check the CPU specs in /proc/cpuinfo. OR dmidecode. c). Get 'em to tell you how to fix it (Change system profile to MaximumPerformance with omconfig, then reboot) 46. Can we check System logs on Hardware layer from OS ?How ? Hints => a). Yes, in dmesg logs or kern.log b). By using respective Hardware CLI commands. c). Specific answers hpasmcli -s \"show iml\" ==> For HP servers omreport system [alertlog | esmlog] ==> For Dell Servers 47. What can be done to improve the performance of an I/O-heavy system? Hints => a). Check RAID Level, if server using any. b). Partition aligned, i.e. starting a partition on sector 2048. c). Mount Options like: noatime - (covers norelatime/nodiratime) Can be enabled only via fstab. d). RAID Controller settings - read/write policy, e.g. write-through(good to ensure data has been changed on disk. write-back(relys on cache, battery). e). nobarrier - depends on whether or not the PERC Controller is making use of a battery or not. f). Adjust the filesystem journaling method - the default mount option is in the middle, data=ordered. This indicates the data is written out to the filesystem before the metadata in the journal is updated. The filesystem can be mounted with data=writeback which allows data to be written to the journal and disk at the same time, with the risk of some data possibly being written to the disk after the metadata that cannot be recovered after a crash. Additionally, the journal can be placed on an external device with another mount option, allowing separation of the writes to increase throughput performance for each write operation. g). discard - For SSDs. Also called fs TRIM. Extends life and improves performance of SSD drives. TRIM enables the controller to cache the expected SSD write cycles to collapse them into a \"do only once\" mechanism to prevent burning out the SSD cells, which can only be written to once with voltage. Note: For LVM, set issue_discards=1 in lvm.conf. Note: On Dell, TRIM/discard is only available on RAID controllers >= PERC H710 h). For SSD, you don't want controller caching for performance reasons. By default HPs turn caching off. Dell: Cut-through IO (CTIO) is an IO accelerator for SSD arrays that boosts the throughput of devices connected to the PERC Controller. It is enabled through disabling the write-back cache (enable writethrough cache) and disabling Read Ahead. HP: On servers with SSD drives, write caching appears to be disabled by default for performance reasons (there is no need for write caching on SSD drives which are fast enough). Writes to SSD drives will not be acknowledged until the data is actually written to disk: i). On HP, HP SSD Smart Path - this is enabled by default by the RAID creation process in the HP RAID controller itself, and can be checked with the standard commands using hpssacli to view controller details and RAID logical device details. The theory is this special method of SSD writing replaces traditional caching techniques with a special HP invented method for improving SSD performance and longevity. j). Disk IO Scheduler deadline (recommended for DBs) noop (recommended for VMs) 48. Why would I choose to use RAID-5 instead of RAID-10? Or vice versa? Hints => RAID-5: Pro: Space efficiency - you only sacrifice 1x HDD to redundancy. Pro: Possible with a minimum of 3x drives, as opposed to RAID-10's minimum-4. Con: Slow writes. Note: Reads are still quick though, so you'd be fine on a read-heavy application. Con: If two drives fail, you lose everything. RAID-10: Pro: Great performance with both reads and writes. Good for write-heavy applications. Pro: Less risk of a catastrophic failure, since more than 1 drive can fail safely (Note: Depends which drives). Con: Too much wastage of Space. Only 50% capacity is uasable. 49. How snapshots work ? Can you please explain. In terms of VMWare or LVM(in case someone asks). Hints => As soon as you create a snapshot, LVM creates a pool of blocks. This pool also contains a full copy of the LVM metadata of the volume. When writes happen to the main volume such as updating an inode, the block being overwritten is copied to this new pool and the new block is written to the main volume. This is the 'copy-on-write'. Because of this, the more data that gets changed between when a snapshot was taken and the current state of the main volume, the more space will get consumed by that snapshot pool. When you mount the snapshot, the meta-data written when the snapshot was taken allows the mapping of snapshot pool blocks over changed blocks in the volume (or higher level snapshot). This way when an access comes for a specific block, LVM knows which block access. As far as the filesystem on that volume is concerned, there are no snapshots. 50. In which scenario we need to use vgcfgrestore and vgcfgbackup commands ? When /etc/lvm/archive contents are written (when does backup happen ?) Hints => a). To restore VG configuration. b). Backup is taken whenever we make change to LVM (create/remove LV/VG) and we can use it to restore to the prior state. 51. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. 52. What is deadlock ? How to identify if process is in deadlock in Linux/Unix ? Hint => Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process 53. How to load kernel module. any commands ? Hint => We can use \"modprobe\" or \"insmod\" commands to load kernel module. 54. Difference between insmod and modprobe commands in Linux ? 55. Different types of file systems? Hint => ext2, ext3, ext4, xfs, vfat etc. 56. Difference between ext4 and xfs? 57. When v create user which files are referred? 58. Differnce between passwd and shadow file? Hint => /etc/passwd contains User's detail like home directory, shell etc. /etc/shadow conatains User's password hashes. 59. What are the contents in passwd file? Hint => root:x:0:0:root:/root:/bin/bash \u2022 login name \u2022 optional encrypted password \u2022 numerical user ID \u2022 numerical group ID \u2022 user name or comment field \u2022 user home directory \u2022 optional user command interpreter 60. List all users who are not given passwd? with cmd Hint => cat /etc/shadow| awk -F \":\" '$2 ~ /[a-z]/ {print $1}' 61. How to check what processess are running on which port? Hint => Using \"netstat\" and \"ss\" command we can relate processes with port. 62. How to extend partition in lvm? and indepth questions in lvm ? Hint => 1. First attach extra storage/disk to server. 2. Create a new partition on the disk using 'fdisk' or 'parted' command. 3. Create a new Physical Volume(PV) on that new partition. e.g. 'pvcreate /dev/sdb1' 4. Exetend existing Volume Group(VG) using new PV. e.g. 'vgextend vg_name /dev/sdb1' 5. Now extend the LV. e.g . 'lvextend -l 100%FREE /dev/mapper/vg_name-lv_name' 6. now execute 'resize2fs' or 'xfs_growfs'. 63. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses, 64. On which port dns works? Hint => DNS works on port 53. Questions from Github \uf0c1 A lot more Questions from nightwolf-cotribution github repo # General Questions: 1. What did you learn yesterday/this week? 2. Talk about your preferred development/administration environment. (OS, Editor, Browsers, Tools etc.) 3. Tell me about the last major Linux project you finished. 4. Tell me about the biggest mistake you've made in [some recent time period] and how you would do it differently today. What did you learn from this experience? 5. Why we must choose you? 6. What function does DNS play on a network? 7. What is HTTP? 8. What is an HTTP proxy and how does it work? 9. Describe briefly how HTTPS works. 10. What is SMTP? Give the basic scenario of how a mail message is delivered via SMTP. 11. What is RAID? What is RAID0, RAID1, RAID5, RAID10? 12. What is a level 0 backup? What is an incremental backup? 13. Describe the general file system hierarchy of a Linux system. 14. Which difference have between public and private SSH key? # Simple Linux Questions: 15. What is the name and the UID of the administrator user? 16. How to list all files, including hidden ones, in a directory? 17. What is the Unix/Linux command to remove a directory and its contents? 18. Which command will show you free/used memory? Does free memory exist on Linux? 19. How to search for the string \"my konfu is the best\" in files of a directory recursively? 20. How to connect to a remote server or what is SSH? 21. How to get all environment variables and how can you use them? 22. I get \"command not found\" when I run ifconfig -a. What can be wrong? 23. What happens if I type TAB-TAB? 24. What command will show the available disk space on the Unix/Linux system? 25. What commands do you know that can be used to check DNS records? 26. What Unix/Linux commands will alter a files ownership, files permissions? 27. What does chmod +x FILENAME do? 28. What does the permission 0750 on a file mean? 29. What does the permission 0750 on a directory mean? 30. How to add a new system user without login permissions? 31. How to add/remove a group from a user? 32. What is a bash alias? 33. How do you set the mail address of the root/a user? 34. What does CTRL-c do? 35. What does CTRL-d do? 36. What does CTRL-z do? 37. What is in /etc/services? 38. How to redirect STDOUT and STDERR in bash? (> /dev/null 2>&1) 39. What is the difference between UNIX and Linux. 40. What is the difference between Telnet and SSH? 41. Explain the three load averages and what do they indicate. What command can be used to view the load averages? 42. Can you name a lower-case letter that is not a valid option for GNU ls? 43. What is a Linux kernel module? 44. Walk me through the steps in booting into single user mode to troubleshoot a problem. 45. Walk me through the steps you'd take to troubleshoot a 404 error on a web application you administer. 46. What is ICMP protocol? Why do you need to use? # Medium Linux Questions: 47. What do the following commands do and how would you use them? tee awk tr cut tac curl wget watch head tail less cat touch sar netstat tcpdump lsof 48. What does an & after a command do? 49. What does & disown after a command do? 50. What is a packet filter and how does it work? 51. What is Virtual Memory? 52. What is swap and what is it used for? 53. What is an A record, an NS record, a PTR record, a CNAME record, an MX record? 54. Are there any other RRs and what are they used for? 55. What is a Split-Horizon DNS? 56. What is the sticky bit? 57. What does the immutable bit do to a file? 58. What is the difference between hardlinks and symlinks? What happens when you remove the source to a symlink/hardlink? 59. What is an inode and what fields are stored in an inode? 60. How to force/trigger a file system check on next reboot? 61. What is SNMP and what is it used for? 62. What is a runlevel and how to get the current runlevel? 63. What is SSH port forwarding? 64. What is the difference between local and remote port forwarding? 65. What are the steps to add a user to a system without using useradd/adduser? 66. What is MAJOR and MINOR numbers of special files? 67. Describe the mknod command and when you'd use it. 68. Describe a scenario when you get a \"filesystem is full\" error, but 'df' shows there is free space. 69. Describe a scenario when deleting a file, but 'df' not showing the space being freed. 70. Describe how 'ps' works. 71. What happens to a child process that dies and has no parent process to wait for it and what\u2019s bad about this? 72. Explain briefly each one of the process states. 73. How to know which process listens on a specific port? 74. What is a zombie process and what could be the cause of it? 75. You run a bash script and you want to see its output on your terminal and save it to a file at the same time. How could you do it? 76. Explain what echo \"1\" > /proc/sys/net/ipv4/ip_forward does. 77. Describe briefly the steps you need to take in order to create and install a valid certificate for the site https://foo.example.com. 78. Can you have several HTTPS virtual hosts sharing the same IP? 79. What is a wildcard certificate? 80. Which Linux file types do you know? 81. What is the difference between a process and a thread? And parent and child processes after a fork system call? 82. What is the difference between exec and fork? 83. What is \"nohup\" used for? 84. What is the difference between these two commands? myvar=hello & export myvar=hello 85. How many NTP servers would you configure in your local ntp.conf? 86. What does the column 'reach' mean in ntpq -p output? 87. You need to upgrade kernel at 100-1000 servers, how you would do this? 88. How can you get Host, Channel, ID, LUN of SCSI disk? 89. How can you limit process memory usage? 90. What is bash quick substitution/caret replace(^x^y)? 91. Do you know of any alternative shells? If so, have you used any? 92. What is a tarpipe (or, how would you go about copying everything, including hardlinks and special files, from one server to another)? 93. How can you tell if the httpd package was already installed? 94. How can you list the contents of a package? 95. How can you determine which package is better: openssh-server-5.3p1-118.1.el6_8.x86_64 or openssh-server-6.6p1-1.el6.x86_64 ? 96. Can you explain to me the difference between block based, and object based storage? # Hard Linux Questions: 97. What is a tunnel and how you can bypass a http proxy? 98. What is the difference between IDS and IPS? 99. What shortcuts do you use on a regular basis? 100. What is the Linux Standard Base? 101. What is an atomic operation? 102. Your freshly configured http server is not running after a restart, what can you do? 103. What kind of keys are in ~/.ssh/authorized_keys and what it is this file used for? 104. I've added my public ssh key into authorized_keys but I'm still getting a password prompt, what can be wrong? 105. Did you ever create RPM's, DEB's or solaris pkg's? 106. What does :(){ :|:& };: do on your system? 107. How do you catch a Linux signal on a script? 108. Can you catch a SIGKILL? 109. What's happening when the Linux kernel is starting the OOM killer and how does it choose which process to kill first? 110. Describe the linux boot process with as much detail as possible, starting from when the system is powered on and ending when you get a prompt. 111. What's a chroot jail? 112. When trying to umount a directory it says it's busy, how to find out which PID holds the directory? 113. What's LD_PRELOAD and when it's used? 114. You ran a binary and nothing happened. How would you debug this? 115. What are cgroups? Can you specify a scenario where you could use them? 116. How can you remove/delete a file with file-name consisting of only non-printable/non-type-able characters? 117. How can you increase or decrease the priority of a process in Linux? # Expert Linux Questions: 118. A running process gets EAGAIN: Resource temporarily unavailable on reading a socket. How can you close this bad socket/file descriptor without killing the process? 119. What do you control with swapiness? 120. How do you change TCP stack buffers? How do you calculate it? 121. What is Huge Tables? Why isn't it enabled by default? Why and when use it? 122. What is LUKS? How to use it? (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Solved list of Linux interview Questions"},{"location":"nightwolf-cotribution/linux_questionairs/#top-100-linux-interview-questions-and-answers-solved","text":"We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Technical Solutions Specialist - Infrastructure), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. Most of these are scenario based Linux interview questions for Experienced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); 1. Explain the below error: ------------------------------------------------------------------------------------------- find: page allocation failure. order:5, mode:0x0 Pid: 22938, comm: find Not tainted 2.6.32-279.el6.i686 #1 Call Trace: [<c04f22bc>] ? __alloc_pages_nodemask+0x6bc/0x870 [<c051f1bc>] ? cache_alloc_refill+0x2bc/0x510 [<c083f035>] ? apic_timer_interrupt+0x31/0x38 [<c051f552>] ? __kmalloc+0x142/0x180 [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb2f0>] ? rpc_wait_bit_killable+0x0/0x80 [sunrpc] ------------------------------------------------------------------------------------------- Hint => Above error is kernel dump due to kmalloc buffer exhaustion. The system is out of memory and cannot satisfy the allocation request at all. This is more likely to happen on swap-less systems that are unable to purge application pages to free up memory. The system may have sufficient free pages in total but not enough that are physically contiguous to satisfy the allocation request. This is referred to as memory fragmentation. - The phrase \"page allocation failure\" - this defines the error message that should be used in a KCS search. - The process name \"find\" - often page allocation failures are related to code that gets executed by specific threads, try searching KCS with and without this term. If the allocation request was performed by an interrupt service routine then the process name will be irrelevant. - The order of the allocation \"order:5\" and mode \"mode:0x0\" can often be the same for every instance of a particular problem, try searching KCS with and without these terms. - The function requesting the memory allocation \"rpc_malloc()\" is likely to be the same for every instance of this particular problem so include that in the KCS search. - Do not search for the CPU number, the process id or function offsets since they will change and may prevent finding a match. For more details https://access.redhat.com/articles/1360023 2. What is order:5, mode:0x0 in above output. 3. Difference between kernel panic due to Memory crunch and page allocation failure. 4. Difference between VMalloc and KMalloc Hint => The kmalloc() & vmalloc() functions are a simple interface for obtaining kernel memory in byte-sized chunks. - The kmalloc() function guarantees that the pages are physically contiguous (and virtually contiguous). - The vmalloc() function works in a similar fashion to kmalloc(), except it allocates memory that is only virtually contiguous and not necessarily physically contiguous. 5. How do you troubleshoot memory performance issue. Please explain the details. 6. Which tools do you use to troubleshoot high Memory troubleshooting. 7. What are zombie process and how to kill/reclaim them. 8. What are system calls. 9. What is strace used for. 10. Difference between fork and exec in Linux. 11. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- 12. Explain few examples of kernel panic. 13. How to blacklist a module. 14. What are D-State processes and what causes these. 15. If Disk is causing D-state processes, what you can check and can do to fix the issue. 16. Explain blk_trace. 17. If you are receiving \"Connection reset by peer\" when trying to connect a server, what can be root cause of this and how you can fix this. 18. How will you troubleshoot high load avg. Which tools will you use and why. Explain in details. 19. What can be reason of high load avg in kernel space i.e. %sy in top. 20. What is CPU affinity . How to check it. 21. How to check all cores stat in top. Hint: by pressing 1 or using mpstat 22. What can be the reasons for server to stuck at boot prompt. and how to troubleshoot further ? 23. What is backporting in Linux. Hint: its related to packages. 24. There are two systems and one is taking less booting time and other than taking more than booting time. 25. Please explain Linux booting processing? 26. How to check the loaded and unloaded modules 27. Please explain few types of kernel errors? 28. How to troubleshoot the system performance if any Linux system is facing slowness? 29. How to troubleshoot high memory usageissue on Linux system. 30. What is CPU load ? How to calculate the load average on the system? 31. What the Zombie and Orphan process? How to kill zombie process? 32. What could be the impacts on the system if there are many zombie process are available? 33. What is affinity in Linux? 1. Binding a process with a CPU core. 2. # taskset < command > 34. What is paging and swapping in Linux? Please explain Page fault ? 35. What is difference between cache and buffer? 36. Explain the TOP, IOSTAT,VMSTAT,IOTOP commands. 37. Difference between RHEL6 and RHEL7 booting process. 38. Difference between systemd and initd 39. How to troubleshoot a issue where a client not able to access a server? 40. What are outputs when you execute the NATCAT (nc) command and explain each ? Hint => - Connected - Connection timeout - Connection refused(if port / server is blocking by the firewall) 41. What are the inodes and how will you free up them? 42. How can we check the packet flow in our system? 43. what is the steal value in top command? 44. Explain what is NUMA. You may know this as \"Node Interleaving\u201d. Give an example where NUMA is beneficial, and another where it's detrimental. Hints => a). NUMA: Non Uniform Memory Access (NUMA) is a design where the memory is split into groups by CPU, making the memory access faster for each CPU to their memory group. b). Beneficial: Multiple processes with smaller memory footprints. Good for applications that fork processes, like Apache. Detrimental: Monolithic, memory-heavy, single processes. May be threaded, example MySQL, Redis, Java-everything, Memcached. Follow-Up Q. - what if there's only a single physical proc. 45. Customer complains that their CPUs aren't running as fast as they're rated. How would you Troubleshoot ? Hints => a). Could be heat damage - check /proc/cpuinfo for \"cpu MHz\". b). Could be the wrong CPUs installed, for that matter. Check the CPU specs in /proc/cpuinfo. OR dmidecode. c). Get 'em to tell you how to fix it (Change system profile to MaximumPerformance with omconfig, then reboot) 46. Can we check System logs on Hardware layer from OS ?How ? Hints => a). Yes, in dmesg logs or kern.log b). By using respective Hardware CLI commands. c). Specific answers hpasmcli -s \"show iml\" ==> For HP servers omreport system [alertlog | esmlog] ==> For Dell Servers 47. What can be done to improve the performance of an I/O-heavy system? Hints => a). Check RAID Level, if server using any. b). Partition aligned, i.e. starting a partition on sector 2048. c). Mount Options like: noatime - (covers norelatime/nodiratime) Can be enabled only via fstab. d). RAID Controller settings - read/write policy, e.g. write-through(good to ensure data has been changed on disk. write-back(relys on cache, battery). e). nobarrier - depends on whether or not the PERC Controller is making use of a battery or not. f). Adjust the filesystem journaling method - the default mount option is in the middle, data=ordered. This indicates the data is written out to the filesystem before the metadata in the journal is updated. The filesystem can be mounted with data=writeback which allows data to be written to the journal and disk at the same time, with the risk of some data possibly being written to the disk after the metadata that cannot be recovered after a crash. Additionally, the journal can be placed on an external device with another mount option, allowing separation of the writes to increase throughput performance for each write operation. g). discard - For SSDs. Also called fs TRIM. Extends life and improves performance of SSD drives. TRIM enables the controller to cache the expected SSD write cycles to collapse them into a \"do only once\" mechanism to prevent burning out the SSD cells, which can only be written to once with voltage. Note: For LVM, set issue_discards=1 in lvm.conf. Note: On Dell, TRIM/discard is only available on RAID controllers >= PERC H710 h). For SSD, you don't want controller caching for performance reasons. By default HPs turn caching off. Dell: Cut-through IO (CTIO) is an IO accelerator for SSD arrays that boosts the throughput of devices connected to the PERC Controller. It is enabled through disabling the write-back cache (enable writethrough cache) and disabling Read Ahead. HP: On servers with SSD drives, write caching appears to be disabled by default for performance reasons (there is no need for write caching on SSD drives which are fast enough). Writes to SSD drives will not be acknowledged until the data is actually written to disk: i). On HP, HP SSD Smart Path - this is enabled by default by the RAID creation process in the HP RAID controller itself, and can be checked with the standard commands using hpssacli to view controller details and RAID logical device details. The theory is this special method of SSD writing replaces traditional caching techniques with a special HP invented method for improving SSD performance and longevity. j). Disk IO Scheduler deadline (recommended for DBs) noop (recommended for VMs) 48. Why would I choose to use RAID-5 instead of RAID-10? Or vice versa? Hints => RAID-5: Pro: Space efficiency - you only sacrifice 1x HDD to redundancy. Pro: Possible with a minimum of 3x drives, as opposed to RAID-10's minimum-4. Con: Slow writes. Note: Reads are still quick though, so you'd be fine on a read-heavy application. Con: If two drives fail, you lose everything. RAID-10: Pro: Great performance with both reads and writes. Good for write-heavy applications. Pro: Less risk of a catastrophic failure, since more than 1 drive can fail safely (Note: Depends which drives). Con: Too much wastage of Space. Only 50% capacity is uasable. 49. How snapshots work ? Can you please explain. In terms of VMWare or LVM(in case someone asks). Hints => As soon as you create a snapshot, LVM creates a pool of blocks. This pool also contains a full copy of the LVM metadata of the volume. When writes happen to the main volume such as updating an inode, the block being overwritten is copied to this new pool and the new block is written to the main volume. This is the 'copy-on-write'. Because of this, the more data that gets changed between when a snapshot was taken and the current state of the main volume, the more space will get consumed by that snapshot pool. When you mount the snapshot, the meta-data written when the snapshot was taken allows the mapping of snapshot pool blocks over changed blocks in the volume (or higher level snapshot). This way when an access comes for a specific block, LVM knows which block access. As far as the filesystem on that volume is concerned, there are no snapshots. 50. In which scenario we need to use vgcfgrestore and vgcfgbackup commands ? When /etc/lvm/archive contents are written (when does backup happen ?) Hints => a). To restore VG configuration. b). Backup is taken whenever we make change to LVM (create/remove LV/VG) and we can use it to restore to the prior state. 51. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. 52. What is deadlock ? How to identify if process is in deadlock in Linux/Unix ? Hint => Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process 53. How to load kernel module. any commands ? Hint => We can use \"modprobe\" or \"insmod\" commands to load kernel module. 54. Difference between insmod and modprobe commands in Linux ? 55. Different types of file systems? Hint => ext2, ext3, ext4, xfs, vfat etc. 56. Difference between ext4 and xfs? 57. When v create user which files are referred? 58. Differnce between passwd and shadow file? Hint => /etc/passwd contains User's detail like home directory, shell etc. /etc/shadow conatains User's password hashes. 59. What are the contents in passwd file? Hint => root:x:0:0:root:/root:/bin/bash \u2022 login name \u2022 optional encrypted password \u2022 numerical user ID \u2022 numerical group ID \u2022 user name or comment field \u2022 user home directory \u2022 optional user command interpreter 60. List all users who are not given passwd? with cmd Hint => cat /etc/shadow| awk -F \":\" '$2 ~ /[a-z]/ {print $1}' 61. How to check what processess are running on which port? Hint => Using \"netstat\" and \"ss\" command we can relate processes with port. 62. How to extend partition in lvm? and indepth questions in lvm ? Hint => 1. First attach extra storage/disk to server. 2. Create a new partition on the disk using 'fdisk' or 'parted' command. 3. Create a new Physical Volume(PV) on that new partition. e.g. 'pvcreate /dev/sdb1' 4. Exetend existing Volume Group(VG) using new PV. e.g. 'vgextend vg_name /dev/sdb1' 5. Now extend the LV. e.g . 'lvextend -l 100%FREE /dev/mapper/vg_name-lv_name' 6. now execute 'resize2fs' or 'xfs_growfs'. 63. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses, 64. On which port dns works? Hint => DNS works on port 53.","title":"Top 100 Linux Interview Questions and Answers - Solved:"},{"location":"nightwolf-cotribution/linux_questionairs/#questions-from-github","text":"A lot more Questions from nightwolf-cotribution github repo # General Questions: 1. What did you learn yesterday/this week? 2. Talk about your preferred development/administration environment. (OS, Editor, Browsers, Tools etc.) 3. Tell me about the last major Linux project you finished. 4. Tell me about the biggest mistake you've made in [some recent time period] and how you would do it differently today. What did you learn from this experience? 5. Why we must choose you? 6. What function does DNS play on a network? 7. What is HTTP? 8. What is an HTTP proxy and how does it work? 9. Describe briefly how HTTPS works. 10. What is SMTP? Give the basic scenario of how a mail message is delivered via SMTP. 11. What is RAID? What is RAID0, RAID1, RAID5, RAID10? 12. What is a level 0 backup? What is an incremental backup? 13. Describe the general file system hierarchy of a Linux system. 14. Which difference have between public and private SSH key? # Simple Linux Questions: 15. What is the name and the UID of the administrator user? 16. How to list all files, including hidden ones, in a directory? 17. What is the Unix/Linux command to remove a directory and its contents? 18. Which command will show you free/used memory? Does free memory exist on Linux? 19. How to search for the string \"my konfu is the best\" in files of a directory recursively? 20. How to connect to a remote server or what is SSH? 21. How to get all environment variables and how can you use them? 22. I get \"command not found\" when I run ifconfig -a. What can be wrong? 23. What happens if I type TAB-TAB? 24. What command will show the available disk space on the Unix/Linux system? 25. What commands do you know that can be used to check DNS records? 26. What Unix/Linux commands will alter a files ownership, files permissions? 27. What does chmod +x FILENAME do? 28. What does the permission 0750 on a file mean? 29. What does the permission 0750 on a directory mean? 30. How to add a new system user without login permissions? 31. How to add/remove a group from a user? 32. What is a bash alias? 33. How do you set the mail address of the root/a user? 34. What does CTRL-c do? 35. What does CTRL-d do? 36. What does CTRL-z do? 37. What is in /etc/services? 38. How to redirect STDOUT and STDERR in bash? (> /dev/null 2>&1) 39. What is the difference between UNIX and Linux. 40. What is the difference between Telnet and SSH? 41. Explain the three load averages and what do they indicate. What command can be used to view the load averages? 42. Can you name a lower-case letter that is not a valid option for GNU ls? 43. What is a Linux kernel module? 44. Walk me through the steps in booting into single user mode to troubleshoot a problem. 45. Walk me through the steps you'd take to troubleshoot a 404 error on a web application you administer. 46. What is ICMP protocol? Why do you need to use? # Medium Linux Questions: 47. What do the following commands do and how would you use them? tee awk tr cut tac curl wget watch head tail less cat touch sar netstat tcpdump lsof 48. What does an & after a command do? 49. What does & disown after a command do? 50. What is a packet filter and how does it work? 51. What is Virtual Memory? 52. What is swap and what is it used for? 53. What is an A record, an NS record, a PTR record, a CNAME record, an MX record? 54. Are there any other RRs and what are they used for? 55. What is a Split-Horizon DNS? 56. What is the sticky bit? 57. What does the immutable bit do to a file? 58. What is the difference between hardlinks and symlinks? What happens when you remove the source to a symlink/hardlink? 59. What is an inode and what fields are stored in an inode? 60. How to force/trigger a file system check on next reboot? 61. What is SNMP and what is it used for? 62. What is a runlevel and how to get the current runlevel? 63. What is SSH port forwarding? 64. What is the difference between local and remote port forwarding? 65. What are the steps to add a user to a system without using useradd/adduser? 66. What is MAJOR and MINOR numbers of special files? 67. Describe the mknod command and when you'd use it. 68. Describe a scenario when you get a \"filesystem is full\" error, but 'df' shows there is free space. 69. Describe a scenario when deleting a file, but 'df' not showing the space being freed. 70. Describe how 'ps' works. 71. What happens to a child process that dies and has no parent process to wait for it and what\u2019s bad about this? 72. Explain briefly each one of the process states. 73. How to know which process listens on a specific port? 74. What is a zombie process and what could be the cause of it? 75. You run a bash script and you want to see its output on your terminal and save it to a file at the same time. How could you do it? 76. Explain what echo \"1\" > /proc/sys/net/ipv4/ip_forward does. 77. Describe briefly the steps you need to take in order to create and install a valid certificate for the site https://foo.example.com. 78. Can you have several HTTPS virtual hosts sharing the same IP? 79. What is a wildcard certificate? 80. Which Linux file types do you know? 81. What is the difference between a process and a thread? And parent and child processes after a fork system call? 82. What is the difference between exec and fork? 83. What is \"nohup\" used for? 84. What is the difference between these two commands? myvar=hello & export myvar=hello 85. How many NTP servers would you configure in your local ntp.conf? 86. What does the column 'reach' mean in ntpq -p output? 87. You need to upgrade kernel at 100-1000 servers, how you would do this? 88. How can you get Host, Channel, ID, LUN of SCSI disk? 89. How can you limit process memory usage? 90. What is bash quick substitution/caret replace(^x^y)? 91. Do you know of any alternative shells? If so, have you used any? 92. What is a tarpipe (or, how would you go about copying everything, including hardlinks and special files, from one server to another)? 93. How can you tell if the httpd package was already installed? 94. How can you list the contents of a package? 95. How can you determine which package is better: openssh-server-5.3p1-118.1.el6_8.x86_64 or openssh-server-6.6p1-1.el6.x86_64 ? 96. Can you explain to me the difference between block based, and object based storage? # Hard Linux Questions: 97. What is a tunnel and how you can bypass a http proxy? 98. What is the difference between IDS and IPS? 99. What shortcuts do you use on a regular basis? 100. What is the Linux Standard Base? 101. What is an atomic operation? 102. Your freshly configured http server is not running after a restart, what can you do? 103. What kind of keys are in ~/.ssh/authorized_keys and what it is this file used for? 104. I've added my public ssh key into authorized_keys but I'm still getting a password prompt, what can be wrong? 105. Did you ever create RPM's, DEB's or solaris pkg's? 106. What does :(){ :|:& };: do on your system? 107. How do you catch a Linux signal on a script? 108. Can you catch a SIGKILL? 109. What's happening when the Linux kernel is starting the OOM killer and how does it choose which process to kill first? 110. Describe the linux boot process with as much detail as possible, starting from when the system is powered on and ending when you get a prompt. 111. What's a chroot jail? 112. When trying to umount a directory it says it's busy, how to find out which PID holds the directory? 113. What's LD_PRELOAD and when it's used? 114. You ran a binary and nothing happened. How would you debug this? 115. What are cgroups? Can you specify a scenario where you could use them? 116. How can you remove/delete a file with file-name consisting of only non-printable/non-type-able characters? 117. How can you increase or decrease the priority of a process in Linux? # Expert Linux Questions: 118. A running process gets EAGAIN: Resource temporarily unavailable on reading a socket. How can you close this bad socket/file descriptor without killing the process? 119. What do you control with swapiness? 120. How do you change TCP stack buffers? How do you calculate it? 121. What is Huge Tables? Why isn't it enabled by default? Why and when use it? 122. What is LUKS? How to use it? (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Questions from Github"},{"location":"nightwolf-cotribution/managerial_interview_questions/","text":"Managerial Interview Questions \uf0c1 We have prepared a set of frequently asked Managerial questions to help Freshers and Experienced Engineers in their preparations for Interview. You will find these questions very helpful in your Managerial interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Tell me what was your most difficult situation you faced and you resolved. When you had issue with your management and how you convince them ? How you have helped your peer ? Did you got a chance to convince cutomers for your work ? How the Jira tasks are assigned to your team ? Tell me what your manager told negative point about you. Tell me you put time significant outside your work. Critical feedback from colleague and how did you handled it. Give me an example of calculated risk where speed was critical. Explain your daily routine ? Explain when you received an agitated customer and how did you handled it. Explain when you worked beyond your role What are your strengths and weaknesses. Any time when you anticipated an issue even before customer raised it and how did you fixed it. Explain a time when you were working on an issue and you have to take an at instance decisions to either revert back or Proceed further because you were reaching SLA. Any time when you worked on issues/technology outside of your comfort zone i.e. Outside Linux. Anytime when you were working on a predefined process and you faced an unexpected issue. How did you dealt with it. What if you have to execute a command which can either break things or fix it ? How will you make decision. The Customer satisfaction level of one of the customer is at 80% right now. What can you do to increase the satisfaction level to 90%. Explain a time where you had a conflict with any of your colleague. And how did you solved the situation. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Managerial Interview Questions"},{"location":"nightwolf-cotribution/managerial_interview_questions/#managerial-interview-questions","text":"We have prepared a set of frequently asked Managerial questions to help Freshers and Experienced Engineers in their preparations for Interview. You will find these questions very helpful in your Managerial interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Tell me what was your most difficult situation you faced and you resolved. When you had issue with your management and how you convince them ? How you have helped your peer ? Did you got a chance to convince cutomers for your work ? How the Jira tasks are assigned to your team ? Tell me what your manager told negative point about you. Tell me you put time significant outside your work. Critical feedback from colleague and how did you handled it. Give me an example of calculated risk where speed was critical. Explain your daily routine ? Explain when you received an agitated customer and how did you handled it. Explain when you worked beyond your role What are your strengths and weaknesses. Any time when you anticipated an issue even before customer raised it and how did you fixed it. Explain a time when you were working on an issue and you have to take an at instance decisions to either revert back or Proceed further because you were reaching SLA. Any time when you worked on issues/technology outside of your comfort zone i.e. Outside Linux. Anytime when you were working on a predefined process and you faced an unexpected issue. How did you dealt with it. What if you have to execute a command which can either break things or fix it ? How will you make decision. The Customer satisfaction level of one of the customer is at 80% right now. What can you do to increase the satisfaction level to 90%. Explain a time where you had a conflict with any of your colleague. And how did you solved the situation. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Managerial Interview Questions"},{"location":"nightwolf-cotribution/manual_testing_interview_questions/","text":"TOP 50 Manual Testing interview questions \uf0c1 We have consolidated a list of frequently asked Manual Testing and QA interview questions for Freshers and Experienced QA Engineers. You will find these questions very helpful in your interviews for Manual Testers or QA Engineer. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is Software Testing? * It is a process of analyzing s/w item to detect the differences between existing and required conditions and to evaluate the features of the s/w item. * It is a verification and validation process. * Process of demonstrating that errors are not present. DIFFERENCE BETWEEN VERIFICATION AND VALIDATION? * Verification => o It is a process of confirming whether the s/w meets it is requirement or not. o Process of examining/reviewing of work product. o Are we building the product right? o It is a QA activity. o It is a static process performed at compile time. o It is performed by a QA team or by developer. o Cost and time effective. o Activities involve in this is testing the application. * Validation => o It is a process of confirming whether the s/w meets user requirement or not. o Process of executing a product & examining how it behaves. o Are we building the right product? o It is a QC activity. o It is a dynamic process performed at run time. o It is performed by a QC team or by tester. o Cost and time taking. o Activities involve in this are inspections, reviews, walk-through. DIFFERENCE BETWEEN QUALITY CONTROL AND QUALITY ASSURANCE? * QA => o It ensures the prevention of defects in the process used to make s/w application. o It involves in process-oriented activities. o Aim to prevent defect. o Eg:- verification o It is the technique of managing the quality. o All team members are responsible for QA. o QA is responsible for SDLC. o It is a process to create the deliverables. * QC => o It executes the program or code to identify the defects in the s/w application. o It involves in product-oriented activities. o Aim to identify and improve. o Eg:- validation o It is a method to verify the quality. o Testing team is responsible for QC. o QC is responsible for STLC. o It is a process to verify that deliverables. WHAT IS SDLC? Ans. Software Development Life Cycle refers to all the activities that are performed during software development, including - requirement analysis, designing, implementation, testing, deployment and maintenance phases. EXPLAIN STLC - Software Testing life cycle ? Software testing life cycle refers to all the activities performed during testing of a software product. The phases include => o Requirement analyses and validation - In this phase the requirements documents are analysed and validated and scope of testing is defined. o Test planning - In this phase test plan strategy is defined, estimation of test effort is defined along with automation strategy and tool selection is done. o Test Design and analysis - In this phase test cases are designed, test data is prepared and automation scripts are implemented. o Test environment setup - A test environment closely simulating the real world environment is prepared. o Test execution - The test cases are prepared, bugs are reported and retested once resolved. o Test closure and reporting - A test closure report is prepared having the final test results summary, learning and test metrics. WHAT IS DYNAMIC TESTING? It involves in the execution of code. It validates the output with the expected outcome. WHAT IS STATIC TESTING? It involves in reviewing the documents to identify the defects in the early stages of SDLC. WHAT IS WHITE BOX TESTING? o This also called as glass-box testing, clear-box and structural testing. o It is based on applications internal code structure. o In this, an internal perspective of the system, as well as programming skills are used to design test cases. o In white box testing, the tester analyses the internal architecture of the system as well as the quality of source code on different parameters like code optimization, code coverage, code reusability etc. o This testing usually was done at the unit level. WHAT IS BLACK BOX TESTING? o It is a process of testing a system component considering input, output and general function. o The tester interact with the system through the interface providing input and validating the received output. o It does not require the knowledge of internal program structure. o In this we test UI & backend (coding/database). o External actions are performed. WHAT IS POSITIVE AND NEGATIVE TESTING? Positive Testing => o It is determine what system supposed to do. o It helps to check whether the application is justifying the requirements or not. Negative Testing => o It is determine what system not supposed to do. o It helps to find the defects from the s/w. WHAT IS GRAY BOX TESTING? It is a combination of both black box and white box testing. The tester who works on this type of testing needs to have access to design documents, this helps to create better test cases. WHAT IS TEST STRATEGY? It is a high-level document and usually developed by project manager. It is a document which captures the approach on how we go about testing the product and achieve the goals. WHAT IS TEST PLAN? It is a document which contains the plan for all the testing activities. WHAT IS TEST SCENARIO? It gives the idea of what we have to test. Or testable part of an application is called TS. WHAT IS TEST CASE? It is a set of conditions under which tester determines whether an application/ software is working correctly or not. WHAT IS TEST BED? An environment configured for testing is called test bed. It consist of hardware, s/w, network configuration. WHAT IS TEST SUITE? Collection of test cases. WHAT IS TEST DATA? It is a document that is basically used to test the s/w program. It is divided into 2 categories:- a) +ve test data which is generally gives to system to generate the expected result. b) \u2013ve test data which is used to test the unhandled condition, unexpected, exceptional input condition. WHAT IS DEFECT LIFE CYCLE? Defect Life Cycle or Bug Life Cycle is the specific set of states that a Bug goes through from discovery to defect fixation. Bug Life Cycle phases/status:- The number of states that a defect goes through varies from project to project. Below lifecycle diagram, covers all possible states => o New: When a new defect is logged and posted for the first time. It is assigned a status as NEW. o Assigned: Once the bug is posted by the tester, the lead of the tester approves the bug and assigns the bug to the developer team. o Open: The developer starts analyzing and works on the defect fix. o Fixed: When a developer makes a necessary code change and verifies the change, he or she can make bug status as \"Fixed.\" o Pending retest: after fixing the defect the developer gives a particular code for retesting the code to the tester. Here the testing is pending on the testers end, the status assigned is \"pending request.\" o Retest: Tester does the retesting of the code at, to check whether the defect is fixed by the developer or not and changes the status to \"Re-test.\" - Verified: The tester re-tests the bug after it got fixed by the developer. If there is no bug detected in the software, then the bug is fixed and the status assigned is \"verified.\" - Reopen: If the bug persists even after the developer has fixed the bug, the tester changes the status to \"reopened\". Once again the bug goes through the life cycle. - Closed: If the bug is no longer exists then tester assigns the status \"Closed.\" - Duplicate: If the defect is repeated twice or the defect corresponds to the same concept of the bug, the status is changed to \"duplicate.\" - Rejected: If the developer feels the defect is not a genuine defect then it changes the defect to \"rejected.\" - Deferred: If the present bug is not of a prime priority and if it is expected to get fixed in the next release, then status \"Deferred\" is assigned to such bugs. - Not a bug: If it does not affect the functionality of the application then the status assign to a bug is \"Not a bug\". WHAT IS SMOKE AND SANITY TESTING? SMOKE => o It is a kind of Software Testing performed after software build to ascertain that the critical functionalities of the program are working fine. o The purpose is to reject a badly broken application so that the QA team does not waste time installing and testing the software application. o In Smoke Testing, the test cases chose to cover the most important functionality or component of the system. The objective is not to perform exhaustive testing, but to verify that the critical functionalities of the system are working fine. SANITY => o Sanity testing is a kind of Software Testing performed after receiving a software build, with minor changes in code, or functionality, to ascertain that the bugs have been fixed and no further issues are introduced due to these changes. #### Smoke Testing Vs Sanity Testing - Key Differences Smoke Testing Sanity Testing Smoke Testing is performed to ascertain that the critical functionalities of the program is working fine Sanity Testing is done to check the new functionality/bugs have been fixed The objective of this testing is to verify the \"stability\" of the system in order to proceed with more rigorous testing The objective of the testing is to verify the \"rationality\" of the system in order to proceed with more rigorous testing This testing is performed by the developers or testers Sanity testing is usually performed by testers Smoke testing is usually documented or scripted Sanity testing is usually not documented and is unscripted Smoke testing is a subset of Acceptance testing Sanity testing is a subset of Regression Testing Smoke testing exercises the entire system from end to end Sanity testing exercises only the particular component of the entire system Smoke testing is like General Health Check Up Sanity Testing is like specialized health check up WHAT IS EXIT AND ENTRY CRITERIA? ENTRY => It describes when to start testing i.e. what we have to test it should be stable enough to test. Ex:- if we want to test home page, the SRS/BRS/FRS document & the test cases must be ready and it should be stable enough to test. EXIT => It describes when to stop testing i.e. once everything mentioned below is fulfilled then s/w release is known as exit criteria:- a. Followed before actually releasing the s/w to client. Checking computer testing is done or not. b. Documents checking:- test matrix (RTM)/summary reports. SUSPENSION CRITERIA => when to stop testing temporarily. WHAT IS BLOCKER? Ans. A blocker is a bug of high priority and high severity. It prevents or blocks testing of some other major portion of the application as well. WHAT IS REGRESSION TESTING? To test whether the changed component has introduced any error to unchanged component or not is called as regression testing. It is perform on QA/production site depends. WHAT IS RETESTING? To test whether the reported bug has been resolved by the developer team or not, is known as retesting. MONKEY/AD-HOC TESTING? It is an informal testing performed without a planning or documentation and without having knowledge of the applications/software functionalities. Monkey testing is a type of testing that is performed randomly without any predefined test cases or test inputs. SEVERITY AND PRIORITY? Priority => o \"How prior we need to fix the bug is priority.\" o It means the occurrences of defect. o Decide by developer team. Types(low, medium, high, critical) SEVERITY => o \"How severe the bug is severity.\" o It means how bad the defect is and what impact it can cause in our application. o Decide by the testing team. Types(minor, medium, major) What is defect priority? Ans. A defect priority is the urgency of the fixing the defect. Normally the defect priority is set on a scale of P0 to P3 with P0 defect having the most urgency to fix. What is defect severity? Ans. Defect severity is the severity of the defect impacting the functionality. Based on the organisation, we can have different levels of defect severity ranging from minor to critical or show stopper. Give an example of Low priority-Low severity, Low priority-High severity, High priority-Low severity, High priority-High severity defects. 1. Low priority-Low severity - A spelling mistake in a page not frequently navigated by users. 2. Low priority-High severity - Application crashing in some very corner case. 3. High priority-Low severity - Slight change in logo color or spelling mistake in company name. 4. High priority-High severity - Issue with login functionality WHAT IS UNIT TESTING? It is also called as module testing /component testing. It is done to check whether the individual unit or module of the source code is working properly. It is done by the developer. INTEGRATION TESTING? It is a process of testing the interface between the two s/w units. It is done by 3 ways:- big-bang , top-down, bottom-up approach. Process of combining & testing multiple components together. Normally done by developer but a tester can also perform if he has the knowledge of coding. SYSTEM TESTING? It is a black box testing technique performed to evaluate the computer system. It include both functional and non-functional testing. Verifying the completed system to ensure that the application works as intended or not. \"The behaviour of the system is tested as defined by the scope of the development project.\" Carried out by specialist tester/independent tester. USER-ACCEPATANCE TESTING? User-requirement testing is done. Done by client as well as end user. It is a final stage of testing before used. ALPHA-BETA TESTING? Alpha => o Developer records all the issues. o Done by the end user at dev site. (involves client or tester+dev) Beta => o Dev go through all the issues after specific period of time. o Done by the end user at the client site. (involves client/user) HOW MONKEY TESTING IS DIFFERENT FROM ADHOC TESTING? In case of adhoc testing although there are no predefined or documented test cases still testers have the understanding of the application. While in case of monkey testing testers does not have any understanding of the application. Explain Agile methodology? Agile methodology of software development is based on interative and incremental approach. In this model, the application is broken down into smaller build on which different cross functional team work together providing rapid delivery along with adapting to changing needs at the same time. o Working is done by individual person. o There is scrum master, who will be either tester/developer from the team or the person who has the knowledge of both testing and coding. o Responsibility of scrum master is to narrating the stories to both the team i.e. testing team and development team. o Scrum meetings can be happen in once a week or in 15 days or once a month. Most of the time client is included in scrum meeting. o Because of this meeting, if the one person is absent the another person from same team can complete his work. So project is not paused and dependency on one person is not happened. This is the main advantage of this model. o Sprint is dividing the project into modules and distributing these modules among both the team so that the team is working parallelly. o When to use:- when the project is big/medium and we have to deliver it as soon as possible then we will use this model. Quality is maintained. What is scrum? A scrum is a process for implementing Agile methodology. In scrum, time is divided into sprints and on completion of sprints, a deliverable is shipped. What are the different roles in scrum? The different roles in scrum are - 1. Product Owner - The product owner owns the whole development of the product, assign tasks to the team and act as an interface between the scrum team(development team) and the stakeholders. 2. Scrum Master - The scrum master monitors that scrum rules get followed in the team and conducts scrum meeting. 3. Scrum Team - A scrum team participate in the scrum meetings and perform the tasks assigned. What is a scrum meeting? A scrum meeting is daily meeting in scrum process. This meeting is conducted by scrum master and update of previous day's work along with next day's task and context is defined in this meeting. Explain TDD (Test Driven Development). Test Driven Development is a software development methodology in which the development of the software is driven by test cases created for the functionality to be implemented. In TDD, first the test cases are created and then code to pass the tests is written. Later the code is refactored as per the standards. Explain equivalence class partitioning. Equivalence class partitioning is a specification based black box testing techniques. ECP means Grouping test data into equivalence classes with the assumpation that all the data items lying in the classes will have same effect on the application. In simple it means diving any module into equal parts and test each part separately. E.g. => 1 :- for testing a Square program(program that prints the square of a number- the equivalence classes can be => Set of Negative numbers, whole numbers, decimal numbers, set of large numbers etc.) 2 :- suppose we have to test 1-100 no\u2019s. So 1st we will divide this no into 5 equal parts. (Like 1-20, 21-40,41-60,61-80,81-100). Now we will select random 3 values and multiply these values with the no of parts. Whatever the no will be, we will checked for that values from all the module in place of checking 100 values. Purpose:- testing a complete module is exhaustive testing and time consuming thats why we use quivalence partioning as it is time saving. What is boundary value analysis? Boundary value analysis is a software testing technique for designing test cases wherein the boundary values of the classes of the equivalence class partitioning are taken as input to the test cases. It is also called as a part of stress and -ve testing. e.g. if the test data lies in the range of 0-100, the boundary value analysis will include test data - 0,1, 99, 100. WHAT ARE SOME DEFECT REPORTING ATTRIBUTES? Ans. Some of the attributes of a Defect resport are => - DefectId - A unique identifier of the defect. - Defect Summary - A one line summary of the defect, more like a defect title. - Defect Description - A detailed description of the defect. - Steps to reproduce - The steps to reproduce the defect. - Expected Result - The expected behaviour from which the application is deviating because of the defect. - Actual Result- The current erroneous state of the application w.r.t. the defect. - Defect Severity - Based on the criticality of the defect, this field can be set to minor, medium, major or show stopper. - Priority - Based on the urgency of the defect, this field can be set on a scale of P0 to P3. What is stub? Ans. In case of top-down integration testing, many a times lower level modules are not developed while beginning testing/integration with top level modules. In those cases Stubs or dummy modules are used that simulate the working of modules by providing hardcoded or expected output based on the input values. What is driver? Ans. In case of bottom-up integration testing, drivers are used to simulate the working of top level modules in order to test the related modules lower in the hierarchy. What are some advantages of automation testing? Ans. Some advantages of automation testing are => - Test execution using automation is fast and saves considerable amount of time. - Carefully written test scripts remove the chance of human error during testing. - Tests execution can be scheduled for nightly run using CI tools like Jenkins which can also be configured to provide daily test results to relevant stakeholders. - Automation testing is very less resource intensive. Once the tests are automated, test execution requires almost no time of QAs. Saving Qa bandwidth for other explratory tasks. What are some disadvantages of automation testing? Ans. Some advantages of automation testing are => - It requries skilled automation testing experts to write test scritps. - Additional effort to write scripts is required upfront. - Automation scripts are limited to verification of the tests that are coded. These tests may miss some error that is very glaring and easily identifiable to human(manual QA). - Even with some minor change in application, script updation and maintenance is required. (adsbygoogle = window.adsbygoogle || []).push({}); (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Manual Testing/QA Interview Questions"},{"location":"nightwolf-cotribution/manual_testing_interview_questions/#top-50-manual-testing-interview-questions","text":"We have consolidated a list of frequently asked Manual Testing and QA interview questions for Freshers and Experienced QA Engineers. You will find these questions very helpful in your interviews for Manual Testers or QA Engineer. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is Software Testing? * It is a process of analyzing s/w item to detect the differences between existing and required conditions and to evaluate the features of the s/w item. * It is a verification and validation process. * Process of demonstrating that errors are not present. DIFFERENCE BETWEEN VERIFICATION AND VALIDATION? * Verification => o It is a process of confirming whether the s/w meets it is requirement or not. o Process of examining/reviewing of work product. o Are we building the product right? o It is a QA activity. o It is a static process performed at compile time. o It is performed by a QA team or by developer. o Cost and time effective. o Activities involve in this is testing the application. * Validation => o It is a process of confirming whether the s/w meets user requirement or not. o Process of executing a product & examining how it behaves. o Are we building the right product? o It is a QC activity. o It is a dynamic process performed at run time. o It is performed by a QC team or by tester. o Cost and time taking. o Activities involve in this are inspections, reviews, walk-through. DIFFERENCE BETWEEN QUALITY CONTROL AND QUALITY ASSURANCE? * QA => o It ensures the prevention of defects in the process used to make s/w application. o It involves in process-oriented activities. o Aim to prevent defect. o Eg:- verification o It is the technique of managing the quality. o All team members are responsible for QA. o QA is responsible for SDLC. o It is a process to create the deliverables. * QC => o It executes the program or code to identify the defects in the s/w application. o It involves in product-oriented activities. o Aim to identify and improve. o Eg:- validation o It is a method to verify the quality. o Testing team is responsible for QC. o QC is responsible for STLC. o It is a process to verify that deliverables. WHAT IS SDLC? Ans. Software Development Life Cycle refers to all the activities that are performed during software development, including - requirement analysis, designing, implementation, testing, deployment and maintenance phases. EXPLAIN STLC - Software Testing life cycle ? Software testing life cycle refers to all the activities performed during testing of a software product. The phases include => o Requirement analyses and validation - In this phase the requirements documents are analysed and validated and scope of testing is defined. o Test planning - In this phase test plan strategy is defined, estimation of test effort is defined along with automation strategy and tool selection is done. o Test Design and analysis - In this phase test cases are designed, test data is prepared and automation scripts are implemented. o Test environment setup - A test environment closely simulating the real world environment is prepared. o Test execution - The test cases are prepared, bugs are reported and retested once resolved. o Test closure and reporting - A test closure report is prepared having the final test results summary, learning and test metrics. WHAT IS DYNAMIC TESTING? It involves in the execution of code. It validates the output with the expected outcome. WHAT IS STATIC TESTING? It involves in reviewing the documents to identify the defects in the early stages of SDLC. WHAT IS WHITE BOX TESTING? o This also called as glass-box testing, clear-box and structural testing. o It is based on applications internal code structure. o In this, an internal perspective of the system, as well as programming skills are used to design test cases. o In white box testing, the tester analyses the internal architecture of the system as well as the quality of source code on different parameters like code optimization, code coverage, code reusability etc. o This testing usually was done at the unit level. WHAT IS BLACK BOX TESTING? o It is a process of testing a system component considering input, output and general function. o The tester interact with the system through the interface providing input and validating the received output. o It does not require the knowledge of internal program structure. o In this we test UI & backend (coding/database). o External actions are performed. WHAT IS POSITIVE AND NEGATIVE TESTING? Positive Testing => o It is determine what system supposed to do. o It helps to check whether the application is justifying the requirements or not. Negative Testing => o It is determine what system not supposed to do. o It helps to find the defects from the s/w. WHAT IS GRAY BOX TESTING? It is a combination of both black box and white box testing. The tester who works on this type of testing needs to have access to design documents, this helps to create better test cases. WHAT IS TEST STRATEGY? It is a high-level document and usually developed by project manager. It is a document which captures the approach on how we go about testing the product and achieve the goals. WHAT IS TEST PLAN? It is a document which contains the plan for all the testing activities. WHAT IS TEST SCENARIO? It gives the idea of what we have to test. Or testable part of an application is called TS. WHAT IS TEST CASE? It is a set of conditions under which tester determines whether an application/ software is working correctly or not. WHAT IS TEST BED? An environment configured for testing is called test bed. It consist of hardware, s/w, network configuration. WHAT IS TEST SUITE? Collection of test cases. WHAT IS TEST DATA? It is a document that is basically used to test the s/w program. It is divided into 2 categories:- a) +ve test data which is generally gives to system to generate the expected result. b) \u2013ve test data which is used to test the unhandled condition, unexpected, exceptional input condition. WHAT IS DEFECT LIFE CYCLE? Defect Life Cycle or Bug Life Cycle is the specific set of states that a Bug goes through from discovery to defect fixation. Bug Life Cycle phases/status:- The number of states that a defect goes through varies from project to project. Below lifecycle diagram, covers all possible states => o New: When a new defect is logged and posted for the first time. It is assigned a status as NEW. o Assigned: Once the bug is posted by the tester, the lead of the tester approves the bug and assigns the bug to the developer team. o Open: The developer starts analyzing and works on the defect fix. o Fixed: When a developer makes a necessary code change and verifies the change, he or she can make bug status as \"Fixed.\" o Pending retest: after fixing the defect the developer gives a particular code for retesting the code to the tester. Here the testing is pending on the testers end, the status assigned is \"pending request.\" o Retest: Tester does the retesting of the code at, to check whether the defect is fixed by the developer or not and changes the status to \"Re-test.\" - Verified: The tester re-tests the bug after it got fixed by the developer. If there is no bug detected in the software, then the bug is fixed and the status assigned is \"verified.\" - Reopen: If the bug persists even after the developer has fixed the bug, the tester changes the status to \"reopened\". Once again the bug goes through the life cycle. - Closed: If the bug is no longer exists then tester assigns the status \"Closed.\" - Duplicate: If the defect is repeated twice or the defect corresponds to the same concept of the bug, the status is changed to \"duplicate.\" - Rejected: If the developer feels the defect is not a genuine defect then it changes the defect to \"rejected.\" - Deferred: If the present bug is not of a prime priority and if it is expected to get fixed in the next release, then status \"Deferred\" is assigned to such bugs. - Not a bug: If it does not affect the functionality of the application then the status assign to a bug is \"Not a bug\". WHAT IS SMOKE AND SANITY TESTING? SMOKE => o It is a kind of Software Testing performed after software build to ascertain that the critical functionalities of the program are working fine. o The purpose is to reject a badly broken application so that the QA team does not waste time installing and testing the software application. o In Smoke Testing, the test cases chose to cover the most important functionality or component of the system. The objective is not to perform exhaustive testing, but to verify that the critical functionalities of the system are working fine. SANITY => o Sanity testing is a kind of Software Testing performed after receiving a software build, with minor changes in code, or functionality, to ascertain that the bugs have been fixed and no further issues are introduced due to these changes. #### Smoke Testing Vs Sanity Testing - Key Differences Smoke Testing Sanity Testing Smoke Testing is performed to ascertain that the critical functionalities of the program is working fine Sanity Testing is done to check the new functionality/bugs have been fixed The objective of this testing is to verify the \"stability\" of the system in order to proceed with more rigorous testing The objective of the testing is to verify the \"rationality\" of the system in order to proceed with more rigorous testing This testing is performed by the developers or testers Sanity testing is usually performed by testers Smoke testing is usually documented or scripted Sanity testing is usually not documented and is unscripted Smoke testing is a subset of Acceptance testing Sanity testing is a subset of Regression Testing Smoke testing exercises the entire system from end to end Sanity testing exercises only the particular component of the entire system Smoke testing is like General Health Check Up Sanity Testing is like specialized health check up WHAT IS EXIT AND ENTRY CRITERIA? ENTRY => It describes when to start testing i.e. what we have to test it should be stable enough to test. Ex:- if we want to test home page, the SRS/BRS/FRS document & the test cases must be ready and it should be stable enough to test. EXIT => It describes when to stop testing i.e. once everything mentioned below is fulfilled then s/w release is known as exit criteria:- a. Followed before actually releasing the s/w to client. Checking computer testing is done or not. b. Documents checking:- test matrix (RTM)/summary reports. SUSPENSION CRITERIA => when to stop testing temporarily. WHAT IS BLOCKER? Ans. A blocker is a bug of high priority and high severity. It prevents or blocks testing of some other major portion of the application as well. WHAT IS REGRESSION TESTING? To test whether the changed component has introduced any error to unchanged component or not is called as regression testing. It is perform on QA/production site depends. WHAT IS RETESTING? To test whether the reported bug has been resolved by the developer team or not, is known as retesting. MONKEY/AD-HOC TESTING? It is an informal testing performed without a planning or documentation and without having knowledge of the applications/software functionalities. Monkey testing is a type of testing that is performed randomly without any predefined test cases or test inputs. SEVERITY AND PRIORITY? Priority => o \"How prior we need to fix the bug is priority.\" o It means the occurrences of defect. o Decide by developer team. Types(low, medium, high, critical) SEVERITY => o \"How severe the bug is severity.\" o It means how bad the defect is and what impact it can cause in our application. o Decide by the testing team. Types(minor, medium, major) What is defect priority? Ans. A defect priority is the urgency of the fixing the defect. Normally the defect priority is set on a scale of P0 to P3 with P0 defect having the most urgency to fix. What is defect severity? Ans. Defect severity is the severity of the defect impacting the functionality. Based on the organisation, we can have different levels of defect severity ranging from minor to critical or show stopper. Give an example of Low priority-Low severity, Low priority-High severity, High priority-Low severity, High priority-High severity defects. 1. Low priority-Low severity - A spelling mistake in a page not frequently navigated by users. 2. Low priority-High severity - Application crashing in some very corner case. 3. High priority-Low severity - Slight change in logo color or spelling mistake in company name. 4. High priority-High severity - Issue with login functionality WHAT IS UNIT TESTING? It is also called as module testing /component testing. It is done to check whether the individual unit or module of the source code is working properly. It is done by the developer. INTEGRATION TESTING? It is a process of testing the interface between the two s/w units. It is done by 3 ways:- big-bang , top-down, bottom-up approach. Process of combining & testing multiple components together. Normally done by developer but a tester can also perform if he has the knowledge of coding. SYSTEM TESTING? It is a black box testing technique performed to evaluate the computer system. It include both functional and non-functional testing. Verifying the completed system to ensure that the application works as intended or not. \"The behaviour of the system is tested as defined by the scope of the development project.\" Carried out by specialist tester/independent tester. USER-ACCEPATANCE TESTING? User-requirement testing is done. Done by client as well as end user. It is a final stage of testing before used. ALPHA-BETA TESTING? Alpha => o Developer records all the issues. o Done by the end user at dev site. (involves client or tester+dev) Beta => o Dev go through all the issues after specific period of time. o Done by the end user at the client site. (involves client/user) HOW MONKEY TESTING IS DIFFERENT FROM ADHOC TESTING? In case of adhoc testing although there are no predefined or documented test cases still testers have the understanding of the application. While in case of monkey testing testers does not have any understanding of the application. Explain Agile methodology? Agile methodology of software development is based on interative and incremental approach. In this model, the application is broken down into smaller build on which different cross functional team work together providing rapid delivery along with adapting to changing needs at the same time. o Working is done by individual person. o There is scrum master, who will be either tester/developer from the team or the person who has the knowledge of both testing and coding. o Responsibility of scrum master is to narrating the stories to both the team i.e. testing team and development team. o Scrum meetings can be happen in once a week or in 15 days or once a month. Most of the time client is included in scrum meeting. o Because of this meeting, if the one person is absent the another person from same team can complete his work. So project is not paused and dependency on one person is not happened. This is the main advantage of this model. o Sprint is dividing the project into modules and distributing these modules among both the team so that the team is working parallelly. o When to use:- when the project is big/medium and we have to deliver it as soon as possible then we will use this model. Quality is maintained. What is scrum? A scrum is a process for implementing Agile methodology. In scrum, time is divided into sprints and on completion of sprints, a deliverable is shipped. What are the different roles in scrum? The different roles in scrum are - 1. Product Owner - The product owner owns the whole development of the product, assign tasks to the team and act as an interface between the scrum team(development team) and the stakeholders. 2. Scrum Master - The scrum master monitors that scrum rules get followed in the team and conducts scrum meeting. 3. Scrum Team - A scrum team participate in the scrum meetings and perform the tasks assigned. What is a scrum meeting? A scrum meeting is daily meeting in scrum process. This meeting is conducted by scrum master and update of previous day's work along with next day's task and context is defined in this meeting. Explain TDD (Test Driven Development). Test Driven Development is a software development methodology in which the development of the software is driven by test cases created for the functionality to be implemented. In TDD, first the test cases are created and then code to pass the tests is written. Later the code is refactored as per the standards. Explain equivalence class partitioning. Equivalence class partitioning is a specification based black box testing techniques. ECP means Grouping test data into equivalence classes with the assumpation that all the data items lying in the classes will have same effect on the application. In simple it means diving any module into equal parts and test each part separately. E.g. => 1 :- for testing a Square program(program that prints the square of a number- the equivalence classes can be => Set of Negative numbers, whole numbers, decimal numbers, set of large numbers etc.) 2 :- suppose we have to test 1-100 no\u2019s. So 1st we will divide this no into 5 equal parts. (Like 1-20, 21-40,41-60,61-80,81-100). Now we will select random 3 values and multiply these values with the no of parts. Whatever the no will be, we will checked for that values from all the module in place of checking 100 values. Purpose:- testing a complete module is exhaustive testing and time consuming thats why we use quivalence partioning as it is time saving. What is boundary value analysis? Boundary value analysis is a software testing technique for designing test cases wherein the boundary values of the classes of the equivalence class partitioning are taken as input to the test cases. It is also called as a part of stress and -ve testing. e.g. if the test data lies in the range of 0-100, the boundary value analysis will include test data - 0,1, 99, 100. WHAT ARE SOME DEFECT REPORTING ATTRIBUTES? Ans. Some of the attributes of a Defect resport are => - DefectId - A unique identifier of the defect. - Defect Summary - A one line summary of the defect, more like a defect title. - Defect Description - A detailed description of the defect. - Steps to reproduce - The steps to reproduce the defect. - Expected Result - The expected behaviour from which the application is deviating because of the defect. - Actual Result- The current erroneous state of the application w.r.t. the defect. - Defect Severity - Based on the criticality of the defect, this field can be set to minor, medium, major or show stopper. - Priority - Based on the urgency of the defect, this field can be set on a scale of P0 to P3. What is stub? Ans. In case of top-down integration testing, many a times lower level modules are not developed while beginning testing/integration with top level modules. In those cases Stubs or dummy modules are used that simulate the working of modules by providing hardcoded or expected output based on the input values. What is driver? Ans. In case of bottom-up integration testing, drivers are used to simulate the working of top level modules in order to test the related modules lower in the hierarchy. What are some advantages of automation testing? Ans. Some advantages of automation testing are => - Test execution using automation is fast and saves considerable amount of time. - Carefully written test scripts remove the chance of human error during testing. - Tests execution can be scheduled for nightly run using CI tools like Jenkins which can also be configured to provide daily test results to relevant stakeholders. - Automation testing is very less resource intensive. Once the tests are automated, test execution requires almost no time of QAs. Saving Qa bandwidth for other explratory tasks. What are some disadvantages of automation testing? Ans. Some advantages of automation testing are => - It requries skilled automation testing experts to write test scritps. - Additional effort to write scripts is required upfront. - Automation scripts are limited to verification of the tests that are coded. These tests may miss some error that is very glaring and easily identifiable to human(manual QA). - Even with some minor change in application, script updation and maintenance is required. (adsbygoogle = window.adsbygoogle || []).push({}); (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"TOP 50 Manual Testing interview questions"},{"location":"nightwolf-cotribution/network/","text":"OS Networking Interview Questions \uf0c1 General questions about basic Networking + OS Networking + Troubleshooting. (adsbygoogle = window.adsbygoogle || []).push({}); 1. How to check if you are receiving connection timeouts when trying to connect to a server. 2. In what condition you will receive connection refused. Hint: 1. if packets are rejected using firewall/iptables action \"REJECT\" 2. If service is not responding or is down. 3. what will you do/check if customer calls and tells you that there is packet drop when he is trying to access a remote server. 4. How will you troubleshoot packet drops? 5. What is TCP re-transmissions ? 6. What if there are no Packet drops/loss and high TCP re-transmissions on server. How will this impact server/services ? 7. What tools can be used to check bandwidth usage between 2 servers/devices. 8. When you poweron a laptop, how does it receives IP every time. Explain the process. 9. What if dhcp lease is over, will IP/connection drop from server. 10. What is APIPA. In what case Server gets APIPA ip? Hint: When server is configured to receive IP from DHCP and there is no DHCP in that network. 11. What happens in backend why you type google.com in browser and hit enter. 12. Explain recursive and Iterative DNS queries. 13. What can be the reason for receiving Request time outs when trying to resolve rackspace.com. 14. What are HOPs in network. In a packet-switching network, a hop is the trip a data packet takes from one router or intermediate point to another in the network. On the Internet (or a network that uses TCP/IP), the number of hops a packet has taken toward its destination (called the \"hop count\") is kept in the packet header. A packet with an exceedingly large hop count is discarded. 15. what is Apache 500 status code (Internel server error) and how to troubleshoot the 500 status code issue. 16. DNS working flow? What happens when we hit www.google.com in browser? 17. What is recursive and iterative query? 18. What are the dns records? Explain each and every record? 19. What is difference between below 3 domains www.google.com Google.com www.google.com. 20. What is the DORA process ? (adsbygoogle = window.adsbygoogle || []).push({}); 21. TCP 3 way handshaking and 4 way termination? 22. SSL/TLS 4 way hand shaking? 23. How to check TCP buffer size in Linux system? 24. How to check packet drops in Linux? 25. Explain the OSI model and working of each and every layer. 26. Difference between TCP and UDP protocol. 27. Why we have only 13 DNS server in the world? 28. A customer complains of website slowness, what will be your troubleshooting approach in network prospective? 29. Difference between Router and switch ? 30. What configuration happens on switch and router? 31. What happens when you open a website in browser? 32. How does a client gets connected to Wifi? or What happens in the backend when client connects to wifi? 33. Remove session layer from the picture and now consider running traceroute on 2 CMD terminals. How does the request differ on both CMD terminals? 34. Can sequence number be duplicate in TCP/IP? 35. How many traceroute hops will be there when you run traceroute from server A to Server B Server A --> Switch A --> Router R1 --> Router R2 --> Switch B --> Server B 36. Please explain a recent issue that you experienced in your environment and your learning associated with it. (adsbygoogle = window.adsbygoogle || []).push({}); Questions from Github \uf0c1 A lot more Questions from nightwolf-cotribution github repo 1. What is localhost and why would ping localhost fail? 2. What is the similarity between \"ping\" & \"traceroute\" ? How is traceroute able to find the hops. 3. What is the command used to show all open ports and/or socket connections on a machine? 4. Is 300.168.0.123 a valid IPv4 address? 5. Which IP ranges/subnets are \"private\" or \"non-routable\" (RFC 1918)? 6. What is a VLAN? 7. What is ARP and what is it used for? 8. What is the difference between TCP and UDP? 9. What is the purpose of a default gateway? 10. What is command used to show the routing table on a Linux box? 11. A TCP connection on a network can be uniquely defined by 4 things. What are those things? 12. When a client running a web browser connects to a web server, what is the source port and what is the destination port of the connection? 13. How do you add an IPv6 address to a specific interface? 14. You have added an IPv4 and IPv6 address to interface eth0. A ping to the v4 address is working but a ping to the v6 address gives you the response sendmsg: operation not permitted. What could be wrong? 15. What is SNAT and when should it be used? 16. Explain how could you ssh login into a Linux system that DROPs all new incoming packets using a SSH tunnel. 17. How do you stop a DDoS attack? 18. How can you see content of an ip packet? 19. What is IPoAC (RFC 1149)? 20. What will happen when you bind port 0? 21. Customer changes an A record in their DNS control panel but calls us because they're still seeing the old version of their web site. How would you troubleshoot? 22. I type \"ping nightwolf.in\" at a command-line. What things does the Linux OS do to turn \"nightwolf.in\" into an IP address? Hints => a). /etc/hosts - used in name resolution whenever the files source is being defined in the hosts database b). nameserver \u2013 IP (IPv4 or IPv6) of the server that will resolve the queries c). /etc/resolv.conf - used by the libresolv library that's used by the libnss_dns library, this is when the source used is DNS. 23. Can you still SSH to a Linux server if its default gateway is set incorrectly? How? Hints => a). You can SSH in, but only from another device in the same subnet, or in a network to which the \"broken\" server has a static route defined. b).some static route is there which can route to destination network. 24. Two servers behind the same firewall are able to communicate by their private IPs, but not via FQDN. What might be causing this, and what can be done to fix it? Hints => a). The FQDNs are resolving to public IPs, and public IPs are unrouteable within the private network behind the firewall. b). Edit /etc/hosts on the all servers behind the firewall to override DNS to point to private IP. c). The \"best\" fix would be to enable DNS doctoring/translation at the firewall. (adsbygoogle = window.adsbygoogle || []).push({}); 25. What are the pros/cons of Load Balancer Health checks? 26. What is persistence (ie: sticky sessions), and what are the pros and cons of it? Hint => Persistence: If source X went to node Y within the past Z minutes, bypass all balancing algorithms and send directly to node Y again. Pros: \u2022 Avoids the breakage of per-node session information (ie: /var/lib/php/session files). Shopping carts, active logins, etc. Cons: \u2022 In general, persistence can lead to some imbalance of active sessions between nodes. If a node is temporarily unavailable (ie: rebooted), all active sessions become persistent to the remaining node(s). \u2022 With LeastConnections, this means the recovering node takes all new sessions, and is potentially overloaded until sessions equalize. \u2022 With RoundRobin, this means the recovering node has significantly fewer sessions than the others for quite a while. \u2022 If one connection is causing a large load, that load is not balanced - persistence keeps it all on one node. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"OS Networking Interview Questions"},{"location":"nightwolf-cotribution/network/#os-networking-interview-questions","text":"General questions about basic Networking + OS Networking + Troubleshooting. (adsbygoogle = window.adsbygoogle || []).push({}); 1. How to check if you are receiving connection timeouts when trying to connect to a server. 2. In what condition you will receive connection refused. Hint: 1. if packets are rejected using firewall/iptables action \"REJECT\" 2. If service is not responding or is down. 3. what will you do/check if customer calls and tells you that there is packet drop when he is trying to access a remote server. 4. How will you troubleshoot packet drops? 5. What is TCP re-transmissions ? 6. What if there are no Packet drops/loss and high TCP re-transmissions on server. How will this impact server/services ? 7. What tools can be used to check bandwidth usage between 2 servers/devices. 8. When you poweron a laptop, how does it receives IP every time. Explain the process. 9. What if dhcp lease is over, will IP/connection drop from server. 10. What is APIPA. In what case Server gets APIPA ip? Hint: When server is configured to receive IP from DHCP and there is no DHCP in that network. 11. What happens in backend why you type google.com in browser and hit enter. 12. Explain recursive and Iterative DNS queries. 13. What can be the reason for receiving Request time outs when trying to resolve rackspace.com. 14. What are HOPs in network. In a packet-switching network, a hop is the trip a data packet takes from one router or intermediate point to another in the network. On the Internet (or a network that uses TCP/IP), the number of hops a packet has taken toward its destination (called the \"hop count\") is kept in the packet header. A packet with an exceedingly large hop count is discarded. 15. what is Apache 500 status code (Internel server error) and how to troubleshoot the 500 status code issue. 16. DNS working flow? What happens when we hit www.google.com in browser? 17. What is recursive and iterative query? 18. What are the dns records? Explain each and every record? 19. What is difference between below 3 domains www.google.com Google.com www.google.com. 20. What is the DORA process ? (adsbygoogle = window.adsbygoogle || []).push({}); 21. TCP 3 way handshaking and 4 way termination? 22. SSL/TLS 4 way hand shaking? 23. How to check TCP buffer size in Linux system? 24. How to check packet drops in Linux? 25. Explain the OSI model and working of each and every layer. 26. Difference between TCP and UDP protocol. 27. Why we have only 13 DNS server in the world? 28. A customer complains of website slowness, what will be your troubleshooting approach in network prospective? 29. Difference between Router and switch ? 30. What configuration happens on switch and router? 31. What happens when you open a website in browser? 32. How does a client gets connected to Wifi? or What happens in the backend when client connects to wifi? 33. Remove session layer from the picture and now consider running traceroute on 2 CMD terminals. How does the request differ on both CMD terminals? 34. Can sequence number be duplicate in TCP/IP? 35. How many traceroute hops will be there when you run traceroute from server A to Server B Server A --> Switch A --> Router R1 --> Router R2 --> Switch B --> Server B 36. Please explain a recent issue that you experienced in your environment and your learning associated with it. (adsbygoogle = window.adsbygoogle || []).push({});","title":"OS Networking Interview Questions"},{"location":"nightwolf-cotribution/network/#questions-from-github","text":"A lot more Questions from nightwolf-cotribution github repo 1. What is localhost and why would ping localhost fail? 2. What is the similarity between \"ping\" & \"traceroute\" ? How is traceroute able to find the hops. 3. What is the command used to show all open ports and/or socket connections on a machine? 4. Is 300.168.0.123 a valid IPv4 address? 5. Which IP ranges/subnets are \"private\" or \"non-routable\" (RFC 1918)? 6. What is a VLAN? 7. What is ARP and what is it used for? 8. What is the difference between TCP and UDP? 9. What is the purpose of a default gateway? 10. What is command used to show the routing table on a Linux box? 11. A TCP connection on a network can be uniquely defined by 4 things. What are those things? 12. When a client running a web browser connects to a web server, what is the source port and what is the destination port of the connection? 13. How do you add an IPv6 address to a specific interface? 14. You have added an IPv4 and IPv6 address to interface eth0. A ping to the v4 address is working but a ping to the v6 address gives you the response sendmsg: operation not permitted. What could be wrong? 15. What is SNAT and when should it be used? 16. Explain how could you ssh login into a Linux system that DROPs all new incoming packets using a SSH tunnel. 17. How do you stop a DDoS attack? 18. How can you see content of an ip packet? 19. What is IPoAC (RFC 1149)? 20. What will happen when you bind port 0? 21. Customer changes an A record in their DNS control panel but calls us because they're still seeing the old version of their web site. How would you troubleshoot? 22. I type \"ping nightwolf.in\" at a command-line. What things does the Linux OS do to turn \"nightwolf.in\" into an IP address? Hints => a). /etc/hosts - used in name resolution whenever the files source is being defined in the hosts database b). nameserver \u2013 IP (IPv4 or IPv6) of the server that will resolve the queries c). /etc/resolv.conf - used by the libresolv library that's used by the libnss_dns library, this is when the source used is DNS. 23. Can you still SSH to a Linux server if its default gateway is set incorrectly? How? Hints => a). You can SSH in, but only from another device in the same subnet, or in a network to which the \"broken\" server has a static route defined. b).some static route is there which can route to destination network. 24. Two servers behind the same firewall are able to communicate by their private IPs, but not via FQDN. What might be causing this, and what can be done to fix it? Hints => a). The FQDNs are resolving to public IPs, and public IPs are unrouteable within the private network behind the firewall. b). Edit /etc/hosts on the all servers behind the firewall to override DNS to point to private IP. c). The \"best\" fix would be to enable DNS doctoring/translation at the firewall. (adsbygoogle = window.adsbygoogle || []).push({}); 25. What are the pros/cons of Load Balancer Health checks? 26. What is persistence (ie: sticky sessions), and what are the pros and cons of it? Hint => Persistence: If source X went to node Y within the past Z minutes, bypass all balancing algorithms and send directly to node Y again. Pros: \u2022 Avoids the breakage of per-node session information (ie: /var/lib/php/session files). Shopping carts, active logins, etc. Cons: \u2022 In general, persistence can lead to some imbalance of active sessions between nodes. If a node is temporarily unavailable (ie: rebooted), all active sessions become persistent to the remaining node(s). \u2022 With LeastConnections, this means the recovering node takes all new sessions, and is potentially overloaded until sessions equalize. \u2022 With RoundRobin, this means the recovering node has significantly fewer sessions than the others for quite a while. \u2022 If one connection is causing a large load, that load is not balanced - persistence keeps it all on one node. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Questions from Github"},{"location":"nightwolf-cotribution/performance/","text":"Linux System Performance Troubleshooting \uf0c1 Definition: System performance is a measure of the amount of useful work done by a System in a time range. OS/CPU Load: The average amount of processes using or waiting for CPU allocation over a period of time. Usually System Performance issues can be identified by observing slowness in services offered by OS and it mostly happen due to any of the below reasons. - high CPU Usage - high Memory Usage - high Disk IO Usage - Network Performance Issues - Software Bugs like memory leaks, Kernel bugs etc. We will try to break your investigation steps into major bullet points and helps you to find the root causes of the issue. Tools helpful in your investigation: \uf0c1 Below listed tools are Linux performance monitoring tools, which will help you find out the root cause of the issue, top mpstat sar free strace iotop iostat netstat pidstat vmstat tcpdump iptraf blktrace lsof ethtool type-clip : A very good tool when you have to paste something into virtual consoles. recap : To capture historic stats of a server. This tool is almost harmless and captures a lot of information. You can add this tool to your default package list in your environment. You can install these tools very easily using below command: yum install -y lsof sysstat iptraf tcpdump procps-ng net-tools strace iotop ethtool blktrace (adsbygoogle = window.adsbygoogle || []).push({}); You can start your investigation by executing small script, which will gather a lot of system stats for you: \uf0c1 bash <(curl -s https://raw.githubusercontent.com/v-nightwolf/nightwolf-cotribution/main/server_stats.sh) The output of above script will look like this: ############################################ Server Uptime: 1days 6:43:23 Load Average: Current 0.26 Load Average: 15min Average 0.26 #### Printing CPU stats: #### 01:15:26 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 01:15:26 PM all 1.47 0.00 0.48 0.02 0.00 0.12 0.00 0.00 0.00 97.91 ###RAM usage### Free Ram: 569.93MB Used RAM: 3171.23MB 15% ram left RAM ALERT: Low! TOP RAM CONSUMER: /opt/mongodb/mms/jdk/bin/mms-app RAM Usage (RSS): 1857.43 MB Total Number of RAM Processes: 2 Total RAM Usage for all /opt/mongodb/mms/jdk/bin/mms-app processes = 2287.56 MB 61.15% used by /opt/mongodb/mms/jdk/bin/mms-app ###CPU usage### Top Process: /opt/mongodb/mms/jdk/bin/mms-app CPU % for SINGLE Top Process = 4.2 number of processes this is running: 2 Total CPU % for /opt/mongodb/mms/jdk/bin/mms-app = 4.8 ############################################ (adsbygoogle = window.adsbygoogle || []).push({}); Troubleshooting Linux perfomance isssue happening due to high CPU Usage \uf0c1 Every performance issue troubleshooting should starts with top command: and the most useful output of top command is: top - 16:18:32 up 2:52, 1 user, load average: 0.09, 0.20, 0.22 Tasks: 99 total, 1 running, 98 sleeping, 0 stopped, 0 zombie %Cpu(s): 3.2 us, 0.0 sy, 0.0 ni, 96.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 2889532 total, 73328 free, 2707940 used, 108264 buff/cache KiB Swap: 2097148 total, 1206268 free, 890880 used. 48052 avail Mem a). 1st line contains: UPTIME and LOAD Averages current time and length of time since last boot total number of users logged in. system load avg over the last 1, 5 and 15 minutes b). 2nd line contains: TASKs This line shows total tasks or threads, depending on the state of the Threads-mode toggle. That total is further classified as: running; sleeping; stopped; zombie c). 3rd line contains CPU state percentages: This will guide your investigation into a specific direction. As a default, percentages for these individual categories are displayed. us, user : time running un-niced user processes sy, system : time running kernel processes ni, nice : time running niced user processes id, idle : time spent in the kernel idle handler wa, IO-wait : time waiting for I/O completion hi : time spent servicing hardware interrupts si : time spent servicing software interrupts st : time stolen from this vm by the hypervisor Above stats might guide your investigation into a specific direction. Depending on above CPU stats, you will be able to decide what to check next. * If only \"%us\"(time running un-niced user processes) is high that means your application is consuming more CPU. Now you should deep dive into process stats and process trace. We will discuss this in more detail soon. * If only \"%sy\"(time running kernel processes) is high that means you kernel level system call are consuming the CPU. This usually happens when there is a bug in kernel packages. * If \"%ni\"(time running niced user processes) is high that means the prioritized processes are consuming the CPU. You should try to depriritize the process. * If \"%id\"(time spent in the kernel idle handler) is high that means your system is ideal and is not doing anything. * If \"%wa\"(time waiting for I/O completion) is high that means your most CPU time is being spent on waiting for I/O completion. These waits can be due to Disk slowness or Network slowness. * If \"%hi\"(time spent servicing hardware interrupts) is high that means CPU is very busy in servicing the Hardware interrupts. Hardware interrupt will cause CPU to stop its current processing and go handle the Hardware interrupt. Ususally hardware interrupts was generated by physical devices like disk, NIC,computer peripherals. You should check the hardwares attached to the machine and see if they are working fine. * If \"%si\"(time spent servicing software interrupts) is high that CPU is busy serving Software interrupts. Usually Software interrupt occure at kernel level. * \"%st\"(time stolen from this vm by the hypervisor) means that virtual CPU is being spent waiting for the Hypervisor to allocate CPU to virtual machine. This stat is only applicable to Virtual machines. If %us is high, that mean some process in user space is consuming the CPU. Next step would be to deep dive into process stats: 1. First identify the process consuming high CPU. This can be identified using \"top\" command. 2. Once you have identified the process, note down its process id(pid). 3. Try tracing the process using stace command. This will throw a lot output onto the screen. starce -t -p PID_OF_PROCESS => This will print wall clock time of each system call. 4. Please check if process is getting stuck at any system call and note down system call and the resource system call is using. 5. If system call is waiting for a File related I/O, then next step is to check file system call is waiting for. 6. Go to /proc/{$PID_OF_PROCESS}/ (adsbygoogle = window.adsbygoogle || []).push({}); Troubleshooting Linux performance issue happening due to high Memory Usage \uf0c1 4th and 5th line in 'top' output contains Memory stats in Kibibytes(kib). There is slight difference between kilibyte and kibibyte i.e 1 kB = 1000 bytes. 1 KiB = 1024 bytes. As a default, line 4 reflects physical memory, classified as: total, free, used and buff/cache line 5 reflects mostly virtual memory(swap), classified as: total, free, used and avail total: Total size of memory available to system. free: Size of memory which in un-utilized. used: Size of memory currently utilized by processes+System. buff/cache: Size of memory utilized by system for kernel buffers(i.e. buff) and page cache and slabs(i.e. cache) (adsbygoogle = window.adsbygoogle || []).push({}); High memory utilization \uf0c1 Excessive memory use on the system can lead to poor performance due to systems need to move data between the RAM memory and swap storage, memory must be cleared to make room for new allocations, or there is little space available for caching data from the file system. Below is sample output from free command. # free total used free shared buff/cache available Mem: 32877556 3236036 20996296 11316 8645224 29120756 Swap: 1953788 5888 1947900 1. Total Mem: Total amount of memory allocated to the server. 2. Used Mem: Total amount of memory currently being utilized by system and applications. 3. Free Mem: Memory amount currently free on system. 4. Shared Mem: Amount of memory used by tmpfs and shared memory betweendifferent processes. 5. buff/cache Mem: Memory used by OS buffer and caching, which is dropped by OS when new memory request comes. 6. Available Mem: Free Mem + buff/cahce Mem. Linux support virtual memory, which allows applications to use more memory than physical RAM installed on the machine. Linux will use the swap partitions to store the data that is not currently being used and automatically move data between RAM memory and disk drives as required. However, it is very slow to move data between physical RAM and disk. what is Swap? \uf0c1 The primary function of Swap space is to utilize disk space from a separate, dedicated partition on the main storage for RAM when the physical main memory fills up and more space is needed. What is vm.swappiness \uf0c1 vm.swappiness is a value which helps OS to decide when to start using swap. The default value of vm.swappiness is 60. vm.swappiness represents the percentage of the free memory before activating swap. The lower the value of vm.swappiness, the less swapping is used and the more memory pages are kept in physical memory. In simpler words, the value of /proc/sys/vm/swappiness dictates how 'aggressively' the Linux kernel will swap memory pages [during memory reclaim]. The 'swappiness' value can be between 0 and 100. For high Memory centric applications like Databases, Please try to keep the vm.swappiness as low as possible, somewhere between 5 to 10. A value of 0 does not prevent swap. In fact, the kernel will initiate swap when the amount of free and file-backed pages is less than the high water mark in a zone irrespective of the swappiness value. If no swap area(s) exists then swappiness is not applicable. (adsbygoogle = window.adsbygoogle || []).push({}); Main Memory Architecture: \uf0c1 - To expand on main memory, Memory is divided into small-small chunks of memory called Memory Pages. The default size of memory pages on most processors is 4KB, and although some processors use 8KB, 16KB, or 64KB as the default page size. => Fixed size block in RAM (Physical Address Space) - Frame => Fixed size block created by CPU (Logical Address Space) - Page - Main memory is divided by two general categories: Page Cache, where the kernel stores page-sized chunks of files for faster loading; and Anonymous Memory, which is mainly comprised from content not backed by storage, such as program stack and temporary variables. - The kernel uses a memory management program that detects blocks (AKA: pages) of anonymous memory, in which the contents have not been used recently. The memory management program swaps out enough of these relatively infrequently used pages of anonymous memory to a special partition on the main storage specifically designated for paging or swapping. This frees up RAM and makes room for more data. - Those pages of anonymous memory that were swapped out to the Swap partition are tracked by the kernel memory management code and can be paged back into RAM if they are needed (AKA: Swapped-in). => PAGE TABLE: A table that has mapping of addresses in RAM of pages and a validation bit which tells either the page is present on that address or not. Linux memory management: \uf0c1 - When a process in Memory tries to access a file, the Memory Management program checks if that File already exists in Memory and what memory pages are mapped to the requested file. For this, Memory Page faults are raised. What are Memory Page Faults: \uf0c1 - It is a condition when a process in execution can not find memory pages for a file in Virtual memory space. - Memory Page Faults are of 2 types: * Major Page Faults * Minor Page Faults What are Major page faults ? \uf0c1 * If page mapping for requested file is not found in Virtual Memory space, a \"Major page Fault\" is raised. This means requested file needs to be bring into memory from either disk or Swap. Bringing file to memory from Disk or Swap is costs heavy penalty in terms of cpu cycles occurred due to swap-in, swap-out. => \"SWAP-IN\": It is a mthod of moving a file/pages from Swap to main Memory. => \"SWAP-OUT\": It is a method of moving file/pages from Memory to Swap. Major page faults, Swap-IN and Swap-OUT are cpu cycle costly processes. If there is high swap-in and swap-out happening on a server/machine, which clearly means OS is mostly busy in just moving the files/pages around instead of actually serving the actual proccesses. This will degrade the system performance. What are Minor page faults ? \uf0c1 * If requested file page mapping is found in Virtual memory, but pages are mapped/used by some other process, a \"Minor page fault\" will be raised. Linux OS then will mark those pages as shared and will allow both old and new processes to share those pages. What is dirty page ? \uf0c1 * When memory pages mapped to a file are edited by some process, those edited pages are called dirty pages until they are written to disk. In simpler words, Edited and uncommied memory pages are called dirty pages. Dirty pages are saved in page cache until they are written/commited to disk. * Dirty pages are periodically transferred (as well as with the system calls sync or fsync) to the underlying storage device. * Up to and including the 2.6.31 version of the Linux kernel, the pdflush threads ensured that dirty pages were periodically written to the underlying storage device. * Since pdflush had several performance disadvantages, Jens Axboe developed a new, more effective writeback mechanism for Linux Kernel version 2.6.32. This approach provides threads for each device attached to a server. These threads are managed by a service called \"flushd\". Memory issue related Investigation: \uf0c1 1. Review the output of free command: $ free -m total used free shared buff/cache available Mem: 32106 3130 20774 11 8201 28448 Swap: 1907 13 1894 2. Review the SAR command output for continuously heavy Swap in/out activity. This is represented by high values of \"pswpin\" / \"pswpout\" Example of sar output : $ sar -W 12:00:00 AM pswpin/s pswpout/s 05:20:00 AM 0.21 0.00 05:30:00 AM 0.08 0.85 05:40:00 AM 0.47 0.00 05:50:00 AM 3.58 1.71 06:00:00 AM 2.48 0.00 06:10:00 AM 39.91 7.17 <<<<----- High swap-in and swap-out detected 06:20:00 AM 0.21 2.72 06:30:00 AM 13.30 1.04 Or you can check these stats using below command, the si/so section to check swapin/swapout.: $ vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 62208 21707724 694072 7281500 0 0 1 29 1 1 1 1 98 0 0 3. Check the number of major page fault happening on the server using below sar command. This suggests that CPU is mostly busy fetching files from disk/swap rather than serving actual processes. $ sar -B 05:20:10 PM pgpgin/s pgpgout/s fault/s majflt/s pgfree/s pgscank/s pgscand/s pgsteal/s %vmeff 05:30:10 PM 0.00 232.94 1502.78 0.00 1296.10 0.00 0.00 0.00 0.00 05:40:10 PM 0.00 624.97 123624.06 123035 682.68 0.00 0.00 0.00 0.00 <<<--- High major faults 05:50:10 PM 0.00 130.77 229.73 121873 238.82 0.00 0.00 0.00 0.00 <<<--- High major faults 06:00:08 PM 0.05 361.87 303.89 0.00 367.08 0.00 0.00 0.00 0.00 4. Check the kbdirty value in below command output. => kbdirty: Amount of memory in kilobytes waiting to get written back to the disk. If this value is very high, It means either there is something wrong with pdflush/flushd service or CPU is unable to write changed pages(dirty pages) back to disk with expected speed. Now you will have to check the disk performance which we will discuss in detail in our next topic. 05:20:10 PM kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty 05:30:10 PM 21267192 29123732 3075004 9.35 716156 7213396 9264428 26.60 4503104 6166304 252 05:40:10 PM 21286860 29144116 3054432 9.29 716524 7213628 9359668 26.87 4481716 6173528 772 05:50:10 PM 21279032 29136796 3061676 9.31 716684 7213984 9265528 26.60 4506340 6178268 436 06:00:08 PM 21266616 29125040 3073608 9.35 717036 7214472 9254568 26.57 4508936 6169368 1368 5. Tune vm.swappiness kernel parameter: Depending on your System's usage and high swap-in and swap-outs on your system, you can lower the value of vm.swappiness kernel parameter accordingly. For high Memory centric applications like Databases, Please try to keep the vm.swappiness as low as possible, somewhere between 5 to 10. $ cat /proc/sys/vm/swappiness 60 => open /etc/sysctl.conf as root. Then, change or add this line to the file: vm.swappiness = 10 => for changing the swappiness value temporarily try this command: $ echo 50 > /proc/sys/vm/swappiness A value of 0 does not prevent swap. In fact, the kernel will initiate swap when the amount of free and file-backed pages is less than the high water mark in a zone irrespective of the swappiness value. (adsbygoogle = window.adsbygoogle || []).push({}); Troubleshooting Linux performance issue happening due to high Disk IO Usage \uf0c1 Troubleshooting Linux OS network performance Issues \uf0c1 If you have any feedback or suggestion for the content of this website, please use the feedbacks and suggestions form for the same. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Linux-Performance"},{"location":"nightwolf-cotribution/performance/#linux-system-performance-troubleshooting","text":"Definition: System performance is a measure of the amount of useful work done by a System in a time range. OS/CPU Load: The average amount of processes using or waiting for CPU allocation over a period of time. Usually System Performance issues can be identified by observing slowness in services offered by OS and it mostly happen due to any of the below reasons. - high CPU Usage - high Memory Usage - high Disk IO Usage - Network Performance Issues - Software Bugs like memory leaks, Kernel bugs etc. We will try to break your investigation steps into major bullet points and helps you to find the root causes of the issue.","title":"Linux System Performance Troubleshooting"},{"location":"nightwolf-cotribution/performance/#tools-helpful-in-your-investigation","text":"Below listed tools are Linux performance monitoring tools, which will help you find out the root cause of the issue, top mpstat sar free strace iotop iostat netstat pidstat vmstat tcpdump iptraf blktrace lsof ethtool type-clip : A very good tool when you have to paste something into virtual consoles. recap : To capture historic stats of a server. This tool is almost harmless and captures a lot of information. You can add this tool to your default package list in your environment. You can install these tools very easily using below command: yum install -y lsof sysstat iptraf tcpdump procps-ng net-tools strace iotop ethtool blktrace (adsbygoogle = window.adsbygoogle || []).push({});","title":"Tools helpful in your investigation:"},{"location":"nightwolf-cotribution/performance/#you-can-start-your-investigation-by-executing-small-script-which-will-gather-a-lot-of-system-stats-for-you","text":"bash <(curl -s https://raw.githubusercontent.com/v-nightwolf/nightwolf-cotribution/main/server_stats.sh) The output of above script will look like this: ############################################ Server Uptime: 1days 6:43:23 Load Average: Current 0.26 Load Average: 15min Average 0.26 #### Printing CPU stats: #### 01:15:26 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 01:15:26 PM all 1.47 0.00 0.48 0.02 0.00 0.12 0.00 0.00 0.00 97.91 ###RAM usage### Free Ram: 569.93MB Used RAM: 3171.23MB 15% ram left RAM ALERT: Low! TOP RAM CONSUMER: /opt/mongodb/mms/jdk/bin/mms-app RAM Usage (RSS): 1857.43 MB Total Number of RAM Processes: 2 Total RAM Usage for all /opt/mongodb/mms/jdk/bin/mms-app processes = 2287.56 MB 61.15% used by /opt/mongodb/mms/jdk/bin/mms-app ###CPU usage### Top Process: /opt/mongodb/mms/jdk/bin/mms-app CPU % for SINGLE Top Process = 4.2 number of processes this is running: 2 Total CPU % for /opt/mongodb/mms/jdk/bin/mms-app = 4.8 ############################################ (adsbygoogle = window.adsbygoogle || []).push({});","title":"You can start your investigation by executing small script, which will gather a lot of system stats for you:"},{"location":"nightwolf-cotribution/performance/#troubleshooting-linux-perfomance-isssue-happening-due-to-high-cpu-usage","text":"Every performance issue troubleshooting should starts with top command: and the most useful output of top command is: top - 16:18:32 up 2:52, 1 user, load average: 0.09, 0.20, 0.22 Tasks: 99 total, 1 running, 98 sleeping, 0 stopped, 0 zombie %Cpu(s): 3.2 us, 0.0 sy, 0.0 ni, 96.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 2889532 total, 73328 free, 2707940 used, 108264 buff/cache KiB Swap: 2097148 total, 1206268 free, 890880 used. 48052 avail Mem a). 1st line contains: UPTIME and LOAD Averages current time and length of time since last boot total number of users logged in. system load avg over the last 1, 5 and 15 minutes b). 2nd line contains: TASKs This line shows total tasks or threads, depending on the state of the Threads-mode toggle. That total is further classified as: running; sleeping; stopped; zombie c). 3rd line contains CPU state percentages: This will guide your investigation into a specific direction. As a default, percentages for these individual categories are displayed. us, user : time running un-niced user processes sy, system : time running kernel processes ni, nice : time running niced user processes id, idle : time spent in the kernel idle handler wa, IO-wait : time waiting for I/O completion hi : time spent servicing hardware interrupts si : time spent servicing software interrupts st : time stolen from this vm by the hypervisor Above stats might guide your investigation into a specific direction. Depending on above CPU stats, you will be able to decide what to check next. * If only \"%us\"(time running un-niced user processes) is high that means your application is consuming more CPU. Now you should deep dive into process stats and process trace. We will discuss this in more detail soon. * If only \"%sy\"(time running kernel processes) is high that means you kernel level system call are consuming the CPU. This usually happens when there is a bug in kernel packages. * If \"%ni\"(time running niced user processes) is high that means the prioritized processes are consuming the CPU. You should try to depriritize the process. * If \"%id\"(time spent in the kernel idle handler) is high that means your system is ideal and is not doing anything. * If \"%wa\"(time waiting for I/O completion) is high that means your most CPU time is being spent on waiting for I/O completion. These waits can be due to Disk slowness or Network slowness. * If \"%hi\"(time spent servicing hardware interrupts) is high that means CPU is very busy in servicing the Hardware interrupts. Hardware interrupt will cause CPU to stop its current processing and go handle the Hardware interrupt. Ususally hardware interrupts was generated by physical devices like disk, NIC,computer peripherals. You should check the hardwares attached to the machine and see if they are working fine. * If \"%si\"(time spent servicing software interrupts) is high that CPU is busy serving Software interrupts. Usually Software interrupt occure at kernel level. * \"%st\"(time stolen from this vm by the hypervisor) means that virtual CPU is being spent waiting for the Hypervisor to allocate CPU to virtual machine. This stat is only applicable to Virtual machines. If %us is high, that mean some process in user space is consuming the CPU. Next step would be to deep dive into process stats: 1. First identify the process consuming high CPU. This can be identified using \"top\" command. 2. Once you have identified the process, note down its process id(pid). 3. Try tracing the process using stace command. This will throw a lot output onto the screen. starce -t -p PID_OF_PROCESS => This will print wall clock time of each system call. 4. Please check if process is getting stuck at any system call and note down system call and the resource system call is using. 5. If system call is waiting for a File related I/O, then next step is to check file system call is waiting for. 6. Go to /proc/{$PID_OF_PROCESS}/ (adsbygoogle = window.adsbygoogle || []).push({});","title":"Troubleshooting Linux perfomance isssue happening due to high CPU Usage"},{"location":"nightwolf-cotribution/performance/#troubleshooting-linux-performance-issue-happening-due-to-high-memory-usage","text":"4th and 5th line in 'top' output contains Memory stats in Kibibytes(kib). There is slight difference between kilibyte and kibibyte i.e 1 kB = 1000 bytes. 1 KiB = 1024 bytes. As a default, line 4 reflects physical memory, classified as: total, free, used and buff/cache line 5 reflects mostly virtual memory(swap), classified as: total, free, used and avail total: Total size of memory available to system. free: Size of memory which in un-utilized. used: Size of memory currently utilized by processes+System. buff/cache: Size of memory utilized by system for kernel buffers(i.e. buff) and page cache and slabs(i.e. cache) (adsbygoogle = window.adsbygoogle || []).push({});","title":"Troubleshooting Linux performance issue happening due to high Memory Usage"},{"location":"nightwolf-cotribution/performance/#high-memory-utilization","text":"Excessive memory use on the system can lead to poor performance due to systems need to move data between the RAM memory and swap storage, memory must be cleared to make room for new allocations, or there is little space available for caching data from the file system. Below is sample output from free command. # free total used free shared buff/cache available Mem: 32877556 3236036 20996296 11316 8645224 29120756 Swap: 1953788 5888 1947900 1. Total Mem: Total amount of memory allocated to the server. 2. Used Mem: Total amount of memory currently being utilized by system and applications. 3. Free Mem: Memory amount currently free on system. 4. Shared Mem: Amount of memory used by tmpfs and shared memory betweendifferent processes. 5. buff/cache Mem: Memory used by OS buffer and caching, which is dropped by OS when new memory request comes. 6. Available Mem: Free Mem + buff/cahce Mem. Linux support virtual memory, which allows applications to use more memory than physical RAM installed on the machine. Linux will use the swap partitions to store the data that is not currently being used and automatically move data between RAM memory and disk drives as required. However, it is very slow to move data between physical RAM and disk.","title":"High memory utilization"},{"location":"nightwolf-cotribution/performance/#what-is-swap","text":"The primary function of Swap space is to utilize disk space from a separate, dedicated partition on the main storage for RAM when the physical main memory fills up and more space is needed.","title":"what is Swap?"},{"location":"nightwolf-cotribution/performance/#what-is-vmswappiness","text":"vm.swappiness is a value which helps OS to decide when to start using swap. The default value of vm.swappiness is 60. vm.swappiness represents the percentage of the free memory before activating swap. The lower the value of vm.swappiness, the less swapping is used and the more memory pages are kept in physical memory. In simpler words, the value of /proc/sys/vm/swappiness dictates how 'aggressively' the Linux kernel will swap memory pages [during memory reclaim]. The 'swappiness' value can be between 0 and 100. For high Memory centric applications like Databases, Please try to keep the vm.swappiness as low as possible, somewhere between 5 to 10. A value of 0 does not prevent swap. In fact, the kernel will initiate swap when the amount of free and file-backed pages is less than the high water mark in a zone irrespective of the swappiness value. If no swap area(s) exists then swappiness is not applicable. (adsbygoogle = window.adsbygoogle || []).push({});","title":"What is vm.swappiness"},{"location":"nightwolf-cotribution/performance/#main-memory-architecture","text":"- To expand on main memory, Memory is divided into small-small chunks of memory called Memory Pages. The default size of memory pages on most processors is 4KB, and although some processors use 8KB, 16KB, or 64KB as the default page size. => Fixed size block in RAM (Physical Address Space) - Frame => Fixed size block created by CPU (Logical Address Space) - Page - Main memory is divided by two general categories: Page Cache, where the kernel stores page-sized chunks of files for faster loading; and Anonymous Memory, which is mainly comprised from content not backed by storage, such as program stack and temporary variables. - The kernel uses a memory management program that detects blocks (AKA: pages) of anonymous memory, in which the contents have not been used recently. The memory management program swaps out enough of these relatively infrequently used pages of anonymous memory to a special partition on the main storage specifically designated for paging or swapping. This frees up RAM and makes room for more data. - Those pages of anonymous memory that were swapped out to the Swap partition are tracked by the kernel memory management code and can be paged back into RAM if they are needed (AKA: Swapped-in). => PAGE TABLE: A table that has mapping of addresses in RAM of pages and a validation bit which tells either the page is present on that address or not.","title":"Main Memory Architecture:"},{"location":"nightwolf-cotribution/performance/#linux-memory-management","text":"- When a process in Memory tries to access a file, the Memory Management program checks if that File already exists in Memory and what memory pages are mapped to the requested file. For this, Memory Page faults are raised.","title":"Linux memory management:"},{"location":"nightwolf-cotribution/performance/#what-are-memory-page-faults","text":"- It is a condition when a process in execution can not find memory pages for a file in Virtual memory space. - Memory Page Faults are of 2 types: * Major Page Faults * Minor Page Faults","title":"What are Memory Page Faults:"},{"location":"nightwolf-cotribution/performance/#what-are-major-page-faults","text":"* If page mapping for requested file is not found in Virtual Memory space, a \"Major page Fault\" is raised. This means requested file needs to be bring into memory from either disk or Swap. Bringing file to memory from Disk or Swap is costs heavy penalty in terms of cpu cycles occurred due to swap-in, swap-out. => \"SWAP-IN\": It is a mthod of moving a file/pages from Swap to main Memory. => \"SWAP-OUT\": It is a method of moving file/pages from Memory to Swap. Major page faults, Swap-IN and Swap-OUT are cpu cycle costly processes. If there is high swap-in and swap-out happening on a server/machine, which clearly means OS is mostly busy in just moving the files/pages around instead of actually serving the actual proccesses. This will degrade the system performance.","title":"What are Major page faults ?"},{"location":"nightwolf-cotribution/performance/#what-are-minor-page-faults","text":"* If requested file page mapping is found in Virtual memory, but pages are mapped/used by some other process, a \"Minor page fault\" will be raised. Linux OS then will mark those pages as shared and will allow both old and new processes to share those pages.","title":"What are Minor page faults ?"},{"location":"nightwolf-cotribution/performance/#what-is-dirty-page","text":"* When memory pages mapped to a file are edited by some process, those edited pages are called dirty pages until they are written to disk. In simpler words, Edited and uncommied memory pages are called dirty pages. Dirty pages are saved in page cache until they are written/commited to disk. * Dirty pages are periodically transferred (as well as with the system calls sync or fsync) to the underlying storage device. * Up to and including the 2.6.31 version of the Linux kernel, the pdflush threads ensured that dirty pages were periodically written to the underlying storage device. * Since pdflush had several performance disadvantages, Jens Axboe developed a new, more effective writeback mechanism for Linux Kernel version 2.6.32. This approach provides threads for each device attached to a server. These threads are managed by a service called \"flushd\".","title":"What is dirty page ?"},{"location":"nightwolf-cotribution/performance/#memory-issue-related-investigation","text":"1. Review the output of free command: $ free -m total used free shared buff/cache available Mem: 32106 3130 20774 11 8201 28448 Swap: 1907 13 1894 2. Review the SAR command output for continuously heavy Swap in/out activity. This is represented by high values of \"pswpin\" / \"pswpout\" Example of sar output : $ sar -W 12:00:00 AM pswpin/s pswpout/s 05:20:00 AM 0.21 0.00 05:30:00 AM 0.08 0.85 05:40:00 AM 0.47 0.00 05:50:00 AM 3.58 1.71 06:00:00 AM 2.48 0.00 06:10:00 AM 39.91 7.17 <<<<----- High swap-in and swap-out detected 06:20:00 AM 0.21 2.72 06:30:00 AM 13.30 1.04 Or you can check these stats using below command, the si/so section to check swapin/swapout.: $ vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 62208 21707724 694072 7281500 0 0 1 29 1 1 1 1 98 0 0 3. Check the number of major page fault happening on the server using below sar command. This suggests that CPU is mostly busy fetching files from disk/swap rather than serving actual processes. $ sar -B 05:20:10 PM pgpgin/s pgpgout/s fault/s majflt/s pgfree/s pgscank/s pgscand/s pgsteal/s %vmeff 05:30:10 PM 0.00 232.94 1502.78 0.00 1296.10 0.00 0.00 0.00 0.00 05:40:10 PM 0.00 624.97 123624.06 123035 682.68 0.00 0.00 0.00 0.00 <<<--- High major faults 05:50:10 PM 0.00 130.77 229.73 121873 238.82 0.00 0.00 0.00 0.00 <<<--- High major faults 06:00:08 PM 0.05 361.87 303.89 0.00 367.08 0.00 0.00 0.00 0.00 4. Check the kbdirty value in below command output. => kbdirty: Amount of memory in kilobytes waiting to get written back to the disk. If this value is very high, It means either there is something wrong with pdflush/flushd service or CPU is unable to write changed pages(dirty pages) back to disk with expected speed. Now you will have to check the disk performance which we will discuss in detail in our next topic. 05:20:10 PM kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty 05:30:10 PM 21267192 29123732 3075004 9.35 716156 7213396 9264428 26.60 4503104 6166304 252 05:40:10 PM 21286860 29144116 3054432 9.29 716524 7213628 9359668 26.87 4481716 6173528 772 05:50:10 PM 21279032 29136796 3061676 9.31 716684 7213984 9265528 26.60 4506340 6178268 436 06:00:08 PM 21266616 29125040 3073608 9.35 717036 7214472 9254568 26.57 4508936 6169368 1368 5. Tune vm.swappiness kernel parameter: Depending on your System's usage and high swap-in and swap-outs on your system, you can lower the value of vm.swappiness kernel parameter accordingly. For high Memory centric applications like Databases, Please try to keep the vm.swappiness as low as possible, somewhere between 5 to 10. $ cat /proc/sys/vm/swappiness 60 => open /etc/sysctl.conf as root. Then, change or add this line to the file: vm.swappiness = 10 => for changing the swappiness value temporarily try this command: $ echo 50 > /proc/sys/vm/swappiness A value of 0 does not prevent swap. In fact, the kernel will initiate swap when the amount of free and file-backed pages is less than the high water mark in a zone irrespective of the swappiness value. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Memory issue related Investigation:"},{"location":"nightwolf-cotribution/performance/#troubleshooting-linux-performance-issue-happening-due-to-high-disk-io-usage","text":"","title":"Troubleshooting Linux performance issue happening due to high Disk IO Usage"},{"location":"nightwolf-cotribution/performance/#troubleshooting-linux-os-network-performance-issues","text":"If you have any feedback or suggestion for the content of this website, please use the feedbacks and suggestions form for the same. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Troubleshooting Linux OS network performance Issues"},{"location":"nightwolf-cotribution/shell_scripting_interview_questions/","text":"Shell Scripting Interview Questions \uf0c1 We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Shell Scripting Interview.. This list includes Google shell scripting interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Write a shell script to fetch the data from rest api and take out the required fileds Write a shell script to extract the number of alphabets characters and digits and tell the count. Write a shell script to extract the ip addresss from a file and the count of similar ipadress. Write a shell script to hit the rest api and modify the json data and put it back to the database. How to declare an array in shell script ? What is crontab and tell some time set you have done? Why do we give #!/bin/sh at start of shell script ? Write a shell script to find old logs and archive it. What is $#,$?,$@ etc.. in linux shell script ? What type of data redirections exists in shell scripting ? What is difference between > /dev/null , 1> /dev/null , 2> /dev/null and &> /dev/null/. Write a script to list top 10 most used words in a file. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Shell-Scripting Interview Questions"},{"location":"nightwolf-cotribution/shell_scripting_interview_questions/#shell-scripting-interview-questions","text":"We have prepared a set of questions to help Freshers and Experienced Linux Admins in their preparations for Shell Scripting Interview.. This list includes Google shell scripting interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); Write a shell script to fetch the data from rest api and take out the required fileds Write a shell script to extract the number of alphabets characters and digits and tell the count. Write a shell script to extract the ip addresss from a file and the count of similar ipadress. Write a shell script to hit the rest api and modify the json data and put it back to the database. How to declare an array in shell script ? What is crontab and tell some time set you have done? Why do we give #!/bin/sh at start of shell script ? Write a shell script to find old logs and archive it. What is $#,$?,$@ etc.. in linux shell script ? What type of data redirections exists in shell scripting ? What is difference between > /dev/null , 1> /dev/null , 2> /dev/null and &> /dev/null/. Write a script to list top 10 most used words in a file. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Shell Scripting Interview Questions"},{"location":"nightwolf-cotribution/terraform_interview_question-2/","text":"Top 250 Questions and Answers For Terraform Associate Certification - 2 \uf0c1 We have consolidated a list of frequently asked questions in Terraform AssociateCertification. Same list consists of frequently asked Terraform interview questions. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is command fmt? The terraform \"fmt\" command is used to rewrite Terraform configuration files to a canonical format and style. This command applies a subset of the Terraform language style conventions, along with other minor adjustments for readability. What is the recommended approach after upgrading terraform? The canonical format may change in minor ways between Terraform versions, so after upgrading Terraform we recommend to proactively run terraform \"fmt\" on your modules along with any other changes you are making to adopt the new version. What is the command usage? terraform fmt [options] [DIR] By default, fmt scans the current directory for configuration files. Is this true? True By default, \"fmt\" scans the current directory for configuration files. If the dir argument is provided then it will scan that given directory instead. If dir is a single dash (-) then fmt will read from standard input (STDIN). You are formatting the configuration files and what is the flag you should use tosee the differences? terraform fmt -diff You are formatting the configuration files and what is the flag you should use toprocess the subdirectories as well? terraform fmt -recursive You are formatting configuration files in a lot of directories and you don\u2019t wantto see the list of file changes. What is the flag that you should use? terraform fmt -list=false What is the command taint? The terraform \"taint\" command manually marks a Terraform-managed resource as tainted, forcing it to be destroyed and recreated on the next apply. This command will not modify infrastructure, but does modify the state file in order to mark a resource as tainted. Once a resource is marked as tainted, the next plan will show that the resource will be destroyed and recreated and the next apply will implement this change. What is the \"taint\" command usage? terraform taint [options] address The address argument is the address of the resource to mark as tainted. The address is in the resource address syntax. When you are tainting a resource terraform reads the default state fileterraform.tfstate. What is the flag you should use to read from a different path? terraform taint -state=path Give an example of tainting a single resource? terraform taint aws_security_group.allow_all The resource aws_security_group.allow_all in the module root has been marked as tainted. Give an example of tainting a resource within a module? terraform taint \"module.couchbase.aws_instance.cb_node[9]\" Resource instance module.couchbase.aws_instance.cb_node[9] has been marked as tainted. What is the command import? The terraform import command is used to import existing resources into Terraform. Terraform is able to import existing infrastructure. This allows you take resources you have created by some other means and bring it under Terraform management. This is a great way to slowly transition infrastructure to Terraform, or to be able to be confident that you can use Terraform in the future if it potentially doesn't support every feature you need today. What is the command import usage? terraform import [options] ADDRESS ID What is the default workspace name? default What are workspaces? Each Terraform configuration has an associated backend that defines how operations are executed and where persistent data such as the Terraform state are stored. The persistent data stored in the backend belongs to a workspace. Initially the backend has only one workspace, called \"default\", and thus there is only one Terraform state associated with that configuration. Certain backends support multiple named workspaces, allowing multiple states to be associated with a single configuration. What is the command to list the workspaces? terraform workspace list What is the command to create a new workspace? terraform workspace new <name> What is the command to show the current workspace? terraform workspace show What is the command to switch the workspace? terraform workspace select <workspace name> What is the command to delete the workspace? terraform workspace delete <workspace name> Can you delete the default workspace? No. You can't ever delete default workspace You are working on the different workspaces and you want to use a differentnumber of instances based on the workspace. How do you achieve that? resource \"aws_instance\" \"example\" { count = \"${terraform.workspace == \"default\" ? 5 : 1}\" # ... other arguments } You are working on the different workspaces and you want to use tags based onthe workspace. How do you achieve that? resource \"aws_instance\" \"example\" { tags = { Name = \"web - ${terraform.workspace}\" } # ... other arguments } You want to create a parallel, distinct copy of a set of infrastructure in order totest a set of changes before modifying the main production infrastructure. How doyou achieve that? Workspaces What is the command state? The terraform state command is used for advanced state management. As your Terraform usage becomes more advanced, there are some cases where you may need to modify the Terraform state. Rather than modify the state directly, the terraform state commands can be used in many cases instead. Reference: https://www.terraform.io/docs/commands/state/index.html What is the command usage? terraform state <subcommand> [options] [args] You are working on terraform files and you want to list all the resources. Whatis the command you should use? terraform state list How do you list the resources for the given name? terraform state list <resource name> What is the command that shows the attributes of a single resource in the statefile? terraform state show 'resource name' How do you do debugging terraform? Terraform has detailed logs which can be enabled by setting the \"TF_LOG\" environment variable to any value. This will cause detailed logs to appear on stderr. You can set \"TF_LOG\" to one of the log levels TRACE, DEBUG, INFO, WARN or ERROR to change the verbosity of the logs. TRACE is the most verbose and it is the default if TF_LOG is set to something other than a log level name. To persist logged output you can set TF_LOG_PATH in order to force the log to always be appended to a specific file when logging is enabled. Note that even when TF_LOG_PATH is set, TF_LOG must be set in order for any logging to be enabled. Reference: https://www.terraform.io/docs/internals/debugging.html If terraform crashes where should you see the logs? crash.log If Terraform ever crashes (a \"panic\" in the Go runtime), it saves a log file with the debug logs from the session as well as the panic message and backtrace to crash.log. Reference: https://www.terraform.io/docs/internals/debugging.html What is the first thing you should do when the terraform crashes? panic message The most interesting part of a crash log is the panic message itself and the backtrace immediately following. So the first thing to do is to search the file for panic. Reference: https://www.terraform.io/docs/internals/debugging.html You are building infrastructure for different environments for example testand dev. How do you maintain separate states? There are two primary methods to separate state between environments: directories workspaces What is the difference between directory-separated and workspace-separatedenvironments? Directory separated environments rely on duplicate Terraform code, which may be useful if your deployments need differ, for example to test infrastructure changes in development. But they can run the risk of creating drift between the environments over time. Workspace-separated environments use the same Terraform code but have different state files, which is useful if you want your environments to stay as similar to each other as possible, for example if you are providing development infrastructure to a team that wants to simulate running in production. What is the command to pull the remote state? terraform state pull This command will download the state from its current location and output the raw format to stdout. Reference: https://www.terraform.io/docs/commands/state/pull.html What is the command is used manually to upload a local state file to a remotestate terraform state push The \"terraform state push\" command is used to manually upload a local state file to remote state. This command also works with local state. Reference: https://www.terraform.io/docs/commands/state/push.html The command terraform taint modifies the state file and doesn\u2019t modify theinfrastructure. Is this true? True This command will not modify infrastructure, but does modify the state file in order to mark a resource as tainted. Once a resource is marked as tainted, the next plan will show that the resource will be destroyed and recreated and the next apply will implement this change. Your team has decided to use terraform in your company and you have existinginfrastructure. How do you migrate your existing resources to terraform and startusing it? You should use terraform import and modify the infrastrcuture in the terraform files and do the terraform workflow (init, plan, apply). When you are working with the workspaces how do you access the currentworkspace in the configuration files? ${terraform.workspace} When you are using workspaces where does the Terraform save the state file forthe local state? terraform.tfstate.d For local state, Terraform stores the workspace states in a directory called \"terraform.tfstate.d\". When you are using workspaces where does the Terraform save the state file forthe remote state? For remote state, the workspaces are stored directly in the configured backend. How do you remove items from the Terraform state? terraform state rm 'packet_device.worker' The \"terraform state rm\" command is used to remove items from the Terraform state. This command can remove single resources, single instances of a resource, entire modules, and more. Reference: https://www.terraform.io/docs/commands/state/rm.html How do you move the state from one source to another? terraform state mv 'module.app' 'module.parent.module.app' The \"terraform state mv\" command is used to move items in a Terraform state. This command can move single resources, single instances of a resource, entire modules, and more. This command can also move items to a completely different state file, enabling efficient refactoring. Reference: https://www.terraform.io/docs/commands/state/mv.html How do you rename a resource in the terraform state file? terraform state mv 'packet_device.worker' 'packet_device.helper' The above example renames the packet_device resource named worker to helper. Where do you find and explore terraform Modules? The Terraform Registry makes it simple to find and use modules. The search query will look at module name, provider, and description to match your search terms. On the results page, filters can be used further refine search results. How do you make sure that modules have stability and compatibility? By default, only verified modules are shown in search results. By using the filters, you can view unverified modules as well. How do you download any modules? You need to add any module in the configuration file like below module \"consul\" { source = \"hashicorp/consul/aws\" version = \"0.1.0\" } terraform init command will download and cache any modules referenced by a configuration. What is the syntax for referencing a registry module? <NAMESPACE>/<NAME>/<PROVIDER> // for example module \"consul\" { source = \"hashicorp/consul/aws\" version = \"0.1.0\" } What is the syntax for referencing a private registry module? <HOSTNAME>/<NAMESPACE>/<NAME>/<PROVIDER> // for example module \"vpc\" { source = \"app.terraform.io/example_corp/vpc/aws\" version = \"0.9.3\" } The terraform recommends that all modules must follow semantic versioning.Is this true? True What is a Terraform Module? A Terraform module is a set of Terraform configuration files in a single directory. Even a simple configuration consisting of a single directory with one or more \".tf\" files is a module. Why do we use modules for? * Organize configuration * Encapsulate configuration * Re-use configuration * Provide consistency and ensure best practices How do you call modules in your configuration? Your configuration can use module blocks to call modules in other directories. When Terraform encounters a module block, it loads and processes that module's configuration files. How many ways you can load modules? Local and remote modules Modules can either be loaded from the local filesystem, or a remote source. Terraform supports a variety of remote sources, including the Terraform Registry, most version control systems, HTTP URLs, and Terraform Cloud or Terraform Enterprise private module registries. What are the best practices for using Modules? 1. Start writing your configuration with modules in mind. Even for modestly complex Terraform configurations managed by a single person, you' willll find the benefits of using modules outweigh the time it takes to use them properly. 2. Use local modules to organize and encapsulate your code. Even if you are not using or publishing remote modules, organizing your configuration in terms of modules from the beginning will significantlty reduce the burden of maintaining and updating your configuration as your infrastructure grows in complexity. 3. Use the public Terraform Registry to find useful modules. This way you can more quickly and confidently implement your configuration by relying on the work of others to implement common infrastructure scenarios. 4. Publish and share modules with your team. Most infrastructure is managed by a team of people, and modules are important way that teams can work together to create and maintain infrastructure. As mentioned earlier, you can publish modules either publicly or privately. We will see how to do this in a future guide in this series. What are the different source types for calling modules? Local paths Terraform Registry GitHub Generic Git, Mercurial repositories Bitbucket HTTP URLs S3 buckets GCS buckets Reference: https://www.terraform.io/docs/modules/sources.html What are the arguments you need for using modules in your configuration? source and version // example module \"consul\" { source = \"hashicorp/consul/aws\" version = \"0.1.0\" } How do you set input variables for the modules? The configuration that calls a module is responsible for setting its input values, which are passed as arguments in the module block. Aside from source and version , most of the arguments to a module block will set variable values. On the Terraform registry page for the AWS VPC module, you will see an Inputs tab that describes all of the input variables that module supports. For example, we have defined a lot of input variables for the modules such as ads, cidr,name, etc. provider \"aws\" { region = \"us-west-2\" } module \"vpc\" { source = \"terraform-aws-modules/vpc/aws\" version = \"2.21.0\" name = var.vpc_name cidr = var.vpc_cidr azs = var.vpc_azs private_subnets = var.vpc_private_subnets public_subnets = var.vpc_public_subnets enable_nat_gateway = var.vpc_enable_nat_gateway tags = var.vpc_tags } module \"ec2_instances\" { source = \"terraform-aws-modules/ec2-instance/aws\" version = \"2.12.0\" name = \"my-ec2-cluster\" instance_count = 2 ami = \"ami-0c5204531f799e0c6\" instance_type = \"t2.micro\" vpc_security_group_ids = [module.vpc.default_security_group_id] subnet_id = module.vpc.public_subnets[0] tags = { Terraform = \"true\" Environment = \"dev\" } } How do you access output variables from the modules? You can access them by referring module.<MODULE NAME>.<OUTPUT NAME> Where do you put output variables in the configuration? Module outputs are usually either passed to other parts of your configuration, or defined as outputs in your root module. You will see both uses in this guide. Inside your configuration's directory, outputs.tf will need to contain: output \"vpc_public_subnets\" { description = \"IDs of the VPC's public subnets\" value = module.vpc.public_subnets } output \"ec2_instance_public_ips\" { description = \"Public IP addresses of EC2 instances\" value = module.ec2_instances.public_ip } How do you pass input variables in the configuration? You can define variables.tf in the root folder variable \"vpc_name\" { description = \"Name of VPC\" type = string default = \"example-vpc\" } Then you can access these varibles in the configuration like this module \"vpc\" { source = \"terraform-aws-modules/vpc/aws\" version = \"2.21.0\" name = var.vpc_name cidr = var.vpc_cidr azs = var.vpc_azs private_subnets = var.vpc_private_subnets public_subnets = var.vpc_public_subnets enable_nat_gateway = var.vpc_enable_nat_gateway tags = var.vpc_tags } What is the child module? A module that is called by another configuration is sometimes referred to as a \"child module\" of that configuration. When you use local modules you don\u2019t have to do the command init or get everytime there is a change in the local module. why? When installing a local module, Terraform will instead refer directly to the source directory. Because of this, Terraform will automatically notice changes to local modules without having to re-run terraform init or terraform get. When you use remote modules what should you do if there is a change in themodule? When installing a remote module, Terraform will download it into the \".terraform\" directory in your configuration's root directory. You should initialize with terraform init. A simple configuration consisting of a single directory with one or more .tf filesis a module. Is this true? True When using a new module for the first time, you must run either terraforminit or terraform get to install the module. Is this true? True When installing the modules and where does the terraform save these modules? .terraform/modules // Example .terraform/modules \u251c\u2500\u2500 ec2_instances \u2502 \u2514\u2500\u2500 terraform-aws-modules-terraform-aws-ec2-instance-ed6dcd9 \u251c\u2500\u2500 modules.json \u2514\u2500\u2500 vpc \u2514\u2500\u2500 terraform-aws-modules-terraform-aws-vpc-2417f60 What is the required argument for the module? source All modules require a source argument, which is a meta-argument defined by Terraform CLI. Its value is either the path to a local directory of the module's configuration files, or a remote module source that Terraform should download and use. This value must be a literal string with no template sequences; arbitrary expressions are not allowed. For more information on possible values for this argument, see Module Sources. What are the other optional meta-arguments along with the source whendefining modules version - (Optional) A version constraint string that specifies which versions of the referenced module are acceptable. The newest version matching the constraint will be used. version is supported only for modules retrieved from module registries. providers - (Optional) A map whose keys are provider configuration names that are expected by child module and whose values are corresponding provider names in the calling module. This allows provider configurations to be passed explicitly to child modules. If not specified, the child module inherits all of the default (un-aliased) provider configurations from the calling module. What is the Core Terraform workflow? The core Terraform workflow has three steps: 1. Write - Author infrastructure as code. 2. Plan - Preview changes before applying. 3. Apply - Provision reproducible infrastructure. What is the workflow when you work as an Individual Practitioner? Please check the below URL for detailed workflow: https://www.terraform.io/intro/core-workflow#working-as-an-individual-practitioner What is the workflow when you work as a team? Please check the below URL for detailed workflow: https://www.terraform.io/intro/core-workflow#working-as-a-team What is the workflow when you work as a large organization? Please check the below URL for detailed workflow: https://www.terraform.io/intro/core-workflow#the-core-workflow-enhanced-by-terraform-cloud What is the command init? The \"terraform init\" command is used to initialize a working directory containing Terraform configuration files. This is the first command that should be run after writing a new Terraform configuration or cloning an existing one from version control. It is safe to run this command multiple times. You recently joined a team and you cloned a terraform configuration files fromthe version control system. What is the first command you should use? terraform init This command performs several different initialization steps in order to prepare a working directory for use. This command is always safe to run multiple times, to bring the working directory up to date with changes in the configuration. Though subsequent runs may give errors, this command will never delete your existing configuration or state. If no arguments are given, the configuration in the current working directory is initialized. It is recommended to run Terraform with the current working directory set to the root directory of the configuration, and omit the DIR argument. Ref: https://www.terraform.io/docs/commands/init.html What is the flag you should use to upgrade modules and plugins a part of theirrespective installation steps? upgrade terraform init -upgrade When you are doing initialization with terraform init, you want to skipbackend initialization. What should you do? terraform init -backend=false When you are doing initialization with terraform init, you want to skip childmodule installation. What should you do? terraform init -get=false When you are doing initialization where do all the plugins stored? On Unix Based operationg systems : ~/.terraform.d/plugins on Windows : %APPDATA%\\terraform.d\\plugins When you are doing initialization with terraform init, you want to skip plugininstallation. What should you do? terraform init -get-plugins=false Skips plugin installation. Terraform will use plugins installed in the user plugins directory, and any plugins already installed for the current working directory. If the installed plugins aren't sufficient for the configuration, init fails. What does the command terraform validate does? The \"terraform validate\" command validates the configuration files in a directory, referring only to the configuration and not accessing any remote services such as remote state, provider APIs, etc. Validate runs checks that verify whether a configuration is syntactically valid and internally consistent, regardless of any provided variables or existing state. It is thus primarily useful for general verification of reusable modules, including correctness of attribute names and value types. Ref: https://www.terraform.io/docs/commands/validate.html What does the command plan do? The \"terraform plan\" command is used to create an execution plan. Terraform performs a refresh, unless explicitly disabled, and then determines what actions are necessary to achieve the desired state specified in the configuration files. What does the command apply do? The \"terraform apply\" command is used to apply the changes required to reach the desired state of the configuration, or the pre-determined set of actions generated by a terraform plan execution plan. Ref: https://www.terraform.io/docs/commands/apply.html You are applying the infrastructure with the command apply and you don\u2019twant to do interactive approval. Which flag should you use? terraform apply -auto-approve Ref: https://www.terraform.io/docs/commands/apply.html What does the command destroy do? The \"terraform destroy\" command is used to destroy the Terraform-managed infrastructure. How do you preview the behavior of the command terraform destroy? terraform plan -destroy What are implicit and explicit dependencies? Implicit dependency: By studying the resource attributes used in interpolation expressions, Terraform can automatically infer when one resource depends on another. Terraform uses this dependency information to determine the correct order in which to create the different resources. Implicit dependencies via interpolation expressions are the primary way to inform Terraform about these relationships, and should be used whenever possible. Explicit dependency: Sometimes there are dependencies between resources that are not visible to Terraform. The depends_on argument is accepted by any resource and accepts a list of resources to create explicit dependencies for. Give an example of implicit dependency? In the example below, the reference to aws_instance.example.id creates an implicit dependency on the aws_instance named example. provider \"aws\" { profile = \"default\" region = \"us-east-1\" } resource \"aws_instance\" \"example\" { ami = \"ami-b374d5a5\" instance_type = \"t2.micro\" } resource \"aws_eip\" \"ip\" { vpc = true instance = aws_instance.example.id } Give an example of explicit dependency? In the example below, an application we will run on our EC2 instance expects to use a specific Amazon S3 bucket, but that dependency is configured inside the application code and thus not visible to Terraform. In that case, we can use depends_on to explicitly declare the dependency. resource \"aws_s3_bucket\" \"example\" { bucket = \"some_bucket\" acl = \"private\" } resource \"aws_instance\" \"example\" { ami = \"ami-2757f631\" instance_type = \"t2.micro\" depends_on = [aws_s3_bucket.example] } How do you save the execution plan? terraform plan -out=tfplan you can use that file with apply: terraform apply tfplan You have started writing terraform configuration and you are using somesample configuration as a basis. How do you copy the example configuration intoyour working directory? terraform init -from-module=MODULE-SOURCE Ref: https://www.terraform.io/docs/commands/init.html#copy-a-source-module What is the flag you should use with the terraform plan to get detailed on theexit codes? terraform plan -detailed-exitcode Return a detailed exit code when the command exits. When provided, this argument changes the exit codes and their meanings to provide more granular information about what the resulting plan contains: * 0 = Succeeded with empty diff (no changes) * 1 = Error * 2 = Succeeded with non-empty diff (changes present) How do you target only specific resources when you run a terraform plan? -target=resource - A Resource Address to target. This flag can be used multiple times. How do you update the state prior to checking differences when you run aterraform plan? terraform plan -refresh=true The behavior of any terraform destroy command can be previewed at any timewith an equivalent terraform plan -destroy command. Is this true? True You have the following file and created two resources docker_image anddocker_container with the command terraform apply and you go to the terminaland delete the container with the command docker rm. You come back to yourconfiguration and run the command again. Does terraform recreates the resource? resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image.nginx.latest name = \"nginxtutorial\" ports { internal = 80 external = 8080 } upload { source = \"${abspath(path.root)}/files/index.html\" file = \"/usr/share/nginx/html/index.html\" } } Yes. Terrsform creates the resource again since the execution plan says two resources and the terraform always maintains the desired state. You created a VM instance on AWS cloud provider with the terraformconfiguration and you log in AWS console and removed the instance. What does thenext apply do? It creates the instance again. You have the following file and created two resources docker_image anddocker_container with the command terraform plan and you go to the terminal anddelete the container with the command docker rm. You come back to yourconfiguration and run the command again. What is the output of the commandplan? resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image.nginx.latest name = \"nginxtutorial\" ports { internal = 80 external = 8080 } upload { source = \"${abspath(path.root)}/files/index.html\" file = \"/usr/share/nginx/html/index.html\" } } Terraform will perform the following actions: # docker_container.nginx will be created Plan: 1 to add, 0 to change, 0 to destroy. Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Terraform Interview Questions and Answers- 2nd"},{"location":"nightwolf-cotribution/terraform_interview_question-2/#top-250-questions-and-answers-for-terraform-associate-certification-2","text":"We have consolidated a list of frequently asked questions in Terraform AssociateCertification. Same list consists of frequently asked Terraform interview questions. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is command fmt? The terraform \"fmt\" command is used to rewrite Terraform configuration files to a canonical format and style. This command applies a subset of the Terraform language style conventions, along with other minor adjustments for readability. What is the recommended approach after upgrading terraform? The canonical format may change in minor ways between Terraform versions, so after upgrading Terraform we recommend to proactively run terraform \"fmt\" on your modules along with any other changes you are making to adopt the new version. What is the command usage? terraform fmt [options] [DIR] By default, fmt scans the current directory for configuration files. Is this true? True By default, \"fmt\" scans the current directory for configuration files. If the dir argument is provided then it will scan that given directory instead. If dir is a single dash (-) then fmt will read from standard input (STDIN). You are formatting the configuration files and what is the flag you should use tosee the differences? terraform fmt -diff You are formatting the configuration files and what is the flag you should use toprocess the subdirectories as well? terraform fmt -recursive You are formatting configuration files in a lot of directories and you don\u2019t wantto see the list of file changes. What is the flag that you should use? terraform fmt -list=false What is the command taint? The terraform \"taint\" command manually marks a Terraform-managed resource as tainted, forcing it to be destroyed and recreated on the next apply. This command will not modify infrastructure, but does modify the state file in order to mark a resource as tainted. Once a resource is marked as tainted, the next plan will show that the resource will be destroyed and recreated and the next apply will implement this change. What is the \"taint\" command usage? terraform taint [options] address The address argument is the address of the resource to mark as tainted. The address is in the resource address syntax. When you are tainting a resource terraform reads the default state fileterraform.tfstate. What is the flag you should use to read from a different path? terraform taint -state=path Give an example of tainting a single resource? terraform taint aws_security_group.allow_all The resource aws_security_group.allow_all in the module root has been marked as tainted. Give an example of tainting a resource within a module? terraform taint \"module.couchbase.aws_instance.cb_node[9]\" Resource instance module.couchbase.aws_instance.cb_node[9] has been marked as tainted. What is the command import? The terraform import command is used to import existing resources into Terraform. Terraform is able to import existing infrastructure. This allows you take resources you have created by some other means and bring it under Terraform management. This is a great way to slowly transition infrastructure to Terraform, or to be able to be confident that you can use Terraform in the future if it potentially doesn't support every feature you need today. What is the command import usage? terraform import [options] ADDRESS ID What is the default workspace name? default What are workspaces? Each Terraform configuration has an associated backend that defines how operations are executed and where persistent data such as the Terraform state are stored. The persistent data stored in the backend belongs to a workspace. Initially the backend has only one workspace, called \"default\", and thus there is only one Terraform state associated with that configuration. Certain backends support multiple named workspaces, allowing multiple states to be associated with a single configuration. What is the command to list the workspaces? terraform workspace list What is the command to create a new workspace? terraform workspace new <name> What is the command to show the current workspace? terraform workspace show What is the command to switch the workspace? terraform workspace select <workspace name> What is the command to delete the workspace? terraform workspace delete <workspace name> Can you delete the default workspace? No. You can't ever delete default workspace You are working on the different workspaces and you want to use a differentnumber of instances based on the workspace. How do you achieve that? resource \"aws_instance\" \"example\" { count = \"${terraform.workspace == \"default\" ? 5 : 1}\" # ... other arguments } You are working on the different workspaces and you want to use tags based onthe workspace. How do you achieve that? resource \"aws_instance\" \"example\" { tags = { Name = \"web - ${terraform.workspace}\" } # ... other arguments } You want to create a parallel, distinct copy of a set of infrastructure in order totest a set of changes before modifying the main production infrastructure. How doyou achieve that? Workspaces What is the command state? The terraform state command is used for advanced state management. As your Terraform usage becomes more advanced, there are some cases where you may need to modify the Terraform state. Rather than modify the state directly, the terraform state commands can be used in many cases instead. Reference: https://www.terraform.io/docs/commands/state/index.html What is the command usage? terraform state <subcommand> [options] [args] You are working on terraform files and you want to list all the resources. Whatis the command you should use? terraform state list How do you list the resources for the given name? terraform state list <resource name> What is the command that shows the attributes of a single resource in the statefile? terraform state show 'resource name' How do you do debugging terraform? Terraform has detailed logs which can be enabled by setting the \"TF_LOG\" environment variable to any value. This will cause detailed logs to appear on stderr. You can set \"TF_LOG\" to one of the log levels TRACE, DEBUG, INFO, WARN or ERROR to change the verbosity of the logs. TRACE is the most verbose and it is the default if TF_LOG is set to something other than a log level name. To persist logged output you can set TF_LOG_PATH in order to force the log to always be appended to a specific file when logging is enabled. Note that even when TF_LOG_PATH is set, TF_LOG must be set in order for any logging to be enabled. Reference: https://www.terraform.io/docs/internals/debugging.html If terraform crashes where should you see the logs? crash.log If Terraform ever crashes (a \"panic\" in the Go runtime), it saves a log file with the debug logs from the session as well as the panic message and backtrace to crash.log. Reference: https://www.terraform.io/docs/internals/debugging.html What is the first thing you should do when the terraform crashes? panic message The most interesting part of a crash log is the panic message itself and the backtrace immediately following. So the first thing to do is to search the file for panic. Reference: https://www.terraform.io/docs/internals/debugging.html You are building infrastructure for different environments for example testand dev. How do you maintain separate states? There are two primary methods to separate state between environments: directories workspaces What is the difference between directory-separated and workspace-separatedenvironments? Directory separated environments rely on duplicate Terraform code, which may be useful if your deployments need differ, for example to test infrastructure changes in development. But they can run the risk of creating drift between the environments over time. Workspace-separated environments use the same Terraform code but have different state files, which is useful if you want your environments to stay as similar to each other as possible, for example if you are providing development infrastructure to a team that wants to simulate running in production. What is the command to pull the remote state? terraform state pull This command will download the state from its current location and output the raw format to stdout. Reference: https://www.terraform.io/docs/commands/state/pull.html What is the command is used manually to upload a local state file to a remotestate terraform state push The \"terraform state push\" command is used to manually upload a local state file to remote state. This command also works with local state. Reference: https://www.terraform.io/docs/commands/state/push.html The command terraform taint modifies the state file and doesn\u2019t modify theinfrastructure. Is this true? True This command will not modify infrastructure, but does modify the state file in order to mark a resource as tainted. Once a resource is marked as tainted, the next plan will show that the resource will be destroyed and recreated and the next apply will implement this change. Your team has decided to use terraform in your company and you have existinginfrastructure. How do you migrate your existing resources to terraform and startusing it? You should use terraform import and modify the infrastrcuture in the terraform files and do the terraform workflow (init, plan, apply). When you are working with the workspaces how do you access the currentworkspace in the configuration files? ${terraform.workspace} When you are using workspaces where does the Terraform save the state file forthe local state? terraform.tfstate.d For local state, Terraform stores the workspace states in a directory called \"terraform.tfstate.d\". When you are using workspaces where does the Terraform save the state file forthe remote state? For remote state, the workspaces are stored directly in the configured backend. How do you remove items from the Terraform state? terraform state rm 'packet_device.worker' The \"terraform state rm\" command is used to remove items from the Terraform state. This command can remove single resources, single instances of a resource, entire modules, and more. Reference: https://www.terraform.io/docs/commands/state/rm.html How do you move the state from one source to another? terraform state mv 'module.app' 'module.parent.module.app' The \"terraform state mv\" command is used to move items in a Terraform state. This command can move single resources, single instances of a resource, entire modules, and more. This command can also move items to a completely different state file, enabling efficient refactoring. Reference: https://www.terraform.io/docs/commands/state/mv.html How do you rename a resource in the terraform state file? terraform state mv 'packet_device.worker' 'packet_device.helper' The above example renames the packet_device resource named worker to helper. Where do you find and explore terraform Modules? The Terraform Registry makes it simple to find and use modules. The search query will look at module name, provider, and description to match your search terms. On the results page, filters can be used further refine search results. How do you make sure that modules have stability and compatibility? By default, only verified modules are shown in search results. By using the filters, you can view unverified modules as well. How do you download any modules? You need to add any module in the configuration file like below module \"consul\" { source = \"hashicorp/consul/aws\" version = \"0.1.0\" } terraform init command will download and cache any modules referenced by a configuration. What is the syntax for referencing a registry module? <NAMESPACE>/<NAME>/<PROVIDER> // for example module \"consul\" { source = \"hashicorp/consul/aws\" version = \"0.1.0\" } What is the syntax for referencing a private registry module? <HOSTNAME>/<NAMESPACE>/<NAME>/<PROVIDER> // for example module \"vpc\" { source = \"app.terraform.io/example_corp/vpc/aws\" version = \"0.9.3\" } The terraform recommends that all modules must follow semantic versioning.Is this true? True What is a Terraform Module? A Terraform module is a set of Terraform configuration files in a single directory. Even a simple configuration consisting of a single directory with one or more \".tf\" files is a module. Why do we use modules for? * Organize configuration * Encapsulate configuration * Re-use configuration * Provide consistency and ensure best practices How do you call modules in your configuration? Your configuration can use module blocks to call modules in other directories. When Terraform encounters a module block, it loads and processes that module's configuration files. How many ways you can load modules? Local and remote modules Modules can either be loaded from the local filesystem, or a remote source. Terraform supports a variety of remote sources, including the Terraform Registry, most version control systems, HTTP URLs, and Terraform Cloud or Terraform Enterprise private module registries. What are the best practices for using Modules? 1. Start writing your configuration with modules in mind. Even for modestly complex Terraform configurations managed by a single person, you' willll find the benefits of using modules outweigh the time it takes to use them properly. 2. Use local modules to organize and encapsulate your code. Even if you are not using or publishing remote modules, organizing your configuration in terms of modules from the beginning will significantlty reduce the burden of maintaining and updating your configuration as your infrastructure grows in complexity. 3. Use the public Terraform Registry to find useful modules. This way you can more quickly and confidently implement your configuration by relying on the work of others to implement common infrastructure scenarios. 4. Publish and share modules with your team. Most infrastructure is managed by a team of people, and modules are important way that teams can work together to create and maintain infrastructure. As mentioned earlier, you can publish modules either publicly or privately. We will see how to do this in a future guide in this series. What are the different source types for calling modules? Local paths Terraform Registry GitHub Generic Git, Mercurial repositories Bitbucket HTTP URLs S3 buckets GCS buckets Reference: https://www.terraform.io/docs/modules/sources.html What are the arguments you need for using modules in your configuration? source and version // example module \"consul\" { source = \"hashicorp/consul/aws\" version = \"0.1.0\" } How do you set input variables for the modules? The configuration that calls a module is responsible for setting its input values, which are passed as arguments in the module block. Aside from source and version , most of the arguments to a module block will set variable values. On the Terraform registry page for the AWS VPC module, you will see an Inputs tab that describes all of the input variables that module supports. For example, we have defined a lot of input variables for the modules such as ads, cidr,name, etc. provider \"aws\" { region = \"us-west-2\" } module \"vpc\" { source = \"terraform-aws-modules/vpc/aws\" version = \"2.21.0\" name = var.vpc_name cidr = var.vpc_cidr azs = var.vpc_azs private_subnets = var.vpc_private_subnets public_subnets = var.vpc_public_subnets enable_nat_gateway = var.vpc_enable_nat_gateway tags = var.vpc_tags } module \"ec2_instances\" { source = \"terraform-aws-modules/ec2-instance/aws\" version = \"2.12.0\" name = \"my-ec2-cluster\" instance_count = 2 ami = \"ami-0c5204531f799e0c6\" instance_type = \"t2.micro\" vpc_security_group_ids = [module.vpc.default_security_group_id] subnet_id = module.vpc.public_subnets[0] tags = { Terraform = \"true\" Environment = \"dev\" } } How do you access output variables from the modules? You can access them by referring module.<MODULE NAME>.<OUTPUT NAME> Where do you put output variables in the configuration? Module outputs are usually either passed to other parts of your configuration, or defined as outputs in your root module. You will see both uses in this guide. Inside your configuration's directory, outputs.tf will need to contain: output \"vpc_public_subnets\" { description = \"IDs of the VPC's public subnets\" value = module.vpc.public_subnets } output \"ec2_instance_public_ips\" { description = \"Public IP addresses of EC2 instances\" value = module.ec2_instances.public_ip } How do you pass input variables in the configuration? You can define variables.tf in the root folder variable \"vpc_name\" { description = \"Name of VPC\" type = string default = \"example-vpc\" } Then you can access these varibles in the configuration like this module \"vpc\" { source = \"terraform-aws-modules/vpc/aws\" version = \"2.21.0\" name = var.vpc_name cidr = var.vpc_cidr azs = var.vpc_azs private_subnets = var.vpc_private_subnets public_subnets = var.vpc_public_subnets enable_nat_gateway = var.vpc_enable_nat_gateway tags = var.vpc_tags } What is the child module? A module that is called by another configuration is sometimes referred to as a \"child module\" of that configuration. When you use local modules you don\u2019t have to do the command init or get everytime there is a change in the local module. why? When installing a local module, Terraform will instead refer directly to the source directory. Because of this, Terraform will automatically notice changes to local modules without having to re-run terraform init or terraform get. When you use remote modules what should you do if there is a change in themodule? When installing a remote module, Terraform will download it into the \".terraform\" directory in your configuration's root directory. You should initialize with terraform init. A simple configuration consisting of a single directory with one or more .tf filesis a module. Is this true? True When using a new module for the first time, you must run either terraforminit or terraform get to install the module. Is this true? True When installing the modules and where does the terraform save these modules? .terraform/modules // Example .terraform/modules \u251c\u2500\u2500 ec2_instances \u2502 \u2514\u2500\u2500 terraform-aws-modules-terraform-aws-ec2-instance-ed6dcd9 \u251c\u2500\u2500 modules.json \u2514\u2500\u2500 vpc \u2514\u2500\u2500 terraform-aws-modules-terraform-aws-vpc-2417f60 What is the required argument for the module? source All modules require a source argument, which is a meta-argument defined by Terraform CLI. Its value is either the path to a local directory of the module's configuration files, or a remote module source that Terraform should download and use. This value must be a literal string with no template sequences; arbitrary expressions are not allowed. For more information on possible values for this argument, see Module Sources. What are the other optional meta-arguments along with the source whendefining modules version - (Optional) A version constraint string that specifies which versions of the referenced module are acceptable. The newest version matching the constraint will be used. version is supported only for modules retrieved from module registries. providers - (Optional) A map whose keys are provider configuration names that are expected by child module and whose values are corresponding provider names in the calling module. This allows provider configurations to be passed explicitly to child modules. If not specified, the child module inherits all of the default (un-aliased) provider configurations from the calling module. What is the Core Terraform workflow? The core Terraform workflow has three steps: 1. Write - Author infrastructure as code. 2. Plan - Preview changes before applying. 3. Apply - Provision reproducible infrastructure. What is the workflow when you work as an Individual Practitioner? Please check the below URL for detailed workflow: https://www.terraform.io/intro/core-workflow#working-as-an-individual-practitioner What is the workflow when you work as a team? Please check the below URL for detailed workflow: https://www.terraform.io/intro/core-workflow#working-as-a-team What is the workflow when you work as a large organization? Please check the below URL for detailed workflow: https://www.terraform.io/intro/core-workflow#the-core-workflow-enhanced-by-terraform-cloud What is the command init? The \"terraform init\" command is used to initialize a working directory containing Terraform configuration files. This is the first command that should be run after writing a new Terraform configuration or cloning an existing one from version control. It is safe to run this command multiple times. You recently joined a team and you cloned a terraform configuration files fromthe version control system. What is the first command you should use? terraform init This command performs several different initialization steps in order to prepare a working directory for use. This command is always safe to run multiple times, to bring the working directory up to date with changes in the configuration. Though subsequent runs may give errors, this command will never delete your existing configuration or state. If no arguments are given, the configuration in the current working directory is initialized. It is recommended to run Terraform with the current working directory set to the root directory of the configuration, and omit the DIR argument. Ref: https://www.terraform.io/docs/commands/init.html What is the flag you should use to upgrade modules and plugins a part of theirrespective installation steps? upgrade terraform init -upgrade When you are doing initialization with terraform init, you want to skipbackend initialization. What should you do? terraform init -backend=false When you are doing initialization with terraform init, you want to skip childmodule installation. What should you do? terraform init -get=false When you are doing initialization where do all the plugins stored? On Unix Based operationg systems : ~/.terraform.d/plugins on Windows : %APPDATA%\\terraform.d\\plugins When you are doing initialization with terraform init, you want to skip plugininstallation. What should you do? terraform init -get-plugins=false Skips plugin installation. Terraform will use plugins installed in the user plugins directory, and any plugins already installed for the current working directory. If the installed plugins aren't sufficient for the configuration, init fails. What does the command terraform validate does? The \"terraform validate\" command validates the configuration files in a directory, referring only to the configuration and not accessing any remote services such as remote state, provider APIs, etc. Validate runs checks that verify whether a configuration is syntactically valid and internally consistent, regardless of any provided variables or existing state. It is thus primarily useful for general verification of reusable modules, including correctness of attribute names and value types. Ref: https://www.terraform.io/docs/commands/validate.html What does the command plan do? The \"terraform plan\" command is used to create an execution plan. Terraform performs a refresh, unless explicitly disabled, and then determines what actions are necessary to achieve the desired state specified in the configuration files. What does the command apply do? The \"terraform apply\" command is used to apply the changes required to reach the desired state of the configuration, or the pre-determined set of actions generated by a terraform plan execution plan. Ref: https://www.terraform.io/docs/commands/apply.html You are applying the infrastructure with the command apply and you don\u2019twant to do interactive approval. Which flag should you use? terraform apply -auto-approve Ref: https://www.terraform.io/docs/commands/apply.html What does the command destroy do? The \"terraform destroy\" command is used to destroy the Terraform-managed infrastructure. How do you preview the behavior of the command terraform destroy? terraform plan -destroy What are implicit and explicit dependencies? Implicit dependency: By studying the resource attributes used in interpolation expressions, Terraform can automatically infer when one resource depends on another. Terraform uses this dependency information to determine the correct order in which to create the different resources. Implicit dependencies via interpolation expressions are the primary way to inform Terraform about these relationships, and should be used whenever possible. Explicit dependency: Sometimes there are dependencies between resources that are not visible to Terraform. The depends_on argument is accepted by any resource and accepts a list of resources to create explicit dependencies for. Give an example of implicit dependency? In the example below, the reference to aws_instance.example.id creates an implicit dependency on the aws_instance named example. provider \"aws\" { profile = \"default\" region = \"us-east-1\" } resource \"aws_instance\" \"example\" { ami = \"ami-b374d5a5\" instance_type = \"t2.micro\" } resource \"aws_eip\" \"ip\" { vpc = true instance = aws_instance.example.id } Give an example of explicit dependency? In the example below, an application we will run on our EC2 instance expects to use a specific Amazon S3 bucket, but that dependency is configured inside the application code and thus not visible to Terraform. In that case, we can use depends_on to explicitly declare the dependency. resource \"aws_s3_bucket\" \"example\" { bucket = \"some_bucket\" acl = \"private\" } resource \"aws_instance\" \"example\" { ami = \"ami-2757f631\" instance_type = \"t2.micro\" depends_on = [aws_s3_bucket.example] } How do you save the execution plan? terraform plan -out=tfplan you can use that file with apply: terraform apply tfplan You have started writing terraform configuration and you are using somesample configuration as a basis. How do you copy the example configuration intoyour working directory? terraform init -from-module=MODULE-SOURCE Ref: https://www.terraform.io/docs/commands/init.html#copy-a-source-module What is the flag you should use with the terraform plan to get detailed on theexit codes? terraform plan -detailed-exitcode Return a detailed exit code when the command exits. When provided, this argument changes the exit codes and their meanings to provide more granular information about what the resulting plan contains: * 0 = Succeeded with empty diff (no changes) * 1 = Error * 2 = Succeeded with non-empty diff (changes present) How do you target only specific resources when you run a terraform plan? -target=resource - A Resource Address to target. This flag can be used multiple times. How do you update the state prior to checking differences when you run aterraform plan? terraform plan -refresh=true The behavior of any terraform destroy command can be previewed at any timewith an equivalent terraform plan -destroy command. Is this true? True You have the following file and created two resources docker_image anddocker_container with the command terraform apply and you go to the terminaland delete the container with the command docker rm. You come back to yourconfiguration and run the command again. Does terraform recreates the resource? resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image.nginx.latest name = \"nginxtutorial\" ports { internal = 80 external = 8080 } upload { source = \"${abspath(path.root)}/files/index.html\" file = \"/usr/share/nginx/html/index.html\" } } Yes. Terrsform creates the resource again since the execution plan says two resources and the terraform always maintains the desired state. You created a VM instance on AWS cloud provider with the terraformconfiguration and you log in AWS console and removed the instance. What does thenext apply do? It creates the instance again. You have the following file and created two resources docker_image anddocker_container with the command terraform plan and you go to the terminal anddelete the container with the command docker rm. You come back to yourconfiguration and run the command again. What is the output of the commandplan? resource \"docker_image\" \"nginx\" { name = \"nginx:latest\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image.nginx.latest name = \"nginxtutorial\" ports { internal = 80 external = 8080 } upload { source = \"${abspath(path.root)}/files/index.html\" file = \"/usr/share/nginx/html/index.html\" } } Terraform will perform the following actions: # docker_container.nginx will be created Plan: 1 to add, 0 to change, 0 to destroy.","title":"Top 250 Questions and Answers For Terraform Associate Certification - 2"},{"location":"nightwolf-cotribution/terraform_interview_question-3/","text":"Top 250 Questions and Answers For Terraform Associate Certification - 3 \uf0c1 We have consolidated a list of frequently asked questions in Terraform AssociateCertification. Same list consists of frequently asked Terraform interview questions. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What are Backends? A \"backend\" in Terraform determines how state is loaded and how an operation such as apply is executed. This abstraction enables non-local file state storage, remote execution, etc. By default, Terraform uses the \"local\" backend, which is the normal behavior of Terraform. What is local Backend? The local backend stores state on the local filesystem, locks that state using system APIs, and performs operations locally. // Example terraform { backend \"local\" { path = \"relative/path/to/terraform.tfstate\" } } What is the default path for the local backend? This defaults to \"terraform.tfstate\" relative to the root module by default. What is State Locking? If supported by your backend, Terraform will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state. State locking happens automatically on all operations that could write state. You will not see any message that it is happening. If state locking fails, Terraform will not continue. Does Terraform continue if state locking fails? No. If state locking fails, Terraform will not continue. Can you disable state locking? Yes. You can disable state locking for most commands with the -lock flag but it is not recommended. What are the types of Backend? Standard: State management, functionality covered in State Storage & Locking. Enhanced: Everything in standard plus remote operations. What are remote Backends? Remote backends allow Terraform to use a shared storage space for state data, so any member of your team can use Terraform to manage the same infrastructure. What is the benefit of using remote backend? Remote state storage makes collaboration easier and keeps state and secret information off your local disk. Remote state is loaded only in memory when it is used. If you want to switch from using remote backend to local backend. Whatshould you do? If you want to move back to local state, you can remove the backend configuration block from your configuration and run terraform init again. Terraform will once again ask if you want to migrate your state back to local. What does the command refresh do? The \"terraform refresh\" command is used to reconcile the state Terraform knows about (via its state file) with the real-world infrastructure. This can be used to detect any drift from the last-known state, and to update the state file. Does the command refresh modify the infrastructure? The command \"refresh\" does not modify infrastructure, but does modify the state file. If the state is changed, this may cause changes to occur during the next plan or apply. How do you backup the state to the remote backend? 1. When configuring a backend for the first time (moving from no defined backend to explicitly configuring one), Terraform will give you the option to migrate your state to the new backend. This lets you adopt backends without losing any existing state. 2. To be extra careful, we always recommend manually backing up your state as well. You can do this by simply copying your \"terraform.tfstate\" file to another location. What is a partial configuration in terms of configuring Backends? You do not need to specify every required argument in the backend configuration. Omitting certain arguments may be desirable to avoid storing secrets, such as access keys, within the main configuration. When some or all of the arguments are omitted, we call this a partial configuration. What are the ways to provide remaining arguments when using partialconfiguration? Interactively: Terraform will interactively ask you for the required values, unless interactive input is disabled. Terraform will not prompt for optional values. File: A configuration file may be specified via the init command line. To specify a file, use the -backend-config=PATH option when running terraform init. If the file contains secrets it may be kept in a secure data store, such as Vault, in which case it must be downloaded to the local disk before running Terraform. Command-line key-value pairs: Key/value pairs can be specified via the init command line. Note that many shells retain command-line flags in a history file, so this isn't recommended for secrets. To specify a single key/value pair, use the -backend-config=\"KEY=VALUE\" option when running terraform init. Ref: https://www.terraform.io/docs/backends/config.html What is the basic requirement when using partial configuration? When using partial configuration, Terraform requires at a minimum that an empty backend configuration is specified in one of the root Terraform configuration files, to specify the backend type. // Example terraform { backend \"consul\" {} } Give an example of passing partial configuration with Command-lineKey/Value pairs? terraform init \\ -backend-config=\"address=demo.consul.io\" \\ -backend-config=\"path=example_app/terraform_state\" \\ -backend-config=\"scheme=https\" How to unconfigure a backend? If you no longer want to use any backend, you can simply remove the configuration from the file. Terraform will detect this like any other change and prompt you to reinitialize. As part of the reinitialization, Terraform will ask if you would like to migrate your state back down to normal local state. Once this is complete then Terraform is back to behaving as it does by default. How do you encrypt sensitive data in the state? Terraform Cloud always encrypts state at rest and protects it with TLS in transit. Terraform Cloud also knows the identity of the user requesting state and maintains a history of state changes. This can be used to control access and track activity. Terraform Enterprise also supports detailed audit logging. The S3 backend supports encryption at rest when the encrypt option is enabled. IAM policies and logging can be used to identify any invalid access. Requests for the state go over a TLS connection. Backends are completely optional. Is this true? Backends are completely optional. You can successfully use Terraform without ever having to learn or use backends. However, they do solve pain points that afflict teams at a certain scale. If you are an individual, you can likely get away with never using backends. What are the benefits of Backends? Working in a team: Backends can store their state remotely and protect that state with locks to prevent corruption. Some backends such as Terraform Cloud even automatically store a history of all state revisions. Keeping sensitive information off disk: State is retrieved from backends on demand and only stored in memory. If you are using a backend such as Amazon S3, the only location the state ever is persisted is in S3. Remote operations: For larger infrastructures or certain changes, terraform apply can take a long, long time. Some backends support remote operations which enable the operation to execute remotely. You can then turn off your computer and your operation will still complete. Paired with remote state storage and locking above, this also helps in team environments. Why should you be very careful with the Force unlocking the state? Terraform has a \"force-unlock\" command to manually unlock the state if unlocking failed. Be very careful with this command. If you unlock the state when someone else is holding the lock it could cause multiple writers. Force unlock should only be used to unlock your own lock in the situation where automatic unlocking failed. To protect you, the \"force-unlock\" command requires a unique lock ID. Terraform will output this lock ID if unlocking fails. This lock ID acts as a nonce , ensuring that locks and unlocks target the correct lock. You should only use force unlock command when automatic unlocking fails. Isthis true? True How do you define a variable? variable \"region\" { default = \"us-east-1\" } This defines the region variable within your Terraform configuration. How do you access the variable in the configuration? // accessing a variable provider \"aws\" { region = var.region } How many ways you can assign variables in the configuration? 1. Command-line flags: terraform apply -var 'region=us-east-1' 2. From a file: To persist variable values, create a file and assign variables within this file. Create a file named terraform.tfvars with the following contents: region = \"us-east-1\" terraform apply \\ -var-file=\"secret.tfvars\" \\ -var-file=\"production.tfvars\" 3. From environment varibles: Terraform will read environment variables in the form of \"TF_VAR_name\" to find the value for a variable. For example, the \"TF_VAR_region\" variable can be set in the shell to set the region variable in Terraform. 4. UI input: If you execute \"terraform apply\" with any variable unspecified, Terraform will ask you to input the values interactively. These values are not saved, but this provides a convenient workflow when getting started with Terraform. UI input is not recommended for everyday use of Terraform. Does environment variables support List and map types? No Environment variables can only populate string-type variables. List and map type variables must be populated via one of the other mechanisms. How do you provision infrastructure in a staging environment or a productionenvironment using the same Terraform configuration? You can use different varible files with the same configuration // Example // For development terraform apply -var-file=\"dev.tfvars\" // For test terraform apply -var-file=\"test.tfvars\" How do you assign default values to variables? If no value is assigned to a variable via any of these methods and the variable has a default key in its declaration, that value will be used for the variable. variable \"region\" { default = \"us-east-1\" } What are the data types for the variables? string number boolian list(<TYPE>) set(<TYPE>) map(<TYPE>) object({<ATTR NAME> = <TYPE>, ... }) tuple([<TYPE>, ...]) Give an example of data type List variables? Lists are defined either explicitly or implicitly. variable \"availability_zone_names\" { type = list(string) default = [\"us-west-1a\"] } Give an example of data type Map variables? variable \"region\" {} variable \"amis\" { type = map(string) } amis = { \"us-east-1\" = \"ami-abc123\" \"us-west-2\" = \"ami-def456\" } // accessing resource \"aws_instance\" \"example\" { ami = var.amis[var.region] instance_type = \"t2.micro\" } What is the Variable Definition Precedence? The above mechanisms for setting variables can be used together in any combination. If the same variable is assigned multiple values, Terraform uses the last value it finds, overriding any previous values. Note that the same variable cannot be assigned multiple values within a single source. Terraform loads variables in the following order, with later sources taking precedence over earlier ones: * Environment variables * The terraform.tfvars file, if present. * The terraform.tfvars.json file, if present. * Any \"*.auto.tfvars\" or \"*.auto.tfvars.json\" files, processed in lexical order of their filenames. * Any -var and -var-file options on the command line, in the order they are provided. (This includes variables set by a Terraform Cloud workspace.) What are the output variables? output variables as a way to organize data to be easily queried and shown back to the Terraform user. Outputs are a way to tell Terraform what data is important. This data is outputted when apply is called, and can be queried using the terraform output command. Hoe do you define an output variable? output \"ip\" { value = aws_eip.ip.public_ip } Multiple output blocks can be defined to specify multiple output variables. How do you view outputs and queries them? You will see the output when you run the command \"terraform apply\" You can query the output with the command \"terraform output ip\" What are the dynamic blocks? Some resource types include repeatable nested blocks in their arguments, which do not accept expressions. You can dynamically construct repeatable nested blocks like setting using a special dynamic block type, which is supported inside resource, data, provider, and provisioner blocks. A dynamic block acts much like a for expression, but produces nested blocks instead of a complex typed value. It iterates over a given complex value, and generates a nested block for each element of that complex value. Example using dynamic blocks: resource \"aws_elastic_beanstalk_environment\" \"tfenvtest\" { name = \"tf-test-name\" application = \"${aws_elastic_beanstalk_application.tftest.name}\" solution_stack_name = \"64bit Amazon Linux 2018.03 v2.11.4 running Go 1.12.6\" dynamic \"setting\" { for_each = var.settings content { namespace = setting.value[\"namespace\"] name = setting.value[\"name\"] value = setting.value[\"value\"] } } } Ref: https://www.terraform.io/docs/configuration/expressions.html#dynamic-blocks What are the best practices for dynamic blocks? Overuse of dynamic blocks can make configuration hard to read and maintain, so we recommend using them only when you need to hide details in order to build a clean user interface for a re-usable module. Always write nested blocks out literally where possible. What are the Built-in Functions? The Terraform language includes a number of built-in functions that you can call from within expressions to transform and combine values. max(5, 12, 9) Does Terraform language support user-defined functions? No The Terraform language does not support user-defined functions, and so only the functions built in to the language are available for use. What is the built-in function to change string to a number? parseint parses the given string as a representation of an integer in the specified base and returns the resulting number. The base must be between 2 and 62 inclusive. > parseint(\"100\", 10) 100 More Number Functions are listed here https://www.terraform.io/docs/configuration/functions/abs.html What is the built-in function to evaluates given expression and returns aboolean whether the expression produced a result without any errors? can condition = can(formatdate(\"\", var.timestamp)) Ref: https://www.terraform.io/docs/configuration/functions/can.html What is the built-in function to evaluates all of its argument expressions inturn and returns the result of the first one that does not produce any errors? try locals { example = try( [tostring(var.example)], tolist(var.example), ) } What is Resource Address? A Resource Address is a string that references a specific resource in a larger infrastructure. An address is made up of two parts: [module path][resource spec] What is the Module path? A module path addresses a module within the tree of modules. It takes the form: module.A.module.B.module.C... Multiple modules in a path indicate nesting. If a module path is specified without a resource spec, the address applies to every resource within the module. If the module path is omitted, this addresses the root module. What is the Resource spec? A resource spec addresses a specific resource in the config. It takes the form: resource_type.resource_name[resource index] * resource_type - Type of the resource being addressed. * resource_name - User-defined name of the resource. * [resource index] - an optional index into a resource with multiple instances, surrounded by square brace characters ([ and ]). // Examples resource \"aws_instance\" \"web\" { # ... count = 4 } aws_instance.web[3] // Refers to only last instance aws_instance.web // Refers to all four \"web\" instances. resource \"aws_instance\" \"web\" { # ... for_each = { \"terraform\": \"value1\", \"resource\": \"value2\", \"indexing\": \"value3\", \"example\": \"value4\", } } aws_instance.web[\"example\"] //Refers to only the \"example\" instance in the config. What are complex types and what are the collection types Terraform supports? A complex type is a type that groups multiple values into a single value. There are two categories of complex types: 1. collection types (for grouping similar values) * list(...): a sequence of values identified by consecutive whole numbers starting with zero. * map(...): a collection of values where each is identified by a string label. * set(...): a collection of unique values that do not have any secondary identifiers or ordering. 2. structural types (for grouping potentially dissimilar values). * object(...): a collection of named attributes that each have their own type. * tuple(...): a sequence of elements identified by consecutive whole numbers starting with zero, where each element has its own type. What are the named values available and how do we refer to? Terraform makes several kinds of named values available. Each of these names is an expression that references the associated value; you can use them as standalone expressions, or combine them with other expressions to compute new values. * <RESOURCE TYPE>.<NAME> is an object representing a managed resource of the given type and name. The attributes of the resource can be accessed using dot or square bracket notation. * var.<NAME> is the value of the input variable of the given name. * local.<NAME> is the value of the local value of the given name. * module.<MODULE NAME>.<OUTPUT NAME> is the value of the specified output value from a child module called by the current module. * data.<DATA TYPE>.<NAME> is an object representing a data resource of the given data source type and name. If the resource has the count argument set, the value is a list of objects representing its instances. If the resource has the for_each argument set, the value is a map of objects representing its instances. * path.module is the filesystem path of the module where the expression is placed. * path.root is the filesystem path of the root module of the configuration. * path.cwd is the filesystem path of the current working directory. In normal use of Terraform this is the same as path.root, but some advanced uses of Terraform run it from a directory other than the root module directory, causing these paths to be different. * terraform.workspace is the name of the currently selected workspace. What is the built-in function that reads the contents of a file at the given pathand returns them as a base64-encoded string? filebase64(path) Ref: https://www.terraform.io/docs/configuration/functions/filebase64.html What is the built-in function that converts a timestamp into a different timeformat? formatdate(spec, timestamp) Ref: https://www.terraform.io/docs/configuration/functions/formatdate.html What is the built-in function encodes a given value to a string using JSONsyntax? jsonencode({\"hello\"=\"world\"}) Ref: https://www.terraform.io/docs/configuration/functions/jsonencode.html What is the built-in function that calculates a full host IP address for a givenhost number within a given IP network address prefix? > cidrhost(\"10.12.127.0/20\", 16) 10.12.112.16 Ref: href=https://www.terraform.io/docs/configuration/functions/cidrhost.html What is Sentinel? Sentinel is an embedded policy-as-code framework integrated with the HashiCorp Enterprise products. It enables fine-grained, logic-based policy decisions, and can be extended to use information from external sources. What is the benefit of Sentinel? Codifying policy removes the need for ticketing queues, without sacrificing enforcement. One of the other benefits of Sentinel is that it also has a full testing framework. Avoiding a ticketing workflow allows organizations to provide more self-service capabilities and end-to-end automation, minimizing the friction for developers and operators. Ref: https://www.hashicorp.com/blog/why-policy-as-code What is the Private Module Registry? Terraform Cloud's private module registry helps you share Terraform modules across your organization. It includes support for module versioning, a searchable and filterable list of available modules, and a configuration designer to help you build new workspaces faster. What is the difference between public and private module registries whendefined source? The public registry uses a three-part <NAMESPACE>/<MODULE NAME>/<PROVIDER> format. The private modules use a four-part <HOSTNAME>/<ORGANIZATION>/<MODULE NAME>/<PROVIDER> format. // example module \"vpc\" { source = \"app.terraform.io/example_corp/vpc/aws\" version = \"1.0.4\" } Where is the Terraform Module Registry available at? https://registry.terraform.io/ What is a workspace? A workspace contains everything Terraform needs to manage a given collection of infrastructure, and separate workspaces function like completely separate working directories. What are the benefits of workspaces? https://www.hashicorp.com/resources/terraform-enterprise-understanding-workspaces-and-modules/ You are configuring a remote backend in the terraform cloud. You didn\u2019t createan organization before you do terraform init. Does it work? While the organization defined in the backend stanza must already exist. You are configuring a remote backend in the terraform cloud. You didn\u2019t create a workspace before you do terraform init. Does it work? Terraform Cloud will create it if necessary. If you opt to use a workspace that already exists, the workspace must not have any existing states. Terraform workspaces when you are working with CLI and Terraformworkspaces in the Terraform cloud. Is this correct? If you are familiar with running Terraform using the CLI, you may have used Terraform workspaces. Terraform Cloud workspaces behave differently than Terraform CLI workspaces. Terraform CLI workspaces allow multiple state files to exist within a single directory, enabling you to use one configuration for multiple environments. Terraform Cloud workspaces contain everything needed to manage a given set of infrastructure, and function like separate working directories. How do you authenticate the CLI with the terraform cloud? Newer Versions: 1. terraform login 2. it will open the terraform cloud and generate the token 3. paste that token back in the CLI https://learn.hashicorp.com/terraform/tfc/tfc_login Older versions: keep the following token in the CLI configuration file: credentials \"app.terraform.io\" { token = \"xxxxxx.atlasv1.zzzzzzzzzzzzz\" } Ref: <https://www.terraform.io/docs/commands/cli-config.html#credentials You are building infrastructure on your local machine and you changed yourbackend to remote backend with the Terraform cloud. What should you do tomigrate the state to the remote backend? terraform init Once you have authenticated the remote backend, you are ready to migrate your local state file to Terraform Cloud. To begin the migration, reinitialize. This causes Terraform to recognize your changed backend configuration. During reinitialization, Terraform presents a prompt saying that it will copy the state file to the new backend. Enter \"yes\" and Terraform will migrate the state from your local machine to Terraform Cloud. Ref: https://learn.hashicorp.com/terraform/tfc/tfc_migration#migrate-the-state-file How do you configure remote backend with the terraform cloud? You need to configure in the terraform block terraform { backend \"remote\" { hostname = \"app.terraform.io\" organization = \"<YOUR-ORG-NAME>\" workspaces { name = \"state-migration\" } } } What is Run Triggers? Terraform Cloud\u2019s run triggers allow you to link workspaces so that a successful apply in a source workspace will queue a run in the workspace linked to it with a run trigger. For example, adding new subnets to your network configuration could trigger an update to your application configuration to rebalance servers across the new subnets. What is the benefit of Run Triggers? When managing complex infrastructure with Terraform Cloud, organizing your configuration into different workspaces helps you to better manage and design your infrastructure. Configuring run triggers between workspaces allows you to set up infrastructure pipelines as part of your overall deployment strategy. What are the available permissions that terraform clouds can have? Terraform Cloud teams can have read, plan, write, or admin permissions on individual workspaces. Who can grant permissions on the workspaces? Organization owners grant permissions by grouping users into teams and giving those teams priviliges based on their need for access to individual workspaces. Which plan do you need to manage teams on Terraform cloud? Team Plan How can you add users to an organization? You can add users to an organization by inviting them using their email address. Even if your team member has not signed up for Terraform Cloud yet, they can still accept the invitation and create a new account. The Terraform Cloud Team plan charges you on a per-user basis. Is this true? Yes. The Terraform Cloud Team plan is charged on a per-user basis so adding new users to your organization incurs cost. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Terraform Interview Questions and Answers- 3rd"},{"location":"nightwolf-cotribution/terraform_interview_question-3/#top-250-questions-and-answers-for-terraform-associate-certification-3","text":"We have consolidated a list of frequently asked questions in Terraform AssociateCertification. Same list consists of frequently asked Terraform interview questions. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What are Backends? A \"backend\" in Terraform determines how state is loaded and how an operation such as apply is executed. This abstraction enables non-local file state storage, remote execution, etc. By default, Terraform uses the \"local\" backend, which is the normal behavior of Terraform. What is local Backend? The local backend stores state on the local filesystem, locks that state using system APIs, and performs operations locally. // Example terraform { backend \"local\" { path = \"relative/path/to/terraform.tfstate\" } } What is the default path for the local backend? This defaults to \"terraform.tfstate\" relative to the root module by default. What is State Locking? If supported by your backend, Terraform will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state. State locking happens automatically on all operations that could write state. You will not see any message that it is happening. If state locking fails, Terraform will not continue. Does Terraform continue if state locking fails? No. If state locking fails, Terraform will not continue. Can you disable state locking? Yes. You can disable state locking for most commands with the -lock flag but it is not recommended. What are the types of Backend? Standard: State management, functionality covered in State Storage & Locking. Enhanced: Everything in standard plus remote operations. What are remote Backends? Remote backends allow Terraform to use a shared storage space for state data, so any member of your team can use Terraform to manage the same infrastructure. What is the benefit of using remote backend? Remote state storage makes collaboration easier and keeps state and secret information off your local disk. Remote state is loaded only in memory when it is used. If you want to switch from using remote backend to local backend. Whatshould you do? If you want to move back to local state, you can remove the backend configuration block from your configuration and run terraform init again. Terraform will once again ask if you want to migrate your state back to local. What does the command refresh do? The \"terraform refresh\" command is used to reconcile the state Terraform knows about (via its state file) with the real-world infrastructure. This can be used to detect any drift from the last-known state, and to update the state file. Does the command refresh modify the infrastructure? The command \"refresh\" does not modify infrastructure, but does modify the state file. If the state is changed, this may cause changes to occur during the next plan or apply. How do you backup the state to the remote backend? 1. When configuring a backend for the first time (moving from no defined backend to explicitly configuring one), Terraform will give you the option to migrate your state to the new backend. This lets you adopt backends without losing any existing state. 2. To be extra careful, we always recommend manually backing up your state as well. You can do this by simply copying your \"terraform.tfstate\" file to another location. What is a partial configuration in terms of configuring Backends? You do not need to specify every required argument in the backend configuration. Omitting certain arguments may be desirable to avoid storing secrets, such as access keys, within the main configuration. When some or all of the arguments are omitted, we call this a partial configuration. What are the ways to provide remaining arguments when using partialconfiguration? Interactively: Terraform will interactively ask you for the required values, unless interactive input is disabled. Terraform will not prompt for optional values. File: A configuration file may be specified via the init command line. To specify a file, use the -backend-config=PATH option when running terraform init. If the file contains secrets it may be kept in a secure data store, such as Vault, in which case it must be downloaded to the local disk before running Terraform. Command-line key-value pairs: Key/value pairs can be specified via the init command line. Note that many shells retain command-line flags in a history file, so this isn't recommended for secrets. To specify a single key/value pair, use the -backend-config=\"KEY=VALUE\" option when running terraform init. Ref: https://www.terraform.io/docs/backends/config.html What is the basic requirement when using partial configuration? When using partial configuration, Terraform requires at a minimum that an empty backend configuration is specified in one of the root Terraform configuration files, to specify the backend type. // Example terraform { backend \"consul\" {} } Give an example of passing partial configuration with Command-lineKey/Value pairs? terraform init \\ -backend-config=\"address=demo.consul.io\" \\ -backend-config=\"path=example_app/terraform_state\" \\ -backend-config=\"scheme=https\" How to unconfigure a backend? If you no longer want to use any backend, you can simply remove the configuration from the file. Terraform will detect this like any other change and prompt you to reinitialize. As part of the reinitialization, Terraform will ask if you would like to migrate your state back down to normal local state. Once this is complete then Terraform is back to behaving as it does by default. How do you encrypt sensitive data in the state? Terraform Cloud always encrypts state at rest and protects it with TLS in transit. Terraform Cloud also knows the identity of the user requesting state and maintains a history of state changes. This can be used to control access and track activity. Terraform Enterprise also supports detailed audit logging. The S3 backend supports encryption at rest when the encrypt option is enabled. IAM policies and logging can be used to identify any invalid access. Requests for the state go over a TLS connection. Backends are completely optional. Is this true? Backends are completely optional. You can successfully use Terraform without ever having to learn or use backends. However, they do solve pain points that afflict teams at a certain scale. If you are an individual, you can likely get away with never using backends. What are the benefits of Backends? Working in a team: Backends can store their state remotely and protect that state with locks to prevent corruption. Some backends such as Terraform Cloud even automatically store a history of all state revisions. Keeping sensitive information off disk: State is retrieved from backends on demand and only stored in memory. If you are using a backend such as Amazon S3, the only location the state ever is persisted is in S3. Remote operations: For larger infrastructures or certain changes, terraform apply can take a long, long time. Some backends support remote operations which enable the operation to execute remotely. You can then turn off your computer and your operation will still complete. Paired with remote state storage and locking above, this also helps in team environments. Why should you be very careful with the Force unlocking the state? Terraform has a \"force-unlock\" command to manually unlock the state if unlocking failed. Be very careful with this command. If you unlock the state when someone else is holding the lock it could cause multiple writers. Force unlock should only be used to unlock your own lock in the situation where automatic unlocking failed. To protect you, the \"force-unlock\" command requires a unique lock ID. Terraform will output this lock ID if unlocking fails. This lock ID acts as a nonce , ensuring that locks and unlocks target the correct lock. You should only use force unlock command when automatic unlocking fails. Isthis true? True How do you define a variable? variable \"region\" { default = \"us-east-1\" } This defines the region variable within your Terraform configuration. How do you access the variable in the configuration? // accessing a variable provider \"aws\" { region = var.region } How many ways you can assign variables in the configuration? 1. Command-line flags: terraform apply -var 'region=us-east-1' 2. From a file: To persist variable values, create a file and assign variables within this file. Create a file named terraform.tfvars with the following contents: region = \"us-east-1\" terraform apply \\ -var-file=\"secret.tfvars\" \\ -var-file=\"production.tfvars\" 3. From environment varibles: Terraform will read environment variables in the form of \"TF_VAR_name\" to find the value for a variable. For example, the \"TF_VAR_region\" variable can be set in the shell to set the region variable in Terraform. 4. UI input: If you execute \"terraform apply\" with any variable unspecified, Terraform will ask you to input the values interactively. These values are not saved, but this provides a convenient workflow when getting started with Terraform. UI input is not recommended for everyday use of Terraform. Does environment variables support List and map types? No Environment variables can only populate string-type variables. List and map type variables must be populated via one of the other mechanisms. How do you provision infrastructure in a staging environment or a productionenvironment using the same Terraform configuration? You can use different varible files with the same configuration // Example // For development terraform apply -var-file=\"dev.tfvars\" // For test terraform apply -var-file=\"test.tfvars\" How do you assign default values to variables? If no value is assigned to a variable via any of these methods and the variable has a default key in its declaration, that value will be used for the variable. variable \"region\" { default = \"us-east-1\" } What are the data types for the variables? string number boolian list(<TYPE>) set(<TYPE>) map(<TYPE>) object({<ATTR NAME> = <TYPE>, ... }) tuple([<TYPE>, ...]) Give an example of data type List variables? Lists are defined either explicitly or implicitly. variable \"availability_zone_names\" { type = list(string) default = [\"us-west-1a\"] } Give an example of data type Map variables? variable \"region\" {} variable \"amis\" { type = map(string) } amis = { \"us-east-1\" = \"ami-abc123\" \"us-west-2\" = \"ami-def456\" } // accessing resource \"aws_instance\" \"example\" { ami = var.amis[var.region] instance_type = \"t2.micro\" } What is the Variable Definition Precedence? The above mechanisms for setting variables can be used together in any combination. If the same variable is assigned multiple values, Terraform uses the last value it finds, overriding any previous values. Note that the same variable cannot be assigned multiple values within a single source. Terraform loads variables in the following order, with later sources taking precedence over earlier ones: * Environment variables * The terraform.tfvars file, if present. * The terraform.tfvars.json file, if present. * Any \"*.auto.tfvars\" or \"*.auto.tfvars.json\" files, processed in lexical order of their filenames. * Any -var and -var-file options on the command line, in the order they are provided. (This includes variables set by a Terraform Cloud workspace.) What are the output variables? output variables as a way to organize data to be easily queried and shown back to the Terraform user. Outputs are a way to tell Terraform what data is important. This data is outputted when apply is called, and can be queried using the terraform output command. Hoe do you define an output variable? output \"ip\" { value = aws_eip.ip.public_ip } Multiple output blocks can be defined to specify multiple output variables. How do you view outputs and queries them? You will see the output when you run the command \"terraform apply\" You can query the output with the command \"terraform output ip\" What are the dynamic blocks? Some resource types include repeatable nested blocks in their arguments, which do not accept expressions. You can dynamically construct repeatable nested blocks like setting using a special dynamic block type, which is supported inside resource, data, provider, and provisioner blocks. A dynamic block acts much like a for expression, but produces nested blocks instead of a complex typed value. It iterates over a given complex value, and generates a nested block for each element of that complex value. Example using dynamic blocks: resource \"aws_elastic_beanstalk_environment\" \"tfenvtest\" { name = \"tf-test-name\" application = \"${aws_elastic_beanstalk_application.tftest.name}\" solution_stack_name = \"64bit Amazon Linux 2018.03 v2.11.4 running Go 1.12.6\" dynamic \"setting\" { for_each = var.settings content { namespace = setting.value[\"namespace\"] name = setting.value[\"name\"] value = setting.value[\"value\"] } } } Ref: https://www.terraform.io/docs/configuration/expressions.html#dynamic-blocks What are the best practices for dynamic blocks? Overuse of dynamic blocks can make configuration hard to read and maintain, so we recommend using them only when you need to hide details in order to build a clean user interface for a re-usable module. Always write nested blocks out literally where possible. What are the Built-in Functions? The Terraform language includes a number of built-in functions that you can call from within expressions to transform and combine values. max(5, 12, 9) Does Terraform language support user-defined functions? No The Terraform language does not support user-defined functions, and so only the functions built in to the language are available for use. What is the built-in function to change string to a number? parseint parses the given string as a representation of an integer in the specified base and returns the resulting number. The base must be between 2 and 62 inclusive. > parseint(\"100\", 10) 100 More Number Functions are listed here https://www.terraform.io/docs/configuration/functions/abs.html What is the built-in function to evaluates given expression and returns aboolean whether the expression produced a result without any errors? can condition = can(formatdate(\"\", var.timestamp)) Ref: https://www.terraform.io/docs/configuration/functions/can.html What is the built-in function to evaluates all of its argument expressions inturn and returns the result of the first one that does not produce any errors? try locals { example = try( [tostring(var.example)], tolist(var.example), ) } What is Resource Address? A Resource Address is a string that references a specific resource in a larger infrastructure. An address is made up of two parts: [module path][resource spec] What is the Module path? A module path addresses a module within the tree of modules. It takes the form: module.A.module.B.module.C... Multiple modules in a path indicate nesting. If a module path is specified without a resource spec, the address applies to every resource within the module. If the module path is omitted, this addresses the root module. What is the Resource spec? A resource spec addresses a specific resource in the config. It takes the form: resource_type.resource_name[resource index] * resource_type - Type of the resource being addressed. * resource_name - User-defined name of the resource. * [resource index] - an optional index into a resource with multiple instances, surrounded by square brace characters ([ and ]). // Examples resource \"aws_instance\" \"web\" { # ... count = 4 } aws_instance.web[3] // Refers to only last instance aws_instance.web // Refers to all four \"web\" instances. resource \"aws_instance\" \"web\" { # ... for_each = { \"terraform\": \"value1\", \"resource\": \"value2\", \"indexing\": \"value3\", \"example\": \"value4\", } } aws_instance.web[\"example\"] //Refers to only the \"example\" instance in the config. What are complex types and what are the collection types Terraform supports? A complex type is a type that groups multiple values into a single value. There are two categories of complex types: 1. collection types (for grouping similar values) * list(...): a sequence of values identified by consecutive whole numbers starting with zero. * map(...): a collection of values where each is identified by a string label. * set(...): a collection of unique values that do not have any secondary identifiers or ordering. 2. structural types (for grouping potentially dissimilar values). * object(...): a collection of named attributes that each have their own type. * tuple(...): a sequence of elements identified by consecutive whole numbers starting with zero, where each element has its own type. What are the named values available and how do we refer to? Terraform makes several kinds of named values available. Each of these names is an expression that references the associated value; you can use them as standalone expressions, or combine them with other expressions to compute new values. * <RESOURCE TYPE>.<NAME> is an object representing a managed resource of the given type and name. The attributes of the resource can be accessed using dot or square bracket notation. * var.<NAME> is the value of the input variable of the given name. * local.<NAME> is the value of the local value of the given name. * module.<MODULE NAME>.<OUTPUT NAME> is the value of the specified output value from a child module called by the current module. * data.<DATA TYPE>.<NAME> is an object representing a data resource of the given data source type and name. If the resource has the count argument set, the value is a list of objects representing its instances. If the resource has the for_each argument set, the value is a map of objects representing its instances. * path.module is the filesystem path of the module where the expression is placed. * path.root is the filesystem path of the root module of the configuration. * path.cwd is the filesystem path of the current working directory. In normal use of Terraform this is the same as path.root, but some advanced uses of Terraform run it from a directory other than the root module directory, causing these paths to be different. * terraform.workspace is the name of the currently selected workspace. What is the built-in function that reads the contents of a file at the given pathand returns them as a base64-encoded string? filebase64(path) Ref: https://www.terraform.io/docs/configuration/functions/filebase64.html What is the built-in function that converts a timestamp into a different timeformat? formatdate(spec, timestamp) Ref: https://www.terraform.io/docs/configuration/functions/formatdate.html What is the built-in function encodes a given value to a string using JSONsyntax? jsonencode({\"hello\"=\"world\"}) Ref: https://www.terraform.io/docs/configuration/functions/jsonencode.html What is the built-in function that calculates a full host IP address for a givenhost number within a given IP network address prefix? > cidrhost(\"10.12.127.0/20\", 16) 10.12.112.16 Ref: href=https://www.terraform.io/docs/configuration/functions/cidrhost.html What is Sentinel? Sentinel is an embedded policy-as-code framework integrated with the HashiCorp Enterprise products. It enables fine-grained, logic-based policy decisions, and can be extended to use information from external sources. What is the benefit of Sentinel? Codifying policy removes the need for ticketing queues, without sacrificing enforcement. One of the other benefits of Sentinel is that it also has a full testing framework. Avoiding a ticketing workflow allows organizations to provide more self-service capabilities and end-to-end automation, minimizing the friction for developers and operators. Ref: https://www.hashicorp.com/blog/why-policy-as-code What is the Private Module Registry? Terraform Cloud's private module registry helps you share Terraform modules across your organization. It includes support for module versioning, a searchable and filterable list of available modules, and a configuration designer to help you build new workspaces faster. What is the difference between public and private module registries whendefined source? The public registry uses a three-part <NAMESPACE>/<MODULE NAME>/<PROVIDER> format. The private modules use a four-part <HOSTNAME>/<ORGANIZATION>/<MODULE NAME>/<PROVIDER> format. // example module \"vpc\" { source = \"app.terraform.io/example_corp/vpc/aws\" version = \"1.0.4\" } Where is the Terraform Module Registry available at? https://registry.terraform.io/ What is a workspace? A workspace contains everything Terraform needs to manage a given collection of infrastructure, and separate workspaces function like completely separate working directories. What are the benefits of workspaces? https://www.hashicorp.com/resources/terraform-enterprise-understanding-workspaces-and-modules/ You are configuring a remote backend in the terraform cloud. You didn\u2019t createan organization before you do terraform init. Does it work? While the organization defined in the backend stanza must already exist. You are configuring a remote backend in the terraform cloud. You didn\u2019t create a workspace before you do terraform init. Does it work? Terraform Cloud will create it if necessary. If you opt to use a workspace that already exists, the workspace must not have any existing states. Terraform workspaces when you are working with CLI and Terraformworkspaces in the Terraform cloud. Is this correct? If you are familiar with running Terraform using the CLI, you may have used Terraform workspaces. Terraform Cloud workspaces behave differently than Terraform CLI workspaces. Terraform CLI workspaces allow multiple state files to exist within a single directory, enabling you to use one configuration for multiple environments. Terraform Cloud workspaces contain everything needed to manage a given set of infrastructure, and function like separate working directories. How do you authenticate the CLI with the terraform cloud? Newer Versions: 1. terraform login 2. it will open the terraform cloud and generate the token 3. paste that token back in the CLI https://learn.hashicorp.com/terraform/tfc/tfc_login Older versions: keep the following token in the CLI configuration file: credentials \"app.terraform.io\" { token = \"xxxxxx.atlasv1.zzzzzzzzzzzzz\" } Ref: <https://www.terraform.io/docs/commands/cli-config.html#credentials You are building infrastructure on your local machine and you changed yourbackend to remote backend with the Terraform cloud. What should you do tomigrate the state to the remote backend? terraform init Once you have authenticated the remote backend, you are ready to migrate your local state file to Terraform Cloud. To begin the migration, reinitialize. This causes Terraform to recognize your changed backend configuration. During reinitialization, Terraform presents a prompt saying that it will copy the state file to the new backend. Enter \"yes\" and Terraform will migrate the state from your local machine to Terraform Cloud. Ref: https://learn.hashicorp.com/terraform/tfc/tfc_migration#migrate-the-state-file How do you configure remote backend with the terraform cloud? You need to configure in the terraform block terraform { backend \"remote\" { hostname = \"app.terraform.io\" organization = \"<YOUR-ORG-NAME>\" workspaces { name = \"state-migration\" } } } What is Run Triggers? Terraform Cloud\u2019s run triggers allow you to link workspaces so that a successful apply in a source workspace will queue a run in the workspace linked to it with a run trigger. For example, adding new subnets to your network configuration could trigger an update to your application configuration to rebalance servers across the new subnets. What is the benefit of Run Triggers? When managing complex infrastructure with Terraform Cloud, organizing your configuration into different workspaces helps you to better manage and design your infrastructure. Configuring run triggers between workspaces allows you to set up infrastructure pipelines as part of your overall deployment strategy. What are the available permissions that terraform clouds can have? Terraform Cloud teams can have read, plan, write, or admin permissions on individual workspaces. Who can grant permissions on the workspaces? Organization owners grant permissions by grouping users into teams and giving those teams priviliges based on their need for access to individual workspaces. Which plan do you need to manage teams on Terraform cloud? Team Plan How can you add users to an organization? You can add users to an organization by inviting them using their email address. Even if your team member has not signed up for Terraform Cloud yet, they can still accept the invitation and create a new account. The Terraform Cloud Team plan charges you on a per-user basis. Is this true? Yes. The Terraform Cloud Team plan is charged on a per-user basis so adding new users to your organization incurs cost. (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Top 250 Questions and Answers For Terraform Associate Certification - 3"},{"location":"nightwolf-cotribution/terraform_interview_question/","text":"Top 250 Questions and Answers For Terraform Associate Certification \uf0c1 We have consolidated a list of frequently asked questions in Terraform AssociateCertification. Same list consists of frequently asked Terraform interview questions. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is Infrastructure as Code? You write and execute the code to define, deploy, update, and destroy your infrastructure What are the benefits of IaC? a. Automation => We can bring up the servers with one script and scale up and down based on our load with the same script. b. Reusability of the code => We can reuse the same code c. Versioning => We can check it into version control and we get versioning. Now we can see an incremental history of who changed what, how is our infrastructure actually defined at any given point of time, and wehave this transparency of documentation. IaC makes changes idempotent, consistent, repeatable, and predictable. How using IaC make it easy to provision infrastructure? IaC makes it easy to provision and apply infrastructure configurations, saving time. It standardizes workflows across different infrastructure providers (e.g., VMware, AWS, Azure, GCP, etc.) by using a common syntax across all of them. What is Ideompodent in terms of IaC? The idempotent characteristic provided by IaC tools ensures that, even if the same code is applied multiple times, the result remains the same. What are Day 0 and Day 1 activities? IaC can be applied throughout the lifecycle, both on the initial build, as well as throughout the life of the infrastructure. Commonly, these are referred to as Day 0 and Day 1 activities. \"Day 0\" code provisions and configures your initial infrastructure. \"Day 1\" refers to OS and application configurations you apply after you have initially built your infrastructure. What are the use cases of Terraform? Heroku App Setup Multi-Tier Applications Self-Service Clusters Software Demos Disposable Environments Software Defined Networking Resource Schedulers Multi-Cloud Deployment Reference: https://www.terraform.io/intro/use-cases.html What are the advantages of Terraform? Platform Agnostic State Management Operator Confidence Reference: https://learn.hashicorp.com/terraform/getting-started/intro Where do you describe all the components or your entire datacenter so thatTerraform provision those? Configuration files ends with *.tf How can Terraform build infrastructure so efficiently? Terraform builds a graph of all your resources, and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure. What is multi-cloud deployment? Provisoning your infrastrcutire into multiple cloud providers to increase fault-tolerance of your applications. How multi-cloud deployment is useful? By using only a single region or cloud provider, fault tolerance is limited by the availability of that provider. Having a multi-cloud deployment allows for more graceful recovery of the loss of a region or entire provider. What is cloud-agnostic in terms of provisioning tools? cloud-agnostic and allows a single configuration to be used to manage multiple providers, and to even handle cross-cloud dependencies. Is Terraform cloud-agostic? Yes What is the use of terraform being cloud-agnostic? It simplifies management and orchestration, helping operators build large-scale multi-cloud infrastructures. What is the Terraform State? Every time you run Terraform, it records information about what infrastructure it created in a Terraform state file By default, when you run Terraform in the folder /some/folder, Terraform creates the file /some/folder/terraform.tfstate This file contains a custom JSON format that records a mapping from the Terraform resources in your configuration files to the representation of those resources in the real world. What is the purpose of the Terraform State? Mapping to the Real World => Terraform requires some sort of database to map Terraform config to the real world because you can not find the same functionality in every cloud provider. You need to have some kind of mechanism to be cloud-agnostic Metadata => Terraform must also track metadata such as resource dependencies, pointer to the provider configuration that was most recently used with the resource in situations where multiple aliased providers are present. Performance => When running a terraform plan, Terraform must know the current state of resources in order to effectively determine the changes that it needs to make to reach your desired configuration. For larger infrastructures, querying every resource is too slow. Many cloud providers do not provide APIs to query multiple resources at once, and the round trip time for each resource is hundreds of milliseconds. So, Terraform stores a cache of the attribute values for all resources in the state. This is the most optional feature of Terraform state and is done only as a performance improvement. Syncing => When two people works on the same file and doing some changes to the infrastructure. Its very important for everyone to be working with the same state so that operations will be applied to the same remote objects. Reference: https://www.terraform.io/docs/state/purpose.html What is the name of the terraform state file? terraform.tfstate How do you install terraform on different OS? // Mac OS brew install terraform // Windows choco install terraform How do you manually install terraform? step 1: Download the zip fille step 2: mv ~/Downloads/terraform /usr/local/bin/terraform Where do you put terraform configurations so that you can configure somebehaviors of Terraform itself? The special terraform configuration block type is used to configure some behaviors of Terraform itself, such as requiring a minimum Terraform version to apply your configuration. terraform { # ... } Only constants are allowed inside the terraform block. Is this correct? Yes Within a terraform block, only constant values can be used; arguments may not refer to named objects such as resources, input variables, etc, and may not use any of the Terraform language built-in functions. What are the Providers? A provider is a plugin that Terraform uses to translate the API interactions with the service. A provider is responsible for understanding API interactions and exposing resources. Because Terraform can interact with any API, you can represent almost any infrastructure type as a resource in Terraform. Reference: https://www.terraform.io/docs/configuration/providers.html How do you configure a Provider? provider \"google\" { project = \"acme-app\" region = \"us-central1\" } The name given in the block header (\"google\" in this example) is the name of the provider to configure. Terraform associates each resource type with a provider by taking the first word of the resource type name (separated by underscores), and so the \"google\" provider is assumed to be the provider for the resource type name google_compute_instance. The body of the block (between { and } ) contains configuration arguments for the provider itself. Most arguments in this section are specified by the provider itself; in this example both project and region are specific to the google provider. What are the meta-arguments that are defined by Terraform itself and availablefor all provider blocks? version: Constraining the allowed provider versions alias: using the same provider with different configurations for different resources What is Provider initialization and why do we need? Each time a new provider is added to configuration -- either explicitly via a provider block or by adding a resource from that provider -- Terraform must initialize the provider before it can be used. Initialization downloads and installs the provider's plugin so that it can later be executed. How do you initialize any Provider? Provider initialization is one of the actions of terraform init. Running this command will download and initialize any providers that are not already initialized. When you run terraform init command, all the providers are installed in thecurrent working directory. Is this true? Providers downloaded by terraform init are only installed for the current working directory; other working directories can have their own installed provider versions. Note that terraform init cannot automatically download providers that are not distributed by HashiCorp. How do you constrain the provider version? To constrain the provider version as suggested, add a required_providers block inside a terraform block: terraform { required_providers { aws = \"~> 1.0\" } } How do you upgrade to the latest acceptable version of the provider? terraform init --upgrade It upgrade to the latest acceptable version of each provider. This command also upgrades to the latest versions of all Terraform modules. How many ways you can configure provider versions? 1. With required_providers blocks under terraform block: terraform { required_providers { aws = \"~> 1.0\" } } 2. Provider version constraints can also be specified using a version argument within a provider block: provider { version= \"1.0\" } How do you configure Multiple Provider Instances? alias You can optionally define multiple configurations for the same provider, and select which one to use on a per-resource or per-module basis. Why do we need Multiple Provider instances? Some of the example scenarios: a. multiple regions for a cloud platform b. targeting multiple Docker hosts c. multiple Consul hosts, etc. How do we define multiple Provider configurations? To include multiple configurations for a given provider, include multiple provider blocks with the same provider name, but set the alias meta-argument to an alias name to use for each additional configuration. # The default provider configuration provider \"aws\" { region = \"us-east-1\" } # Additional provider configuration for west coast region provider \"aws\" { alias = \"west\" region = \"us-west-2\" } How do you select alternate providers? By default, resources use a default provider configuration inferred from the first word of the resource type name. For example, a resource of type aws_instance uses the default (un-aliased) aws provider configuration unless otherwise stated. resource \"aws_instance\" \"foo\" { provider = aws.west # ... } What is the location of the user plugins directory? Windows => %APPDATA%\\terraform.d\\plugins Unix based systems => ~/.terraform.d/plugins Third-party plugins should be manually installed. Is that true? True The command terraform init cannot install third-party plugins? True or false? True Install third-party providers by placing their plugin executables in the user plugins directory. The user plugins directory is in one of the following locations, depending on the host operating system. Once a plugin is installed, terraform init can initialize it normally. You must run this command from the directory where the configuration files are located. What is the naming scheme for provider plugins? \"terraform-provider-<NAME>_vX.Y.Z\" What is the CLI configuration File? The CLI configuration file configures per-user settings for CLI behaviors, which apply across all Terraform working directories. It is named either \".terraformrc\" or \"terraform.rc\" . Where is the location of the CLI configuration File? On Windows, the file must be named named \"terraform.rc\" and placed in the relevant user's %APPDATA% directory. On all other systems, the file must be named \".terraformrc\" (note the leading period) and placed directly in the home directory of the relevant user. The location of the Terraform CLI configuration file can also be specified using the TF_CLI_CONFIG_FILE environment variable. What is Provider Plugin Cache? By default, terraform init downloads plugins into a subdirectory of the working directory so that each working directory is self-contained. As a consequence, if you have multiple configurations that use the same provider then a separate copy of its plugin will be downloaded for each configuration. Given that provider plugins can be quite large (on the order of hundreds of megabytes), this default behavior can be inconvenient for those with slow or metered Internet connections. Therefore Terraform optionally allows the use of a local directory as a shared plugin cache, which then allows each distinct plugin binary to be downloaded only once. How do you enable Provider Plugin Cache? To enable the plugin cache, use the plugin_cache_dir setting in the CLI configuration file. plugin_cache_dir = \"$HOME/.terraform.d/plugin-cache\" Alternatively, the TF_PLUGIN_CACHE_DIR environment variable can be used to enable caching or to override an existing cache directory within a particular shell session. When you are using plugin cache you end up growing cache directory withdifferent versions. Whose responsibility to clean it? User Terraform will never itself delete a plugin from the plugin cache once it has been placed there. Over time, as plugins are upgraded, the cache directory may grow to contain several unused versions which must be manually deleted. Why do we need to initialize the directory? When you create a new configuration - or check out an existing configuration from version control - you need to initialize the directory. // Example provider \"aws\" { profile = \"default\" region = \"us-east-1\" } resource \"aws_instance\" \"example\" { ami = \"ami-2757f631\" instance_type = \"t2.micro\" } Initializing a configuration directory downloads and installs providers used in the configuration, which in this case is the aws provider. Subsequent commands will use local settings and data during initialization. What is the command to initialize the directory? terraform init If different teams are working on the same configuration. How do you make filesto have consistent formatting? terraform fmt This command automatically updates configurations in the current directory for easy readability and consistency. If different teams are working on the same configuration. How do you make filesto have syntactically valid and internally consistent? terraform validate This command will check and report errors within modules, attribute names, and value types. Validate your configuration. If your configuration is valid, Terraform will return a success message. What is the command to create infrastructure? terraform apply What is the command to show the execution plan and not apply? terraform plan How do you inspect the current state of the infrastructure applied? terraform show When you applied your configuration, Terraform wrote data into a file called terraform.tfstate. This file now contains the IDs and properties of the resources Terraform created so that it can manage or destroy those resources going forward. If your state file is too big and you want to list the resources from your state.What is the command? terraform state list What is plug-in based architecture? Defining additional features as plugins to your core platform or core application. This provides extensibility, flexibility and isolation What are Provisioners? If you need to do some initial setup on your instances, then provisioners let you upload files, run shell scripts, or install and trigger other software like configuration management tools, etc. How do you define provisioners? resource \"aws_instance\" \"example\" { ami = \"ami-b374d5a5\" instance_type = \"t2.micro\" provisioner \"local-exec\" { command = \"echo hello > hello.txt\" } } Provisioner block within the resource block. Multiple provisioner blocks can be added to define multiple provisioning steps. Terraform supports multiple provisioners What are the types of provisioners? local-exec remote-exec What is a local-exec provisioner and when do we use it? The local-exec provisioner executing a command locally on your machine running Terraform. We use this when we need to do something on our local machine without needing any external URL. What is a remote-exec provisioner and when do we use it? Another useful provisioner is remote-exec which invokes a script on a remote resource after it is created. This can be used to run a configuration management tool, bootstrap into a cluster, etc. Are provisioners runs only when the resource is created or destroyed? Provisioners are only run when a resource is created or destroyed. Provisioners that are run while destroying are Destroy provisioners. They are not a replacement for configuration management and changing the software of an already-running server, and are instead just meant as a way to bootstrap a server. What do we need to use a remote-exec? In order to use a remote-exec provisioner, you must choose an ssh or winrm connection in the form of a connection block within the provisioner. Here is an example provider \"aws\" { profile = \"default\" region = \"us-west-2\" } resource \"aws_key_pair\" \"example\" { key_name = \"examplekey\" public_key = file(\"~/.ssh/terraform.pub\") } resource \"aws_instance\" \"example\" { key_name = aws_key_pair.example.key_name ami = \"ami-04590e7389a6e577c\" instance_type = \"t2.micro\" connection { type = \"ssh\" user = \"ec2-user\" private_key = file(\"~/.ssh/terraform\") host = self.public_ip } provisioner \"remote-exec\" { inline = [ \"sudo amazon-linux-extras enable nginx1.12\", \"sudo yum -y install nginx\", \"sudo systemctl start nginx\" ] } } When terraform mark the resources are tainted? If a resource successfully creates but fails during provisioning, Terraform will error and mark the resource as \"tainted\". A resource that is tainted has been physically created, but can not be considered safe to use since provisioning failed. You applied the infrastructure with terraform apply and you have some taintedresources. You run an execution plan now what happens to those tainted resources? When you generate your next execution plan, Terraform will not attempt to restart provisioning on the same resource because it is not guaranteed to be safe. Instead, Terraform will remove any tainted resources and create new resources, attempting to provision them again after creation. Terraform also does not automatically roll back and destroy the resource duringthe apply when the failure happens. Why? Terraform also does not automatically roll back and destroy the resource during the apply when the failure happens, because that would go against the execution plan: the execution plan would have said a resource will be created, but does not say it will ever be deleted. If you create an execution plan with a tainted resource, however, the plan will clearly state that the resource will be destroyed because it is tainted. How do you manually taint a resource? terraform taint resource.id Does the taint command modify the infrastructure? terraform taint resource.id This command will not modify infrastructure, but does modify the state file in order to mark a resource as tainted. Once a resource is marked as tainted, the next plan will show that the resource will be destroyed and recreated and the next apply will implement this change. By default, provisioners that fail will also cause the Terraform apply itself to fail.Is this true? True By default, provisioners that fail will also cause the Terraform apply itself to fail.How do you change this? The 'on_failure' setting can be used to change this. The allowed values are: continue: Ignore the error and continue with creation or destruction. fial: Raise an error and stop applying (the default behavior). If this is a creation provisioner, taint the resource. // Example resource \"aws_instance\" \"web\" { # ... provisioner \"local-exec\" { command = \"echo The server's IP address is ${self.private_ip}\" on_failure = \"continue\" } } How do you define destroy provisioner and give an example? You can define destroy provisioner with the parameter when: provisioner \"remote-exec\" { when = \"destroy\" # <...snip...> } How do you apply constraints for the provider versions? The \"required_providers\" setting is a map specifying a version constraint for each provider required by your configuration. terraform { required_providers { aws = \">= 2.7.0\" } } What should you use to set both a lower and upper bound on versions for eachprovider? terraform { required_providers { aws = \"~> 2.7.0\" } } How do you try experimental features? In releases where experimental features are available, you can enable them on a per-module basis by setting the experiments argument inside a terraform block: terraform { experiments = [example] } When does the terraform does not recommend using provisions? Passing data into virtual machines and other compute resources Running configuration management software References: https://www.terraform.io/docs/provisioners/#passing-data-into-virtual-machines-and-other-compute-resources https://www.terraform.io/docs/provisioners/#running-configuration-management-software Expressions in provisioner blocks cannot refer to their parent resource by name.Is this true? True The \"self\" object represents the provisioner's parent resource, and has all of that resource's attributes. For example, use \"self.public_ip\" to reference an aws_instance 'public_ip' attribute. What does this symbol version = \"~> 1.0\" mean when defining versions? Any version more than 1.0 and less than 2.0. Terraform supports both cloud and on-premises infrastructure platforms. Is this true? True Terraform assumes an empty default configuration for any provider that is notexplicitly configured. A provider block can be empty. Is this true? True How do you configure the required version of Terraform CLI can be used withyour configuration? The required_version setting can be used to constrain which versions of the Terraform CLI can be used with your configuration. If the running version of Terraform does not match the constraints specified, Terraform will produce an error and exit without taking any further actions. Terraform CLI versions and provider versions are independent of each other. Isthis true? True You are configuring aws provider and it is always recommended to hard codeaws credentials in *.tf files. Is this true? False nightwolf recommends that you never hard-code credentials into \"*.tf\" configuration files. We are explicitly defining the default AWS config profile here to illustrate how Terraform should access sensitive credentials. If you leave out your AWS credentials, Terraform will automatically search for saved API credentials (for example, in ~/.aws/credentials ) or IAM instance profile credentials. This is cleaner when \".tf\" files are checked into source control or if there is more than one admin user You are provisioning the infrastructure with the command terraform apply andyou noticed one of the resources failed. How do you remove that resource withoutaffecting the whole infrastructure? You can taint the resource ans the next apply will destroy the resource terraform taint <resource.id> Next Page (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Terraform Interview Questions and Answers- 1st"},{"location":"nightwolf-cotribution/terraform_interview_question/#top-250-questions-and-answers-for-terraform-associate-certification","text":"We have consolidated a list of frequently asked questions in Terraform AssociateCertification. Same list consists of frequently asked Terraform interview questions. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. (adsbygoogle = window.adsbygoogle || []).push({}); What is Infrastructure as Code? You write and execute the code to define, deploy, update, and destroy your infrastructure What are the benefits of IaC? a. Automation => We can bring up the servers with one script and scale up and down based on our load with the same script. b. Reusability of the code => We can reuse the same code c. Versioning => We can check it into version control and we get versioning. Now we can see an incremental history of who changed what, how is our infrastructure actually defined at any given point of time, and wehave this transparency of documentation. IaC makes changes idempotent, consistent, repeatable, and predictable. How using IaC make it easy to provision infrastructure? IaC makes it easy to provision and apply infrastructure configurations, saving time. It standardizes workflows across different infrastructure providers (e.g., VMware, AWS, Azure, GCP, etc.) by using a common syntax across all of them. What is Ideompodent in terms of IaC? The idempotent characteristic provided by IaC tools ensures that, even if the same code is applied multiple times, the result remains the same. What are Day 0 and Day 1 activities? IaC can be applied throughout the lifecycle, both on the initial build, as well as throughout the life of the infrastructure. Commonly, these are referred to as Day 0 and Day 1 activities. \"Day 0\" code provisions and configures your initial infrastructure. \"Day 1\" refers to OS and application configurations you apply after you have initially built your infrastructure. What are the use cases of Terraform? Heroku App Setup Multi-Tier Applications Self-Service Clusters Software Demos Disposable Environments Software Defined Networking Resource Schedulers Multi-Cloud Deployment Reference: https://www.terraform.io/intro/use-cases.html What are the advantages of Terraform? Platform Agnostic State Management Operator Confidence Reference: https://learn.hashicorp.com/terraform/getting-started/intro Where do you describe all the components or your entire datacenter so thatTerraform provision those? Configuration files ends with *.tf How can Terraform build infrastructure so efficiently? Terraform builds a graph of all your resources, and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure. What is multi-cloud deployment? Provisoning your infrastrcutire into multiple cloud providers to increase fault-tolerance of your applications. How multi-cloud deployment is useful? By using only a single region or cloud provider, fault tolerance is limited by the availability of that provider. Having a multi-cloud deployment allows for more graceful recovery of the loss of a region or entire provider. What is cloud-agnostic in terms of provisioning tools? cloud-agnostic and allows a single configuration to be used to manage multiple providers, and to even handle cross-cloud dependencies. Is Terraform cloud-agostic? Yes What is the use of terraform being cloud-agnostic? It simplifies management and orchestration, helping operators build large-scale multi-cloud infrastructures. What is the Terraform State? Every time you run Terraform, it records information about what infrastructure it created in a Terraform state file By default, when you run Terraform in the folder /some/folder, Terraform creates the file /some/folder/terraform.tfstate This file contains a custom JSON format that records a mapping from the Terraform resources in your configuration files to the representation of those resources in the real world. What is the purpose of the Terraform State? Mapping to the Real World => Terraform requires some sort of database to map Terraform config to the real world because you can not find the same functionality in every cloud provider. You need to have some kind of mechanism to be cloud-agnostic Metadata => Terraform must also track metadata such as resource dependencies, pointer to the provider configuration that was most recently used with the resource in situations where multiple aliased providers are present. Performance => When running a terraform plan, Terraform must know the current state of resources in order to effectively determine the changes that it needs to make to reach your desired configuration. For larger infrastructures, querying every resource is too slow. Many cloud providers do not provide APIs to query multiple resources at once, and the round trip time for each resource is hundreds of milliseconds. So, Terraform stores a cache of the attribute values for all resources in the state. This is the most optional feature of Terraform state and is done only as a performance improvement. Syncing => When two people works on the same file and doing some changes to the infrastructure. Its very important for everyone to be working with the same state so that operations will be applied to the same remote objects. Reference: https://www.terraform.io/docs/state/purpose.html What is the name of the terraform state file? terraform.tfstate How do you install terraform on different OS? // Mac OS brew install terraform // Windows choco install terraform How do you manually install terraform? step 1: Download the zip fille step 2: mv ~/Downloads/terraform /usr/local/bin/terraform Where do you put terraform configurations so that you can configure somebehaviors of Terraform itself? The special terraform configuration block type is used to configure some behaviors of Terraform itself, such as requiring a minimum Terraform version to apply your configuration. terraform { # ... } Only constants are allowed inside the terraform block. Is this correct? Yes Within a terraform block, only constant values can be used; arguments may not refer to named objects such as resources, input variables, etc, and may not use any of the Terraform language built-in functions. What are the Providers? A provider is a plugin that Terraform uses to translate the API interactions with the service. A provider is responsible for understanding API interactions and exposing resources. Because Terraform can interact with any API, you can represent almost any infrastructure type as a resource in Terraform. Reference: https://www.terraform.io/docs/configuration/providers.html How do you configure a Provider? provider \"google\" { project = \"acme-app\" region = \"us-central1\" } The name given in the block header (\"google\" in this example) is the name of the provider to configure. Terraform associates each resource type with a provider by taking the first word of the resource type name (separated by underscores), and so the \"google\" provider is assumed to be the provider for the resource type name google_compute_instance. The body of the block (between { and } ) contains configuration arguments for the provider itself. Most arguments in this section are specified by the provider itself; in this example both project and region are specific to the google provider. What are the meta-arguments that are defined by Terraform itself and availablefor all provider blocks? version: Constraining the allowed provider versions alias: using the same provider with different configurations for different resources What is Provider initialization and why do we need? Each time a new provider is added to configuration -- either explicitly via a provider block or by adding a resource from that provider -- Terraform must initialize the provider before it can be used. Initialization downloads and installs the provider's plugin so that it can later be executed. How do you initialize any Provider? Provider initialization is one of the actions of terraform init. Running this command will download and initialize any providers that are not already initialized. When you run terraform init command, all the providers are installed in thecurrent working directory. Is this true? Providers downloaded by terraform init are only installed for the current working directory; other working directories can have their own installed provider versions. Note that terraform init cannot automatically download providers that are not distributed by HashiCorp. How do you constrain the provider version? To constrain the provider version as suggested, add a required_providers block inside a terraform block: terraform { required_providers { aws = \"~> 1.0\" } } How do you upgrade to the latest acceptable version of the provider? terraform init --upgrade It upgrade to the latest acceptable version of each provider. This command also upgrades to the latest versions of all Terraform modules. How many ways you can configure provider versions? 1. With required_providers blocks under terraform block: terraform { required_providers { aws = \"~> 1.0\" } } 2. Provider version constraints can also be specified using a version argument within a provider block: provider { version= \"1.0\" } How do you configure Multiple Provider Instances? alias You can optionally define multiple configurations for the same provider, and select which one to use on a per-resource or per-module basis. Why do we need Multiple Provider instances? Some of the example scenarios: a. multiple regions for a cloud platform b. targeting multiple Docker hosts c. multiple Consul hosts, etc. How do we define multiple Provider configurations? To include multiple configurations for a given provider, include multiple provider blocks with the same provider name, but set the alias meta-argument to an alias name to use for each additional configuration. # The default provider configuration provider \"aws\" { region = \"us-east-1\" } # Additional provider configuration for west coast region provider \"aws\" { alias = \"west\" region = \"us-west-2\" } How do you select alternate providers? By default, resources use a default provider configuration inferred from the first word of the resource type name. For example, a resource of type aws_instance uses the default (un-aliased) aws provider configuration unless otherwise stated. resource \"aws_instance\" \"foo\" { provider = aws.west # ... } What is the location of the user plugins directory? Windows => %APPDATA%\\terraform.d\\plugins Unix based systems => ~/.terraform.d/plugins Third-party plugins should be manually installed. Is that true? True The command terraform init cannot install third-party plugins? True or false? True Install third-party providers by placing their plugin executables in the user plugins directory. The user plugins directory is in one of the following locations, depending on the host operating system. Once a plugin is installed, terraform init can initialize it normally. You must run this command from the directory where the configuration files are located. What is the naming scheme for provider plugins? \"terraform-provider-<NAME>_vX.Y.Z\" What is the CLI configuration File? The CLI configuration file configures per-user settings for CLI behaviors, which apply across all Terraform working directories. It is named either \".terraformrc\" or \"terraform.rc\" . Where is the location of the CLI configuration File? On Windows, the file must be named named \"terraform.rc\" and placed in the relevant user's %APPDATA% directory. On all other systems, the file must be named \".terraformrc\" (note the leading period) and placed directly in the home directory of the relevant user. The location of the Terraform CLI configuration file can also be specified using the TF_CLI_CONFIG_FILE environment variable. What is Provider Plugin Cache? By default, terraform init downloads plugins into a subdirectory of the working directory so that each working directory is self-contained. As a consequence, if you have multiple configurations that use the same provider then a separate copy of its plugin will be downloaded for each configuration. Given that provider plugins can be quite large (on the order of hundreds of megabytes), this default behavior can be inconvenient for those with slow or metered Internet connections. Therefore Terraform optionally allows the use of a local directory as a shared plugin cache, which then allows each distinct plugin binary to be downloaded only once. How do you enable Provider Plugin Cache? To enable the plugin cache, use the plugin_cache_dir setting in the CLI configuration file. plugin_cache_dir = \"$HOME/.terraform.d/plugin-cache\" Alternatively, the TF_PLUGIN_CACHE_DIR environment variable can be used to enable caching or to override an existing cache directory within a particular shell session. When you are using plugin cache you end up growing cache directory withdifferent versions. Whose responsibility to clean it? User Terraform will never itself delete a plugin from the plugin cache once it has been placed there. Over time, as plugins are upgraded, the cache directory may grow to contain several unused versions which must be manually deleted. Why do we need to initialize the directory? When you create a new configuration - or check out an existing configuration from version control - you need to initialize the directory. // Example provider \"aws\" { profile = \"default\" region = \"us-east-1\" } resource \"aws_instance\" \"example\" { ami = \"ami-2757f631\" instance_type = \"t2.micro\" } Initializing a configuration directory downloads and installs providers used in the configuration, which in this case is the aws provider. Subsequent commands will use local settings and data during initialization. What is the command to initialize the directory? terraform init If different teams are working on the same configuration. How do you make filesto have consistent formatting? terraform fmt This command automatically updates configurations in the current directory for easy readability and consistency. If different teams are working on the same configuration. How do you make filesto have syntactically valid and internally consistent? terraform validate This command will check and report errors within modules, attribute names, and value types. Validate your configuration. If your configuration is valid, Terraform will return a success message. What is the command to create infrastructure? terraform apply What is the command to show the execution plan and not apply? terraform plan How do you inspect the current state of the infrastructure applied? terraform show When you applied your configuration, Terraform wrote data into a file called terraform.tfstate. This file now contains the IDs and properties of the resources Terraform created so that it can manage or destroy those resources going forward. If your state file is too big and you want to list the resources from your state.What is the command? terraform state list What is plug-in based architecture? Defining additional features as plugins to your core platform or core application. This provides extensibility, flexibility and isolation What are Provisioners? If you need to do some initial setup on your instances, then provisioners let you upload files, run shell scripts, or install and trigger other software like configuration management tools, etc. How do you define provisioners? resource \"aws_instance\" \"example\" { ami = \"ami-b374d5a5\" instance_type = \"t2.micro\" provisioner \"local-exec\" { command = \"echo hello > hello.txt\" } } Provisioner block within the resource block. Multiple provisioner blocks can be added to define multiple provisioning steps. Terraform supports multiple provisioners What are the types of provisioners? local-exec remote-exec What is a local-exec provisioner and when do we use it? The local-exec provisioner executing a command locally on your machine running Terraform. We use this when we need to do something on our local machine without needing any external URL. What is a remote-exec provisioner and when do we use it? Another useful provisioner is remote-exec which invokes a script on a remote resource after it is created. This can be used to run a configuration management tool, bootstrap into a cluster, etc. Are provisioners runs only when the resource is created or destroyed? Provisioners are only run when a resource is created or destroyed. Provisioners that are run while destroying are Destroy provisioners. They are not a replacement for configuration management and changing the software of an already-running server, and are instead just meant as a way to bootstrap a server. What do we need to use a remote-exec? In order to use a remote-exec provisioner, you must choose an ssh or winrm connection in the form of a connection block within the provisioner. Here is an example provider \"aws\" { profile = \"default\" region = \"us-west-2\" } resource \"aws_key_pair\" \"example\" { key_name = \"examplekey\" public_key = file(\"~/.ssh/terraform.pub\") } resource \"aws_instance\" \"example\" { key_name = aws_key_pair.example.key_name ami = \"ami-04590e7389a6e577c\" instance_type = \"t2.micro\" connection { type = \"ssh\" user = \"ec2-user\" private_key = file(\"~/.ssh/terraform\") host = self.public_ip } provisioner \"remote-exec\" { inline = [ \"sudo amazon-linux-extras enable nginx1.12\", \"sudo yum -y install nginx\", \"sudo systemctl start nginx\" ] } } When terraform mark the resources are tainted? If a resource successfully creates but fails during provisioning, Terraform will error and mark the resource as \"tainted\". A resource that is tainted has been physically created, but can not be considered safe to use since provisioning failed. You applied the infrastructure with terraform apply and you have some taintedresources. You run an execution plan now what happens to those tainted resources? When you generate your next execution plan, Terraform will not attempt to restart provisioning on the same resource because it is not guaranteed to be safe. Instead, Terraform will remove any tainted resources and create new resources, attempting to provision them again after creation. Terraform also does not automatically roll back and destroy the resource duringthe apply when the failure happens. Why? Terraform also does not automatically roll back and destroy the resource during the apply when the failure happens, because that would go against the execution plan: the execution plan would have said a resource will be created, but does not say it will ever be deleted. If you create an execution plan with a tainted resource, however, the plan will clearly state that the resource will be destroyed because it is tainted. How do you manually taint a resource? terraform taint resource.id Does the taint command modify the infrastructure? terraform taint resource.id This command will not modify infrastructure, but does modify the state file in order to mark a resource as tainted. Once a resource is marked as tainted, the next plan will show that the resource will be destroyed and recreated and the next apply will implement this change. By default, provisioners that fail will also cause the Terraform apply itself to fail.Is this true? True By default, provisioners that fail will also cause the Terraform apply itself to fail.How do you change this? The 'on_failure' setting can be used to change this. The allowed values are: continue: Ignore the error and continue with creation or destruction. fial: Raise an error and stop applying (the default behavior). If this is a creation provisioner, taint the resource. // Example resource \"aws_instance\" \"web\" { # ... provisioner \"local-exec\" { command = \"echo The server's IP address is ${self.private_ip}\" on_failure = \"continue\" } } How do you define destroy provisioner and give an example? You can define destroy provisioner with the parameter when: provisioner \"remote-exec\" { when = \"destroy\" # <...snip...> } How do you apply constraints for the provider versions? The \"required_providers\" setting is a map specifying a version constraint for each provider required by your configuration. terraform { required_providers { aws = \">= 2.7.0\" } } What should you use to set both a lower and upper bound on versions for eachprovider? terraform { required_providers { aws = \"~> 2.7.0\" } } How do you try experimental features? In releases where experimental features are available, you can enable them on a per-module basis by setting the experiments argument inside a terraform block: terraform { experiments = [example] } When does the terraform does not recommend using provisions? Passing data into virtual machines and other compute resources Running configuration management software References: https://www.terraform.io/docs/provisioners/#passing-data-into-virtual-machines-and-other-compute-resources https://www.terraform.io/docs/provisioners/#running-configuration-management-software Expressions in provisioner blocks cannot refer to their parent resource by name.Is this true? True The \"self\" object represents the provisioner's parent resource, and has all of that resource's attributes. For example, use \"self.public_ip\" to reference an aws_instance 'public_ip' attribute. What does this symbol version = \"~> 1.0\" mean when defining versions? Any version more than 1.0 and less than 2.0. Terraform supports both cloud and on-premises infrastructure platforms. Is this true? True Terraform assumes an empty default configuration for any provider that is notexplicitly configured. A provider block can be empty. Is this true? True How do you configure the required version of Terraform CLI can be used withyour configuration? The required_version setting can be used to constrain which versions of the Terraform CLI can be used with your configuration. If the running version of Terraform does not match the constraints specified, Terraform will produce an error and exit without taking any further actions. Terraform CLI versions and provider versions are independent of each other. Isthis true? True You are configuring aws provider and it is always recommended to hard codeaws credentials in *.tf files. Is this true? False nightwolf recommends that you never hard-code credentials into \"*.tf\" configuration files. We are explicitly defining the default AWS config profile here to illustrate how Terraform should access sensitive credentials. If you leave out your AWS credentials, Terraform will automatically search for saved API credentials (for example, in ~/.aws/credentials ) or IAM instance profile credentials. This is cleaner when \".tf\" files are checked into source control or if there is more than one admin user You are provisioning the infrastructure with the command terraform apply andyou noticed one of the resources failed. How do you remove that resource withoutaffecting the whole infrastructure? You can taint the resource ans the next apply will destroy the resource terraform taint <resource.id>","title":"Top 250 Questions and Answers For Terraform Associate Certification"},{"location":"nightwolf-cotribution/yum_vs_apt/","text":"Package Management (yum vs apt): \uf0c1 Redhat and Ubuntu OS families have different package management tools like yum and apt. As an Administrator, we need to have the commands ready for any package mgmt related work for both the OS families. We have outlined few commands which comes handy in such scenarios. (adsbygoogle = window.adsbygoogle || []).push({}); General Packaging System Information \uf0c1 Redhat Family(yum) Ubuntu(apt) Package file extension *.rpm *.deb Repository location configuration /etc/yum.conf OR /etc/yum.repos.d/ /etc/apt/sources.list Adding, Removing and Upgrading Package \uf0c1 Task Redhat Family(yum) Ubuntu(apt) Refresh list of available packages Yum refreshes each time it's used apt-get update Install a package from a repository yum install package_name apt-get install package_name Install a package file yum install package.rpm OR rpm -i package.rpm dpkg --install package.deb Remove a package rpm -e package_name apt-get remove package_name Remove a package with configuration files yum remove package_name apt-get purge package_name Check for package upgrades yum check-update apt-get -s upgrade OR apt-get -s dist-upgrade Upgrade packages yum update OR rpm -Uvh [args] apt-get upgrade Upgrade the entire system yum upgrade apt-get dist-upgrade (adsbygoogle = window.adsbygoogle || []).push({}); Package Information \uf0c1 Task Redhat Family(yum) Ubuntu(apt) Get information about an available package yum search package_name apt-cache search package_name Show available packages yum list available apt-cache dumpavail List all installed packages yum list installed OR rpm -qa dpkg --list Get information about a package yum info package_name apt-cache show package_name Get information about an installed package rpm -qi package_name dpkg --status package_name List files in an installed package rpm -ql package_name dpkg --listfiles package_name List configuration files in an installed package rpm -qc package_name dpkg-query --show -f '${Conffiles}\\n' package_name Show the packages a given package depends on rpm -qR package_name apt-cache depends Show other packages that depend on a given package (reverse dependency) rpm -q --whatrequires [args] apt-cache rdepends Misc. Packaging System Tools \uf0c1 Task Redhat Family(yum) Ubuntu(apt) Show stats about the package cache - apt-cache stats Verify all installed packages rpm -Va debsums Remove packages from the local cache directory yum clean package apt-get clean Remove only obsolete packages from the local cache directory - apt-get autoclean Remove header files from the local cache directory (forcing a new download of same on next use) yum clean headers apt-file purge (adsbygoogle = window.adsbygoogle || []).push({}); Package File Information \uf0c1 Task Redhat Family(yum) Ubuntu(apt) Get information about a package file rpm -qpi package.rpm dpkg --info package.deb List files in a package file rpm -qpl package.rpm dpkg --contents package.deb List documentation files in a package file rpm -qpd package.rpm - List configuration files in a package file rpm -qpc package.rpm - Extract files in a package rpm2cpio package.rpm cpio -vid Find package that installed a file rpm -qf filename dpkg --search filename Find package that provides a particular file yum provides filename apt-file search filename (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Yum vs APT cheatsheet"},{"location":"nightwolf-cotribution/yum_vs_apt/#package-management-yum-vs-apt","text":"Redhat and Ubuntu OS families have different package management tools like yum and apt. As an Administrator, we need to have the commands ready for any package mgmt related work for both the OS families. We have outlined few commands which comes handy in such scenarios. (adsbygoogle = window.adsbygoogle || []).push({});","title":"Package Management (yum vs apt):"},{"location":"nightwolf-cotribution/yum_vs_apt/#general-packaging-system-information","text":"Redhat Family(yum) Ubuntu(apt) Package file extension *.rpm *.deb Repository location configuration /etc/yum.conf OR /etc/yum.repos.d/ /etc/apt/sources.list","title":"General Packaging System Information"},{"location":"nightwolf-cotribution/yum_vs_apt/#adding-removing-and-upgrading-package","text":"Task Redhat Family(yum) Ubuntu(apt) Refresh list of available packages Yum refreshes each time it's used apt-get update Install a package from a repository yum install package_name apt-get install package_name Install a package file yum install package.rpm OR rpm -i package.rpm dpkg --install package.deb Remove a package rpm -e package_name apt-get remove package_name Remove a package with configuration files yum remove package_name apt-get purge package_name Check for package upgrades yum check-update apt-get -s upgrade OR apt-get -s dist-upgrade Upgrade packages yum update OR rpm -Uvh [args] apt-get upgrade Upgrade the entire system yum upgrade apt-get dist-upgrade (adsbygoogle = window.adsbygoogle || []).push({});","title":"Adding, Removing and Upgrading Package"},{"location":"nightwolf-cotribution/yum_vs_apt/#package-information","text":"Task Redhat Family(yum) Ubuntu(apt) Get information about an available package yum search package_name apt-cache search package_name Show available packages yum list available apt-cache dumpavail List all installed packages yum list installed OR rpm -qa dpkg --list Get information about a package yum info package_name apt-cache show package_name Get information about an installed package rpm -qi package_name dpkg --status package_name List files in an installed package rpm -ql package_name dpkg --listfiles package_name List configuration files in an installed package rpm -qc package_name dpkg-query --show -f '${Conffiles}\\n' package_name Show the packages a given package depends on rpm -qR package_name apt-cache depends Show other packages that depend on a given package (reverse dependency) rpm -q --whatrequires [args] apt-cache rdepends","title":"Package Information"},{"location":"nightwolf-cotribution/yum_vs_apt/#misc-packaging-system-tools","text":"Task Redhat Family(yum) Ubuntu(apt) Show stats about the package cache - apt-cache stats Verify all installed packages rpm -Va debsums Remove packages from the local cache directory yum clean package apt-get clean Remove only obsolete packages from the local cache directory - apt-get autoclean Remove header files from the local cache directory (forcing a new download of same on next use) yum clean headers apt-file purge (adsbygoogle = window.adsbygoogle || []).push({});","title":"Misc. Packaging System Tools"},{"location":"nightwolf-cotribution/yum_vs_apt/#package-file-information","text":"Task Redhat Family(yum) Ubuntu(apt) Get information about a package file rpm -qpi package.rpm dpkg --info package.deb List files in a package file rpm -qpl package.rpm dpkg --contents package.deb List documentation files in a package file rpm -qpd package.rpm - List configuration files in a package file rpm -qpc package.rpm - Extract files in a package rpm2cpio package.rpm cpio -vid Find package that installed a file rpm -qf filename dpkg --search filename Find package that provides a particular file yum provides filename apt-file search filename (adsbygoogle = window.adsbygoogle || []).push({}); You may also refer to other interview preparation articles: Linux Interview Questions for Freshers Linux Interview Questions for Freshers - 2 Linux Interview Questions for Freshers and Experienced - L1 Linux Interview Questions for Experienced Linux Admins - L2 Advanced Linux Interview Questions for Experienced Admins - L3 Shell-Scripting interview questions OS Network Interview Questions GCP ACE Practice Questions - 1st GCP ACE Practice Questions - 2nd AWS Certified SysOps Administrator - Questions and Answers-1st AWS Certified SysOps Administrator - Questions and Answers-2nd AWS interview questions for experienced professionals - 1st AWS interview questions for experienced professionals - 2nd Ansible interview questions Kubernetes interview questions DevOps Interview Questions for Freshers and Experienced DevOps Interview Questions for Freshers and Experienced - 2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer Terraform Interview Questions for Experienced DevOps Engineer - 2 Terraform Interview Questions for Experienced DevOps Engineer - 3 JAVA Interview Questions JAVA Interview Questions - 2 Manual Testing Interview Questions for QA Engineers/Testers DBMS Interview Questions Managerial interview questions (adsbygoogle = window.adsbygoogle || []).push({}); You may also explore other articles in our bucket, Happy reading !!: What is Artificial Intelligence ? What is Machine Learning ? Unleashing the Power of Artificial Intelligence What is Neural Networks ? What is Internet of Things (IoT) ? What is Natural Language Processing (NLP) ? What is Robotics and Automation ? Quantum Computing: The Next Frontier Blockchain: A Primer for Beginners What is Virtual Reality (VR) ? Cybersecurity: Protecting our digital world What is AI Chat Open Assistant Chatbot ? What is Bard chatbot ? (adsbygoogle = window.adsbygoogle || []).push({});","title":"Package File Information"}]}