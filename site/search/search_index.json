{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"nightWolf Linux System Performance \uf0c1 System performance is one of the major and reoccurring issue in IT infrastructure and Linux performance troubleshooting is one of the most tedious task in system administration. We have consolidated few major issues and bottlenecks for you to understand the root cause of the performance issues. This Article includes: Troubleshooting Linux Perfomance isssues due to high CPU Usage Troubleshooting Linux Performance issues due to high Memory Usage Troubleshooting Linux Performance issues due to high Disk IO Usage Troubleshooting Linux OS Network Performance Issues Interview Preparations \uf0c1 We have created a small interview question database and preparation material which will help you to prepare for your interviews. Please utilise these questions and tips for your interview preparation. This Article include: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer Docker Interview Questions for DevOps Roles Interview materials Cheatsheets \uf0c1 Cheatsheets for different techs for your references: This include: Kubernetes cheatsheet GIT cheatsheet Linux YUM cheatsheet Linux \uf0c1 Linux OS Basics Filesystem Basics Packsage Management OS Networking and Security OS Security and Hardening RAIDs Database \uf0c1 mysql mongoDB Network \uf0c1 Contribute to nightWolf \uf0c1","title":"Home"},{"location":"#linux-system-performance","text":"System performance is one of the major and reoccurring issue in IT infrastructure and Linux performance troubleshooting is one of the most tedious task in system administration. We have consolidated few major issues and bottlenecks for you to understand the root cause of the performance issues. This Article includes: Troubleshooting Linux Perfomance isssues due to high CPU Usage Troubleshooting Linux Performance issues due to high Memory Usage Troubleshooting Linux Performance issues due to high Disk IO Usage Troubleshooting Linux OS Network Performance Issues","title":"Linux System Performance"},{"location":"#interview-preparations","text":"We have created a small interview question database and preparation material which will help you to prepare for your interviews. Please utilise these questions and tips for your interview preparation. This Article include: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer Docker Interview Questions for DevOps Roles Interview materials","title":"Interview Preparations"},{"location":"#cheatsheets","text":"Cheatsheets for different techs for your references: This include: Kubernetes cheatsheet GIT cheatsheet Linux YUM cheatsheet","title":"Cheatsheets"},{"location":"#linux","text":"Linux OS Basics Filesystem Basics Packsage Management OS Networking and Security OS Security and Hardening RAIDs","title":"Linux"},{"location":"#database","text":"mysql mongoDB","title":"Database"},{"location":"#network","text":"","title":"Network"},{"location":"#contribute-to-nightwolf","text":"","title":"Contribute to nightWolf"},{"location":"cheat/","text":"Kubernetes cheatsheet \uf0c1 GIT cheatsheet \uf0c1 Linux YUM cheatsheet \uf0c1","title":"Cheat"},{"location":"cheat/#kubernetes-cheatsheet","text":"","title":"Kubernetes cheatsheet"},{"location":"cheat/#git-cheatsheet","text":"","title":"GIT cheatsheet"},{"location":"cheat/#linux-yum-cheatsheet","text":"","title":"Linux YUM cheatsheet"},{"location":"contribute/","text":"Your contributions and feedbacks are highly appreciated. This will help us to help people like you to enhance their knowledge and score in their current and future roles. \uf0c1 You can contribute through nightwolf-cotribution github repo page by updating the content and creating a pull request. That pull request will be reviewd thoroughly and then committed, which will display on our website nightwolf.in. You can use below command to contribute through git cli: mkdir nightwolf cd nightwolf git clone git@github.com:v-nightwolf/nightwolf-cotribution.git You will start seeing multiple files there. Contribute in appropriate file and commit & push your changes back to git repo. To read more about how to work with remote repost, please read the Article . Otherwise you can submit your suggestions through below form. \uf0c1 Loading\u2026","title":"Contribute"},{"location":"contribute/#your-contributions-and-feedbacks-are-highly-appreciated-this-will-help-us-to-help-people-like-you-to-enhance-their-knowledge-and-score-in-their-current-and-future-roles","text":"You can contribute through nightwolf-cotribution github repo page by updating the content and creating a pull request. That pull request will be reviewd thoroughly and then committed, which will display on our website nightwolf.in. You can use below command to contribute through git cli: mkdir nightwolf cd nightwolf git clone git@github.com:v-nightwolf/nightwolf-cotribution.git You will start seeing multiple files there. Contribute in appropriate file and commit & push your changes back to git repo. To read more about how to work with remote repost, please read the Article .","title":"Your contributions and feedbacks are highly appreciated. This will help us to help people like you to enhance their knowledge and score in their current and future roles."},{"location":"contribute/#otherwise-you-can-submit-your-suggestions-through-below-form","text":"Loading\u2026","title":"Otherwise you can submit your suggestions through below form."},{"location":"db/","text":"mysql \uf0c1 mongoDB \uf0c1","title":"Db"},{"location":"db/#mysql","text":"","title":"mysql"},{"location":"db/#mongodb","text":"","title":"mongoDB"},{"location":"interview/","text":"Linux Interview Questions for Freshers \uf0c1 Linux Interview Questions for Freshers-2 \uf0c1 Linux L1 interview Questions \uf0c1 Linux L2 interview Questions \uf0c1 Linux L3 interview Questions \uf0c1 OS Network interview Questions \uf0c1 AWS Interview Questions \uf0c1 DevOps Interview Questions for Freshers and Experianced \uf0c1 DevOps Interview Questions for Freshers and Experianced-2 \uf0c1 GIT Interview Questions \uf0c1 Jenkins Interview Questions \uf0c1 Docker Interview Questions \uf0c1 Azure Interview Questions \uf0c1 Terraform Interview Questions \uf0c1","title":"Interview"},{"location":"interview/#linux-interview-questions-for-freshers","text":"","title":"Linux Interview Questions for Freshers"},{"location":"interview/#linux-interview-questions-for-freshers-2","text":"","title":"Linux Interview Questions for Freshers-2"},{"location":"interview/#linux-l1-interview-questions","text":"","title":"Linux L1 interview Questions"},{"location":"interview/#linux-l2-interview-questions","text":"","title":"Linux L2 interview Questions"},{"location":"interview/#linux-l3-interview-questions","text":"","title":"Linux L3 interview Questions"},{"location":"interview/#os-network-interview-questions","text":"","title":"OS Network interview Questions"},{"location":"interview/#aws-interview-questions","text":"","title":"AWS Interview Questions"},{"location":"interview/#devops-interview-questions-for-freshers-and-experianced","text":"","title":"DevOps Interview Questions for Freshers and Experianced"},{"location":"interview/#devops-interview-questions-for-freshers-and-experianced-2","text":"","title":"DevOps Interview Questions for Freshers and Experianced-2"},{"location":"interview/#git-interview-questions","text":"","title":"GIT Interview Questions"},{"location":"interview/#jenkins-interview-questions","text":"","title":"Jenkins Interview Questions"},{"location":"interview/#docker-interview-questions","text":"","title":"Docker Interview Questions"},{"location":"interview/#azure-interview-questions","text":"","title":"Azure Interview Questions"},{"location":"interview/#terraform-interview-questions","text":"","title":"Terraform Interview Questions"},{"location":"mongo/","text":"mongoDB \uf0c1 MongoDb University is a good place to start.","title":"mongoDB"},{"location":"mongo/#mongodb","text":"MongoDb University is a good place to start.","title":"mongoDB"},{"location":"mysql/","text":"MySql Tuner \uf0c1 Below script can help you in tunning your MYSQL database for best performance. Please note that do not blindly trust what this script says. Always think before you impliment the recommendations. wget https://raw.github.com/major/MySQLTuner-perl/master/mysqltuner.pl; chmod +x mysqltuner.pl; ./mysqltuner.pl The output of this script will look like: [root@linux~]# ./mysqltuner.pl >> MySQLTuner 1.4.0 - Major Hayden <major@mhtx.net> >> Bug reports, feature requests, and downloads at http://mysqltuner.com/ >> Run with '--help' for additional options and output filtering [OK] Currently running supported MySQL version 5.1.73 [OK] Operating on 64-bit architecture -------- Storage Engine Statistics ------------------------------------------- [--] Status: +CSV +InnoDB +MRG_MYISAM [!!] InnoDB is enabled but isn't being used [OK] Total fragmented tables: 0 -------- Security Recommendations ------------------------------------------- [!!] User '@bash-test' has no password set. [!!] User '@localhost' has no password set. -------- Performance Metrics ------------------------------------------------- [--] Up for: 44m 11s (13 q [0.005 qps], 7 conn, TX: 16K, RX: 662) [--] Reads / Writes: 100% / 0% [--] Total buffers: 34.0M global + 2.7M per thread (1000 max threads) [!!] Maximum possible memory usage: 2.7G (280% of installed RAM) [OK] Slow queries: 0% (0/13) [OK] Highest usage of available connections: 0% (1/1000) [OK] Key buffer size / total MyISAM indexes: 8.0M/90.0K [!!] Query cache is disabled [OK] Temporary tables created on disk: 0% (0 on disk / 3 total) [!!] Thread cache is disabled [OK] Table cache hit rate: 53% (8 open / 15 opened) [OK] Open file limit used: 0% (16/5K) [OK] Table locks acquired immediately: 100% (18 immediate / 18 locks) -------- Recommendations ----------------------------------------------------- General recommendations: Add skip-innodb to MySQL configuration to disable InnoDB MySQL started within last 24 hours - recommendations may be inaccurate Reduce your overall MySQL memory footprint for system stability Enable the slow query log to troubleshoot bad queries Set thread_cache_size to 4 as a starting value Variables to adjust: *** MySQL's maximum memory usage is dangerously high *** *** Add RAM before increasing MySQL buffer variables *** query_cache_size (>= 8M) thread_cache_size (start at 4)","title":"Mysql"},{"location":"mysql/#mysql-tuner","text":"Below script can help you in tunning your MYSQL database for best performance. Please note that do not blindly trust what this script says. Always think before you impliment the recommendations. wget https://raw.github.com/major/MySQLTuner-perl/master/mysqltuner.pl; chmod +x mysqltuner.pl; ./mysqltuner.pl The output of this script will look like: [root@linux~]# ./mysqltuner.pl >> MySQLTuner 1.4.0 - Major Hayden <major@mhtx.net> >> Bug reports, feature requests, and downloads at http://mysqltuner.com/ >> Run with '--help' for additional options and output filtering [OK] Currently running supported MySQL version 5.1.73 [OK] Operating on 64-bit architecture -------- Storage Engine Statistics ------------------------------------------- [--] Status: +CSV +InnoDB +MRG_MYISAM [!!] InnoDB is enabled but isn't being used [OK] Total fragmented tables: 0 -------- Security Recommendations ------------------------------------------- [!!] User '@bash-test' has no password set. [!!] User '@localhost' has no password set. -------- Performance Metrics ------------------------------------------------- [--] Up for: 44m 11s (13 q [0.005 qps], 7 conn, TX: 16K, RX: 662) [--] Reads / Writes: 100% / 0% [--] Total buffers: 34.0M global + 2.7M per thread (1000 max threads) [!!] Maximum possible memory usage: 2.7G (280% of installed RAM) [OK] Slow queries: 0% (0/13) [OK] Highest usage of available connections: 0% (1/1000) [OK] Key buffer size / total MyISAM indexes: 8.0M/90.0K [!!] Query cache is disabled [OK] Temporary tables created on disk: 0% (0 on disk / 3 total) [!!] Thread cache is disabled [OK] Table cache hit rate: 53% (8 open / 15 opened) [OK] Open file limit used: 0% (16/5K) [OK] Table locks acquired immediately: 100% (18 immediate / 18 locks) -------- Recommendations ----------------------------------------------------- General recommendations: Add skip-innodb to MySQL configuration to disable InnoDB MySQL started within last 24 hours - recommendations may be inaccurate Reduce your overall MySQL memory footprint for system stability Enable the slow query log to troubleshoot bad queries Set thread_cache_size to 4 as a starting value Variables to adjust: *** MySQL's maximum memory usage is dangerously high *** *** Add RAM before increasing MySQL buffer variables *** query_cache_size (>= 8M) thread_cache_size (start at 4)","title":"MySql Tuner"},{"location":"new/","text":"What is localhost and why would ping localhost fail? What is the similarity between \"ping\" & \"traceroute\" ? How is traceroute able to find the hops. What is the command used to show all open ports and/or socket connections on a machine? Is 300.168.0.123 a valid IPv4 address? Which IP ranges/subnets are \"private\" or \"non-routable\" (RFC 1918)? What is a VLAN? What is ARP and what is it used for? What is the difference between TCP and UDP? What is the purpose of a default gateway? What is command used to show the routing table on a Linux box? A TCP connection on a network can be uniquely defined by 4 things. What are those things? When a client running a web browser connects to a web server, what is the source port and what is the destination port of the connection? How do you add an IPv6 address to a specific interface? You have added an IPv4 and IPv6 address to interface eth0. A ping to the v4 address is working but a ping to the v6 address gives you the response sendmsg: operation not permitted. What could be wrong? What is SNAT and when should it be used? Explain how could you ssh login into a Linux system that DROPs all new incoming packets using a SSH tunnel. How do you stop a DDoS attack? How can you see content of an ip packet? What is IPoAC (RFC 1149)? What will happen when you bind port 0?","title":"New"},{"location":"reference/","text":"References \uf0c1 Some questions are 'borrowed' from other great references like: https://github.com/darcyclarke/Front-end-Developer-Interview-Questions https://github.com/kylejohnson/linux-sysadmin-interview-questions/blob/master/test.md http://slideshare.net/kavyasri790693/linux-admin-interview-questions","title":"Reference"},{"location":"reference/#references","text":"Some questions are 'borrowed' from other great references like: https://github.com/darcyclarke/Front-end-Developer-Interview-Questions https://github.com/kylejohnson/linux-sysadmin-interview-questions/blob/master/test.md http://slideshare.net/kavyasri790693/linux-admin-interview-questions","title":"References"},{"location":"nightwolf-cotribution/","text":"nightwolf-cotribution \uf0c1","title":"nightwolf-cotribution"},{"location":"nightwolf-cotribution/#nightwolf-cotribution","text":"","title":"nightwolf-cotribution"},{"location":"nightwolf-cotribution/aws/","text":"AWS Certified SysOps Administrator - Questions and answers \uf0c1 These are AWS interview questions for experianced professionals. You will find these questions very helpful in your AWS professional role interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. QUESTION NO: 1 You are currently hosting multiple applications in a VPC and have logged numerous port scans coming in from a specific IP address block. Your security team has requested that all access from the offending IP address block be denied tor the next 24 hours. Which of the following is the best method to quickly and temporarily deny access from the specified IP address block? A. Create an AD policy to modify Windows Firewall settings on all hosts in the VPC to deny access from the IP address block B. Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP address block C. Add a rule to all of the VPC 5 Security Groups to deny access from the IP address block D. Modify the Windows Firewall settings on all Amazon Machine Images (AMIs) that your organization uses in that VPC to deny access from the IP address block Answer: C Reference: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html QUESTION NO: 2 When preparing for a compliance assessment of your system built inside of AWS. what are three best practices for you to prepare for anaudit? Choose any 3 answers. A. Gather evidence of your IT operational controls B. Request and obtain applicable third-party audited AWS compliance reports and certifications C. Request and obtain a compliance and security tour of an AWS data center for a pre-assessment security review D. Request and obtain approval from AWS to perform relevant network scans and indepth penetration tests of your system and endpoints E. Schedule meetings with AWS third-party auditors to provide evidence of AWS compliance that maps to your control objectives Answer: B,D,E QUESTION NO: 3 You have started a new job and are reviewing your company infrastructure on AWS You notice one web application where they have an Elastic Load Balancer (&B) in front of web instances in an Auto Scaling Group. When you check the metrics for the ELB in CloudWatch you see four healthy instances In Availability Zone (AZ) A and zero in AZ B There are zero unhealthy instances. What do you need to fix to balance the instances across AZs? A. Set the ELB to only be attached to another AZ B. Make sure Auto Scaling is configured to launch in both AZs C. Make sure your AMI is available in both AZs D. Make sure the maximum size of the Auto Scaling Group is greater than 4 Answer: B QUESTION NO: 4 You have been asked to leverage Amazon VPC BC2 and SOS to implement an application that submits and receives millions of messages per second to a message queue. You want to ensure your application has sufficient bandwidth between your EC2 instances and SQS Which option will provide (he most scalable solution for bandwidth between the application and SOS? A. Ensure the application instances are properly configured with an Elastic Load Balancer. B. Ensure the application instances are launched in private subnets with the EBS-optimized option enabled. C. Ensure the application instances are launched in public subnets with the associate-publicIP-address=true option enabled D. Launch application instances in private subnets with an Auto Scaling group and Auto Scaling triggers configured to watch the SOS queue size Answer: C Reference: http://www.cardinalpath.com/autoscaling-your-website-with-amazon-web-services-part-2/ QUESTION NO: 5 You have identified network throughput as a bottleneck on your ml small EC2 instance when uploading data into Amazon S3 In the same region. How do you remedy this situation? A. Add an additional ENI B. Change to a larger Instance C. Use DirectConnect between EC2 and S3 D. Use EBS PIOPS on the local volume Answer: B Reference: https://media.amazonwebservices.com/AWS_Amazon_EMR_Best_Practices.pdf QUESTION NO: 6 When attached to an Amazon VPC which two components provide connectivity with external networks? Choose 2 answers A. Elastic IPS (EIP) B. NAT Gateway (NAT) C. Internet Gateway {IGW) D. Virtual Private Gateway (VGW) Answer: C,D QUESTION NO: 7 Your application currently leverages AWS Auto Scaling to grow and shrink as load Increases/decreases and has been performing well. Your marketing team expects a steady ramp up in traffic to follow an upcoming campaign that will result in a 20x growth in traffic over 4 weeks. Your forecast for the approximate number of Amazon EC2 instances necessary to meet the peak demand is 175. What should you do to avoid potential service disruptions during the ramp up in traffic? 456789 A. Ensure that you have pre-allocated 175 Elastic IP addresses so that each server will be able to obtain one as it launches B. Check the service limits in Trusted Advisor and adjust as necessary so the forecasted count remains within limits. C. Change your Auto Scaling configuration to set a desired capacity of 175 prior to the launch of the marketing campaign. D. Pre-warm your Elastic Load Balancer to match the requests per second anticipated during peak demand prior to the marketing campaign. Answer: D QUESTION NO: 8 You have an Auto Scaling group associated with an Elastic Load Balancer (ELB). You have noticed that instances launched via the Auto Scaling group are being marked unhealthy due to an ELB health check, but these unhealthy instances are not being terminated. What do you need to do to ensure trial instances marked unhealthy by the ELB will be terminated and replaced? A. Change the thresholds set on the Auto Scaling group health check B. Add an Elastic Load Balancing health check to your Auto Scaling group C. Increase the value for the Health check interval set on the Elastic Load Balancer D. Change the health check set on the Elastic Load Balancer to use TCP rather than HTTP checks Answer: B Reference: http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/as-add-elb-healthcheck.html Add an Elastic Load Balancing Health Check to your Auto Scaling GroupBy default, an Auto Scaling group periodically reviews the results of EC2 instance status to determine the health state of each instance. However, if you have associated your Auto Scaling group with an Elastic Load Balancing load balancer, you can choose to use the Elastic Load Balancing health check. In this case, Auto Scaling determines the health status of your instances by checking the results of both the EC2 instance status check and the Elastic Load Balancing instance health check. For information about EC2 instance status checks, see Monitor Instances With Status Checks in the Amazon EC2 User Guide for Linux Instances. For information about Elastic Load Balancing health checks, see Health Check in the Elastic Load Balancing Developer Guide. This topic shows you how to add an Elastic Load Balancing health check to your Auto Scaling group, assuming that you have created a load balancer and have registered the load balancer with your Auto Scaling group. If you have not registered the load balancer with your Auto Scaling group, see Set Up a Scaled and Load-Balanced Application. Auto Scaling marks an instance unhealthy if the calls to the Amazon EC2 action DescribeInstanceStatus return any state other than running, the system status shows impaired, or the calls to Elastic Load Balancing action DescribeInstanceHealth returns OutOfService in the instance state field. If there are multiple load balancers associated with your Auto Scaling group, Auto Scaling checks the health state of your EC2 instances by making health check calls to each load balancer. For each call, if the Elastic Load Balancing action returns any state other than InService, the instance is marked as unhealthy. After Auto Scaling marks an instance as unhealthy, it remains in that state, even if subsequent calls from other load balancers return an InService state for the same instance. QUESTION NO: 9 Which two AWS services provide out-of-the-box user configurable automatic backup-as-a-service and backup rotation options? Choose any 2 answers. A. Amazon S3 B. Amazon RDS C. Amazon EBS D. Amazon Red shift Answer: C,D QUESTION NO: 10 An organization has configured a VPC with an Internet Gateway (IGW). pairs of public and private subnets (each with one subnet per Availability Zone), and an Elastic Load Balancer (ELB) configured to use the public subnets. The application web tier leverages the ELB, Auto Scaling and a mum-AZ RDS database instance. The Organization would like to eliminate any potential single points ft failure in this design. What step should you take to achieve this organization's objective? A. Nothing, there are no single points of failure in this architecture. B. Create and attach a second IGW to provide redundant internet connectivity. C. Create and configure a second Elastic Load Balancer to provide a redundant load balancer. D. Create a second multi-AZ RDS instance in another Availability Zone and configurereplication to provide a redundant database. Answer: C QUESTION NO: 11 Which of the following are characteristics of Amazon VPC subnets? Choose any 2 answers. A. Each subnet maps to a single Availability Zone B. A CIDR block mask of /25 is the smallest range supported C. Instances in a private subnet can communicate with the internet only if they have an Elastic IP. D. By default, all subnets can route between each other, whether they are private or public E. V Each subnet spans at least 2 Availability zones to provide a high-availability environment. Answer: C,E QUESTION NO: 12 You are creating an Auto Scaling group whose Instances need to insert a custom metric into CloudWatch. Which method would be the best way to authenticate your CloudWatch PUT request? A. Create an IAM role with the Put MetricData permission and modify the Auto Scaling launch configuration to launch instances in that role. B. Create an IAM user with the PutMetricData permission and modify the Auto Scaling launch configuration to inject the userscredentials into the instance User Data. C. Modify the appropriate Cloud Watch metric policies to allow the Put MetricData permission to instances from the Auto Scaling group. D. Create an IAM user with the PutMetricData permission and put the credentials in a private repository and have applications on the server pull the credentials as needed. Answer: B QUESTION NO: 13 When an EC2 instance that is backed by an S3-based AMI Is terminated, what happens to the data on me root volume? A. Data is automatically saved as an E8S volume. B. Data is automatically saved as an ESS snapshot. C. Data is automatically deleted. D. Data is unavailable until the instance is restarted. Answer: D Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html QUESTION NO: 14 You have a web application leveraging an Elastic Load Balancer (ELB) In front of the web servers deployed using an Auto Scaling Group Your database is running on Relational Database Service (RDS) The application serves out technical articles and responses to them in general there are more views of an article than there are responses to the article. On occasion, an article on the site becomes extremely popular resulting in significant traffic Increases that causes the site to go down. What could you do to help alleviate the pressure on the infrastructure while maintaining availability during these events? Choose 3 answers A. Leverage CloudFront for the delivery of the articles. B. Add RDS read-replicas for the read traffic going to your relational database C. Leverage ElastiCache for caching the most frequently used data. D. Use SOS to queue up the requests for the technical posts and deliver them out of the queue. E. Use Route53 health checks to fail over to an S3 bucket for an error page. Answer: A,C,E QUESTION NO: 15 The majority of your Infrastructure is on premises and you have a small footprint on AWS Your company has decided to roll out a new application that is heavily dependent on low latency connectivity to LOAP for authentication Your security policy requires minimal changes to the company's existing application user management processes. What option would you implement to successfully launch this application1? A. Create a second, independent LOAP server in AWS for your application to use for authentication B. Establish a VPN connection so your applications can authenticate against your existing on-premises LDAP servers C. Establish a VPN connection between your data center and AWS create a LDAP replica on AWS and configure your application to use the LDAP replica for authentication D. Create a second LDAP domain on AWS establish a VPN connection to establish a trust relationship between your new and existing domains and use the new domain for authentication Answer: D Reference: http://msdn.microsoft.com/en-us/library/azure/jj156090.aspx QUESTION NO: 16 You need to design a VPC for a web-application consisting of an Elastic Load Balancer (ELB). a fleet of web/application servers, and an RDS database The entire Infrastructure must be distributed over 2 availability zones. Which VPC configuration works while assuring the database is not available from the Internet? A. One public subnet for ELB one public subnet for the web-servers, and one private subnet for the database B. One public subnet for ELB two private subnets for the web-servers, two private subnets for RDS C. Two public subnets for ELB two private subnets for the web-servers and two private subnets for RDS D. Two public subnets for ELB two public subnets for the web-servers, and two public subnets for RDS Answer: A QUESTION NO: 17 An application that you are managing has EC2 instances & Dynamo OB tables deployed to several AWS Regions In order to monitor the performance of the application globally, you would like to see two graphs 1) Avg CPU Utilization across all EC2 instances and 2) Number of Throttled Requests for all DynamoDB tables. How can you accomplish this? A. Tag your resources with the application name, and select the tag name as the dimension in the Cloudwatch Management console to view the respective graphs B. Use the Cloud Watch CLI tools to pull the respective metrics from each regional endpoint Aggregate the data offline & store it for graphing in CloudWatch. C. Add SNMP traps to each instance and DynamoDB table Leverage a central monitoring server to capture data from each instance and table Put the aggregate data into Cloud Watch for graphing. D. Add a CloudWatch agent to each instance and attach one to each DynamoDB table. When configuring the agent set the appropriate application name & view the graphs in CloudWatch. Answer: C QUESTION NO: 18 When assessing an organization s use of AWS API access credentials which of the following three credentials should be evaluated? Choose 3 answers A. Key pairs B. Console passwords 10 C. Access keys D. Signing certificates E. Security Group memberships Answer: A,C,D Reference: http://media.amazonwebservices.com/AWS_Operational_Checklists.pdf QUESTION NO: 19 You have a Linux EC2 web server instance running inside a VPC The instance is In a public subnet and has an EIP associated with it so you can connect to It over the Internet via HTTP or SSH The instance was also fully accessible when you last logged in via SSH. and was also serving web requests on port 80. Now you are not able to SSH into the host nor does it respond to web requests on port 80 that were working fine last time you checked You have double-checked that all networking configuration parameters (security groups route tables. IGW'EIP. NACLs etc) are properly configured {and you haven\u2019t made any changes to those anyway since you were last able to reach the Instance). You look at the EC2 console and notice that system status check shows \"impaired.\" Which should be your next step in troubleshooting and attempting to get the instance back to a healthy state so that you can log in again? A. Stop and start the instance so that it will be able to be redeployed on a healthy host system that most likely will fix the \"impaired\" system status B. Reboot your instance so that the operating system will have a chance to boot in a clean healthy state that most likely will fix the 'impaired\" system status C. Add another dynamic private IP address to me instance and try to connect via mat new path, since the networking stack of the OS may be locked up causing the \u201cimpaired\u201d system status. D. Add another Elastic Network Interface to the instance and try to connect via that new path since the networking stack of the OS may be locked up causing the \"impaired\" system status E. un-map and then re-map the EIP to the instance, since the IGWVNAT gateway may not be working properly, causing the \"impaired\" system status Answer: B QUESTION NO: 20 What is a placement group? A. A collection of Auto Scaling groups in the same Region B. Feature that enables EC2 instances to interact with each other via nigh bandwidth, low latency connections C. A collection of Elastic Load Balancers in the same Region or Availability Zone D. A collection of authorized Cloud Front edge locations for a distribution Answer: C Reference: http://aws.amazon.com/ec2/faqs/ QUESTION NO: 21 Your entire AWS infrastructure lives inside of one Amazon VPC You have an Infrastructure monitoring application running on an Amazon instance in Availability Zone (AZ) A of the region, and another application instance running in AZ B. The monitoring application needs to make use of ICMP ping to confirm network reachability of the instance hosting the application. Can you configure the security groups for these instances to only allow the ICMP ping to pass from the monitoringinstance to the application instance and nothing else\" If so how? A. No Two instances in two different AZ's can't talk directly to each other via ICMP ping as that protocol is not allowed across subnet (iebroadcast) boundaries B. Yes Both the monitoring instance and the application instance have to be a part of the same security group, and that security group needs to allow inbound ICMP C. Yes, The security group for the monitoring instance needs to allow outbound ICMP and the application instance's security group needs to allow Inbound ICMP D. Yes, Both the monitoring instance's security group and the application instance's security group need to allow both inbound and outbound ICMP ping packets since ICMP is not a connection-oriented protocol Answer: D QUESTION NO: 22 You have two Elastic Compute Cloud (EC2) instances inside a Virtual Private Cloud (VPC) in the same Availability Zone (AZ) but in different subnets.One instance is running a database and the other instance an application that will interface with the database. You want to confirm that they can talk to each other for your application to work properly. Which two things do we need to confirm in the VPC settings so that these EC2 instances can communicate inside the VPC? Choose 2 answers A. A network ACL that allows communication between the two subnets. B. Both instances are the same instance class and using the same Key-pair. C. That the default route is set to a NAT instance or internet Gateway (IGW) for them to communicate. D. Security groups are set to allow the application host to talk to the database on the right port/protocol. Answer: A,C QUESTION NO: 23 Which services allow the customer to retain full administrative privileges of the underlying EC2 instances? Choose 2 answers A. Amazon Elastic Map Reduce B. Elastic Load Balancing C. AWS Elastic Beanstalk D. I\" Amazon Elasticache E. Amazon Relational Database service Answer: B,C QUESTION NO: 24 You have a web-style application with a stateless but CPU and memory-intensive web tier running on a cc2 8xlarge EC2 instance inside of a VPC The instance when under load is having 134 problems returning requests within the SLA as defined by your business The application maintains its state in a DynamoDB table, but the data tier is properly provisioned and responses are consistently fast. How can you best resolve the issue of the application responses not meeting your SLA? A. Add another cc2 8xlarge application instance, and put both behind an Elastic Load Balancer B. Move the cc2 8xlarge to the same Availability Zone as the DynamoDB table C. Cache the database responses in ElastiCache for more rapid access D. Move the database from DynamoDB to RDS MySQL in scale-out read-replica configuration Answer: B Reference: http://aws.amazon.com/elasticmapreduce/faqs/ QUESTION NO: 25 You are managing a legacy application Inside VPC with hard coded IP addresses in its configuration. Which two mechanisms will allow the application to failover to new instances without the need for reconfiguration? Choose 2 answers A. Create an ELB to reroute traffic to a failover instance B. Create a secondary ENI that can be moved to a failover instance C. Use Route53 health checks to fail traffic over to a failover instance D. Assign a secondary private IP address to the primary ENIO that can De moved to a failover instance Answer: A,D QUESTION NO: 26 You are designing a system that has a Bastion host. This component needs to be highly available without human intervention. Which of the following approaches would you select? A. Run the bastion on two instances one in each AZ B. Run the bastion on an active Instance in one AZ and have an AMI ready to boot up in the event of failure C. Configure the bastion instance in an Auto Scaling group Specify the Auto Scaling group to include multiple AZs but have a min-size of 1 and max-size of 1 D. Configure an ELB in front of the bastion instance Answer: C QUESTION NO: 27 Which of the following statements about this S3 bucket policy is true? 15 A. Denies the server with the IP address 192 168 100 0 full access to the \"mybucket\" bucket B. Denies the server with the IP address 192 168 100 188 full access to the \"mybucket\" bucket C. Grants all the servers within the 192 168 100 0/24 subnet full access to the \"mybucket\" bucket D. Grants all the servers within the 192 168 100 188/32 subnet full access to the \"mybucket\" bucket Answer: C QUESTION NO: 28 Which of the following requires a custom CloudWatch metric to monitor? A. Data transfer of an EC2 instance B. Disk usage activity of an EC2 instance C. Memory Utilization of an EC2 instance D. CPU Utilization of an EC2mstance Answer: B Reference: http://aws.amazon.com/cloudwatch/ QUESTION NO: 29 You run a web application where web servers on EC2 Instances are In an Auto Scaling group Monitoring over the last 6 months shows that 6 web servers are necessary to handle the minimum load During the day up to 12 servers are needed Five to six days per year, the number of web servers required might go up to 15. What would you recommend to minimize costs while being able to provide hill availability? A. 6 Reserved instances (heavy utilization). 6 Reserved instances {medium utilization), rest covered by On-Demand instances B. 6 Reserved instances (heavy utilization). 6 On-Demand instances, rest covered by Spot Instances C. 6 Reserved instances (heavy utilization) 6 Spot instances, rest covered by OnDemand instances D. 6 Reserved instances (heavy utilization) 6 Reserved instances (medium utilization) rest covered by Spot instances 1678 Answer: C QUESTION NO: 30 You have been asked to propose a multi-region deployment of a web-facing application where a controlled portion of your traffic is being processed by an alternate region. Which configuration would achieve that goal? A. Route53 record sets with weighted routing policy B. Route53 record sets with latency based routing policy C. Auto Scaling with scheduled scaling actions set D. Elastic Load Balancing with health checks enabled Answer: D Reference: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/TerminologyandKeyConcepts.html QUESTION NO: 31 You have set up Individual AWS accounts for each project. You have been asked to make sure your AWS Infrastructure costs do not exceed the budget set per project for each month. Which of the following approaches can help ensure that you do not exceed the budget each month? A. Consolidate your accounts so you have a single bill for all accounts and projects B. Set up auto scaling with CloudWatch alarms using SNS to notify you when you are running too many Instances in a given account C. Set up CloudWatch billing alerts for all AWS resources used by each project, with a notification occurring when the amount for each resource tagged to a particular project matches the budget allocated to the project. D. Set up CloudWatch billing alerts for all AWS resources used by each account, with email notifications when it hits 50%. 80% and 90% of its budgeted monthly spend Answer: C QUESTION NO: 32 When creation of an EBS snapshot Is initiated but not completed the EBS volume? A. Cannot De detached or attached to an EC2 instance until me snapshot completes B. Can be used in read-only mode while me snapshot is in progress C. Can be used while me snapshot Is in progress D. Cannot be used until the snapshot completes Answer: C Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html QUESTION NO: 33 You are using ElastiCache Memcached to store session state and cache database queries in your infrastructure You notice in Cloud Watch that Evictions and GetMisses are Doth very high. What two actions could you take to rectify this? Choose 2 answers A. Increase the number of nodes in your cluster B. Tweak the max-item-size parameter C. Shrink the number of nodes in your cluster D. Increase the size of the nodes in the duster Answer: B,D QUESTION NO: 34 You are running a database on an EC2 instance, with the data stored on Elastic Block Store (EBS) for persistence At times throughout the day, you are seeing large variance in the response times of the database queries Looking into the instance with the isolate command you see a lot of wait time on the disk volume that the database's data is stored on. What two ways can you improve the performance of the database's storage while maintaining the current persistence of the data? Choose 2 answers A. Move to an SSD backed instance B. Move the database to an EBS-Optimized Instance C. T Use Provisioned IOPs EBS D. Use the ephemeral storage on an m2 4xiarge Instance Instead Answer: A,B QUESTION NO: 35 Your EC2-Based Multi-tier application includes a monitoring instance that periodically makes application -level read only requests of various application components and if any of those fail more than three times 30 seconds calls CloudWatch lo fire an alarm, and the alarm notifies your operations team by email and SMS of a possible application health problem. However, you also need to watch the watcher -the monitoring instance itself - and be notified if it becomes unhealthy. Which of the following Is a simple way to achieve that goal? A. Run another monitoring instance that pings the monitoring instance and fires a could watch alarm mat notifies your operations teamshould the primary monitoring instance become unhealthy. B. Set a Cloud Watch alarm based on EC2 system and instance status checks and have the alarm notify your operations team of anydetected problem with the monitoring instance. C. Set a Cloud Watch alarm based on the CPU utilization of the monitoring instance and nave the alarm notify your operations team if C r the CPU usage exceeds 50% few more than one minute: then have your monitoring application go into a CPU-bound loop should itDetect any application problems. D. Have the monitoring instances post messages to an SOS queue and then dequeue those messages on another instance should D c- the queue cease to have new messages, the second instance should first terminate the original monitoring instance start anotherbackup monitoring instance and assume (he role of the previous monitoring instance and beginning adding messages to the SOSqueue. 19 Answer: D QUESTION NO: 36 You have decided to change the Instance type for instances running In your application tier that are using Auto Scaling. In which area below would you change the instance type definition? A. Auto Scaling launch configuration B. Auto Scaling group C. Auto Scaling policy D. Auto Scaling tags Answer: B Reference: http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/WhatIsAutoScaling.html QUESTION NO: 37 You are attempting to connect to an instance in Amazon VPC without success You have already verified that the VPC has an Internet Gateway (IGW) the instance has an associated Elastic IP (EIP) and correct security group rules are in place. Which VPC component should you evaluate next? A. The configuration of a MAT instance B. The configuration of the Routing Table C. The configuration of the internet Gateway (IGW) D. The configuration of SRC/DST checking Answer: C Reference: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/UserScenariosForVPC.html QUESTION NO: 38 You are tasked with the migration of a highly trafficked Node JS application to AWS In order to comply with organizational standards Chef recipes must be used to configure the application servers that host this application and to support application lifecycle events. Which deployment option meets these requirements while minimizing administrative burden? A. Create a new stack within Opsworks add the appropriate layers to the stack and deploy the application B. Create a new application within Elastic Beanstalk and deploy this application to a new environment C. Launch a Mode JS server from a community AMI and manually deploy the application to the launched EC2 instance D. Launch and configure Chef Server on an EC2 instance and leverage the AWS CLI to launch application servers and configure those instances using Chef. Answer: B Reference: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deployment.html QUESTION NO: 39 You have been asked to automate many routine systems administrator backup and recovery activities Your current plan is to leverage AWS-managed solutions as much as possible and automate the rest with the AWS CU and scripts. Which task would be best accomplished with a script? A. Creating daily EBS snapshots with a monthly rotation of snapshots B. Creating daily ROS snapshots with a monthly rotation of snapshots C. Automatically detect and stop unused or underutilized EC2 instances D. Automatically add Auto Scaled EC2 instances to an Amazon Elastic Load Balancer Answer: C QUESTION NO: 40 Your organization's security policy requires that all privileged users either use frequently rotated passwords or one-time access credentials in addition to username/password. Which two of the following options would allow an organization to enforce this policy for AWS users? Choose 2 answers A. Configure multi-factor authentication for privileged 1AM users B. Create 1AM users for privileged accounts C. Implement identity federation between your organization's Identity provider leveraging the 1AM Security Token Service D. Enable the 1AM single-use password policy option for privileged users Answer: C,D QUESTION NO: 41 What are characteristics of Amazon S3? Choose 2 answers A. Objects are directly accessible via a URL B. S3 should be used to host a relational database C. S3 allows you to store objects or virtually unlimited size D. S3 allows you to store virtually unlimited amounts of data E. S3 offers Provisioned IOPS Answer: B,C QUESTION NO: 42 You receive a frantic call from a new DBA who accidentally dropped a table containing all your customers. Which Amazon RDS feature will allow you to reliably restore your database to within 5 minutes of when the mistake was made? A. Multi-AZ RDS B. RDS snapshots C. RDS read replicas D. RDS automated backup Answer: B Reference: http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.BackingUpAndRestoringAmazonRDSInstances.html QUESTION NO: 43 A media company produces new video files on-premises every day with a total size of around 100GBS after compression All files have a size of 1 -2 GB and need to be uploaded to Amazon S3 every night in a fixed time window between 3am and 5am Current upload takes almost 3 hours, although less than half of the available bandwidth is used. What step(s) would ensure that the file uploads are able to complete in the allotted time window? A. Increase your network bandwidth to provide faster throughput to S3 B. Upload the files in parallel to S3 C. Pack all files into a single archive, upload it to S3, then extract the files in AWS D. Use AWS Import/Export to transfer the video files Answer: D Reference: http://aws.amazon.com/importexport/faqs/ QUESTION NO: 44 You are running a web-application on AWS consisting of the following components an Elastic Load Balancer (ELB) an Auto-Scaling Group of EC2 instances running Linux/PHP/Apache, and Relational DataBase Service (RDS) MySQL. Which security measures fall into AWS's responsibility? A. Protect the EC2 instances against unsolicited access by enforcing the principle of least-privilege access B. Protect against IP spoofing or packet sniffing C. Assure all communication between EC2 instances and ELB is encrypted D. Install latest security patches on ELB. RDS and EC2 instances Answer: B QUESTION NO: 45 You use S3 to store critical data for your company Several users within your group currently have lull permissions to your S3 buckets You need to come up with a solution mat does not impact your users and also protect against the accidental deletion of objects. Which two options will address this issue? Choose 2 answers A. Enable versioning on your S3 Buckets B. Configure your S3 Buckets with MFA delete C. Create a Bucket policy and only allow read only permissions to all users at the bucket level D. Enable object life cycle policies and configure the data older than 3 months to be archived in Glacier Answer: B,C QUESTION NO: 46 An organization's security policy requires multiple copies of all critical data to be replicated across at least a primary and backup data center. The organization has decided to store some critical data on Amazon S3. 245 Which option should you implement to ensure this requirement is met? A. Use the S3 copy API to replicate data between two S3 buckets in different regions B. You do not need to implement anything since S3 data is automatically replicated between regions C. Use the S3 copy API to replicate data between two S3 buckets in different facilities within an AWS Region D. You do not need to implement anything since S3 data is automatically replicated between multiple facilities within an AWS Region Answer: C QUESTION NO: 47 You are tasked with setting up a cluster of EC2 Instances for a NoSOL database The database requires random read 10 disk performance up to a 100.000 IOPS at 4KB block side per node Which of the following EC2 instances will perform the best for this workload? A. A High-Memory Quadruple Extra Large (m2 4xlarge) with EBS-Optimized set to true and a PIOPs EBS volume B. A Cluster Compute Eight Extra Large (cc2 8xlarge) using instance storage C. High I/O Quadruple Extra Large (hil 4xiarge) using instance storage D. A Cluster GPU Quadruple Extra Large (cg1 4xlarge) using four separate 4000 PIOPS EBS volumes in a RAID 0 configuration Answer: B Reference: http://aws.amazon.com/ec2/instance-types/ QUESTION NO: 48 When an EC2 EBS-backed (EBS root) instance is stopped, what happens to the data on any ephemeral store volumes? A. Data will be deleted and win no longer be accessible B. Data Is automatically saved in an EBS volume. C. Data Is automatically saved as an E8S snapshot D. Data is unavailable until the instance is restarted Answer: D QUESTION NO: 49 Your team Is excited about theuse of AWS because now they have access to programmable Infrastructure\" You have been asked to manage your AWS infrastructure In a manner similar to the way you might manage application code You want to be able to deploy exact copies of different versions of your infrastructure, stage changes into different environments, revert back to previous versions, and identify what versions are running at any particular time (development test QA. production). Which approach addresses this requirement? A. Use cost allocation reports and AWS Opsworks to deploy and manage your infrastructure. B. Use AWS CloudWatch metrics and alerts along with resource tagging to deploy and manage your infrastructure. C. Use AWS Beanstalk and a version control system like GIT to deploy and manage your infrastructure. D. Use AWS CloudFormation and a version control system like GIT to deploy and manage your infrastructure. Answer: B Reference: http://aws.amazon.com/opsworks/faqs/ QUESTION NO: 50 You have a server with a 5O0GB Amazon EBS data volume. The volume is 80% full. You need to back up the volume at regular intervals and be able to re-create the volume in a new Availability Zone in the shortest time possible. All applications using the volume can be paused for a period of a few minutes with no discernible user impact. Which of the following backup methods will best fulfill your requirements? A. Take periodic snapshots of the EBS volume 2678930 B. Use a third party Incremental backup application to back up to Amazon Glacier C. Periodically back up all data to a single compressed archive and archive to Amazon S3 using a parallelized multi-part upload D. Create another EBS volume in the second Availability Zone attach it to the Amazon EC2 instance, and use a disk manager to mirror me two disks Answer: D Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html QUESTION NO: 51 Your company Is moving towards tracking web page users with a small tracking Image loaded on each page Currently you are serving this image out of US-East, but are starting to get concerned about the time It takes to load the image for users on the west coast. What are the two best ways to speed up serving this image? Choose 2 answers A. Use Route 53's Latency Based Routing and serve the image out of US-West-2 as well as US-East-1 B. Serve the image out through CloudFront C. Serve the image out of S3 so that it isn't being served oft of your web application tier D. Use EBS PIOPs to serve the image faster out of your EC2 instances Answer: A,B QUESTION NO: 52 If you want to launch Amazon Elastic Compute Cloud (EC2) Instances and assign each Instance a predetermined private IP address you should: A. Assign a group or sequential Elastic IP address to the instances B. Launch the instances in a Placement Group C. Launch the instances in the Amazon virtual Private Cloud (VPC). D. Use standard EC2 instances since each instance gets a private Domain Name Service (DNS) already E. Launch the Instance from a private Amazon Machine image (Mil) Answer: C Reference: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-ip-addressing.html QUESTION NO: 53 A customer has a web application that uses cookie Based sessions to track logged in users It Is deployed on AWS using ELB and Auto Scaling The customer observes that when load increases. Auto Scaling launches new Instances but the load on the easting Instances does not decrease, causing all existing users to have a sluggish experience. Which two answer choices independently describe a behavior that could be the cause of the sluggish user experience? Choose 2 answers A. ELB's normal behavior sends requests from the same user to the same backend instance B. ELB's behavior when sticky sessions are enabled causes ELB to send requests in the same session to the same backend instance C. A faulty browser is not honoring the TTL of the ELB DNS name. D. The web application uses long polling such as comet or websockets. Thereby keeping a connection open to a web server tor a long time E. The web application uses long polling such as comet or websockets. Thereby keeping a connection open to a web server for a long time. Answer: B,D QUESTION NO: 54 What would happen to an RDS (Relational Database Service) multi-Availability Zone deployment of the primary OB instance fails? A. The IP of the primary DB instance is switched to the standby OB instance B. The RDS (Relational Database Service) DB instance reboots C. A new DB instance is created in the standby availability zone D. The canonical name record (CNAME) is changed from primary to standby Answer: B QUESTION NO: 55 How can the domain's zone apex for example \"myzoneapexdomain com\" be pointed towards an Elastic Load Balancer? A. By using an AAAA record B. By using an A record C. By using an Amazon Route 53 CNAME record D. By using an Amazon Route 53 Alias record Answer: C Reference: http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosingalias-non-alias.html Topic 2, Volume B QUESTION NO: 56 An organization has created 5 IAM users. The organization wants to give them the same login ID but different passwords. How can the organization achieve this? A. The organization should create a separate login ID but give the IAM users the same alias so that each one can login with their alias B. The organization should create each user in a separate region so that they have their own URL to login C. It is not possible to have the same login ID for multiple IAM users of the same account D. The organization should create various groups and add each user with the same login ID to different groups. The user can login with their own group ID Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Whenever the organization is creating an IAM user, there should be a unique ID for each user. It is not possible to have the same login ID for multiple users. The names of users,groups, roles, instance profiles must be alphanumeric, including the following common characters: plus(+), equal(=), comma(,), period(.), at(@), and dash (-) QUESTION NO: 57 A user is planning to evaluate AWS for their internal use. The user does not want to incur any charge on his account during the evaluation. Which of the below mentioned AWS services would incur a charge if used? A. AWS S3 with 1 GB of storage B. AWS micro instance running 24 hours daily C. AWS ELB running 24 hours a day D. AWS PIOPS volume of 10 GB size Answer: D Explanation: AWS is introducing a free usage tier for one year to help the new AWS customers get started in Cloud. The free tier can be used for anything that the user wants to run in the Cloud. AWS offers a handful of AWS services as a part of this which includes 750 hours of free micro instances and 750 hours of ELB. It includes the AWS S3 of 5 GB and AWS EBS general purpose volume upto 30 GB. PIOPS is not part of free usage tier. QUESTION NO: 58 A user has developed an application which is required to send the data to a NoSQL database. The user wants to decouple the data sending such that the application keeps processing and sending data but does not wait for an acknowledgement of DB. Which of the below mentioned applications helps in this scenario? A. AWS Simple Notification Service B. AWS Simple Workflow C. AWS Simple Queue Service D. AWS Simple Query Service Answer: C Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. In this case, the user can use AWS SQS to send messages which are received from an application and sent to DB. The application can continue processing data without waiting for any acknowledgement from DB. The user can use SQS to transmit any volume of data without losing messages or requiring other services to always be available. QUESTION NO: 59 An organization has created 50 IAM users. The organization has introduced a new policy which will change the access of an IAM user. How can the organization implement this effectively so that there is no need to apply the policy at the individual user level? A. Use the IAM groups and add users as per their role to different groups and apply policy to group B. The user can create a policy and apply it to multiple users in a single go with the AWS CLI C. Add each user to the IAM role as per their organization role to achieve effective policy setup D. Use the IAM role and implement access at the role level Answer: A Explanation: With AWS IAM, a group is a collection of IAM users. A group allows the user to specify permissions for a collection of users, which can make it easier to manage the permissions for those users. A group helps an organization manage access in a better way; instead of applying at the individual level, the organization can apply at the group level which is applicable to all the users who are a part of that group. QUESTION NO: 60 A user is planning to use AWS Cloud formation for his automatic deployment requirements. Which of the below mentioned components are required as a part of the template? A. Parameters B. Outputs C. Template version D. Resources Answer: D Explanation: AWS Cloud formation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The template is a JSON-format, text-based file that describes all the AWS resources required to deploy and run an application. It can have option fields, such as Template Parameters, Output, Data tables, and Template file format version. The only mandatory value is Resource. The user can define the AWS services which will be used/ created by this template inside the Resource section QUESTION NO: 61 A user has recently started using EC2. The user launched one EC2 instance in the default subnet in EC2-VPC Which of the below mentioned options is not attached or available with the EC2 instance when it is launched? A. Public IP address B. Internet gateway C. Elastic IP D. Private IP address Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to a user\u2019s AWS account. A subnet is a range of IP addresses in the VPC. The user can launch the AWS resources into a subnet. There are two supported platforms into which a user can launch instances: EC2-Classic and EC2-VPC (default subnet.. A default VPC has all the benefits of EC2-VPC and the ease of use of EC2- Classic. Each instance that the user launches into a default subnet has a private IP address and a public IP address. These instances can communicate with the internet through an internet gateway. An internet gateway enables the EC2 instances to connect to the internet through the Amazon EC2 network edge. QUESTION NO: 62 A user has launched an EC2 instance. The user is planning to setup the CloudWatch alarm. Which of the below mentioned actions is not supported by the CloudWatch alarm? A. Notify the Auto Scaling launch config to scale up B. Send an SMS using SNS C. Notify the Auto Scaling group to scale down D. Stop the EC2 instance Answer: B Explanation: A user can create a CloudWatch alarm that takes various actions when the alarm changes state. An alarm watches a single metric over the time period that the user has specified, and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The actions could be sending a notification to an Amazon Simple Notification Service topic (SMS, Email, and HTTP end point.,notifying the Auto Scaling policy or changing the state of the instance to Stop/Terminate. QUESTION NO: 63 A user is trying to delete an Auto Scaling group from CLI. Which of the below mentioned steps are to be performed by the user? A. Terminate the instances with the ec2-terminate-instance command B. Terminate the Auto Scaling instances with the as-terminate-instance command C. Set the minimum size and desired capacity to 0 D. There is no need to change the capacity. Run the as-delete-group command and it will reset all values to 0 Answer: C Explanation: If the user wants to delete the Auto Scaling group, the user should manually set the values of the minimum and desired capacity to 0. Otherwise Auto Scaling will not allow for the deletion of the group from CLI. While trying from the AWS console, the user need not set the values to 0 as the Auto Scaling console will automatically do so. QUESTION NO: 64 An organization is planning to create 5 different AWS accounts considering various security requirements. The organization wants to use a single payee account by using the consolidated billing option. Which of the below mentioned statements is true with respect to the above information? A. Master (Payee. account will get only the total bill and cannot see the cost incurred by each account B. Master (Payee. account can view only the AWS billing details of the linked accounts C. It is not recommended to use consolidated billing since the payee account will have access to the linked accounts D. Each AWS account needs to create an AWS billing policy to provide permission to the payee account Answer: B Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. Consolidated billing enables the organization to see a combined view of the AWS charges incurred by each account as well as obtain a detailed cost report for each of the individual AWS accounts associated with the paying account. The payee account will not have any other access than billing data of linked accounts. QUESTIONNO: 64-A A user has deployed an application on his private cloud. The user is using his own monitoring tool. He wants to configure that whenever there is an error, the monitoring tool should notify him via SMS. Which of the below mentioned AWS services will help in this scenario? A. None because the user infrastructure is in the private cloud/ B. AWS SNS C. AWS SES D. AWS SMS Answer: B Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can be used to make push notifications to mobile devices. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. In this case user can use the SNS apis to send SMS. QUESTION NO: 65 A user has created a web application with Auto Scaling. The user is regularly monitoring the application and he observed that the traffic is highest on Thursday and Friday between 8 AM to 6 PM. What is the best solution to handle scaling in this case? A. Add a new instance manually by 8 AM Thursday and terminate the same by 6 PM Friday B. Schedule Auto Scaling to scale up by 8 AM Thursday and scale down after 6 PM on Friday C. Schedule a policy which may scale up every day at 8 AM and scales down by 6 PM D. Configure a batch process to add a instance by 8 AM and remove it by Friday 6 PM Answer: B Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. In this case the load increases by Thursday and decreases by Friday. Thus, the user can setup the scaling activity based on the predictable traffic patterns of the web application using Auto Scaling scale by Schedule. QUESTION NO: 66 A user has setup a CloudWatch alarm on an EC2 action when the CPU utilization is above 75%. The alarm sends a notification to SNS on the alarm state. If the user wants to simulate the alarm action how can he achieve this? A. Run activities on the CPU such that its utilization reaches above 75% B. From the AWS console change the state to \u2018Alarm\u2019 C. The user can set the alarm state to \u2018Alarm\u2019 using CLI D. Run the SNS action manually Answer: C Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods.The user can test an alarm by setting it to any state using the SetAlarmState API (mon-set-alarm-state command.. This temporary state change lasts only until the next alarm comparison occurs. QUESTION 13 A user is trying to setup a scheduled scaling activity using Auto Scaling. The user wants to setup the recurring schedule. Which of the below mentioned parameters is not required in this case? A. Maximum size B. Auto Scaling group name C. End time D. Recurrence value Answer: A Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can also configure the recurring schedule action which will follow the Linux cron format. If the user is setting a recurring event, it is required that the user specifies the Recurrence value (in a cron format., end time (not compulsory but recurrence will stop after this. and the Auto Scaling group for which the scaling activity is to be scheduled. QUESTION NO: 67 A user has setup a billing alarm using CloudWatch for $200. The usage of AWS exceeded $200 after some days. The user wants to increase the limit from $200 to $400? What should the user do? A. Create a new alarm of $400 and link it with the first alarm B. It is not possible to modify the alarm once it has crossed the usage limit C. Update the alarm to set the limit at $400 instead of $200 D. Create a new alarm for the additional $200 amount Answer: C Explanation: AWS CloudWatch supports enabling the billing alarm on the total AWS charges. The estimated charges are calculated and sent several times daily to CloudWatch in the form of metric data. This data will be stored for 14 days. This data also includes the estimated charges for every service in AWS used by the user, as well as the estimated overall AWS charges. If the user wants to increase the limit, the user can modify the alarm and specify a new threshold. QUESTION NO: 68 A sys admin has created the below mentioned policy and applied to an S3 object named aws.jpg. The aws.jpg is inside a bucket named cloudacademy. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg\"] }] A. It is not possible to define a policy at the object level B. It will make all the objects of the bucket cloudacademy as public C. It will make the bucket cloudacademy as public D. the aws.jpg object as public Answer: A Explanation: A system admin can grant permission to the S3 objects or buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. It cannot be applied at the object level. QUESTION NO: 69 A user is trying to save some cost on the AWS services. Which of the below mentioned options will not help him save cost? A. Delete the unutilized EBS volumes once the instance is terminated B. Delete the AutoScaling launch configuration after the instances are terminated C. Release the elastic IP if not required once the instance is terminated D. Delete the AWS ELB after the instances are terminated Answer: B Explanation: AWS bills the user on a as pay as you go model. AWS will charge the user once the AWS resource is allocated. Even though the user is not using the resource, AWS will charge if it is in service or allocated. Thus, it is advised that once the user\u2019s work is completed he should: Terminate the EC2 instance Delete the EBS volumes Release the unutilized Elastic IPs Delete ELB The AutoScaling launch configuration does not cost the user. Thus, it will not make any difference to the cost whether it is deleted or not. QUESTION NO: 70 A user is trying to aggregate all the CloudWatch metric data of the last 1 week. Which of the below mentioned statistics is not available for the user as a part of data aggregation? A. Aggregate B. Sum C. Sample data D. Average Answer: A Explanation: Amazon CloudWatch is basically a metrics repository. Either the user can send the custom data or an AWS product can put metrics into the repository, and the user can retrieve the statistics based on those metrics. The statistics are metric data aggregations over specified periods of time. Aggregations are made using the namespace, metric name, dimensions, and the data point unit of measure, within the time period that is specified by the user. CloudWatch supports Sum, Min, Max, Sample Data and Average statistics aggregation. QUESTION NO: 71 An organization is planning to use AWS for their production roll out. The organization wants to implement automation for deployment such that it will automatically create a LAMP stack, download the latest PHP installable from S3 and setup the ELB. Which of the below mentioned AWS services meets the quirement for making an orderly deployment of the software? A. AWS Elastic Beanstalk B. AWS Cloudfront C. AWS Cloudformation D. AWS DevOps Answer: C Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. Cloudformation provides an easy way to create and delete the collection of related AWS resources and provision them in an orderly way. AWS CloudFormation automates and simplifies the task of repeatedly and predictably creating groups of related resources that power the user\u2019s applications. AWS Cloudfront is a CDN; Elastic Beanstalk does quite a few of the required tasks. However, it is a PAAS which uses a ready AMI. AWS Elastic Beanstalk provides an environment to easily develop and run applications in the cloud. QUESTION NO: 72 A user has created a subnet with VPC and launched an EC2 instance in that subnet with only default settings.Which of the below mentioned options is ready to use on the EC2 instance as soon as it is launched? A. Elastic IP B. Private IP C. Public IP D. I nternet gateway Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to a user\u2019s AWS account. A subnet is a range of IP addresses in the VPC. The user can launch the AWS resources into a subnet. There are two supported platforms into which a user can launch instances: EC2-Classic and EC2-VPC. When the user launches an instance which is not a part of the non-default subnet, it will only have a private IP assigned to it. The instances part of a subnet can communicate with each other but cannot communicate over the internet or to the AWS services, such as RDS / S3. QUESTION NO: 73 An organization is setting up programmatic billing access for their AWS account. Which of the below mentioned services is not required or enabled when the organization wants to use programmatic access? A. Programmatic access B. AWS bucket to hold the billing report C. AWS billing alerts D. Monthly Billing report Answer: C Explanation: AWS provides an option to have programmatic access to billing. Programmatic Billing Access leverages the existing Amazon Simple Storage Service (Amazon S3. APIs. Thus, the user can build applications that reference his billing data from a CSV (comma-separated value. file stored in an Amazon S3 bucket. To enable programmatic access, the user has to first enable the monthly billing report. Then the user needs to provide an AWS bucket name where the billing CSV will be uploaded. The user should also enable the Programmatic access option. QUESTION NO: 74 A user has configured the Auto Scaling group with the minimum capacity as 3 and the maximum capacity as 5. When the user configures the AS group, how many instances will Auto Scaling launch? A. 3 B. 0 C. 5 D. 2 Answer: C Explanation: When the user configures the launch configuration and the Auto Scaling group, the Auto Scaling group will start instances by launching the minimum number (or the desired number, if specified. of EC2 instances. If there are no other scaling conditions attached to the Auto Scaling group, it will maintain the minimum number of running instances at all times. QUESTION NO: 75 An admin is planning to monitor the ELB. Which of the below mentioned services does not help the admin capture the monitoring information about the ELB activity? A. ELB Access logs B. ELB health check C. CloudWatch metrics D. ELB API calls with CloudTrail Answer: B Explanation: The admin can capture information about Elastic Load Balancer using either: CloudWatch Metrics ELB Logs files which are stored in the S3 bucket CloudTrail with API calls which can notify the user as well generate logs for each API calls The health check is internally performed by ELB and does not help the admin get the ELB activity. QUESTION NO: 76 A user is planning to use AWS Cloudformation. Which of the below mentioned functionalities does not help him to correctly understand Cloudfromation? A. Cloudformation follows the DevOps model for the creation of Dev & Test B. AWS Cloudfromation does not charge the user for its service but only charges for the AWS resources created with it C. Cloudformation works with a wide variety of AWS services, such as EC2, EBS, VPC, IAM, S3, RDS, ELB, etc D. CloudFormation provides a set of application bootstrapping scripts which enables the user to install Software Answer: A Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. It supports a wide variety of AWS services, such as EC2, EBS, AS, ELB, RDS, VPC, etc. It also provides application bootstrapping scripts which enable the user to install software packages or create folders. It is free of the cost and only charges the user for the services created with it. The only challenge is that it does not follow any model, such as DevOps; instead customers can define templates and use them to provision and manage the AWS resources in an orderly way. QUESTION NO: 77 A user has launched 10 instances from the same AMI ID using Auto Scaling. The user is trying to see the average CPU utilization across all instances of the last 2 weeks under the CloudWatch console. How can the user achieve this? A. View the Auto Scaling CPU metrics B. Aggregate the data over the instance AMI ID C. The user has to use the CloudWatchanalyser to find the average data across instances D. It is not possible to see the average CPU utilization of the same AMI ID since the instance ID is different Answer: B Explanation: Amazon CloudWatch is basically a metrics repository. Either the user can send the custom data or an AWS product can put metrics into the repository, and the user can retrieve the statistics based on those metrics. The statistics are metric data aggregations over specified periods of time. Aggregations are made using the namespace, metric name, dimensions, and the data point unit of measure, within the time period that is specified by the user. To aggregate the data across instances launched with AMI, the user should select the AMI ID under EC2 metrics and select the aggregate average to view the data. QUESTION NO: 78 A user is trying to understand AWS SNS. To which of the below mentioned end points is SNS unable to send a notification? A. Email JSON B. HTTP C. AWS SQS D. AWS SES Answer: D Explanation: Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can select one the following transports as part of the subscription requests: \u201cHTTP\u201d, \u201cHTTPS\u201d,\u201dEmail\u201d, \u201cEmailJSON\u201d, \u201cSQS\u201d, \u201cand SMS\u201d. QUESTION NO: 79 A user has configured an Auto Scaling group with ELB. The user has enabled detailed CloudWatch monitoring on Auto Scaling. Which of the below mentioned statements will help the user understand the functionality better? A. It is not possible to setup detailed monitoring for Auto Scaling B. In this case, Auto Scaling will send data every minute and will charge the user extra C. Detailed monitoring will send data every minute without additional charges D. Auto Scaling sends data every minute only and does not charge the user Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Auto Scaling includes 7 metrics and 1 dimension, and sends data to CloudWatch every 5 minutes by default. The user can enable detailed monitoring for Auto Scaling, which sends data to CloudWatch every minute. However, this will have some extra-costs. QUESTION NO: 80 A system admin is planning to setup event notifications on RDS. Which of the below mentioned services will help the admin setup notifications? A. AWS SES B. AWS Cloudtrail C. AWS Cloudwatch D. AWS SNS Answer: D Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. These notifications can be in any notification form supported by Amazon SNS for an AWS region, such as an email, a text message or a call to an HTTP endpoint QUESTION NO: 81 You are building an online store on AWS that uses SQS to process your customer orders. Your backend system needs those messages in the same sequence the customer orders have been put in. How can you achieve that? A. It is not possible to do this with SQS B. You can use sequencing information on each message C. You can do this with SQS but you also need to use SWF D. Messages will arrive in the same order by default Answer: B Explanation: Amazon SQS is engineered to always be available and deliver messages. One of the resulting tradeoffs is that SQSdoes not guarantee first in, first out delivery of messages. For many distributed applications, each message can stand on its own, and as long as all messages are delivered, the order is not important. If your system requires that order be preserved, you can place sequencing information in each message, so that you can reorder the messages when the queue returns them. QUESTION NO: 82 An organization wants to move to Cloud. They are looking for a secure encrypted database storage option. Which of the below mentioned AWS functionalities helps them to achieve this? A. AWS MFA with EBS B. AWS EBS encryption C. Multi-tier encryption with Redshift D. AWS S3 server side storage Answer: B Explanation: AWS EBS supports encryption of the volume while creating new volumes. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. The data at rest, the I/O as well as all the snapshots of EBS will be encrypted. The encryption occurs on the servers that host the EC2 instances, providing encryption of data as it moves between the EC2 instances and EBS storage. EBS encryption is based on the AES-256 cryptographic algorithm, which is the industry standard QUESTION NO: 83 A user wants to disable connection draining on an existing ELB. Which of the below mentioned statements helps the user disable connection draining on the ELB? A. The user can only disable connection draining from CLI B. It is not possible to disable the connection draining feature once enabled C. The user can disable the connection draining feature from EC2 -> ELB console or from CLI D. The user needs to stop all instances before disabling connection draining Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. The user can enable or disable connection draining from the AWS EC2 console -> ELB or using CLI. QUESTION NO: 84 A user has a refrigerator plant. The user is measuring the temperature of the plant every 15 minutes. If the user wants to send the data to CloudWatch to view the data visually, which of the below mentioned statements is true with respect to the information given above? A. The user needs to use AWS CLI or API to upload the data B. The user can use the AWS Import Export facility to import data to CloudWatch C. The user will upload data from the AWS console D. The user cannot upload data to CloudWatch since it is not an AWS service metric Answer: A Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. While sending the data the user has to include the metric name, namespace and timezone as part of the request. QUESTION NO: 85 A system admin is managing buckets, objects and folders with AWS S3. Which of the below mentioned statements is true and should be taken in consideration by the sysadmin? A. The folders support only ACL B. Both the object and bucket can have an Access Policy but folder cannot have policy C. Folders can have a policy D. Both the object and bucket can have ACL but folders cannot have ACL Answer: A Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. It cannot be applied at the object level. The folders are similar to objects with no content. Thus, folders can have only ACL and cannot have a policy. QUESTION NO: 86 A user has created an ELB with three instances. How many security groups will ELB create by default? A. 3 B. 5 C. 2 D. 1 Answer: C Explanation: Elastic Load Balancing provides a special Amazon EC2 source security group that the user can use to ensure that back-end EC2 instances receive traffic only from Elastic Load Balancing. This feature needs two security groups: the source security group and a security group that defines the ingress rules for the back-end instances. To ensure that traffic only flows between the load balancer and the back-end instances, the user can add or modify a rule to the back-end security group which can limit the ingress traffic. Thus, it can come only from the source security group provided by Elastic load Balancing. QUESTION NO: 87 An organization has created 50 IAM users. The organization wants that each user can change their password but cannot change their access keys. How can the organization achieve this? A. The organization has to create a special password policy and attach it to each user B. The root account owner has to use CLI which forces each IAM user to change their password on first login C. By default each IAM user can modify their passwords D. The root account owner can set the policy from the IAM console under the password policy screen Answer: D Explanation: With AWS IAM, organizations can use the AWS Management Console to display, create, change or delete a password policy. As a part of managing the password policy, the user can enable all users to manage their own passwords. If the user has selected the option which allows the IAM users to modify their password, he does not need to set a separate policy for the users. This option in the AWS console allows changing only the password. QUESTION NO: 88 A user has created a photo editing software and hosted it on EC2. The software accepts requests from the user about the photo format and resolution and sends a message to S3 to enhance the picture accordingly.Which of the below mentioned AWS services will help make a scalable software with the AWS infrastructure in this scenario? A. AWS Glacier B. AWS Elastic Transcoder C. AWS Simple Notification Service D. AWS Simple Queue Service Answer: D Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. The user can configure SQS, which will decouple the call between the EC2 application and S3. Thus, the application does not keep waiting for S3 to provide the data. QUESTION NO: 89 An application is generating a log file every 5 minutes. The log file is not critical but may be required only for verification in case of some major issue. The file should be accessible over the internet whenever required. Which of the below mentioned options is a best possible storage solution for it? A. AWS S3 B. AWS Glacier C. AWS RDS D. AWS RRS Answer: D Explanation: Amazon S3 stores objects according to their storage class. There are three major storage classes: Standard, Reduced Redundancy Storage and Glacier. Standard is for AWS S3 and provides very high durability. However, the costs are a little higher. Glacier is for archival and the files are not available over the internet. Reduced Redundancy Storage is for less critical files. Reduced Redundancy is little cheaper as it provides less durability in comparison to S3. In this case since the log files are not mission critical files, RRS will be a better option. QUESTION NO: 90 A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25. The user is trying to create the private subnet with CIDR 20.0.0.128/25. Which of the below mentioned statements is true in this scenario? A. It will not allow the user to create the private subnet due to a CIDR overlap B. It will allow the user to create a private subnet with CIDR as 20.0.0.128/25 C. This statement is wrong as AWS does not allow CIDR 20.0.0.0/25 D. It will not allow the user to create a private subnet due to a wrong CIDR range Answer: B Explanation: When the user creates a subnet in VPC, he specifies the CIDR block for the subnet. The CIDR block of a subnet can be the same as the CIDR block for the VPC (for a single subnet in the VPC., or a subset (to enable multiple subnets.. If the user creates more than one subnet in a VPC, the CIDR blocks of the subnets must not overlap. Thus, in this case the user has created a VPC with the CIDR block 20.0.0.0/24, which supports 256 IP addresses (20.0.0.0 to 20.0.0.255.. The user can break this CIDR block into two subnets, each supporting 128 IP addresses. One subnet uses the CIDR block 20.0.0.0/25 (for addresses 20.0.0.0 - 20.0.0.127. and the other uses the CIDR block 20.0.0.128/25 (for addresses 20.0.0.128 - 20.0.0.255.. QUESTION NO: 91 A user has created an S3 bucket which is not publicly accessible. The bucket is having thirty objects which are also private. If the user wants to make the objects public, how can he configure this with minimal efforts? A. The user should select all objects from the console and apply a single policy to mark them public B. The user can write a program which programmatically makes all objects public using S3 SDK C. Set the AWS bucket policy which marks all objects as public D. Make the bucket ACL as public so it will also mark all objects as public Answer: C Explanation: A system admin can grant permission of the S3 objects or buckets to any user or make the objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. QUESTION NO: 92 A sys admin is maintaining an application on AWS. The application is installed on EC2 and user has configured ELB and Auto Scaling. Considering future load increase, the user is planning to launch new servers proactively so that they get registered with ELB. How can the user add these instances with Auto Scaling? A. Increase the desired capacity of the Auto Scaling group B. Increase the maximum limit of the Auto Scaling group C. Launch an instance manually and register it with ELB on the fly D. Decrease the minimum limit of the Auto Scaling grou Answer: A Explanation: A user can increase the desired capacity of the Auto Scaling group and Auto Scaling will launch a new instance as per the new capacity. The newly launched instances will be registered with ELB if Auto Scaling group is configured with ELB. If the user decreases the minimum size the instances will be removed from Auto Scaling. Increasing the maximum size will not add instances but only set the maximum instance cap. QUESTION NO: 93 An organization, which has the AWS account ID as 999988887777, has created 50 IAM users. All the users are added to the same group cloudacademy. If the organization has enabled that each IAM user can login with the AWS console, which AWS login URL will the IAM users use? A. https:// 999988887777.signin.aws.amazon.com/console/ B. https:// signin.aws.amazon.com/cloudacademy/ C. https:// cloudacademy.signin.aws.amazon.com/999988887777/console/ D. https:// 999988887777.aws.amazon.com/ cloudacademy/ Answer: A Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Once the organization has created the IAM users, they will have a separate AWS console URL to login to the AWS console. The console login URL for the IAM user will be https:// AWS_Account_ID.signin.aws.amazon.com/console/. It uses only the AWS account ID and does not depend on the group or user ID. QUESTION NO: 94 A user has setup connection draining with ELB to allow in-flight requests to continue while the instance is being deregistered through Auto Scaling. If the user has not specified the draining time, how long will ELB allow inflight requests traffic to continue? A. 600 seconds B. 3600 seconds C. 300 seconds D. 0 seconds Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. The user can specify a maximum time (3600 seconds. for the load balancer to keep the connections alive before reporting the instance as deregistered. If the user does not specify the maximum timeout period, by default, the load balancer will close the connections to the deregistering instance after 300 seconds. QUESTION NO: 95 A root AWS account owner is trying to understand various options to set the permission to AWS S3. Which of the below mentioned options is not the right option to grant permission for S3? A. User Access Policy B. S3 Object Access Policy C. S3 Bucket Access Policy D. S3 ACL Answer: B Explanation: Amazon S3 provides a set of operations to work with the Amazon S3 resources. Managing S3 resource access refers to granting others permissions to work with S3. There are three ways the root account owner can define access with S3: S3 ACL: The user can use ACLs to grant basic read/write permissions to other AWS accounts. S3 Bucket Policy: The policy is used to grant other AWS accounts or IAM users permissions for the bucket and the objects in it. User Access Policy: Define an IAM user and assign him the IAM policy which grants him access to S3. QUESTION NO: 96 A sys admin has created a shopping cart application and hosted it on EC2. The EC2 instances are running behind ELB. The admin wants to ensure that the end user request will always go to the EC2 instance where the user session has been created. How can the admin configure this? A. Enable ELB cross zone load balancing B. Enable ELB cookie setup C. Enable ELB sticky session D. Enable ELB connection draining Answer: C Explanation: Generally AWS ELB routes each request to a zone with the minimum load. The Elastic Load Balancer provides a feature called sticky session which binds the user\u2019s session with a specific EC2 instance. If the sticky session is enabled the first request from the user will be redirected to any of the EC2 instances. But, henceforth, all requests from the same user will be redirected to the same EC2 instance. This ensures that all requests coming from the user during the session will be sent to the same application instance. QUESTION NO: 97 A user has configured ELB with three instances. The user wants to achieve High Availabilityi as well as redundancy with ELB. Which of the below mentioned AWS services helps the user achieve this for ELB? A. Route 53 B. AWS Mechanical Turk C. Auto Scaling D. AWS EMR Answer: A Explanation: The user can provide high availability and redundancy for applications running behind Elastic Load Balancer by enabling the Amazon Route 53 Domain Name System (DNS. failover for the load balancers. Amazon Route 53 is a DNS service that provides reliable routing to the user\u2019s infrastructure. QUESTION NO: 98 An organization is using AWS since a few months. The finance team wants to visualize the pattern of AWS spending. Which of the below AWS tool will help for this requirement? A. AWS Cost Manager B. AWS Cost Explorer C. AWS CloudWatch D. AWS Consolidated Billing Answer: B Explanation: The AWS Billing and Cost Management console includes the Cost Explorer tool for viewing AWS cost data as a graph. It does not charge extra to user for this service. With Cost Explorer the user can filter graphs using resource tags or with services in AWS. If the organization is using Consolidated Billing it helps generate report based on linked accounts. This will help organization to identify areas that require further inquiry. The organization can view trends and use that to understand spend and to predict future costs. QUESTION NO: 99 A user has launched an ELB which has 5 instances registered with it. The user deletes the ELB by mistake. What will happen to the instances? A. ELB will ask the user whether to delete the instances or not B. Instances will be terminated C. ELB cannot be deleted if it has running instances registered with it D. Instances will keep running Answer: D Explanation: When the user deletes the Elastic Load Balancer, all the registered instances will be deregistered. However, they will continue to run. The user will incur charges if he does not take any action on those instances. QUESTION NO: 100 A user is planning to setup notifications on the RDS DB for a snapshot. Which of the below mentioned event categories is not supported by RDS for this snapshot source type? A. Backup B. Creation C. Deletion D. Restoration Answer: A Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. Event categories for a snapshot source type include: Creation, Deletion, and Restoration. The Backup is a part of DB instance source type. QUESTION NO: 101 A customer is using AWS for Dev and Test. The customer wants to setup the Dev environment with Cloudformation. Which of the below mentioned steps are not required while using Cloudformation? A. Create a stack B. Configure a service C. Create and upload the template D. Provide the parameters configured as part of the template Answer: B Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. AWS CloudFormation introduces two concepts: the template and the stack. The template is a JSON-format, text-based file that describes all the AWS resources required to deploy and run an application. The stack is a collection of AWS resources which are created and managed as a single unit when AWS CloudFormation instantiates a template. While creating a stack, the user uploads the template and provides the data for the parameters if required. QUESTION NO: 102 A user has configured the AWS CloudWatch alarm for estimated usage charges in the US East region. Which of the below mentioned statements is not true with respect to the estimated charges? A. It will store the estimated charges data of the last 14 days B. It will include the estimated charges of every AWS service C. The metric data will represent the data of all the regions D. The metric data will show data specific to that region Answer: D Explanation: When the user has enabled the monitoring of estimated charges for the AWS account with AWS CloudWatch, the estimated charges are calculated and sent several times daily to CloudWatch in the form of metric data. This data will be stored for 14 days. The billing metric data is stored in the US East (Northern Virginia. Region and represents worldwide charges. This data also includes the estimated charges for every service in AWS used by the user, as well as the estimated overall AWS charges. QUESTION NO: 103 A user is accessing RDS from an application. The user has enabled the Multi AZ feature with the MS SQL RDS DB. During a planned outage how will AWS ensure that a switch from DB to a standby replica will not affect access to the application? A. RDS will have an internal IP which will redirect all requests to the new DB B. RDS uses DNS to switch over to stand by replica for seamless transition C. The switch over changes Hardware so RDS does not need to worry about access D. RDS will have both the DBs running independently and the user has to manually switch over Answer: B Explanation: In the event of a planned or unplanned outage of a DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone if the user has enabled Multi AZ. The automatic failover mechanism simply changes the DNS record of the DB instance to point to the standby DB instance. As a result, the user will need to re-establish any existing connections to the DB instance. However, as the DNS is the same, the application can access DB seamlessly. QUESTION NO: 104 An organization is generating digital policy files which are required by the admins for verification. Once the files are verified they may not be required in the future unless there is some compliance issue. If the organization wants to save them in a cost effective way, which is the best possible solution? A. AWS RRS B. AWS S3 C. AWS RDS D. AWS Glacier Answer: D Explanation: Amazon S3 stores objects according to their storage class. There are three major storage classes: Standard, Reduced Redundancy and Glacier. Standard is for AWS S3 and provides very high durability. However, the costs are a little higher. Reduced redundancy is for less critical files. Glacier is for archival and the files which are accessed infrequently. It is an extremely low-cost storage service that provides secure and durable storage for data archiving and backup. QUESTION NO: 105 A user has launched an EBS backed instance. The user started the instance at 9 AM in the morning. Between 9 AM to 10 AM, the user is testing some script. Thus, he stopped the instance twice and restarted it. In the same hour the user rebooted the instance once. For how many instance hours will AWS charge the user? A. 3 hours B. 4 hours C. 2 hours D. 1 hour Answer: A Explanation: A user can stop/start or reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. When the instance is rebooted AWS will not charge the user for the extra hours. In case the user stops the instance, AWS does not charge the running cost but charges only the EBS storage cost. If the user starts and stops the instance multiple times in a single hour, AWS will charge the user for every start and stop. In this case, since the instance was rebooted twice, it will cost the user for 3 instance hours. QUESTION NO: 106 An organization has configured the custom metric upload with CloudWatch. The organization has given permission to its employees to upload data using CLI as well SDK. How can the user track the calls made to CloudWatch? A. The user can enable logging with CloudWatch which logs all the activities B. Use CloudTrail to monitor the API calls C. Create an IAM user and allow each user to log the data using the S3 bucket D. Enable detailed monitoring with CloudWatch Answer: B Explanation: AWS CloudTrail is a web service which will allow the user to monitor the calls made to the Amazon CloudWatch API for the organization\u2019s account, including calls made by the AWS Management Console, Command Line Interface (CLI., and other services. When CloudTrail logging is turned on, CloudWatch will write log files into the Amazon S3 bucket, which is specified during the CloudTrail configuration. QUESTION NO: 107 A user has created a queue named \u201cmyqueue\u201d with SQS. There are four messages published to queue which are not received by the consumer yet. If the user tries to delete the queue, what will happen? A. A user can never delete a queue manually. AWS deletes it after 30 days of inactivity on queue B. It will delete the queue C. It will initiate the delete but wait for four days before deleting until all messages are deleted automatically. D. It will ask user to delete the messages first Answer: B Explanation: SQS allows the user to move data between distributed components of applications so they can perform different tasks without losing messages or requiring each component to be always available. The user can delete a queue at any time, whether it is empty or not. It is important to note that queues retain messages for a set period of time. By default, a queue retains messages for four days. QUESTION NO: 108 A user has launched a large EBS backed EC2 instance in the US-East-1a region. The user wants to achieve Disaster Recovery (DR. for that instance by creating another small instance in Europe. How can the user achieve DR? A. Copy the running instance using the \u201cInstance Copy\u201d command to the EU region B. Create AMI of the instance and copy the AMI to the EU region. Then launch the instance from the EU AMI. C. Copy the instance from the US East region to the EU region D. Use the \u201cLaunch more like this\u201d option to copy the instance from one region to another Answer: B Explanation: To launch an EC2 instance it is required to have an AMI in that region. If the AMI is not available in that region, then create a new AMI or use the copy command to copy the AMI from one region to the other region. QUESTION NO: 109 A user has created numerous EBS volumes. What is the general limit for each AWS account for the maximum number of EBS volumes that can be created? A. 10000 B. 5000 C. 100 D. 1000 Answer: B Explanation: A user can attach multiple EBS volumes to the same instance within the limits specified by his AWS account. Each AWS account has a limit on the number of Amazon EBS volumes that the user can create, and the total storage available. The default limit for the maximum number of volumes that can be created is 5000. QUESTION NO: 110 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24. and VPN only subnets CIDR (20.0.1.0/24. along with the VPN gateway (vgw-12345. to connect to the user\u2019s data centre. Which of the below mentioned options is a valid entry for the main route table in this scenario? A. Destination: 20.0.0.0/24 and Target: vgw-12345 B. Destination: 20.0.0.0/16 and Target: ALL C. Destination: 20.0.1.0/16 and Target: vgw-12345 D. Destination: 0.0.0.0/0 and Target: vgw-12345 Answer: D Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. Here are the valid entries for the main route table in this scenario: Destination: 0.0.0.0/0 & Target: vgw-12345 (To route all internet traffic to the VPN gateway. Destination: 20.0.0.0/16 & Target: local (To allow local routing in VPC. QUESTION NO: 111 A user has stored data on an encrypted EBS volume. The user wants to share the data with his friend\u2019s AWS account. How can user achieve this? A. Create an AMI from the volume and share the AMI B. Copy the data to an unencrypted volume and then share C. Take a snapshot and share the snapshot with a friend D. If both the accounts are using the same encryption key then the user can share the volume directly Answer: B Explanation: AWS EBS supports encryption of the volume. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. If the user is having data on an encrypted volume and is trying to share it with others, he has to copy the data from the encrypted volume to a new unencrypted volume. Only then can the user share it as an encrypted volume data. Otherwise the snapshot cannot be shared. QUESTION NO: 112 A user has enabled the Multi AZ feature with the MS SQL RDS database server. Which of the below mentioned statements will help the user understand the Multi AZ feature better? A. In a Multi AZ, AWS runs two DBs in parallel and copies the data asynchronously to the replica copy B. In a Multi AZ, AWS runs two DBs in parallel and copies the data synchronously to the replica copy C. In a Multi AZ, AWS runs just one DB but copies the data synchronously to the standby replica D. AWS MS SQL does not support the Multi AZ feature Answer: C Explanation: Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption.Note that the high-availability feature is not a scaling solution for read-only scenarios; you cannot use a standby replica to serve read traffic. To service read-only traffic, you should use a read replica. QUESTION NO: 113 An organization is using cost allocation tags to find the cost distribution of different departments and projects. One of the instances has two separate tags with the key/ value as \u201cInstanceName/ HR\u201d, \u201cCostCenter/HR\u201d. What will AWS do in this case? A. InstanceName is a reserved tag for AWS. Thus, AWS will not allow this tag B. AWS will not allow the tags as the value is the same for different keys C. AWS will allow tags but will not show correctly in the cost allocation report due to the same value of the two separate keys D. AWS will allow both the tags and show properly in the cost distribution report Answer: D Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources, AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. Each tag will have a key-value and can be applied to services, such as EC2, S3, RDS, EMR, etc. It is required that the key should be different for each tag. The value can be the same for different keys. In this case since the value is different, AWS will properly show the distribution report with the correct values. QUESTION NO: 114 A user is publishing custom metrics to CloudWatch. Which of the below mentioned statements will help the user understand the functionality better? A. The user can use the CloudWatch Import tool B. The user should be able to see the data in the console after around 15 minutes C. If the user is uploading the custom data, the user must supply the namespace, timezone, and metric name as part of the command D. The user can view as well as upload data using the console, CLI and APIs Answer: B Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user has to always include the namespace as a part of the request. However, the other parameters are optional. If the user has uploaded data using CLI, he can view it as a graph inside the console. The data will take around 2 minutes to upload but can be viewed only after around 15 minutes. QUESTION NO: 115 A user is launching an EC2 instance in the US East region. Which of the below mentioned options is recommended by AWS with respect to the selection of the availability zone? A. Always select the US-East-1-a zone for HA B. Do not select the AZ; instead let AWS select the AZ C. The user can never select the availability zone while launching an instance D. Always select the AZ while launching an instance Answer: B Explanation: When launching an instance with EC2, AWS recommends not to select the availability zone (AZ.. AWS specifies that the default Availability Zone should be accepted. This is because it enables AWS to select the best Availability Zone based on the system health and available capacity. If the user launches additional instances, only then an Availability Zone should be specified. This is to specify the same or different AZ from the running instances. QUESTION NO: 116 A user has created a VPC with CIDR 20.0.0.0/16 with only a private subnet and VPN connection using the VPC wizard. The user wants to connect to the instance in a private subnet over SSH. How should the user define the security rule for SSH? A. Allow Inbound traffic on port 22 from the user\u2019s network B. The user has to create an instance in EC2 Classic with an elastic IP and configure the security group of a private subnet to allow SSH from that elastic IP C. The user can connect to a instance in a private subnet using the NAT instance D. Allow Inbound traffic on port 80 and 22 to allow the user to connect to a private subnet over the Internet Answer: A Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, the user can setup a case with a VPN only subnet (private. which uses VPN access to connect with his data centre. When the user has configured this setup with Wizard, all network connections to the instances in the subnet will come from his data centre. The user has to configure the security group of the private subnet which allows the inbound traffic on SSH (port 22. from the data centre\u2019s network range. QUESTION NO: 117 A user has created an ELB with the availability zone US-East-1A. The user wants to add more zones to ELB to achieve High Availability. How can the user add more zones to the existing ELB? A. It is not possible to add more zones to the existing ELB B. The only option is to launch instances in different zones and add to ELB C. The user should stop the ELB and add zones and instances as required D. The user can add zones on the fly from the AWS console Answer: D Explanation: The user has created an Elastic Load Balancer with the availability zone and wants to add more zones to the existing ELB. The user can do so in two ways: From the console or CLI, add new zones to ELB; Launch instances in a separate AZ and add instances to the existing ELB. QUESTION NO: 118 A user has configured an Auto Scaling group with ELB. The user has enabled detailed CloudWatch monitoring on Elastic Load balancing. Which of the below mentioned statements will help the user understand this functionality better? A. ELB sends data to CloudWatch every minute only and does not charge the user B. ELB will send data every minute and will charge the user extra C. ELB is not supported by CloudWatch D. It is not possible to setup detailed monitoring for ELB Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Elastic Load Balancing includes 10 metrics and 2 dimensions, and sends data to CloudWatch every minute. This does not cost extra. QUESTION NO: 119 A user has configured ELB with two EBS backed EC2 instances. The user is trying to understand the DNS access and IP support for ELB. Which of the below mentioned statements may not help the user understand the IP mechanism supported by ELB? A. The client can connect over IPV4 or IPV6 using Dualstack B. ELB DNS supports both IPV4 and IPV6 C. Communication between the load balancer and back-end instances is always through IPV4 D. The ELB supports either IPV4 or IPV6 but not both Answer: D Explanation: Elastic Load Balancing supports both Internet Protocol version 6 (IPv6. and Internet Protocol version 4 (IPv4.. Clients can connect to the user\u2019s load balancer using either IPv4 or IPv6 (in EC2- Classic. DNS. However, communication between the load balancer and its back-end instances uses only IPv4. The user can use the Dualstack-prefixed DNS name to enable IPv6 support for communications between the client and the load balancers. Thus, the clients are able to access the load balancer using either IPv4 or IPv6 as their individual connectivity needs dictate. QUESTION NO: 120 A user has received a message from the support team that an issue occurred 1 week back between 3 AM to 4 AM and the EC2 server was not reachable. The user is checking the CloudWatch metrics of that instance. How can the user find the data easily using the CloudWatch console? A. The user can find the data by giving the exact values in the time Tab under CloudWatch metrics B. The user can find the data by filtering values of the last 1 week for a 1 hour period in the Relative tab under CloudWatch metrics C. It is not possible to find the exact time from the console. The user has to use CLI to provide the specific time D. The user can find the data by giving the exact values in the Absolute tab under CloudWatch metrics Answer: D Explanation: If the user is viewing the data inside the CloudWatch console, the console provides options to filter values either using the relative period, such as days /hours or using the Absolute tab where the user can provide data with a specific date and time. The console also provides the option to search using the local timezone under the time range caption in the console. QUESTION NO: 121 A user has setup Auto Scaling with ELB on the EC2 instances. The user wants to configure that whenever the CPU utilization is below 10%, Auto Scaling should remove one instance. How can the user configure this? A. The user can get an email using SNS when the CPU utilization is less than 10%. The user can use the desired capacity of Auto Scaling to remove the instance B. Use CloudWatch to monitor the data and Auto Scaling to remove the instances using scheduled actions C. Configure CloudWatch to send a notification to Auto Scaling Launch configuration when the CPU utilization is less than 10% and configure the Auto Scaling policy to remove the instance D. Configure CloudWatch to send a notification to the Auto Scaling group when the CPU Utilization is less than 10% and configure the Auto Scaling policy to remove the instance Answer: D Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup to receive a notification on the Auto Scaling group with the CloudWatch alarm when the CPU utilization is below a certain threshold. The user can configure the Auto Scaling policy to take action for removing the instance. When the CPU utilization is below 10% CloudWatch will send an alarm to the Auto Scaling group to execute the policy. QUESTION NO: 122 A user has enabled detailed CloudWatch metric monitoring on an Auto Scaling group. Which of the below mentioned metrics will help the user identify the total number of instances in an Auto Scaling group cluding pending, terminating and running instances? A. GroupTotalInstances B. GroupSumInstances C. It is not possible to get a count of all the three metrics together. The user has to find the individual number of running, terminating and pending instances and sum it D. GroupInstancesCount Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. For Auto Scaling, CloudWatch provides various metrics to get the group information, such as the Number of Pending, Running or Terminating instances at any moment. If the user wants to get the total number of Running, Pending and Terminating instances at any moment, he can use the GroupTotalInstances metric. QUESTION NO: 123 A user is trying to configure the CloudWatch billing alarm. Which of the below mentioned steps should be performed by the user for the first time alarm creation in the AWS Account Management section? A. Enable Receiving Billing Reports B. Enable Receiving Billing Alerts C. Enable AWS billing utility D. Enable CloudWatch Billing Threshold Answer: B Explanation: AWS CloudWatch supports enabling the billing alarm on the total AWS charges. Before the user can create an alarm on the estimated charges, he must enable monitoring of the estimated AWS charges, by selecting the option \u201cEnable receiving billing alerts\u201d. It takes about 15 minutes before the user can view the billing data. The user can then create the alarms. QUESTION NO: 124 A user is checking the CloudWatch metrics from the AWS console. The user notices that the CloudWatch data is coming in UTC. The user wants to convert the data to a local time zone. How can the user perform this? A. In the CloudWatch dashboard the user should set the local timezone so that CloudWatch shows the data only in the local time zone B. In the CloudWatch console select the local timezone under the Time Range tab to 712 view the data as per the local timezone C. The CloudWatch data is always in UTC; the user has to manually convert the data D. The user should have send the local timezone while uploading the data so that CloudWatch will show the data only in the local timezone Answer: B Explanation: If the user is viewing the data inside the CloudWatch console, the console provides options to filter values either using the relative period, such as days/hours or using the Absolute tab where the use can provide data with a specific date and time. The console also provides the option to search using the local timezone under the time range caption in the console because the time range tab allows the user to change the time zone. QUESTION NO: 125 An organization (Account ID 123412341234. has attached the below mentioned IAM policy to a user. What does this policy statement entitle the user to perform? \"Statement\": [ { \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*AccessKey*\", ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] } ] Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (Account ID 123412341234. wants some of their users to manage keys (access and secret access keys. of all IAM users, the organization should set the below mentioned policy which entitles the IAM user to modify keys of all IAM users with CLI, SDK or API. \"Statement\": [ { \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*AccessKey*\", ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] } ] QUESTION NO: 126 A user is trying to connect to a running EC2 instance using SSH. However, the user gets a connection time out error. Which of the below mentioned options is not a possible reason for rejection? A. The access key to connect to the instance is wrong B. The security group is not configured properly C. The private key used to launch the instance is not correct D. The instance CPU is heavily loaded Answer: A Explanation: If the user is trying to connect to a Linux EC2 instance and receives the connection time out error the probable reasons are: - Security group is not configured with the SSH port - The private key pair is not right - The user name to login is wrong - The instance CPU is heavily loaded, so it does not allow more connections QUESTION NO: 127 A user has configured Elastic Load Balancing by enabling a Secure Socket Layer (SSL) negotiation configuration known as a Security Policy. Which of the below mentioned options is not part of this secure policy while negotiating the SSL connection between the user and the client? A. SSL Protocols B. Client Order Preference C. SSL Ciphers D. Server Order Preference Answer: B Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the load balancer. A security policy is a combination of SSL Protocols, SSL Ciphers, and the Server order Preference option. QUESTION NO: 128 A user has configured CloudWatch monitoring on an EBS backed EC2 instance. If the user has not attached any additional device, which of the below mentioned metrics will always show a 0 value? A. DiskReadBytes 7456 B. NetworkIn C. NetworkOut D. CPUUtilization Answer: A Explanation: CloudWatch is used to monitor AWS as the well custom services. For EC2 when the user is monitoring the EC2 instances, it will capture the 7 Instance level and 3 system check parameters for the EC2 instance. Since this is an EBS backed instance, it will not have ephermal storage attached to it. Out of the 7 EC2 metrics, the 4 metrics DiskReadOps, DiskWriteOps, DiskReadBytes and DiskWriteBytes are disk related data and available only when there is ephermal storage attached to an instance. For an EBS backed instance without any additional device, this data will be 0. QUESTION NO: 129 A user has launched an EBS backed EC2 instance. What will be the difference while performing the restart or stop/start options on that instance? A. For restart it does not charge for an extra hour, while every stop/start it will be charged as a separate hour B. Every restart is charged by AWS as a separate hour, while multiple start/stop actions during a single hour will be counted as a single hour C. For every restart or start/stop it will be charged as a separate hour D. For restart it charges extra only once, while for every stop/start it will be charged as a separate hour Answer: A Explanation: For an EC2 instance launched with an EBS backed AMI, each time the instance state is changed from stop to start/ running, AWS charges a full instance hour, even if these transitions happen multiple times within a single hour. Anyway, rebooting an instance AWS does not charge a new instance billing hour. QUESTION NO: 130 A user has created a queue named \u201cmyqueue\u201d in US-East region with AWS SQS. The user\u2019s AWS account ID is 123456789012. If the user wants to perform some action on this queue, which of the below Queue URL should he use? A. http://sqs.us-east-1.amazonaws.com/123456789012/myqueue B. http://sqs.amazonaws.com/123456789012/myqueue C. http://sqs. 123456789012.us-east-1.amazonaws.com/myqueue D. http:// 123456789012.sqs. us-east-1.amazonaws.com/myqueue Answer: A Explanation: When creating a new queue in SQS, the user must provide a queue name that is unique within the scope of all queues of user\u2019s account. If the user creates queues using both the latest WSDL and a previous version, he will have a single namespace for all his queues. Amazon SQS assigns each queue created by user an identifier called a queue URL, which includes the queue name and other components that Amazon SQS determines. Whenever the user wants to perform an action on a queue, he must provide its queue URL. The queue URL for the account id 123456789012 & queue name \u201cmyqueue\u201d in US-East-1 region will be http:// sqs.us-east1.amazonaws.com/123456789012/myqueue. QUESTION NO: 131 A sys admin is trying to understand the Auto Scaling activities. Which of the below mentioned processes is not performed by Auto Scaling? A. Reboot Instance B. Schedule Actions C. Replace Unhealthy D. Availability Zone Balancing Answer: A Explanation: There are two primary types of Auto Scaling processes: Launch and Terminate, which launch or terminate instances, respectively. Some other actions performed by Auto Scaling are: AddToLoadbalancer, AlarmNotification, HealthCheck, AZRebalance, ReplaceUnHealthy, and ScheduledActions. QUESTION NO: 132 A sys admin is trying to understand EBS snapshots. Which of the below mentioned statements will not be useful to the admin to understand the concepts about a snapshot? A. The snapshot is synchronous B. It is recommended to stop the instance before taking a snapshot for consistent data C. The snapshot is incremental D. The snapshot captures the data that has been written to the hard disk when the snapshot command was executed Answer: A Explanation: The AWS snapshot is a point in time backup of an EBS volume. When the snapshot command is executed it will capture the current state of the data that is written on the drive and take a backup. For a better and consistent snapshot of the root EBS volume, AWS recommends stopping the instance. For additional volumes it is recommended to unmount the device. The snapshots are asynchronous and incremental. QUESTION NO: 133 A root account owner has created an S3 bucket testmycloud. The account owner wants to allow everyone to upload the objects as well as enforce that the person who uploaded the object should manage the permission of those objects. Which is the easiest way to achieve this? A. The root account owner should create a bucket policy which allows the IAM users to upload the object B. The root account owner should create the bucket policy which allows the other account owners to set the object policy of that bucket C. The root account should use ACL with the bucket to allow everyone to upload the object D. The root account should create the IAM users and provide them the permission to upload content to the bucket Answer: C Explanation: Each AWS S3 bucket and object has an ACL (Access Control List. associated with it. An ACL is a list of grants identifying the grantee and the permission granted. The user can use ACLs to grant basic read/write permissions to other AWS accounts. ACLs use an Amazon S3\u2013specific XML schema. The user cannot grant permissions to other users in his account. ACLs are suitable for specific scenarios. For example, if a bucket owner allows other AWS accounts to upload objects, permissions to these objects can only be managed using the object ACL by the AWS account that owns the object. QUESTION NO: 134 An organization has setup consolidated billing with 3 different AWS accounts. Which of the below mentioned advantages will organization receive in terms of the AWS pricing? A. The consolidated billing does not bring any cost advantage for the organization B. All AWS accounts will be charged for S3 storage by combining the total storage of each account C. The EC2 instances of each account will receive a total of 750*3 micro instance hours free D. The free usage tier for all the 3 accounts will be 3 years and not a single year Answer: B Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. For billing purposes, AWS treats all the accounts on the consolidated bill as one account. Some services, such as Amazon EC2 and Amazon S3 have volume pricing tiers across certain usage dimensions that give the user lower prices when he uses the service more. QUESTION NO: 135 A user has launched two EBS backed EC2 instances in the US-East-1a region. The user wants to change the zone of one of the instances. How can the user change it? A. Stop one of the instances and change the availability zone B. The zone can only be modified using the AWS CLI C. From the AWS EC2 console, select the Actions - > Change zones and specify new zone D. Create an AMI of the running instance and launch the instance in a separate AZ Answer: D Explanation: With AWS EC2, when a user is launching an instance he can select the availability zone (AZ. at the time of launch. If the zone is not selected, AWS selects it on behalf of the user. Once the instance is launched, the user cannot change the zone of that instance unless he creates an AMI of that instance and launches a new instance from it. QUESTION NO: 136 A user wants to make so that whenever the CPU utilization of the AWS EC2 instance is above 90%, the redlight of his bedroom turns on. Which of the below mentioned AWS services is helpful for this purpose? A. AWS CloudWatch + AWS SES B. AWS CloudWatch + AWS SNS C. None. It is not possible to configure the light with the AWS infrastructure services D. AWS CloudWatch and a dedicated software turning on the light Answer: B Explanation: Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can configure some sensor devices at his home which receives data on the HTTP end point (REST calls. and turn on the red light. The user can configure the CloudWatch alarm to send a notification to the AWS SNS HTTP end point (the sensor device. and it will turn the light red when there is an alarm condition. QUESTION NO: 137 An organization has added 3 of his AWS accounts to consolidated billing. One of the AWS accounts has purchased a Reserved Instance (RI. of a small instance size in the US-East-1a zone. All other AWS accounts are running instances of a small size in the same zone. What will happen in this case for the RI pricing? A. Only the account that has purchased the RI will get the advantage of RI pricing B. One instance of a small size and running in the US-East-1a zone of each AWS account will get the benefit of RI pricing C. Any single instance from all the three accounts can get the benefit of AWS RI pricing if they are running in the same zone and are of the same size D. If there are more than one instances of a small size running across multiple accounts in the same zone no one will get the benefit of RI Answer: C Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. For billing purposes, consolidated billing treats all the accounts on the consolidated bill as one account. This means that all accounts on a consolidated bill can receive the hourly cost benefit of the Amazon EC2 Reserved Instances purchased by any other account. In this case only one Reserved Instance has been purchased by one account. Thus, only a single instance from any of the accounts will get the advantage of RI. AWS will implement the blended rate for each instance if more than one instance is running concurrently. QUESTION NO: 138 An organization is planning to use AWS for 5 different departments. The finance department is responsible to pay for all the accounts. However, they want the cost separation for each account to map with the right cost centre. How can the finance department achieve this? A. Create 5 separate accounts and make them a part of one consolidate billing B. Create 5 separate accounts and use the IAM cross account access with the roles for better management C. Create 5 separate IAM users and set a different policy for their access D. Create 5 separate IAM groups and add users as per the department\u2019s employees Answer: A Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. Consolidated billing enables the organization to see a combined view of the AWS charges incurred by each account as well as obtain a detailed cost report for each of the individual AWS accounts associated with the paying account. QUESTION NO: 139 A user has setup an EBS backed instance and a CloudWatch alarm when the CPU utilization is more than 65%. The user has setup the alarm to watch it for 5 periods of 5 minutes each. The CPU utilization is 60% between 9 AM to 6 PM. The user has stopped the EC2 instance for 15 minutes between 11 AM to 11:15 AM. What will be the status of the alarm at 11:30 AM? A. Alarm B. OK C. Insufficient Data D. Error Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The state of the alarm will be OK for the whole day. When the user stops the instance for three periods the alarm may not receive the data QUESTION NO: 140 A user is running one instance for only 3 hours every day. The user wants to save some cost with the instance. Which of the below mentioned Reserved Instance categories is advised in this case? A. The user should not use RI; instead only go with the on-demand pricing B. The user should use the AWS high utilized RI C. The user should use the AWS medium utilized RI D. The user should use the AWS low utilized RI Answer: A Explanation: The AWS Reserved Instance provides the user with an option to save some money by paying a one-time fixed amount and then save on the hourly rate. It is advisable that if the user is having 30% or more usage of an instance per day, he should go for a RI. If the user is going to use an EC2 instance for more than 2200-2500 hours per year, RI will help the user save some cost. Here, the instance is not going to run for less than 1500 hours. Thus, it is advisable that the user should use the on-demand pricing. QUESTION NO: 141 A user has setup an RDS DB with Oracle. The user wants to get notifications when someone modifies the security group of that DB. How can the user configure that? A. It is not possible to get the notifications on a change in the security group B. Configure SNS to monitor security group changes C. Configure event notification on the DB security group D. Configure the CloudWatch alarm on the DB for a change in the security group Answer: C Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. These events can be configured for source categories, such as DB instance, DB security group, DB snapshot and DB parameter group. If the user is subscribed to a Configuration Change category for a DB security group, he will be notified when the DB security group is changed. QUESTION NO: 142 A user is trying to setup a recurring Auto Scaling process. The user has setup one process to scale up every day at 8 am and scale down at 7 PM. The user is trying to setup another recurring process which scales up on the 1st of every month at 8 AM and scales down the same day at 7 PM. What will Auto Scaling do in this scenario? A. Auto Scaling will execute both processes but will add just one instance on the 1st B. Auto Scaling will add two instances on the 1st of the month C. Auto Scaling will schedule both the processes but execute only one process randomly D. Auto Scaling will throw an error since there is a conflict in the schedule of two separate Auto Scaling Processes Answer: D Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can also configure the recurring schedule action which will follow the Linux cron format. As per Auto Scaling, a scheduled action must have a unique time value. If the user attempts to schedule an activity at a time when another existing activity is already scheduled, the call will be rejected with an error message noting the conflict. QUESTION NO: 143 A user is planning to setup infrastructure on AWS for the Christmas sales. The user is planning to use Auto Scaling based on the schedule for proactive scaling. What advise would you give to the user? A. It is good to schedule now because if the user forgets later on it will not scale up B. The scaling should be setup only one week before Christmas C. Wait till end of November before scheduling the activity D. It is not advisable to use scheduled based scaling Answer: C Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can specify any date in the future to scale up or down during that period. As per Auto Scaling the user can schedule an action for up to a month in the future. Thus, it is recommended to wait until end of November before scheduling for Christmas. QUESTION NO: 144 A user is trying to understand the ACL and policy for an S3 bucket. Which of the below mentioned policy permissions is equivalent to the WRITE ACL on a bucket? A. s3:GetObjectAcl B. s3:GetObjectVersion C. s3:ListBucketVersions D. s3:DeleteObject Answer: D Explanation: Amazon S3 provides a set of operations to work with the Amazon S3 resources. Each AWS S3 bucket can have an ACL (Access Control List. or bucket policy associated with it. The WRITE ACL list allows the other AWS accounts to write/modify to that bucket. The equivalent S3 bucket policy permission for it is s3:DeleteObject. QUESTION NO: 145 A user has created an ELB with Auto Scaling. Which of the below mentioned offerings from ELB helps the user to stop sending new requests traffic from the load balancer to the EC2 instance when the instance is being deregistered while continuing in-flight requests? A. ELB sticky session B. ELB deregistration check C. ELB connection draining D. ELB auto registration Off Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. QUESTION NO: 146 A user has launched an EC2 instance from an instance store backed AMI. The infrastructure team wants to create an AMI from the running instance. Which of the below mentioned steps will not be performed while creating the AMI? A. Define the AMI launch permissions B. Upload the bundled volume C. Register the AMI D. Bundle the volume Answer: A Explanation: When the user has launched an EC2 instance from an instance store backed AMI, it will need to follow certain steps, such as \u201cBundling the root volume\u201d, \u201cUploading the bundled volume\u201d and \u201cRegister the AMI\u201d. Once the AMI is created the user can setup the launch permission. However, it is not required to setup during the launch. QUESTION NO: 147 You are managing the AWS account of a big organization. The organization has more than 1000+ employees and they want to provide access to the various services to most of the employees. Which of the below mentioned options is the best possible solution in this case? A. The user should create a separate IAM user for each employee and provide access to them as per the policy B. The user should create an IAM role and attach STS with the role. The user should attach that role to the EC2 instance and setup AWS authentication on that server C. The user should create IAM groups as per the organization\u2019s departments and add each user to the group for better access control D. Attach an IAM role with the organization\u2019s authentication service to authorize each user for various AWS services Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user is managing an AWS account for an organization that already has an identity system, such as the login system for the corporate network (SSO.. In this case, instead of creating individual IAM users or groups for each user who need AWS access, it may be more practical to use a proxy server to translate the user identities from the organization network into the temporary AWS security credentials. This proxy server will attach an IAM role to the user after authentication. QUESTION NO: 148 A user has configured a VPC with a new subnet. The user has created a security group. The user wants to configure that instances of the same subnet communicate with each other. How can the user configure this with the security group? A. There is no need for a security group modification as all the instances can communicate with each other inside the same subnet B. Configure the subnet as the source in the security group and allow traffic on all the protocols and ports C. Configure the security group itself as the source and allow traffic on all the protocols and ports D. The user has to use VPC peering to configure this Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features that the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level. If the user is using the default security group it will have a rule which allows the instances to communicate with other. For a new security group the user has to specify the rule, add it to define the source as the security group itself, and select all the protocols and ports for that source. QUESTION NO: 149 A user is launching an instance. He is on the \u201cTag the instance\u201d screen. Which of the below mentioned information will not help the user understand the functionality of an AWS tag? A. Each tag will have a key and value B. The user can apply tags to the S3 bucket C. The maximum value of the tag key length is 64 unicode characters D. AWS tags are used to find the cost distribution of various resources Answer: C Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources, AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. Each tag will have a key-value and can be applied to services, such as EC2, S3, RDS, EMR, etc. The maximum size of a tag key is 128 unicode characters. QUESTION NO: 150 A user has created a VPC with CIDR 20.0.0.0/16. The user has created public and VPN only subnets along with hardware VPN access to connect to the user\u2019s datacenter. The user wants to make so that all traffic coming to the public subnet follows the organization\u2019s proxy policy. How can the user make this happen? A. Setting up a NAT with the proxy protocol and configure that the public subnet receives traffic from NAT B. Settin up a proxy policy in the internet gateway connected with the public subnet C. It is not possible to setup the proxy policy for a public subnet D. Setting the route table and security group of the public subnet which receives traffic from a virtual private gateway Answer: D Explanation: The user can create subnets within a VPC. If the user wants to connect to VPC from his own data centre, he can setup public and VPN only subnets which uses hardware VPN access to connect with his data centre. When the user has configured this setup, it will update the main route table used with the VPN-only subnet, create a custom route table and associate it with the public subnet. It also creates an internet gateway for the public subnet. By default the internet traffic of the VPN subnet is routed to a virtual private gateway while the internet traffic of the public subnet is routed through the internet gateway. The user can set up the route and security group rules. These rules enable the traffic to come from the organization\u2019s network over the virtual private gateway to the public subnet to allow proxy settings on that public subnet. QUESTION NO: 151 A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25 and a private subnet with CIDR 20.0.0.128/25. The user has launched one instance each in the private and public subnets. Which of the below mentioned options cannot be the correct IP address (private IP. assigned to an instance in the public or private subnet? A. 20.0.0.255 B. 20.0.0.132 C. 20.0.0.122 D. 20.0.0.55 Answer: A Explanation: When the user creates a subnet in VPC, he specifies the CIDR block for the subnet. In this case the user has created a VPC with the CIDR block 20.0.0.0/24, which supports 256 IP addresses (20.0.0.0 to 20.0.0.255.. The public subnet will have IP addresses between 20.0.0.0 - 20.0.0.127 and the private subnet will have IP addresses between 20.0.0.128 - 20.0.0.255. AWS reserves the first four IP addresses and the last IP address in each subnet\u2019s CIDR block. These are not available for the user to use. Thus, the instance cannot have an IP address of 20.0.0.255 QUESTION NO: 152 A user has launched an EBS backed EC2 instance. The user has rebooted the instance. Which of the below mentioned statements is not true with respect to the reboot action? A. The private and public address remains the same B. The Elastic IP remains associated with the instance C. The volume is preserved D. The instance runs on a new host computer Answer: D Explanation: A user can reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. However, it is recommended that the user use the Amazon EC2 to reboot the instance instead of running the operating system reboot command from the instance. The instance remains on the same host computer and maintains its public DNS name, private IP address, and any data on its instance store volumes. It typically takes a few minutes for the reboot to complete, but the time it takes to reboot depends on the instance configuration. QUESTION NO: 153 A user has setup a web application on EC2. The user is generating a log of the application performance at every second. There are multiple entries for each second. If the user wants to send that data to CloudWatch every minute, what should he do? A. The user should send only the data of the 60th second as CloudWatch will map the receive data timezone with the sent data timezone B. It is not possible to send the custom metric to CloudWatch every minute C. Give CloudWatch the Min, Max, Sum, and SampleCount of a number of every minute D. Calculate the average of one minute and send the data to CloudWatch Answer: C Explanation: Amazon CloudWatch aggregates statistics according to the period length that the user has specified while getting data from CloudWatch. The user can publish as many data points as he wants with the same or similartime stamps. CloudWatch aggregates them by the period length when the user calls get statistics about those data points. CloudWatch records the average (sum of all items divided by the number of items. of the values received for every 1-minute period, as well as the number of samples, maximum value, and minimum value for the same time period. CloudWatch will aggregate all the data which have time stamps within a one-minute period. QUESTION NO: 154 An AWS root account owner is trying to create a policy to access RDS. Which of the below mentioned statements is true with respect to the above information? A. Create a policy which allows the users to access RDS and apply it to the RDS instances B. The user cannot access the RDS database if he is not assigned the correct IAM policy C. The root account owner should create a policy for the IAM user and give him access to the RDS services D. The policy should be created for the user and provide access for RDS Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the account owner wants to create a policy for RDS, the owner has to create an IAM user and define the policy which entitles the IAM user with various RDS services such as Launch Instance, Manage security group, Manage parameter group etc. QUESTION NO: 155 A user is using a small MySQL RDS DB. The user is experiencing high latency due to the Multi AZ feature.Which of the below mentioned options may not help the user in this situation? A. Schedule the automated back up in non-working hours B. Use a large or higher size instance C. Use PIOPS D. Take a snapshot from standby Replica Answer: D Explanation: An RDS DB instance which has enabled Multi AZ deployments may experience increased write and commit latency compared to a Single AZ deployment, due to synchronous data replication. The user may also face changes in latency if deployment fails over to the standby replica. For production workloads, AWS recommends the user to use provisioned IOPS and DB instance classes (m1.large and larger. as they are optimized for provisioned IOPS to give a fast, and consistent performance. With Multi AZ feature, the user can not have option to take snapshot from replica. QUESTION NO: 156 A user is displaying the CPU utilization, and Network in and Network out CloudWatch metrics data of a single instance on the same graph. The graph uses one Y-axis for CPU utilization and Network in and another Y-axis for Network out. Since Network in is too high, the CPU utilization data is not visible clearly on graph to the user. How can the data be viewed better on the same graph? A. It is not possible to show multiple metrics with the different units on the same graph B. Add a third Y-axis with the console to show all the data in proportion C. Change the axis of Network by using the Switch command from the graph D. Change the units of CPU utilization so it can be shown in proportion with Network Answer: C Explanation: Amazon CloudWatch provides the functionality to graph the metric data generated either by the AWS services or the custom metric to make it easier for the user to analyse. It is possible to show the multiple metrics with different units on the same graph. If the graph is not plotted properly due to a difference in the unit data over two metrics, the user can change the Y-axis of one of the graph by selecting that graph and clicking on the Switch option. QUESTION NO: 157 A user is planning to use AWS services for his web application. If the user is trying to set up his own billing management system for AWS, how can he configure it? A. Set up programmatic billing access. Download and parse the bill as per the requirement B. It is not possible for the user to create his own billing management service with AWS C. Enable the AWS CloudWatch alarm which will provide APIs to download the alarm data D. Use AWS billing APIs to download the usage report of each service from the AWS billing console Answer: A Explanation: AWS provides an option to have programmatic access to billing. Programmatic Billing Access leverages the existing Amazon Simple Storage Service (Amazon S3. APIs. Thus, the user can build applications that reference his billing data from a CSV (comma-separated value. file stored in an Amazon S3 bucket. AWS will upload the bill to the bucket every few hours and the user can download the bill CSV from the bucket, parse itand create a billing system as per the requirement. QUESTION NO: 158 A user is planning to schedule a backup for an EBS volume. The user wants security of the snapshot data. How can the user achieve data encryption with a snapshot? A. Use encrypted EBS volumes so that the snapshot will be encrypted by AWS B. While creating a snapshot select the snapshot with encryption C. By default the snapshot is encrypted by AWS D. Enable server side encryption for the snapshot using S3 Answer: A Explanation: AWS EBS supports encryption of the volume. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. The data at rest, the I/O as well as all the snapshots of the encrypted EBS will also be encrypted. EBS encryption is based on the AES-256 cryptographic algorithm, which is the industry standard. QUESTION NO: 159 A user has created a public subnet with VPC and launched an EC2 instance within it. The user is trying to delete the subnet. What will happen in this scenario? A. It will delete the subnet and make the EC2 instance as a part of the default subnet B. It will not allow the user to delete the subnet until the instances are terminated C. It will delete the subnet as well as terminate the instances D. The subnet can never be deleted independently, but the user has to delete the VPC first Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When an instance is launched it will have a network interface attached with it. The user cannot delete the subnet until he terminates the instance and deletes the network interface. QUESTION NO: 160 A user has setup an EBS backed instance and attached 2 EBS volumes to it. The user has setup a CloudWatch alarm on each volume for the disk data. The user has stopped the EC2 instance and detached the EBS volumes. What will be the status of the alarms on the EBS volume? A. OK B. Insufficient Data C. Alarm D. The EBS cannot be detached until all the alarms are removed Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. Alarms invoke actions only for sustained state changes. There are three states of the alarm: OK, Alarm and Insufficient data. In this case since the EBS is detached and inactive the state will be Insufficient. QUESTION NO: 161 A user has launched an EC2 instance from an instance store backed AMI. The infrastructure team wants to create an AMI from the running instance. Which of the below mentioned credentials is not required while creating the AMI? A. AWS account ID B. X.509 certificate and private key C. AWS login ID to login to the console D. Access key and secret access key Answer: C Explanation: When the user has launched an EC2 instance from an instance store backed AMI and the admin team wants to create an AMI from it, the user needs to setup the AWS AMI or the API tools first. Once the tool is setup the user will need the following credentials: AWS account ID; AWS access and secret access key; X.509 certificate with private key. QUESTION NO: 162 A user has configured an SSL listener at ELB as well as on the back-end instances. Which of the below mentioned statements helps the user understand ELB traffic handling with respect to the SSL listener? A. It is not possible to have the SSL listener both at ELB and back-end instances B. ELB will modify headers to add requestor details C. ELB will intercept the request to add the cookie details if sticky session is enabled D. ELB will not modify the headers Answer: D Explanation: When the user has configured Transmission Control Protocol (TCP. or Secure Sockets Layer (SSL. for both front-end and back-end connections of the Elastic Load Balancer, the load balancer forwards the request to the back-end instances without modifying the request headers unless the proxy header is enabled. SSL does not support sticky sessions. If the user has enabled a proxy protocol it adds the source and destination IP to the header. QUESTION NO: 163 A user has created a Cloudformation stack. The stack creates AWS services, such as EC2 instances, ELB, AutoScaling, and RDS. While creating the stack it created EC2, ELB and AutoScaling but failed to create RDS. What will Cloudformation do in this scenario? A. Cloudformation can never throw an error after launching a few services since it verifies all the steps before launching. B. It will warn the user about the error and ask the user to manually create RDS C. Rollback all the changes and terminate all the created services D. It will wait for the user\u2019s input about the error and correct the mistake after the input Answer: C Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The AWS Cloudformation stack is a collection of AWS resources which are created and managed as a single unit when AWS CloudFormation instantiates a template. If any of the services fails to launch, Cloudformation will rollback all the changes and terminate or delete all the created services. QUESTION NO: 164 A user is trying to launch an EBS backed EC2 instance under free usage. The user wants to achieve encryption of the EBS volume. How can the user encrypt the data at rest? A. Use AWS EBS encryption to encrypt the data at rest B. The user cannot use EBS encryption and has to encrypt the data manually or using a third party tool C. The user has to select the encryption enabled flag while launching the EC2 instance D. Encryption of volume is not available as a part of the free usage tier Answer: B Explanation: AWS EBS supports encryption of the volume while creating new volumes. It supports encryption of the data at rest, the I/O as well as all the snapshots of the EBS volume. The EBS supports encryption for the selected instance type and the newer generation instances, such as m3, c3, cr1, r3, g2. It is not supported with a micro instance. QUESTION NO: 165 A user has created a VPC with public and private subnets using the VPC wizard. The user has not launched any instance manually and is trying to delete the VPC. What will happen in this scenario? A. It will not allow to delete the VPC as it has subnets with route tables B. It will not allow to delete the VPC since it has a running route instance C. It will terminate the VPC along with all the instances launched by the wizard D. It will not allow to delete the VPC since it has a running NAT instance Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance with an elastic IP. If the user is trying to delete the VPC it will not allow as the NAT instance is still running. QUESTION NO: 166 An organization is measuring the latency of an application every minute and storing data inside a file in the JSON format. The organization wants to send all latency data to AWS CloudWatch. How can the organization achieve this? A. The user has to parse the file before uploading data to CloudWatch B. It is not possible to upload the custom data to CloudWatch C. The user can supply the file as an input to the CloudWatch command D. The user can use the CloudWatch Import command to import data from the file to CloudWatch Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user has to always include the namespace as part of the request. If the user wants to upload the custom data from a file, he can supply file name along with the parameter -- metric-data to command put-metric-data. QUESTION NO: 167 A user has launched an EBS backed instance with EC2-Classic. The user stops and starts the instance. Which of the below mentioned statements is not true with respect to the stop/start action? A. The instance gets new private and public IP addresses B. The volume is preserved C. The Elastic IP remains associated with the instance D. The instance may run on a anew host computer Answer: C Explanation: A user can always stop/start an EBS backed EC2 instance. When the user stops the instance, it first enters the stopping state, and then the stopped state. AWS does not charge the running cost but charges only for the EBS storage cost. If the instance is running in EC2-Classic, it receives a new private IP address; as the Elastic IP address (EIP. associated with the instance is no longer associated with that instance. QUESTION NO: 168 A user has launched an RDS postgreSQL DB with AWS. The user did not specify the maintenance window during creation. The user has configured RDS to update the DB instance type from micro to large. If the user wants to have it during the maintenance window, what will AWS do? A. AWS will not allow to update the DB until the maintenance window is configured B. AWS will select the default maintenance window if the user has not provided it C. AWS will ask the user to specify the maintenance window during the update D. It is not possible to change the DB size from micro to large with RDS Answer: B Explanation: AWS RDS has a compulsory maintenance window which by default is 30 minutes. If the user does not specify the maintenance window during the creation of RDS then AWS will select a 30-minute maintenance window randomly from an 8-hour block of time per region. In this case, Amazon RDS assigns a 30-minute maintenance window on a randomly selected day of the week. QUESTION NO: 169 A user has created a subnet in VPC and launched an EC2 instance within it. The user has not selected the option to assign the IP address while launching the instance. The user has 3 elastic IPs and is trying to assign one of the Elastic IPs to the VPC instance from the console. The console does not show any instance in the IP assignment screen. What is a possible reason that the instance is unavailable in the assigned IP console? A. The IP address may be attached to one of the instances B. The IP address belongs to a different zone than the subnet zone C. The user has not created an internet gateway D. The IP addresses belong to EC2 Classic; so they cannot be assigned to VPC Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When the user is launching an instance he needs toselect an option which attaches a public IP to the instance. If the user has not selected the option to attach the public IP then it will only have a private IP when launched. If the user wants to connect to an instance from the internet he should create an elastic IP with VPC. If the elastic IP is a part of EC2 Classic it cannot be assigned to a VPC instance. QUESTION NO: 170 A user has launched multiple EC2 instances for the purpose of development and testing in the same region. The user wants to find the separate cost for the production and development instances. How can the user find the cost distribution? A. The user should download the activity report of the EC2 services as it has the instance ID wise data B. It is not possible to get the AWS cost usage data of single region instances separately C. The user should use Cost Distribution Metadata and AWS detailed billing D. The user should use Cost Allocation Tags and AWS billing reports Answer: D Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources (such as Amazon EC2 instances or Amazon S3 buckets., AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. The user can apply tags which represent business categories (such as cost centres, application names, or instance type \u2013 Production/Dev. to organize usage costs across multiple services. QUESTION NO: 171 A user has created a VPC with CIDR 20.0.0.0/16 using VPC Wizard. The user has created a public CIDR (20.0.0.0/24) and a VPN only subnet CIDR (20.0.1.0/24) along with the hardware VPN access to connect to the user\u2019s data centre. Which of the below mentioned components is not present when the VPC is setup with the wizard? A. Main route table attached with a VPN only subnet B. A NAT instance configured to allow the VPN subnet instances to connect with the internet C. Custom route table attached with a public subnet D. An internet gateway for a public subnet Answer: B Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will update the main route table used with the VPN-only subnet, create a custom route table and associate it with the public subnet. It also creates an internet gateway for the public subnet. The wizard does not create a NAT instance by default. The user can create it manually and attach it with a VPN only subnet. QUESTION NO: 172 A user has created a VPC with the public subnet. The user has created a security group for that VPC. Which of the below mentioned statements is true when a security group is created? A. It can connect to the AWS services, such as S3 and RDS by default B. It will have all the inbound traffic by default C. It will have all the outbound traffic by default D. It will by default allow traffic to the internet gateway Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level while ACLs work at the subnet level. When a user creates a security group with AWS VPC, by default it will allow all the outbound traffic but block all inbound traffic. QUESTION NO: 173 A user has setup an Auto Scaling group. The group has failed to launch a single instance for more than 24 hours. What will happen to Auto Scaling in this condition? A. Auto Scaling will keep trying to launch the instance for 72 hours B. Auto Scaling will suspend the scaling process C. Auto Scaling will start an instance in a separate region D. The Auto Scaling group will be terminated automatically Answer: B Explanation: If Auto Scaling is trying to launch an instance and if the launching of the instance fails continuously, it will suspend the processes for the Auto Scaling groups since it repeatedly failed to launch an instance. This is known as an administrative suspension. It commonly applies to the Auto Scaling group that has no running instances which is trying to launch instances for more than 24 hours, and has not succeeded in that to do so. QUESTION NO: 174 A user is planning to set up the Multi AZ feature of RDS. Which of the below mentioned conditions won't take advantage of the Multi AZ feature? A. Availability zone outage B. A manual failover of the DB instance using Reboot with failover option C. Region outage D. When the user changes the DB instance\u2019s server type Answer: C Explanation: Amazon RDS when enabled with Multi AZ will handle failovers automatically. Thus, the user can resume database operations as quickly as possible without administrative intervention. The primary DB instance switches over automatically to the standby replica if any of the following conditions occur: An Availability Zone outage The primary DB instance fails The DB instance's server type is changed The DB instance is undergoing software patching A manual failover of the DB instance was initiated using Reboot with failover QUESTION NO: 175 An organization has configured Auto Scaling with ELB. One of the instance health check returns the status as Impaired to Auto Scaling. What will Auto Scaling do in this scenario? A. Perform a health check until cool down before declaring that the instance has failed B. Terminate the instance and launch a new instance C. Notify the user using SNS for the failed state D. Notify ELB to stop sending traffic to the impaired instance Answer: B Explanation: The Auto Scaling group determines the health state of each instance periodically by checking the results of the Amazon EC2 instance status checks. If the instance status description shows any other state other than \u201crunning\u201d or the system status description shows impaired, Auto Scaling considers the instance to be unhealthy. Thus, it terminates the instance and launches a replacement. QUESTION NO: 176 A user is using Cloudformation to launch an EC2 instance and then configure an application after the instance is launched. The user wants the stack creation of ELB and AutoScaling to wait until the EC2 instance is launched and configured properly. How can the user configure this? A. It is not possible that the stack creation will wait until one service is created and launched B. The user can use the HoldCondition resource to wait for the creation of the other dependent resources C. The user can use the DependentCondition resource to hold the creation of the other dependent resources D. The user can use the WaitCondition resource to hold the creation of the other 1034 dependent resources Answer: D Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. AWS CloudFormation provides a WaitCondition resource which acts as a barrier and blocks the creation of other resources until a completion signal is received from an external source, such as a user application or management system. QUESTION NO: 177 An organization has configured two single availability zones. The Auto Scaling groups are configured in separate zones. The user wants to merge the groups such that one group spans across multiple zones. How can the user configure this? A. Run the command as-join-auto-scaling-group to join the two groups B. Run the command as-update-auto-scaling-group to configure one group to span across zones and delete the other group C. Run the command as-copy-auto-scaling-group to join the two groups D. Run the command as-merge-auto-scaling-group to merge the groups Answer: B Explanation: If the user has configured two separate single availability zone Auto Scaling groups and wants to merge them then he should update one of the groups and delete the other one. While updating the first group it is recommended that the user should increase the size of the minimum, maximum and desired capacity as a summation of both the groups. QUESTION NO: 178 An AWS account wants to be part of the consolidated billing of his organization\u2019s payee account. How can the owner of that account achieve this? A. The payee account has to request AWS support to link the other accounts with his account B. The owner of the linked account should add the payee account to his master account list from the billing console C. The payee account will send a request to the linked account to be a part of consolidated billing D. The owner of the linked account requests the payee account to add his account to consolidated billing Answer: C Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. To add a particular account (linked. to the master (payee. account, the payee account has to request the linked account to join consolidated billing. Once the linked account accepts the request henceforth all charges incurred by the linked account will be paid by the payee account. QUESTION NO: 179 A sysadmin has created the below mentioned policy on an S3 bucket named cloudacademy. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy] }] A. It will make the cloudacademy bucket as well as all its objects as public B. It will allow everyone to view the ACL of the bucket C. It will give an error as no object is defined as part of the policy while the action defines the rule about the object D. It will make the cloudacademy bucket as public Answer: D Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. In the sample policy the action says \u201cS3:ListBucket\u201d for effect Allow on Resource arn:aws:s3:::cloudacademy. This will make the cloudacademy bucket public. \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy] }] QUESTION NO: 180 A user has launched two EBS backed EC2 instances in the US-East-1a region. The user wants to change the zone of one of the instances. How can the user change it? A. The zone can only be modified using the AWS CLI B. It is not possible to change the zone of an instance after it is launched C. Stop one of the instances and change the availability zone D. From the AWS EC2 console, select the Actions - > Change zones and specify the new zone Answer: B Explanation: With AWS EC2, when a user is launching an instance he can select the availability zone (AZ. at the time of launch. If the zone is not selected, AWS selects it on behalf of the user. Once the instance is launched, the user cannot change the zone of that instance unless he creates an AMI of that instance and launches a new instance from it. QUESTION NO: 181 An organization (account ID 123412341234. has configured the IAM policy to allow the user to modify his credentials. What will the below mentioned statement allow the user to perform? { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"iam:AddUserToGroup\", \"iam:RemoveUserFromGroup\", \"iam:GetGroup\" ], \"Resource\": \"arn:aws:iam:: 123412341234:group/TestingGroup\" }] A. The IAM policy will throw an error due to an invalid resource name B. The IAM policy will allow the user to subscribe to any IAM group C. Allow the IAM user to update the membership of the group called TestingGroup D. Allow the IAM user to delete the TestingGroup Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (account ID 123412341234. wants their users to manage their subscription to the groups, they should create a relevant policy for that. The below mentioned policy allows the respective IAM user to update the membership of the group called MarketingGroup. { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"iam:AddUserToGroup\", \"iam:RemoveUserFromGroup\", \"iam:GetGroup\" ], \"Resource\": \"arn:aws:iam:: 123412341234:group/ TestingGroup \" }] QUESTION NO: 182 A user has configured ELB with two EBS backed instances. The user has stopped the instances for 1 week to save costs. The user restarts the instances after 1 week. Which of the below mentioned statements will help the user to understand the ELB and instance registration better? A. There is no way to register the stopped instances with ELB B. The user cannot stop the instances if they are registered with ELB C. If the instances have the same Elastic IP assigned after reboot they will be registered with ELB D. The instances will automatically get registered with ELB Answer: C Explanation: Elastic Load Balancing registers the user\u2019s load balancer with his EC2 instance using the associated IP address. When the instances are stopped and started back they will have a different IP address. Thus, they will not get registered with ELB unless the user manually registers them. If the instances are assigned the same Elastic IP after reboot they will automatically get registered with ELB. QUESTION NO: 183 A user is trying to connect to a running EC2 instance using SSH. However, the user gets a Host key not found error. Which of the below mentioned options is a possible reason for rejection? A. The user has provided the wrong user name for the OS login B. The instance CPU is heavily loaded C. The security group is not configured properly D. The access key to connect to the instance is wrong Answer: A Explanation: If the user is trying to connect to a Linux EC2 instance and receives the Host Key not found error the probable reasons are: The private key pair is not right The user name to login is wrong QUESTION NO: 184 A user has hosted an application on EC2 instances. The EC2 instances are configured with ELB and Auto Scaling. The application server session time out is 2 hours. The user wants to configure connection draining to ensure that all in-flight requests are supported by ELB even though the instance is being deregistered. What time out period should the user specify for connection draining? A. 5 minutes B. 1 hour C. 30 minutes D. 2 hours Answer: B QUESTION NO: 185 A user is using the AWS EC2. The user wants to make so that when there is an issue in the EC2 server, such as instance status failed, it should start a new instance in the user\u2019s private cloud. Which AWS service helps to achieve this automation? A. AWS CloudWatch + Cloudformation B. AWS CloudWatch + AWS AutoScaling + AWS ELB C. AWS CloudWatch + AWS VPC D. AWS CloudWatch + AWS SNS Answer: D Explanation: Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can configure a web service (HTTP End point. in his data centre which receives data and launches an instance in the private cloud. The user should configure the CloudWatch alarm to send a notification to SNS when the \u201cStatusCheckFailed\u201d metric is true for the EC2 instance. The SNS topic can be configured to send a notification to the user\u2019s HTTP end point which launches an instance in the private cloud. QUESTION NO: 186 A sys admin has enabled logging on ELB. Which of the below mentioned fields will not be a part of the log file name? A. Load Balancer IP B. EC2 instance IP C. S3 bucket name D. Random string Answer: B Explanation: Elastic Load Balancing access logs capture detailed information for all the requests made to the load balancer. Elastic Load Balancing publishes a log file from each load balancer node at the interval that the user has specified. The load balancer can deliver multiple logs for the same period. Elastic Load Balancing creates log file names in the following format: \u201c{Bucket}/{Prefix}/ AWSLogs/{AWS AccountID}/elasticloadbalancing/{Region}/{Year}/{Month}/{Day}/{AWS Account ID}_elasticloadbalancing_{Region}_{Load Balancer Name}_{End Time}_{Load Balancer IP}_{Random String}.log\u201c QUESTION NO: 187 A user has created a queue named \u201cawsmodule\u201d with SQS. One of the consumers of queue is down for 3 days and then becomes available. Will that component receive message from queue? A. Yes, since SQS by default stores message for 4 days B. No, since SQS by default stores message for 1 day only C. No, since SQS sends message to consumers who are available that time D. Yes, since SQS will not delete message until it is delivered to all consumers Answer: A Explanation: SQS allows the user to move data between distributed components of applications so they can perform different tasks without losing messages or requiring each component to be always available. Queues retain messages for a set period of time. By default, a queue retains messages for four days. However, the user can configure a queue to retain messages for up to 14 days after the message has been sent. QUESTION NO: 188 An organization has setup multiple IAM users. The organization wants that each IAM user accesses the IAM console only within the organization and not from outside. How can it achieve this? A. Create an IAM policy with the security group and use that security group for AWS console login B. Create an IAM policy with a condition which denies access when the IP address range is not from the organization C. Configure the EC2 instance security group which allows traffic only from the organization\u2019s IP range D. Create an IAM policy with VPC and allow a secure gateway between the organization and AWS Console Answer: B Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user can add conditions as a part of the IAM policies. The condition can be set on AWS Tags, Time, and Client IP as well as on many other parameters. If the organization wants the user to access only from a specific IP range, they should set an IAM policy condition which denies access when the IP is not in a certain range. E.g. The sample policy given below denies all traffic when the IP is not in a certain range. \"Statement\": [{ \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\", \"Condition\": { \"NotIpAddress\": { \"aws:SourceIp\": [\"10.10.10.0/24\", \"20.20.30.0/24\"] } } }] QUESTION NO: 189 An organization has created one IAM user and applied the below mentioned policy to the user. What entitlements do the IAM users avail with this policy? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\" \"Action\": [ \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:Describe*\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"autoscaling:Describe*\", \"Resource\": \"*\" } ] } A. The policy will allow the user to perform all read only activities on the EC2 services B. The policy will allow the user to list all the EC2 resources except EBS C. The policy will allow the user to perform all read and write activities on the EC2 services D. The policy will allow the user to perform all read only activities on the EC2 services except load Balancing Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If an organization wants to setup read only access to EC2 for a particular user, they should mention the action in the IAM policy which entitles the user for Describe rights for EC2, CloudWatch, Auto Scaling and ELB. In the policy shown below, the user will have read only access for EC2 and EBS, CloudWatch and Auto Scaling. Since ELB is not mentioned as a part of the list, the user will not have access to ELB. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:Describe*\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"autoscaling:Describe*\", \"Resource\": \"*\" } ] } QUESTION NO: 190 A user has enabled session stickiness with ELB. The user does not want ELB to manage the cookie; instead he wants the application to manage the cookie. What will happen when the server instance, which is bound to a cookie, crashes? A. The response will have a cookie but stickiness will be deleted B. The session will not be sticky until a new cookie is inserted C. ELB will throw an error due to cookie unavailability D. The session will be sticky and ELB will route requests to another server as ELB keeps replicating the Cookie Answer: B Explanation: With Elastic Load Balancer, if the admin has enabled a sticky session with application controlled stickiness, the load balancer uses a special cookie generated by the application to associate the session with the original server which handles the request. ELB follows the lifetime of the application-generated cookie corresponding to the cookie name specified in the ELB policy configuration. The load balancer only inserts a new stickiness cookie if the application response includes a new application cookie. The load balancer stickiness cookie does not update with each request. If the application cookie is explicitly removed or expires, the session stops being sticky until a new application cookie is issued. QUESTION NO: 191 A user is observing the EC2 CPU utilization metric on CloudWatch. The user has observed some interesting patterns while filtering over the 1 week period for a particular hour. The user wants to zoom that data point to a more granular period. How can the user do that easily with CloudWatch? A. The user can zoom a particular period by selecting that period with the mouse and then releasing the mouse B. The user can zoom a particular period by double clicking on that period with the mouse C. The user can zoom a particular period by specifying the aggregation data for that period D. The user can zoom a particular period by specifying the period in the Time Range Answer: A QUESTION NO: 192 A user has created an Auto Scaling group with default configurations from CLI. The user wants to setup the CloudWatch alarm on the EC2 instances, which are launched by the Auto Scaling group. The user has setup an alarm to monitor the CPU utilization every minute. Which of the below mentioned statements is true? A. It will fetch the data at every minute but the four data points [corresponding to 4 minutes] will not have value since the EC2 basic monitoring metrics are collected every five minutes B. It will fetch the data at every minute as detailed monitoring on EC2 will be enabled by the default launch configuration of Auto Scaling C. The alarm creation will fail since the user has not enabled detailed monitoring on the EC2 instances D . The user has to first enable detailed monitoring on the EC2 instances to support alarm monitoring at every minute Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates an Auto Scaling launch config using CLI, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. Thus, by default detailed monitoring will be enabled for Auto Scaling as well as for all the instances launched by that Auto Scaling group. QUESTION NO: 193 A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is not true in this scenario? A. The VPC will create a routing instance and attach it with a public subnet B. The VPC will create two subnets 116789 C. The VPC will create one internet gateway and attach it to VPC D. The VPC will launch one NAT instance with an elastic IP Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance with an elastic IP. Wizard will also create two subnets with route tables. It will also create an internet gateway and attach it to the VPC. QUESTION NO: 194 A user has configured ELB with a TCP listener at ELB as well as on the back-end instances. The user wants to enable a proxy protocol to capture the source and destination IP information in the header. Which of the below mentioned statements helps the user understand a proxy protocol with TCP configuration? A. If the end user is requesting behind a proxy server then the user should not enable a proxy protocol on ELB B. ELB does not support a proxy protocol when it is listening on both the load balancer and the backend instances C. Whether the end user is requesting from a proxy server or directly, it does not make a difference for the proxy protocol D. If the end user is requesting behind the proxy then the user should add the \u201cisproxy\u201d flag to the ELB Configuration Answer: A Explanation: When the user has configured Transmission Control Protocol (TCP. or Secure Sockets Layer (SSL. for both front-end and back-end connections of the Elastic Load Balancer, the load balancer forwards the request to the back-end instances without modifying the request headers unless the proxy header is enabled. If the end user is requesting from a Proxy Protocol enabled proxy server, then the ELB admin should not enable the Proxy Protocol on the load balancer. If the Proxy Protocol is enabled on both the proxy server and the load balancer, the load balancer will add another header to the request which already has a header from the proxy server. This duplication may result in errors. QUESTION NO: 195 A user has launched 5 instances in EC2-CLASSIC and attached 5 elastic IPs to the five different instances in the US East region. The user is creating a VPC in the same region. The user wants to assign an elastic IP to the VPC instance. How can the user achieve this? A. The user has to request AWS to increase the number of elastic IPs associated with the account B. AWS allows 10 EC2 Classic IPs per region ; so it will allow to allocate new Elastic IPs to the same region C. The AWS will not allow to create a new elastic IP in VPC; it will throw an error D. The user can allocate a new IP address in VPC as it has a different limit than EC2 Answer: D Explanation: Section: (none) A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. A user can have 5 IP addresses per region with EC2 Classic. The user can have 5 separate IPs with VPC in the same region as it has a separate limit than EC2 Classic. QUESTION NO: 196 A user has created a subnet in VPC and launched an EC2 instance within it. The user has not selected the option to assign the IP address while launching the instance. Which of the below mentioned statements is true with respect to this scenario? A. The instance will always have a public DNS attached to the instance by default B. The user can directly attach an elastic IP to the instance C. The instance will never launch if the public IP is not assigned D. The user would need to create an internet gateway and then attach an elastic IP to the instance to connect from internet Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When the user is launching an instance he needs to select an option which attaches a public IP to the instance. If the user has not selected the option to attach the public IP then it will only have a private IP when launched. The user cannot connect to the instance from the internet. If the user wants an elastic IP to connect to the instance from the internet he should create an internet gateway and assign an elastic IP to instance. QUESTION NO: 197 An organization has applied the below mentioned policy on an IAM group which has selected the IAM users. What entitlements do the IAM users avail with this policy? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" } ] } A. The policy is not created correctly. It will throw an error for wrong resource name B. The policy is for the group. Thus, the IAM user cannot have any entitlement to this C. It allows full access to all AWS services for the IAM users who are a part of this group D. If this policy is applied to the EC2 resource, the users of the group will have full access to the EC2 Resources Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The IAM group allows the organization to specify permissions for a collection of users. With the below mentioned policy, it will allow the group full access (Admin. to all AWS services. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" } ] } QUESTION NO: 198 A user is configuring a CloudWatch alarm on RDS to receive a notification when the CPU utilization of RDS is higher than 50%. The user has setup an alarm when there is some iinactivity on RDS, such as RDS unavailability. How can the user configure this? A. Setup the notification when the CPU is more than 75% on RDS B. Setup the notification when the state is Insufficient Data C. Setup the notification when the CPU utilization is less than 10% D. It is not possible to setup the alarm on RDS Answer: B Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The alarm has three states: Alarm, OK and Insufficient data. The Alarm will change to Insufficient Data when any of the three situations arise: when the alarm has just started, when the metric is not available or when enough data is not available for the metric to determine the alarm state. If the user wants to find that RDS is not available, he can setup to receive the notification when the state is in Insufficient data. QUESTION NO: 199 George has shared an EC2 AMI created in the US East region from his AWS account with Stefano. George copies the same AMI to the US West region. Can Stefano access the copied AMI of George\u2019s account from the US West region? A. No, copy AMI does not copy the permission B. It is not possible to share the AMI with a specific account C. Yes, since copy AMI copies all private account sharing permissions D. Yes, since copy AMI copies all the permissions attached with the AMI Answer: A Explanation: Within EC2, when the user copies an AMI, the new AMI is fully independent of the source AMI; there is no link to the original (source. AMI. AWS does not copy launch the permissions, userdefined tags or the Amazon S3 bucket permissions from the source AMI to the new AMI. Thus, in this case by default Stefano will not have access to the AMI in the US West region. QUESTION NO: 200 A user has created a VPC with a subnet and a security group. The user has launched an instance in that subnet and attached a public IP. The user is still unable to connect to the instance. The internet gateway has also been created. What can be the reason for the error? A. The internet gateway is not configured with the route table B. The private IP is not present C. The outbound traffic on the security group is disabled D. The internet gateway is not configured with the security group Answer: A Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level. When a user launches an instance and wants to connect to an instance, he needs an internet gateway. The internet gateway should be configured with the route table to allow traffic from the internet. QUESTION NO: 201 A user is trying to setup a security policy for ELB. The user wants ELB to meet the cipher supported by the client by configuring the server order preference in ELB security policy. Which of the below mentioned preconfigured policies supports this feature? A. ELBSecurity Policy-2014-01 B. ELBSecurity Policy-2011-08 C. ELBDefault Negotiation Policy D. ELBSample- OpenSSLDefault Cipher Policy Answer: A Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. If the load balancer is configured to support the Server Order Preference, then load balancer gets to select the first cipher in its list that matches any one of the ciphers in client's list. When the user verifies the preconfigured policies supported by ELB, the policy \u201cELBSecurity Policy-2014-01\u201d supports server order preference. QUESTION NO: 202 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling AlarmNotification (which notifies Auto Scaling for CloudWatch alarms. process for a while. What will Auto Scaling do during this period? A. AWS will not receive the alarms from CloudWatch B. AWS will receive the alarms but will not execute the Auto Scaling policy C. Auto Scaling will execute the policy but it will not launch the instances until the process is resumed D. It is not possible to suspend the AlarmNotification process Answer: B Explanation: Auto Scaling performs various processes, such as Launch, Terminate Alarm Notification etc. The user can also suspend individual process. The AlarmNotification process type accepts notifications from the Amazon CloudWatch alarms that are associated with the Auto Scaling group. If the user suspends this process type, Auto Scaling will not automatically execute the scaling policies that would be triggered by the alarms. QUESTION NO: 203 George has launched three EC2 instances inside the US-East-1a zone with his AWS account. Ray has launched two EC2 instances in the US-East-1a zone with his AWS account. Which of the below entioned statements will help George and Ray understand the availability zone (AZ. concept better? A. The instances of George and Ray will be running in the same data centre B. All the instances of George and Ray can communicate over a private IP with a minimal cost C. All the instances of George and Ray can communicate over a private IP without any cost D. The US-East-1a region of George and Ray can be different availability zones Answer: D Explanation: Each AWS region has multiple, isolated locations known as Availability Zones. To ensure that the AWS resources are distributed across the Availability Zones for a region, AWS independently maps the Availability Zones to identifiers for each account. In this case the Availability Zone US-East-1a where George\u2019s EC2 instances are running might not be the same location as the US-East-1a zone of Ray\u2019s EC2 instances. There is no way for the user to coordinate the Availability Zones between accounts. QUESTION NO: 204 A user had aggregated the CloudWatch metric data on the AMI ID. The user observed some abnormal behaviour of the CPU utilization metric while viewing the last 2 weeks of data. The user wants to share that data with his manager. How can the user achieve this easily with the AWS console? A. The user can use the copy URL functionality of CloudWatch to share the exact details B. The user can use the export data option from the CloudWatch console to export the current data point C. The user has to find the period and data and provide all the aggregation information to the manager D. The user can use the CloudWatch data copy functionality to copy the current data points Answer: A Explanation: Amazon CloudWatch provides the functionality to graph the metric data generated either by the AWS services or the custom metric to make it easier for the user to analyse. The console provides the option to save the URL or bookmark it so that it can be used in the future by typing the same URL. The Copy URL functionality is available under the console when the user selects any metric to view. QUESTION NO: 205 A user has setup a CloudWatch alarm on the EC2 instance for CPU utilization. The user has setup to receive a notification on email when the CPU utilization is higher than 60%. The user is running a virus scan on the same instance at a particular time. The user wants to avoid receiving an email at this time. What should the user do? A. Remove the alarm B. Disable the alarm for a while using CLI C. Modify the CPU utilization by removing the email alert D. Disable the alarm for a while using the console Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. When the user has setup an alarm and it is know that for some unavoidable event the status may change to Alarm, the user can disable the alarm using the DisableAlarmActions API or from the command line mon-disable-alarm-actions. QUESTION NO: 206 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned SSL protocols is not supported by the security policy? A. TLS 1.3 B. TLS 1.2 C. SSL 2.0 D. SSL 3.0 Answer: A Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and loadbalancer. Elastic Load Balancing supports the following versions of the SSL protocol: TLS 1.2 TLS 1.1 TLS 1.0 SSL 3.0 SSL 2.0 QUESTION NO: 207 A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group for the public subnet (WebSecGrp. and the private subnet(DBSecGrp). Which of the below mentioned entries is required in the private subnet database security group (DBSecGrp)? A. Allow Inbound on port 3306 for Source Web Server Security Group (WebSecGrp. B. Allow Inbound on port 3306 from source 20.0.0.0/16 C. Allow Outbound on port 3306 for Destination Web Server Security Group (WebSecGrp. D. Allow Outbound on port 80 for Destination NAT Instance IP Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the private subnet can receive inbound traffic from the public subnet on the DB port. Thus, configure port 3306 in Inbound with the source as the Web Server Security Group (WebSecGrp.. The user should configure ports 80 and 443 for Destination 0.0.0.0/0 as the route table directs traffic to the NAT instance from the private subnet. QUESTION NO: 208 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created public and VPN only subnets along with hardware VPN access to connect to the user\u2019s data centre. The user has not yet launched any instance as well as modified or deleted any setup. He wants to delete this VPC from the console. Will the console allow the user to delete the VPC? A. Yes, the console will delete all the setups and also delete the virtual private gateway B. No, the console will ask the user to manually detach the virtual private gateway first and then allow deleting the VPC C. Yes, the console will delete all the setups and detach the virtual private gateway D. No, since the NAT instance is running Answer: C Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. If the virtual private gateway is attached with VPC and the user deletes the VPC from the console it will first detach the gateway automatically and only then delete the VPC. QUESTION NO: 209 A user is trying to create a PIOPS EBS volume with 4000 IOPS and 100 GB size. AWS does not allow the user to create this volume. What is the possible root cause for this? A. The ratio between IOPS and the EBS volume is higher than 30 B. The maximum IOPS supported by EBS is 3000 C. The ratio between IOPS and the EBS volume is lower than 50 D. PIOPS is supported for EBS higher than 500 GB size Answer: A Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30; for example, a volume with 3000 IOPS must be at least 100 GB. QUESTION NO: 210 A user has setup a custom application which generates a number in decimals. The user wants to track that number and setup the alarm whenever the number is above a certain limit. The application is sending the data to CloudWatch at regular intervals for this purpose. Which of the below mentioned statements is not true with respect to the above scenario? A. The user can get the aggregate data of the numbers generated over a minute and send it to CloudWatch B. The user has to supply the timezone with each data point C. CloudWatch will not truncate the number until it has an exponent larger than 126 (i.e. (1 x 10^126) ). D. The user can create a file in the JSON format with the metric name and value and supply it to CloudWatch Answer: B QUESTION NO: 211 A user has launched an EC2 Windows instance from an instance store backed AMI. The user has also set the Instance initiated shutdown behavior to stop. What will happen when the user shuts down the OS? A. It will not allow the user to shutdown the OS when the shutdown behaviour is set to Stop B. It is not possible to set the termination behaviour to Stop for an Instance store backed AMI instance C. The instance will stay running but the OS will be shutdown D. The instance will be terminated Answer: B Explanation: When the EC2 instance is launched from an instance store backed AMI, it will not allow the user to configure the shutdown behaviour to \u201cStop\u201d. It gives a warning that the instance does not have the EBS root volume. QUESTION NO: 212 A user has enabled versioning on an S3 bucket. The user is using server side encryption for data at Rest. If the user is supplying his own keys for encryption (SSE-C., which of the below mentioned statements is true? A. The user should use the same encryption key for all versions of the same object B. It is possible to have different encryption keys for different versions of the same object C. AWS S3 does not allow the user to upload his own keys for server side encryption D. The SSE-C does not work when versioning is enabled Answer: B Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. If the bucket is versioningenabled, each object version uploaded by the user using the SSE-C feature can have its own encryption key. The user is responsible for tracking which encryption key was used for which object's version QUESTION NO: 213 A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 in this VPC. The user is trying to create another subnet with the same VPC for CIDR 20.0.0.1/24. What will happen in this scenario? A. The VPC will modify the first subnet CIDR automatically to allow the second subnet IP range B. It is not possible to create a subnet with the same CIDR as VPC C. The second subnet will be created D. It will throw a CIDR overlaps error Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. The user can create a subnet with the same size of VPC. However, he cannot create any other subnet since the CIDR of the second subnet will conflict with the first subnet. QUESTION NO: 214 A user has launched an RDS MySQL DB with the Multi AZ feature. The user has scheduled the scaling of instance storage during maintenance window. What is the correct order of events during maintenance window? Perform maintenance on standby Promote standby to primary Perform maintenance on original primary Promote original master back as primary A. 1, 2, 3, 4 B. 1, 2, 3 C. 2, 3, 1, 4 Answer: B Explanation: Running MySQL on the RDS DB instance as a Multi-AZ deployment can help the user reduce the impact of a maintenance event, as the Amazon will conduct maintenance by following the steps in the below mentioned order: Perform maintenance on standby Promote standby to primary Perform maintenance on original primary, which becomes the new standby. QUESTION NO: 215 A sys admin is using server side encryption with AWS S3. Which of the below mentioned statements helps the user understand the S3 encryption functionality? A. The server side encryption with the user supplied key works when versioning is enabled B. The user can use the AWS console, SDK and APIs to encrypt or decrypt the content for server side encryption with the user supplied key. C. The user must send an AES-128 encrypted key D. The user can upload his own encryption key to the S3 console Answer: A Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key. The encryption with the user supplied key (SSE-C. does not work with the AWS console. The S3 does not store the keys and the user has to send a key with each request. The SSE-C works when the user has enabled versioning. QUESTION NO: 216 A root account owner is trying to understand the S3 bucket ACL. Which of the below mentioned options cannot be used to grant ACL on the object using the authorized predefined group? A. Authenticated user group B. All users group C. Log Delivery Group D. Canonical user group Answer: D Explanation: An S3 bucket ACL grantee can be an AWS account or one of the predefined Amazon S3 groups. Amazon S3 has a set of predefined groups. When granting account access to a group, the user can specify one of the URLs of that group instead of a canonical user ID. AWS S3 has the following predefined groups: Authenticated Users group: It represents all AWS accounts. All Users group: Access permission to this group allows anyone to access the resource. Log Delivery group: WRITE permission on a bucket enables this group to write server access logs to the bucket. QUESTION NO: 217 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24. and VPN only subnets CIDR (20.0.1.0/24. along with the VPN gateway (vgw-12345. to connect to the user\u2019s data centre. The user\u2019s data centre has CIDR 172.28.0.0/12. The user has also setup a NAT instance (i-123456. to allow traffic to the internet from the VPN subnet. Which of the below mentioned options is not a valid entry for the main route table in this scenario? A. Destination: 20.0.1.0/24 and Target: i-12345 B. Destination: 0.0.0.0/0 and Target: i-12345 C. Destination: 172.28.0.0/12 and Target: vgw-12345 D. Destination: 20.0.0.0/16 and Target: local Answer: A Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. If the user has setup a NAT instance to route all the internet requests then all requests to the internet should be routed to it. All requests to the organization\u2019s DC will be routed to the VPN gateway. Here are the valid entries for the main route table in this scenario: Destination: 0.0.0.0/0 & Target: i-12345 (To route all internet traffic to the NAT Instance. Destination: 172.28.0.0/12 & Target: vgw-12345 (To route all the organization\u2019s data centre traffic to the VPN gateway. Destination: 20.0.0.0/16 & Target: local (To allow local routing in VPC. QUESTION NO: 218 A user has created a VPC with public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24 . The NAT instance ID is i-a12345. Which of the below mentioned entries are required in the main route table attached with the private subnet to allow instances to connect with the internet? A. Destination: 0.0.0.0/0 and Target: i-a12345 B. Destination: 20.0.0.0/0 and Target: 80 C. Destination: 20.0.0.0/0 and Target: i-a12345 D. Destination: 20.0.0.0/24 and Target: i-a12345 Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create two route tables and attach to the subnets. The main route table will have the entry \u201cDestination: 0.0.0.0/0 and Target: ia12345\u201d, which allows all the instances in the private subnet to connect to the internet using NAT. QUESTION NO: 219 A root account owner has given full access of his S3 bucket to one of the IAM users using the bucket ACL. When the IAM user logs in to the S3 console, which actions can he perform? A. He can just view the content of the bucket B. He can do all the operations on the bucket C. It is not possible to give access to an IAM user using ACL D. The IAM user can perform all operations on the bucket using only API/SDK Answer: C Explanation: Each AWS S3 bucket and object has an ACL (Access Control List. associated with it. An ACL is a list of grants identifying the grantee and the permission granted. The user can use ACLs to grant basic read/write permissions to other AWS accounts. ACLs use an Amazon S3\u2013specific XML schema. The user cannot grant permissions to other users (IAM users. in his account. QUESTION NO: 220 An organization has configured Auto Scaling with ELB. There is a memory issue in the application which is causing CPU utilization to go above 90%. The higher CPU usage triggers an event for Auto Scaling as per the scaling policy. If the user wants to find the root cause inside the application without triggering a scaling activity, how can he achieve this? A. Stop the scaling process until research is completed B. It is not possible to find the root cause from that instance without triggering scaling C. Delete Auto Scaling until research is completed D. Suspend the scaling process until research is completed Answer: D Explanation: Auto Scaling allows the user to suspend and then resume one or more of the Auto Scaling processes in the Auto Scaling group. This is very useful when the user wants to investigate a configuration problem or some other issue, such as a memory leak with the web application and then make changes to the application, without triggering the Auto Scaling process. QUESTION NO: 221 A sys admin is planning to subscribe to the RDS event notifications. For which of the below mentioned source categories the subscription cannot be configured? A. DB security group B. DB snapshot C. DB options group D. DB parameter group Answer: C Explanation: Amazon RDS uses the Amazon Simple Notification Service (SNS. to provide a notification when an Amazon RDS event occurs. These events can be configured for source categories, such as DB instance, DB security group, DB snapshot and DB parameter group. QUESTION NO: 222 A user has launched an EC2 instance. The instance got terminated as soon as it was launched. Which of the below mentioned options is not a possible reason for this? A. The user account has reached the maximum EC2 instance limit B. The snapshot is corrupt C. The AMI is missing. It is the required part D. The user account has reached the maximum volume limit Answer: A Explanation: When the user account has reached the maximum number of EC2 instances, it will not be allowed to launch an instance. AWS will throw an \u2018InstanceLimitExceeded\u2019 error. For all other reasons, such as \u201cAMI is missing part\u201d, \u201cCorrupt Snapshot\u201d or \u201dVolume limit has reached\u201d it will launch an EC2 instance and then terminate it. QUESTION NO: 223 A user is trying to understand the detailed CloudWatch monitoring concept. Which of the below mentioned services does not provide detailed monitoring with CloudWatch? A. AWS EMR B. AWS RDS C. AWS ELB D. AWS Route53 Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Services, such as RDS, EC2, Auto Scaling, ELB, and Route 53 can provide the monitoring data every minute. QUESTION NO: 224 A user is measuring the CPU utilization of a private data centre machine every minute. The machine provides the aggregate of data every hour, such as Sum of data\u201d, \u201cMin value\u201d, \u201cMax value, and \u201cNumber of Data points\u201d. The user wants to send these values to CloudWatch. How can the user achieve this? A. Send the data using the put-metric-data command with the aggregate-values parameter B. Send the data using the put-metric-data command with the average-values parameter C. Send the data using the put-metric-data command with the statistic-values parameter D. Send the data using the put-metric-data command with the aggregate \u2013data parameter Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish the data to CloudWatch as single data points or as an aggregated set of data points called a statistic set using the command put-metric-data. When sending the aggregate data, the user needs to send it with the parameter statistic-values: awscloudwatch put-metric-data --metric-name <Name> --namespace <Custom namespace> -- timestamp <UTC Format> --statistic-values Sum=XX,Minimum=YY,Maximum=AA,SampleCount=BB --unit Milliseconds QUESTION NO: 225 A user has enabled detailed CloudWatch monitoring with the AWS Simple Notification Service. Which of the below mentioned statements helps the user understand detailed monitoring better? A. SNS will send data every minute after configuration B. There is no need to enable since SNS provides data every minute C. AWS CloudWatch does not support monitoring for SNS D. SNS cannot provide data every minute Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. The AWS SNS service sends data every 5 minutes. Thus, it supports only the basic monitoring. The user cannot enable detailed monitoring with SNS. QUESTION NO: 226 A user has setup a VPC with CIDR 20.0.0.0/16. The VPC has a private subnet (20.0.1.0/24) and a public subnet (20.0.0.0/240). The user\u2019s data centre has CIDR of 20.0.54.0/24 and 20.1.0.0/24 If the private subnet wants to communicate with the data centre, what will happen? A. It will allow traffic communication on both the CIDRs of the data centre B. It will not allow traffic with data centre on CIDR 20.1.0.0/24 but allows traffic communication on 20.0.54.0/24 C. It will not allow traffic communication on any of the data centre CIDRs D. It will allow traffic with data centre on CIDR 20.1.0.0/24 but does not allow on 20.0.54.0/24 Answer: D Explanation: VPC allows the user to set up a connection between his VPC and corporate or home network data centre. If the user has an IP address prefix in the VPC that overlaps with one of the networks' prefixes, any traffic to the network's prefix is dropped. In this case CIDR 20.0.54.0/24 falls in the VPC\u2019s CIDR range of 20.0.0.0/16. Thus, it will not allow traffic on that IP. In the case of 20.1.0.0/24, it does not fall in the VPC\u2019s CIDR range. Thus, traffic will be allowed on it. QUESTION NO: 227 A user wants to find the particular error that occurred on a certain date in the AWS MySQL RDS DB. Which of the below mentioned activities may help the user to get the data easily? A. It is not possible to get the log files for MySQL RDS B. Find all the transaction logs and query on those records C. Direct the logs to the DB table and then query that table D. Download the log file to DynamoDB and search for the record Answer: C Explanation: The user can view, download, and watch the database logs using the Amazon RDS console, the Command Line Interface (CLI. or the Amazon RDS API. For the MySQL RDS, the user can view the error log, slow query log, and general logs. The user can also view the MySQL logs easily by directing the logs to a database table in the main database and querying that table. QUESTION NO: 228 A user is trying to send custom metrics to CloudWatch using the PutMetricData APIs. Which of the below mentioned points should the user needs to take care while sending the data to CloudWatch? A. The size of a request is limited to 8KB for HTTP GET requests and 40KB for HTTP POST requests B. The size of a request is limited to 128KB for HTTP GET requests and 64KB for HTTP POST requests C. The size of a request is limited to 40KB for HTTP GET requests and 8KB for HTTP POST requests D. The size of a request is limited to 16KB for HTTP GET requests and 80KB for HTTP POST requests Answer: A Explanation: With AWS CloudWatch, the user can publish data points for a metric that share not only the same time stamp, but also the same namespace and dimensions. CloudWatch can accept multiple data points in the same PutMetricData call with the same time stamp. The only thing that the user needs to take care of is that the size of a PutMetricData request is limited to 8KB for HTTP GET requests and 40KB for HTTP POST requests. QUESTION NO: 229 An AWS account owner has setup multiple IAM users. One IAM user only has CloudWatch access. He has setup the alarm action which stops the EC2 instances when the CPU utilization is below the threshold limit. What will happen in this case? A. It is not possible to stop the instance using the CloudWatch alarm B. CloudWatch will stop the instance when the action is executed C. The user cannot set an alarm on EC2 since he does not have the permission D. The user can setup the action but it will not be executed if the user does not have EC2 rights Answer: D Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup an action which stops the instances when their CPU utilization is below a certain threshold for a certain period of time. The EC2 action can either terminate or stop the instance as part of the EC2 action. If the IAM user has read/write permissions for Amazon CloudWatch but not for Amazon EC2, he can still create an alarm. However, the stop or terminate actions will not be performed on the Amazon EC2 instance. QUESTION NO: 230 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling terminate process only for a while. What will happen to the availability zone rebalancing process (AZRebalance. during this period? A. Auto Scaling will not launch or terminate any instances B. Auto Scaling will allow the instances to grow more than the maximum size C. Auto Scaling will keep launching instances till the maximum instance size D. It is not possible to suspend the terminate process while keeping the launch active Answer: B Explanation: Auto Scaling performs various processes, such as Launch, Terminate, Availability Zone Rebalance (AZRebalance. etc. The AZRebalance process type seeks to maintain a balanced number of instances across Availability Zones within a region. If the user suspends the Terminate process, the AZRebalance process can cause the Auto Scaling group to grow up to ten percent larger than the maximum size. This is because Auto Scaling allows groups to temporarily grow larger than the maximum size during rebalancing activities. If Auto Scaling cannot terminate instances, the Auto Scaling group could remain up to ten percent larger than the maximum size until the user resumes the Terminate process type. QUESTION NO: 231 A user has created a mobile application which makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK and root account access/secret access key to connect to DynamoDB from mobile. Which of the below mentioned statements is true with respect to the best practice for security in this scenario? A. The user should create a separate IAM user for each mobile application and provide DynamoDB access with it. B. The user should create an IAM role with DynamoDB and EC2 access. Attach the role with EC2 and route all calls from the mobile through EC2. C. The application should use an IAM role with web identity federation which validates calls to DynamoDB with identity providers, such as Google, Amazon, and Facebook. D. Create an IAM Role with DynamoDB access and attach it with the mobile application Answer: C Explanation: With AWS IAM a user is creating an application which runs on an EC2 instance and makes requests to AWS, such as DynamoDB or S3 calls. Here it is recommended that the user should not create an IAM user and pass the user's credentials to the application or embed those credentials inside the application. If the user is creating an app that runs on a mobile phone and makes requests to AWS, the user should not create an IAMuser and distribute the user's access key with the app. Instead, he should use an identity provider, such as Login with Amazon, Facebook, or Google to authenticate the users, and then use that identity to get temporary security credentials. QUESTION NO: 232 A user is configuring the Multi AZ feature of an RDS DB. The user came to know that this RDS DB does not use the AWS technology, but uses server mirroring to achieve HA. Which DB is the user using right now? A. My SQL B. Oracle C. MS SQL D. PostgreSQL Answer: C Explanation: Amazon RDS provides high availability and failover support for DB instances using Multi AZ deployments. In a Multi AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Multi AZ deployments for Oracle, PostgreSQL, and MySQL DB instances use Amazon technology, while SQL Server (MS SQL. DB instances use SQL Server Mirroring. QUESTION NO: 233 A user is receiving a notification from the RDS DB whenever there is a change in the DB security group. The user does not want to receive these notifications for only a month. Thus, he does not want to delete the notification. How can the user configure this? A. Change the Disable button for notification to \u201cYes\u201d in the RDS console B. Set the send mail flag to false in the DB event notification console C. The only option is to delete the notification from the console D. Change the Enable button for notification to \u201cNo\u201d in the RDS console Answer: D Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. Event notifications are sent to the addresses that the user has provided while creating the subscription. The user can easily turn off the notification without deleting a subscription by setting the Enabled radio button to No in the Amazon RDS console or by setting the Enabled parameter to false using the CLI or Amazon RDS API. QUESTION NO: 234 A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 by mistake. The user is trying to create another subnet of CIDR 20.0.0.1/24. How can the user create the second subnet? A. There is no need to update the subnet as VPC automatically adjusts the CIDR of the first subnet based on the second subnet\u2019s CIDR B. The user can modify the first subnet CIDR from the console C. It is not possible to create a second subnet as one subnet with the same CIDR as the VPC has been created D. The user can modify the first subnet CIDR with AWS CLI Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside the subnet. The user can create a subnet with the same size of VPC. However, he cannot create any other subnet since the CIDR of the second subnet will conflict with the first subnet. The user cannot modify the CIDR of a subnet once it is created. Thus, in this case if required, the user has to delete the subnet and create new subnets. QUESTION NO: 235 A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group for the public subnet (WebSecGrp. and the private subnet (DBSecGrp.. Which of the below mentioned entries is required in the web server security group (WebSecGrp.? A. Configure Destination as DB Security group ID (DbSecGrp. for port 3306 Outbound B. 80 for Destination 0.0.0.0/0 Outbound C. Configure port 3306 for source 20.0.0.0/24 InBound D. Configure port 80 InBound for source 20.0.0.0/16 Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the public subnet can receive inbound traffic directly from the internet. Thus, the user should configure port 80 with source 0.0.0.0/0 in InBound. The user should configure that the instance in the public subnet can send traffic to the private subnet instances on the DB port. Thus, the user should configure the DB security group of the private subnet (DbSecGrp. as the destination for port 3306 in Outbound. QUESTION NO: 236 A user is trying to understand the detailed CloudWatch monitoring concept. Which of the below mentioned services provides detailed monitoring with CloudWatch without charging the user extra? A. AWS Auto Scaling B. AWS Route 53 C. AWS EMR D. AWS SNS Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Services, such as RDS, ELB, OpsWorks, and Route 53 can provide the monitoring data every minute without charging the user. QUESTION NO: 237 A user is trying to understand the CloudWatch metrics for the AWS services. It is required that the user should first understand the namespace for the AWS services. Which of the below mentioned is not a valid namespace for the AWS services? A. AWS/StorageGateway B. AWS/CloudTrail C. AWS/ElastiCache D. AWS/SWF Answer: B Explanation: Amazon CloudWatch is basically a metrics repository. The AWS product puts metrics into this repository, and the user can retrieve the data or statistics based on those metrics. To distinguish the data for each service, the CloudWatch metric has a namespace. Namespaces are containers for metrics. All AWS services that provide the Amazon CloudWatch data use a namespace string, beginning with \"AWS/\". All the services which are supported by CloudWatch will have some namespace. CloudWatch does not monitor CloudTrail. Thus, the namespace \u201cAWS/CloudTrail\u201d is incorrect. QUESTION NO: 238 A system admin is planning to encrypt all objects being uploaded to S3 from an application. The system admin does not want to implement his own encryption algorithm; instead he is planning to use server side encryption by supplying his own key (SSE-C.. Which parameter is not required while making a call for SSE-C? A. x-amz-server-side-encryption-customer-key-AES-256 B. x-amz-server-side-encryption-customer-key C. x-amz-server-side-encryption-customer-algorithm D. x-amz-server-side-encryption-customer-key-MD5 Answer: A Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. When the user is supplying his own encryption key, the user has to send the below mentioned parameters as a part of the API calls: x-amz-server-side-encryption-customer-algorithm: Specifies the encryption algorithm x-amzserver-side-encryption-customer-key: To provide the base64-encoded encryption key x-amzserver-side-encryption-customer-key-MD5: To provide the base64-encoded 128-bit MD5 digest of the encryption key QUESTION NO: 239 A user is using the AWS SQS to decouple the services. Which of the below mentioned operations is not supported by SQS? A. SendMessageBatch B. DeleteMessageBatch C. CreateQueue D. DeleteMessageQueue Answer: D Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. The user can perform the following set of operations using the Amazon SQS: CreateQueue, ListQueues, DeleteQueue, SendMessage, SendMessageBatch, ReceiveMessage, DeleteMessage, DeleteMessageBatch, ChangeMessageVisibility, ChangeMessageVisibilityBatch, SetQueueAttributes, GetQueueAttributes, GetQueueUrl, AddPermission and RemovePermission. Operations can be performed only by the AWS account owner or an AWS account that the account owner has delegated to. QUESTION NO: 240 A user has configured Auto Scaling with 3 instances. The user had created a new AMI after updating one of the instances. If the user wants to terminate two specific instances to ensure that Auto Scaling launches an instances with the new launch configuration, which command should he run? A. as-delete-instance-in-auto-scaling-group <Instance ID> --no-decrement-desired-capacity B. as-terminate-instance-in-auto-scaling-group <Instance ID> --update-desired-capacity C. as-terminate-instance-in-auto-scaling-group <Instance ID> --decrement-desired-capacity D. as-terminate-instance-in-auto-scaling-group <Instance ID> --no-decrement-desired-capacity Answer: D Explanation: The Auto Scaling command as-terminate-instance-in-auto-scaling-group <Instance ID> will terminate the specific instance ID. The user is required to specify the parameter as \u2013no-decrement-desiredcapacity to ensure that it launches a new instance from the launch config after terminating the instance. If the user specifies the parameter --decrement-desired-capacity then Auto Scaling will terminate the instance and decrease the desired capacity by 1. QUESTION NO: 241 A user has launched an EC2 instance from an instance store backed AMI. If the user restarts the instance, what will happen to the ephermal storage data? A. All the data will be erased but the ephermal storage will stay connected B. All data will be erased and the ephermal storage is released C. It is not possible to restart an instance launched from an instance store backed AMI D. The data is preserved Answer: D Explanation: A user can reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. However, it is recommended that the user use Amazon EC2 to reboot the instance instead of running the operating system reboot command from the instance. When an instance launched from an instance store backed AMI is rebooted all the ephermal storage data is still preserved. QUESTION NO: 242 A user has launched an EC2 instance. However, due to some reason the instance was terminated. If the user wants to find out the reason for termination, where can he find the details? A. It is not possible to find the details after the instance is terminated B. The user can get information from the AWS console, by checking the Instance description under the State transition reason label C. The user can get information from the AWS console, by checking the Instance description under the Instance Status Change reason label D. The user can get information from the AWS console, by checking the Instance description under the Instance Termination reason label Answer: D Explanation: An EC2 instance, once terminated, may be available in the AWS console for a while after termination. The user can find the details about the termination from the description tab under the label State transition reason. If the instance is still running, there will be no reason listed. If the user has explicitly stopped or terminated the instance, the reason will be \u201cUser initiated shutdown\u201d. QUESTION NO: 243 A user has created a VPC with CIDR 20.0.0.0/24. The user has used all the IPs of CIDR and wants to increase the size of the VPC. The user has two subnets: public (20.0.0.0/28. and private (20.0.1.0/28). How can the user change the size of the VPC? A. The user can delete all the instances of the subnet. Change the size of the subnets to 20.0.0.0/32 and 20.0.1.0/32, respectively. Then the user can increase the size of the VPC using CLI B. It is not possible to change the size of the VPC once it has been created C. The user can add a subnet with a higher range so that it will automatically increase the size of the VPC. D. The user can delete the subnets first and then modify the size of the VPC Answer: B Explanation: Once the user has created a VPC, he cannot change the CIDR of that VPC. The user has to terminate all the instances, delete the subnets and then delete the VPC. Create a new VPC with a higher size and launch instances with the newly created VPC and subnets. QUESTION NO: 244 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned security policies is supported by ELB? A. Dynamic Security Policy B. All the other options C. Predefined Security Policy D. Default Security Policy Answer: C Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. ELB supports two policies: Predefined Security Policy: which comes with predefined cipher and SSL protocols; Custom Security Policy: which allows the user to configure a policy. QUESTION NO: 245 A user has granted read/write permission of his S3 bucket using ACL. Which of the below mentioned options is a valid ID to grant permission to other AWS accounts (grantee. using ACL? A. IAM User ID B. S3 Secure ID C. Access ID D. Canonical user ID Answer: D Explanation: An S3 bucket ACL grantee can be an AWS account or one of the predefined Amazon S3 groups. The user can grant permission to an AWS account by the email address of that account or by the canonical user ID. If the user provides an email in the grant request, Amazon S3 finds the canonical user ID for that account and adds it to the ACL. The resulting ACL will always contain the canonical user ID for the AWS account, and not the AWS account's email address. QUESTION NO: 246 A user has configured an ELB to distribute the traffic among multiple instances. The user instances are facing some issues due to the back-end servers. Which of the below mentioned CloudWatch metrics helps the user understand the issue with the instances? A. HTTPCode_Backend_3XX B. HTTPCode_Backend_4XX C. HTTPCode_Backend_2XX D. HTTPCode_Backend_5XX Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. For ELB, CloudWatch provides various metrics including error code by ELB as well as by back-end servers (instances.. It gives data for the count of the number of HTTP response codes generated by the back-end instances. This metric does not include any response codes generated by the load balancer. These metrics are: The 2XX class status codes represents successful actions The 3XX class status code indicates that the user agent requires action The 4XX class status code represents client errors The 5XX class status code represents back-end server errors QUESTION NO: 247 A user has launched an EC2 instance store backed instance in the US-East-1a zone. The user created AMI #1 and copied it to the Europe region. After that, the user made a few updates to the application running in the US-East-1a zone. The user makes an AMI#2 after the changes. If the user launches a new instance in Europe from the AMI #1 copy, which of the below mentioned statements is true? A. The new instance will have the changes made after the AMI copy as AWS just copies the reference of the original AMI during the copying. Thus, the copied AMI will have all the updated data. B. The new instance will have the changes made after the AMI copy since AWS keeps updating the AMI. C. It is not possible to copy the instance store backed AMI from one region to another. D. The new instance in the EU region will not have the changes made after the AMI copy. Answer: D Explanation: Within EC2, when the user copies an AMI, the new AMI is fully independent of the source AMI; there is no link to the original (source. AMI. The user can modify the source AMI without affecting the new AMI and vice a versa. Therefore, in this case even if the source AMI is modified, the copied AMI of the EU region will not have the changes. Thus, after copy the user needs to copy the new source AMI to the destination region to get those changes. QUESTION NO: 248 A user runs the command \u201cdd if=/dev/zero of=/dev/xvdfbs=1M\u201d on a fresh blank EBS volume attached to a Linux instance. Which of the below mentioned activities is the user performing with the command given above? A. Creating a file system on the EBS volume B. Mounting the device to the instance C. Pre warming the EBS volume D. Formatting the EBS volume Answer: C Explanation: When the user creates a new EBS volume and is trying to access it for the first time it will encounter reduced IOPS due to wiping or initiating of the block storage. To avoid this as well as achieve the best performance it is required to pre warm the EBS volume. For a blank volume attached with a Linux OS, the \u201cdd\u201d command is used to write to all the blocks on the device. In the command \u201cdd if=/dev/zero of=/dev/xvdfbs=1M\u201d the parameter \u201cif =import file\u201d should be set to one of the Linux virtual devices, such as /dev/zero. The \u201cof=output file\u201d parameter should be set to the drive that the user wishes to warm. The \u201cbs\u201d parameter sets the block size of the write operation; for optimal performance, this should be set to 1 MB. QUESTION NO: 249 A user has created an Auto Scaling group using CLI. The user wants to enable CloudWatch detailed monitoring for that group. How can the user configure this? A. When the user sets an alarm on the Auto Scaling group, it automatically enables detail monitoring B. By default detailed monitoring is enabled for Auto Scaling C. Auto Scaling does not support detailed monitoring D. Enable detail monitoring from the AWS console Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates an Auto Scaling launch config as the first step for creating an Auto Scaling group, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. Thus, the user does not need to set this flag if he wants detailed monitoring. QUESTION NO: 250 A user has created a VPC with a public subnet. The user has terminated all the instances which are part of the subnet. Which of the below mentioned statements is true with respect to this scenario? A. The user cannot delete the VPC since the subnet is not deleted B. All network interface attached with the instances will be deleted C. When the user launches a new instance it cannot use the same subnet D. The subnet to which the instances were launched with will be deleted Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When an instance is launched it will have a network interface attached with it. The user cannot delete the subnet until he terminates the instance and deletes the network interface. When the user terminates the instance all the network interfaces attached with it are also deleted. QUESTION NO: 251 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. The ELB security policy supports various ciphers. Which of the below mentioned options helps identify the matching cipher at the client side to the ELB cipher list when client is requesting ELB DNS over SSL? A. Cipher Protocol B. Client Configuration Preference C. Server Order Preference D. Load Balancer Preference Answer: C Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. When client is requesting ELB DNS over SSL and if the load balancer is configured to support the Server Order Preference, then the load balancer gets to select the first cipher in its list that matches any one of the ciphers in the client's list. Server Order Preference ensures that the load balancer determines which cipher is used for the SSL connection. QUESTION NO: 252 A user has created a VPC with public and private subnets. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.1.0/24 and the public subnet uses CIDR 20.0.0.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group of the NAT instance. Which of the below mentioned entries is not required for the NAT security group? A. For Inbound allow Source: 20.0.1.0/24 on port 80 B. For Outbound allow Destination: 0.0.0.0/0 on port 80 C. For Inbound allow Source: 20.0.0.0/24 on port 80 D. For Outbound allow Destination: 0.0.0.0/0 on port 443 Answer: C Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the private subnet can connect to the internet using the NAT instances. The user should first configure that NAT can receive traffic on ports 80 and 443 from the private subnet. Thus, allow ports 80 and 443 in Inbound for the private subnet 20.0.1.0/24. Now to route this traffic to the internet configure ports 80 and 443 in Outbound with destination 0.0.0.0/0. The NAT should not have an entry for the public subnet CIDR. QUESTION NO: 253 A user has created an application which will be hosted on EC2. The application makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK to connect with from the EC2 instance. Which of the below mentioned statements is true with respect to the best practice for security in this scenario? A. The user should attach an IAM role with DynamoDB access to the EC2 instance B. The user should create an IAM user with DynamoDB access and use its credentials within the application to connect with DynamoDB C. The user should create an IAM role, which has EC2 access so that it will allow deploying the application D. The user should create an IAM user with DynamoDB and EC2 access. Attach the user with the application so that it does not use the root account credentials Answer: A Explanation: With AWS IAM a user is creating an application which runs on an EC2 instance and makes requests to AWS, such as DynamoDB or S3 calls. Here it is recommended that the user should not create an IAM user and pass the user's credentials to the application or embed those credentials inside the application. Instead, the user should use roles for EC2 and give that role access to DynamoDB /S3. When the roles are attached to EC2, it will give temporary security credentials to the application hosted on that EC2, to connect with DynamoDB / S3. QUESTION NO: 254 An organization (Account ID 123412341234. has attached the below mentioned IAM policy to a user. What does this policy statement entitle the user to perform? { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*LoginProfile\", \"iam:*AccessKey*\", \"iam:*SigningCertificate*\" ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] }] } A. The policy allows the IAM user to modify all IAM user\u2019s credentials using the console, SDK, CLI or APIs B. The policy will give an invalid resource error C. The policy allows the IAM user to modify all credentials using only the console D. The policy allows the user to modify all IAM user\u2019s password, sign in certificates and access keys using only CLI, SDK or APIs Answer: D Explanation: WS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (Account ID 123412341234. wants some of their users to manage credentials (access keys, password, and sing in certificates. of all IAM users, they should set an applicable policy to that user or group of users. The below mentioned policy allows the IAM user to modify the credentials of all IAM user\u2019s using only CLI, SDK or APIs. The user cannot use the AWS console for this activity since he does not have list permission for the IAM users. { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\" \"Action\": [ \"iam:*LoginProfile\", \"iam:*AccessKey*\", \"iam:*SigningCertificate*\" ], \"Resource\": [\"arn:aws:iam::123412341234:user/${aws:username}\"] }] } QUESTION NO: 255 A sys admin is trying to understand the sticky session algorithm. Please select the correct sequence of steps, both when the cookie is present and when it is not, to help the admin understand the implementation of the sticky session: ELB inserts the cookie in the response ELB chooses the instance based on the load balancing algorithm Check the cookie in the service request The cookie is found in the request The cookie is not found in the request A. 3,1,4,2 [Cookie is not Present] & 3,1,5,2 [Cookie is Present] B. 3,4,1,2 [Cookie is not Present] & 3,5,1,2 [Cookie is Present] C. 3,5,2,1 [Cookie is not Present] & 3,4,2,1 [Cookie is Present] D. 3,2,5,4 [Cookie is not Present] & 3,2,4,5 [Cookie is Present] Answer: C Explanation: Generally AWS ELB routes each request to a zone with the minimum load. The Elastic Load Balancer provides a feature called sticky session which binds the user\u2019s session with a specific EC2 instance. The load balancer uses a special load-balancer-generated cookie to track the application instance for each request. When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the request is sent to the application instance specified in the cookie. If there is no cookie, the load balancer chooses an application instance based on the existing load balancing algorithm. A cookie is inserted into the response for binding subsequent requests from the same user to that application instance. QUESTION NO: 256 A user has a weighing plant. The user measures the weight of some goods every 5 minutes and sends data to AWS CloudWatch for monitoring and tracking. Which of the below mentioned parameters is mandatory for the user to include in the request list? A. Value B. Namespace C. Metric Name D. Timezone Answer: B Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish the data to CloudWatch as single data points or as an aggregated set of data points called a statistic set. The user has to always include the namespace as part of the request. The user can supply a file instead of the metric name. If the user does not supply the timezone, it accepts the current time. If the user is sending the data as a single data point it will have parameters, such as value. However, if the user is sending as an aggregate it will have parameters, such as statistic-values. QUESTION NO: 257 An organization has configured Auto Scaling for hosting their application. The system admin wants to understand the Auto Scaling health check process. If the instance is unhealthy, Auto Scaling launches an instance and terminates the unhealthy instance. What is the order execution? A. Auto Scaling launches a new instance first and then terminates the unhealthy instance B. Auto Scaling performs the launch and terminate processes in a random order C. Auto Scaling launches and terminates the instances simultaneously D. Auto Scaling terminates the instance first and then launches a new instance Answer: D Explanation: Auto Scaling keeps checking the health of the instances at regular intervals and marks the instance for replacement when it is unhealthy. The ReplaceUnhealthy process terminates instances which are marked as unhealthy and subsequently creates new instances to replace them. This process first terminates the instance and then launches a new instance. QUESTION NO: 258 A user is trying to connect to a running EC2 instance using SSH. However, the user gets an Unprotected Private Key File error. Which of the below mentioned options can be a possible reason for rejection? A. The private key file has the wrong file permission B. The ppk file used for SSH is read only C. The public key file has the wrong permission D. The user has provided the wrong user name for the OS login Answer: A Explanation: While doing SSH to an EC2 instance, if you get an Unprotected Private Key File error it means that the private key file's permissions on your computer are too open. Ideally the private key should have the Unix permission of 0400. To fix that, run the command: # chmod 0400 /path/to/private.key QUESTION NO: 259 A user has provisioned 2000 IOPS to the EBS volume. The application hosted on that EBS is experiencing less IOPS than provisioned. Which of the below mentioned options does not affect the IOPS of the volume? A. The application does not have enough IO for the volume B. The instance is EBS optimized C. The EC2 instance has 10 Gigabit Network connectivity D. The volume size is too large Answer: D Explanation: When the application does not experience the expected IOPS or throughput of the PIOPS EBS volume that was provisioned, the possible root cause could be that the EC2 bandwidth is the limiting factor and the instance might not be either EBS-optimized or might not have 10 Gigabit network connectivity. Another possible cause for not experiencing the expected IOPS could also be that the user is not driving enough I/O to the EBS volumes. The size of the volume may not affect IOPS. QUESTION NO: 260 A storage admin wants to encrypt all the objects stored in S3 using server side encryption. The user does not want to use the AES 256 encryption key provided by S3. How can the user achieve this? A. The admin should upload his secret key to the AWS console and let S3 decrypt the objects B. The admin should use CLI or API to upload the encryption key to the S3 bucket. When making a call to the S3 API mention the encryption key URL in each request C. S3 does not support client supplied encryption keys for server side encryption D. The admin should send the keys and encryption algorithm with each API call Answer: D Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API callto supply his own encryption key. Amazon S3 never stores the user\u2019s encryption key. The user has to supply it for each encryption or decryption call. QUESTION NO: 261 A user is trying to create a PIOPS EBS volume with 8 GB size and 200 IOPS. Will AWS create the volume? A. Yes, since the ratio between EBS and IOPS is less than 30 B. No, since the PIOPS and EBS size ratio is less than 30 C. No, the EBS size is less than 10 GB D. Yes, since PIOPS is higher than 100 Answer: C Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30; for example, a volume with 3000 IOPS must be at least 100 GB. QUESTION NO: 262 A user has scheduled the maintenance window of an RDS DB on Monday at 3 AM. Which of the below mentioned events may force to take the DB instance offline during the maintenance window? A. Enabling Read Replica B. Making the DB Multi AZ C. DB password change D. Security patching Answer: D Explanation: Amazon RDS performs maintenance on the DB instance during a user-definable maintenance window. The system may be offline or experience lower performance during that window. The only maintenance events that may require RDS to make the DB instance offline are: Scaling compute operations Software patching. Required software patching is automatically scheduled only for patches that are security and durability related. Such patching occurs infrequently (typically once every few months. and seldom requires more than a fraction of the maintenance window. QUESTION NO: 263 An organization has launched 5 instances: 2 for production and 3 for testing. The organization wants that one particular group of IAM users should only access the test instances and not the production ones. How can the organization set that as a part of the policy? A. Launch the test and production instances in separate regions and allow region wise access to the group B. Define the IAM policy which allows access based on the instance ID C. Create an IAM policy with a condition which allows access to only small instances D. Define the tags on the test and production servers and add a condition to the IAM policy which allows access to specific tags Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user can add conditions as a part of the IAM policies. The condition can be set on AWS Tags, Time, and Client IP as well as on various parameters. If the organization wants the user to access only specific instances he should define proper tags and add to the IAM policy condition. The sample policy is shown below. \"Statement\": [ { \"Action\": \"ec2:*\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/InstanceType\": \"Production\" } } } ] QUESTION NO: 264 A user has configured Auto Scaling with the minimum capacity as 2 and the desired capacity as 2. The user is trying to terminate one of the existing instance with the command: as-terminate-instance-in-auto-scaling-group<Instance ID> --decrement-desired-capacity What will Auto Scaling do in this scenario? A. Terminates the instance and does not launch a new instance B. Terminates the instance and updates the desired capacity to 1 C. Terminates the instance and updates the desired capacity and minimum size to 1 D. Throws an error Answer: D Explanation: The Auto Scaling command as-terminate-instance-in-auto-scaling-group <Instance ID> will terminate the specific instance ID. The user is required to specify the parameter as --decrement-desiredcapacity. Then Auto Scaling will terminate the instance and decrease the desired capacity by 1. In this case since the minimum size is 2, Auto Scaling will not allow the desired capacity to go below 2. Thus, it will throw an error. QUESTION NO: 265 A user is collecting 1000 records per second. The user wants to send the data to CloudWatch using the custom namespace. Which of the below mentioned options is recommended for this activity? A. Aggregate the data with statistics, such as Min, max, Average, Sum and Sample data and send the data to CloudWatch B. Send all the data values to CloudWatch in a single command by separating them with a comma. CloudWatch will parse automatically C. Create one csv file of all the data and send a single file to CloudWatch D. It is not possible to send all the data in one call. Thus, it should be sent one by one. CloudWatch will aggregate the data automatically Answer: A Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish data to CloudWatch as single data points or as an aggregated set of data points called a statistic set using the command put-metric-data. It is recommended that when the user is having multiple data points per minute, he should aggregate the data so that it will minimize the number of calls to put-metric-data. In this case it will be single call to CloudWatch instead of 1000 calls if the data is aggregated. QUESTION NO: 266 A user is trying to create an EBS volume with the highest PIOPS supported by EBS. What is the minimum size of EBS required to have the maximum IOPS? A. 124 B. 150 C. 134 D. 128 Answer: C Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30. QUESTION NO: 267 An organization is trying to create various IAM users. Which of the below mentioned options is not a valid IAM username? A. John.cloud B. john@cloud C. John=cloud D. john#cloud Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Whenever the organization is creating an IAM user, there should be a unique ID for each user. The names of users, groups, roles, instance profiles must be alphanumeric, including the following common characters: plus (+., equal (=., comma (,., period (.., at (@., and dash (-.. QUESTION NO: 268 A user is having data generated randomly based on a certain event. The user wants to upload that data to CloudWatch. It may happen that event may not have data generated for some period due to andomness. Which of the below mentioned options is a recommended option for this case? A. For the period when there is no data, the user should not send the data at all B. For the period when there is no data the user should send a blank value C. For the period when there is no data the user should send the value as 0 D. The user must upload the data to CloudWatch as having no data for some period will cause an error at CloudWatch monitoring Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. When the user data is more random and not generated at regular intervals, there can be a period which has no associated data. The user can either publish the zero (0. Value for that period or not publish the data at all. It is recommended that the user should publish zero instead of no value to monitor the health of the application. This is helpful in an alarm as well as in the generation of the sample data count. QUESTION NO: 269 A user is sending the data to CloudWatch using the CloudWatch API. The user is sending data 90 minutes in the future. What will CloudWatch do in this case? A. CloudWatch will accept the data B. It is not possible to send data of the future C. It is not possible to send the data manually to CloudWatch D. The user cannot send data for more than 60 minutes in the future Answer: A Explanation: With Amazon CloudWatch, each metric data point must be marked with a time stamp. The user can send the data using CLI but the time has to be in the UTC format. If the user does not provide the time, CloudWatch will take the data received time in the UTC timezone. The time stamp sent by the user can be up to two weeks in the past and up to two hours into the future. QUESTION NO: 270 A user wants to upload a complete folder to AWS S3 using the S3 Management console. How can the user perform this activity? A. Just drag and drop the folder using the flash tool provided by S3 B. Use the Enable Enhanced Folder option from the S3 console while uploading objects C. The user cannot upload the whole folder in one go with the S3 management console D. Use the Enable Enhanced Uploader option from the S3 console while uploading objects Answer: D Explanation: AWS S3 provides a console to upload objects to a bucket. The user can use the file upload screen to upload the whole folder in one go by clicking on the Enable Enhanced Uploader option. When the user uploads afolder, Amazon S3 uploads all the files and subfolders from the specified folder to the user\u2019s bucket. It then assigns a key value that is a combination of the uploaded file name and the folder name. QUESTION NO: 271 Which of the below mentioned AWS RDS logs cannot be viewed from the console for MySQL? A. Error Log B. Slow Query Log C. Transaction Log D. General Log Answer: C Explanation: The user can view, download, and watch the database logs using the Amazon RDS console, the Command Line Interface (CLI., or the Amazon RDS API. For the MySQL RDS, the user can view the error log, slow querylog, and general logs. RDS does not support viewing the transaction logs. QUESTION NO: 272 A user has launched an EBS backed EC2 instance in the US-East-1a region. The user stopped the instance and started it back after 20 days. AWS throws up an \u2018InsufficientInstanceCapacity\u2019 error. What can be the possible reason for this? A. AWS does not have sufficient capacity in that availability zone B. AWS zone mapping is changed for that user account C. There is some issue with the host capacity on which the instance is launched D. The user account has reached the maximum EC2 instance limit Answer: A Explanation: When the user gets an \u2018InsufficientInstanceCapacity\u2019 error while launching or starting an EC2 instance, it means that AWS does not currently have enough available capacity to service the user request. If the user is requesting a large number of instances, there might not be enough server capacity to host them. The user can either try again later, by specifying a smaller number of instances or changing the availability zone if launching a fresh instance. QUESTION NO: 273 A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is true in this scenario? A. The AWS VPC will automatically create a NAT instance with the micro size B. VPC bounds the main route table with a private subnet and a custom route table with a public subnet C. The user has to manually create a NAT instance D. VPC bounds the main route table with a public subnet and a custom route table with a private subnet Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance of a smaller or higher size, respectively. The VPC has an implied router and the VPC wizard updates the main route table used with the private subnet, creates a custom route table and associates it with the public subnet. QUESTION NO: 274 The CFO of a company wants to allow one of his employees to view only the AWS usage report page. Which of the below mentioned IAM policy statements allows the user to have access to the AWS usage report page? A. \"Effect\": \"Allow\", \"Action\": [\u201cDescribe\u201d], \"Resource\": \"Billing\" B. \"Effect\": \"Allow\", \"Action\": [\"AccountUsage], \"Resource\": \"*\" C. \"Effect\": \"Allow\", \"Action\": [\"aws-portal:ViewUsage\"], \"Resource\": \"*\" D. \"Effect\": \"Allow\", \"Action\": [\"aws-portal: ViewBilling\"], \"Resource\": \"*\" Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the CFO wants to allow only AWS usage report page access, the policy for that IAM user will be as given below: { \"Version\": \"2012-10-17\", \"Statement\": [ 168 { \"Effect\": \"Allow\", \"Action\": [ \"aws-portal:ViewUsage\" ], \"Resource\": \"*\" } ] } QUESTION NO: 275 An organization has created 10 IAM users. The organization wants each of the IAM users to have access to a separate DyanmoDB table. All the users are added to the same group and the organization wants to setup a group level policy for this. How can the organization achieve this? A. Define the group policy and add a condition which allows the access based on the IAM name B. Create a DynamoDB table with the same name as the IAM user name and define the policy rule which grants access based on the DynamoDB ARN using a variable C. Create a separate DynamoDB database for each user and configure a policy in the group based on the DB variable. D. It is not possible to have a group level policy which allows different IAM users to different DynamoDB Tables. Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. AWS DynamoDB has only tables and the organization cannot makeseparate databases. The organization should create a table with the same name as the IAM user name and use the ARN of DynamoDB as part of the group policy. The sample policy is shown below: { \"Version\": \"2012-10-17\", \"Statement\": [{ 169 \"Effect\": \"Allow\", \"Action\": [\"dynamodb:*\"], \"Resource\": \"arn:aws:dynamodb:region:account-number-without-hyphens:table/ ${aws:username}\" } ] } QUESTION NO: 276 A user has configured an HTTPS listener on an ELB. The user has not configured any security policy which can help to negotiate SSL between the client and ELB. What will ELB do in this scenario? A. By default ELB will select the first version of the security policy B. By default ELB will select the latest version of the policy C. ELB creation will fail without a security policy D. It is not required to have a security policy since SSL is already installed Answer: B Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. If the user has created an HTTPS/SSL listener without associating any security policy, Elastic Load Balancing will, bydefault, associate the latest version of the ELBSecurityPolicyYYYY-MM with the load balancer. QUESTION NO: 277 A user is creating a Cloudformation stack. Which of the below mentioned limitations does not hold true for Cloudformation? A. One account by default is limited to 100 templates B. The user can use 60 parameters and 60 outputs in a single template C. The template, parameter, output, and resource description fields are limited to 4096 characters D. One account by default is limited to 20 stacks Answer: A Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The limitations given below apply to the Cloudformation template and stack. There are no limits to the number of templates but each AWS CloudFormation account is limited to a maximum of 20 stacks by default. The Template, Parameter, Output, and Resource description fields are limited to 4096 characters. The user can include up to 60 parameters and 60 outputs in a template. QUESTION NO: 278 A user has two EC2 instances running in two separate regions. The user is running an internal memory management tool, which captures the data and sends it to CloudWatch in US East, using a CLI with the same namespace and metric. Which of the below mentioned options is true with respect to the above statement? A. The setup will not work as CloudWatch cannot receive data across regions B. CloudWatch will receive and aggregate the data based on the namespace and metric C. CloudWatch will give an error since the data will conflict due to two sources D. CloudWatch will take the data of the server, which sends the data first Answer: B Explanation: Amazon CloudWatch does not differentiate the source of a metric when receiving custom data. If the user is publishing a metric with the same namespace and dimensions from different sources, CloudWatch will treat them as a single metric. If the data is coming with the same timezone within a minute, CloudWatch will aggregate the data. It treats these as a single metric, allowing the user to get the statistics, such as minimum, maximum, average, and the sum of all across all servers. QUESTION NO: 279 An organization has created a Queue named \u201cmodularqueue\u201d with SQS. The organization is not performing any operations such as SendMessage, ReceiveMessage, DeleteMessage, GetQueueAttributes, SetQueueAttributes, AddPermission, and RemovePermission on the queue. What can happen in this scenario? A. AWS SQS sends notification after 15 days for inactivity on queue B. AWS SQS can delete queue after 30 days without notification C. AWS SQS marks queue inactive after 30 days D. AWS SQS notifies the user after 2 weeks and deletes the queue after 3 weeks. Answer: B Explanation: Amazon SQS can delete a queue without notification if one of the following actions hasn't been performed on it for 30 consecutive days: SendMessage, ReceiveMessage, DeleteMessage, GetQueueAttributes, SetQueueAttributes, AddPermission, and RemovePermission. QUESTION NO: 280 An organization has setup Auto Scaling with ELB. Due to some manual error, one of the instances got rebooted. Thus, it failed the Auto Scaling health check. Auto Scaling has marked it for replacement. How can the system admin ensure that the instance does not get terminated? A. Update the Auto Scaling group to ignore the instance reboot event B. It is not possible to change the status once it is marked for replacement C. Manually add that instance to the Auto Scaling group after reboot to avoid replacement D. Change the health of the instance to healthy using the Auto Scaling commands Answer: D Explanation: After an instance has been marked unhealthy by Auto Scaling, as a result of an Amazon EC2 or ELB health check, it is almost immediately scheduled for replacement as it will never automatically recover its health. If the user knows that the instance is healthy then he can manually call the SetInstanceHealth action (or the as-setinstance- health command from CLI. to set the instance's health status back to healthy. Auto Scaling will throw an error if the instance is already terminating or else it will mark it healthy. QUESTION NO: 281 A system admin wants to add more zones to the existing ELB. The system admin wants to perform this activity from CLI. Which of the below mentioned command helps the system admin to add new zones to the existing ELB? A. elb-enable-zones-for-lb B. elb-add-zones-for-lb C. It is not possible to add more zones to the existing ELB D. elb-configure-zones-for-lb Answer: A Explanation: The user has created an Elastic Load Balancer with the availability zone and wants to add more zones to the existing ELB. The user can do so in two ways: From the console or CLI, add new zones to ELB; QUESTION NO: 282 An organization is planning to create a user with IAM. They are trying to understand the limitations of IAM so that they can plan accordingly. Which of the below mentioned statements is not true with respect to the limitations of IAM? A. One IAM user can be a part of a maximum of 5 groups B. The organization can create 100 groups per AWS account C. One AWS account can have a maximum of 5000 IAM users D. One AWS account can have 250 roles Answer: A Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The default maximums for each of the IAM entities is given below: Groups per AWS account: 100 Users per AWS account: 5000 Roles per AWS account: 250 Number of groups per user: 10 (that is, one user can be part of these many groups. QUESTION NO: 283 A user is planning to scale up an application by 8 AM and scale down by 7 PM daily using Auto Scaling. What should the user do in this case? A. Setup the scaling policy to scale up and down based on the CloudWatch alarms B. The user should increase the desired capacity at 8 AM and decrease it by 7 PM manually C. The user should setup a batch process which launches the EC2 instance at a specific time D. Setup scheduled actions to scale up or down at a specific time Answer: A Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. To configure the Auto Scaling group to scale based on a schedule, the user needs to create scheduled actions. A scheduled action tells Auto Scaling to perform a scaling action at a certain time in the future. QUESTION NO: 284 A user has created a VPC with two subnets: one public and one private. The user is planning to run the patch update for the instances in the private subnet. How can the instances in the private subnet connect to theinternet? A. Use the internet gateway with a private IP B. Allow outbound traffic in the security group for port 80 to allow internet updates C. The private subnet can never connect to the internet D. Use NAT with an elastic IP Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created two subnets (one private and one public., he would need a Network Address Translation (NAT. instance with the elastic IP address. This enables the instances in the private subnet to send requests to the internet (for example, to perform software updates.. QUESTION NO: 285 A user has configured an EC2 instance in the US-East-1a zone. The user has enabled detailed monitoring of the instance. The user is trying to get the data from CloudWatch using a CLI. Which of the below mentioned CloudWatch endpoint URLs should the user use? A. monitoring.us-east-1.amazonaws.com B. monitoring.us-east-1-a.amazonaws.com C. monitoring.us-east-1a.amazonaws.com D. cloudwatch.us-east-1a.amazonaws.com Answer: A Explanation: The CloudWatch resources are always region specific and they will have the end point as region specific. If the user is trying to access the metric in the US-East-1 region, the endpoint URL will be: monitoring.us-east- 1.amazonaws.com QUESTION NO: 286 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling AddToLoadBalancer (which adds instances to the load balancer. process for a while. What will happen to the instances launched during the suspension period? A. The instances will not be registered with ELB and the user has to manually register when the process is resumed B. The instances will be registered with ELB only once the process has resumed C. Auto Scaling will not launch the instance during this period due to process suspension D. It is not possible to suspend only the AddToLoadBalancer process Answer: A Explanation: Auto Scaling performs various processes, such as Launch, Terminate, add to Load Balancer etc. The user can also suspend the individual process. The AddToLoadBalancer process type adds instances to the load balancer when the instances are launched. If this process is suspended, Auto Scaling will launch the instances but will not add them to the load balancer. When the user resumes this process, Auto Scaling will resume adding new instances launched after resumption to the load balancer. However, it will not add running instances that were launched while the process was suspended; those instances must be added manually. QUESTION NO: 287 A sys admin has enabled a log on ELB. Which of the below mentioned activities are not captured by the log? A. Response processing time B. Front end processing time C. Backend processing time D. Request processing time Answer: B Explanation: Elastic Load Balancing access logs capture detailed information for all the requests made to the load balancer. Each request will have details, such as client IP, request path, ELB IP, time, and latencies. The time will have information, such as Request Processing time, Backend Processing time and Response Processing time. QUESTION NO: 288 A user has moved an object to Glacier using the life cycle rules. The user requests to restore the archive after 6 months. When the restore request is completed the user accesses that archive. Which of the below mentioned statements is not true in this condition? A. The archive will be available as an object for the duration specified by the user during the restoration request B. The restored object\u2019s storage class will be RRS C. The user can modify the restoration period only by issuing a new restore request with the updated period D. The user needs to pay storage for both RRS (restored) and Glacier (Archive) Rates. Answer: B Explanation: AWS Glacier is an archival service offered by AWS. AWS S3 provides lifecycle rules to archive and restore objects from S3 to Glacier. Once the object is archived their storage class will change to Glacier. If the user sends a request for restore, the storage class will still be Glacier for the restored object. The user will be paying for both the archived copy as well as for the restored object. The object is available only for the duration specified in the restore request and if the user wants to modify that period, he has to raise another restore request with the updated duration. QUESTION NO: 289 A user is running a batch process on EBS backed EC2 instances. The batch process starts a few instances to process hadoop Map reduce jobs which can run between 50 \u2013 600 minutes or sometimes for more time. The user wants to configure that the instance gets terminated only when the process is completed. How can the user configure this with CloudWatch? A. Setup the CloudWatch action to terminate the instance when the CPU utilization is less than 5% B. Setup the CloudWatch with Auto Scaling to terminate all the instances C. Setup a job which terminates all instances after 600 minutes D. It is not possible to terminate instances automatically Answer: D Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup an action which terminates the instances when their CPU utilization is below a certain threshold for a certain period of time. The EC2 action can either terminate or stop the instance as part of the EC2 action. QUESTION NO: 290 A user has enabled versioning on an S3 bucket. The user is using server side encryption for data at rest. If the user is supplying his own keys for encryption (SSE-C., what is recommended to the user for the purpose of security? A. The user should not use his own security key as it is not secure B. Configure S3 to rotate the user\u2019s encryption key at regular intervals C. Configure S3 to store the user\u2019s keys securely with SSL D. Keep rotating the encryption key manually at the client side Answer: D Explanation: AWS S3 supports client side or server side encryption to encrypt all data at Rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. Since S3 does not store the encryption keys in SSE-C, it is recommended that the user should manage keys securely and keep rotating them regularly at the client side version. QUESTION NO: 291 A user runs the command \u201cdd if=/dev/xvdf of=/dev/null bs=1M\u201d on an EBS volume created from a snapshot and attached to a Linux instance. Which of the below mentioned activities is the user performing with the step given above? A. Pre warming the EBS volume B. Initiating the device to mount on the EBS volume C. Formatting the volume D. Copying the data from a snapshot to the device Answer: A Explanation: When the user creates an EBS volume and is trying to access it for the first time it will encounter reduced IOPS due to wiping or initiating of the block storage. To avoid this as well as achieve the best performance it is required to pre warm the EBS volume. For a volume created from a snapshot and attached with a Linux OS, the \u201cdd\u201d command pre warms the existing data on EBS and any restored snapshots of volumes that have been previously fully pre warmed. This command maintains incremental snapshots; however, because this operation is read-only, it does not pre warm unused space that has never been written to on the original volume. In the command \u201cdd if=/dev/xvdf of=/dev/null bs=1M\u201d , the parameter \u201cif=input file\u201d should be set to the drive that the user wishes to warm. The \u201cof=output file\u201d parameter should be set to the Linux null virtual device, /dev/null. The \u201cbs\u201d parameter sets the block size of the read operation; for optimal performance, this should be set to 1 MB. QUESTION NO: 292 A user has launched an EC2 Windows instance from an instance store backed AMI. The user wants to convert the AMI to an EBS backed AMI. How can the user convert it? A. Attach an EBS volume to the instance and unbundle all the AMI bundled data inside the EBS B. A Windows based instance store backed AMI cannot be converted to an EBS backed AMI C. It is not possible to convert an instance store backed AMI to an EBS backed AMI D. Attach an EBS volume and use the copy command to copy all the ephermal content to the EBS Volume Answer: B Explanation: Generally when a user has launched an EC2 instance from an instance store backed AMI, it can be converted to an EBS backed AMI provided the user has attached the EBS volume to the instance and unbundles the AMI data to it. However, if the instance is a Windows instance, AWS does not allow this. In this case, since the instance is a Windows instance, the user cannot convert it to an EBS backed AMI. QUESTION NO: 293 A user has created a VPC with public and private subnets using the VPC Wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24. Which of the below mentioned entries are required in the main route table to allow the instances in VPC to communicate with each other? A. Destination : 20.0.0.0/24 and Target : VPC B. Destination : 20.0.0.0/16 and Target : ALL C. Destination : 20.0.0.0/0 and Target : ALL D. Destination : 20.0.0.0/16 and Target : Local Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create two route tables and attach to the subnets. The main route table will have the entry \u201cDestination: 20.0.0.0/24 and Target: Local\u201d, which allows all instances in the VPC to communicate with each other. QUESTION NO: 294 A sysadmin has created the below mentioned policy on an S3 bucket named cloudacademy. The bucket has both AWS.jpg and index.html objects. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg] }] A. It will make all the objects as well as the bucket public B. It will throw an error for the wrong action and does not allow to save the policy C. It will make the AWS.jpg object as public D. It will make the AWS.jpg as well as the cloudacademy bucket as public Answer: B Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. In the below policy the action says \u201cS3:ListBucket\u201d for effect Allow and when there is no bucket name mentioned as a part of the resource, it will throw an error and not save the policy. \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg] }] QUESTION NO: 295 A user has launched an EC2 instance and deployed a production application in it. The user wants to prohibit any mistakes from the production team to avoid accidental termination. How can the user achieve this? A. The user can the set DisableApiTermination attribute to avoid accidental termination B. It is not possible to avoid accidental termination C. The user can set the Deletion termination flag to avoid accidental termination D. The user can set the InstanceInitiatedShutdownBehavior flag to avoid accidental termination Answer: A Explanation: It is always possible that someone can terminate an EC2 instance using the Amazon EC2 console, command line interface or API by mistake. If the admin wants to prevent the instance from being accidentally terminated, he can enable termination protection for that instance. The DisableApiTermination attribute controls whether the instance can be terminated using the console, CLI or API. By default, termination protection is disabled for an EC2 instance. When it is set it will not allow the user to terminate the instance from CLI, API or the console. QUESTION NO: 296 A user has created a launch configuration for Auto Scaling where CloudWatch detailed monitoring is disabled. The user wants to now enable detailed monitoring. How can the user achieve this? A. Update the Launch config with CLI to set InstanceMonitoringDisabled = false B. The user should change the Auto Scaling group from the AWS console to enable detailed monitoring C. Update the Launch config with CLI to set InstanceMonitoring.Enabled = true D. Create a new Launch Config with detail monitoring enabled and update the Auto Scaling group Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates the AutoScaling launch config as the first step for creating an Auto Scaling group, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. When the user has created a launch configuration with InstanceMonitoring.Enabled = false it will involve multiple steps to enable detail monitoring. The steps are: Create a new Launch config with detailed monitoring enabled Update the Auto Scaling group with a new launch config Enable detail monitoring on each EC2 instance QUESTION NO: 297 A user is trying to pre-warm a blank EBS volume attached to a Linux instance. Which of the below mentioned steps should be performed by the user? A. There is no need to pre-warm an EBS volume B. Contact AWS support to pre-warm C. Unmount the volume before pre-warming D. Format the device Answer: C Explanation: When the user creates a new EBS volume or restores a volume from the snapshot, the backend storage blocks are immediately allocated to the user EBS. However, the first time when the user is trying to access a block of the storage, it is recommended to either be wiped from the new volumes or instantiated from the snapshot (for restored volumes. before the user can access the block. This preliminary action takes time and can cause a 5 to 50 percent loss of IOPS for the volume when the block is accessed for the first time. To avoid this it is required to pre warm the volume. Prewarming an EBS volume on a Linux instance requires that the user should unmount the blank device first and then write all the blocks on the device using a command, such as \u201cdd\u201d. QUESTION NO: 298 A user has launched an EC2 instance from an instance store backed AMI. The user has attached an additional instance store volume to the instance. The user wants to create an AMI from the running instance. Will the AMI have the additional instance store volume data? A. Yes, the block device mapping will have information about the additional instance store volume B. No, since the instance store backed AMI can have only the root volume bundled C. It is not possible to attach an additional instance store volume to the existing instance store backed AMI instance D. No, since this is ephermal storage it will not be a part of the AMI Answer: A Explanation: When the user has launched an EC2 instance from an instance store backed AMI and added an instance store volume to the instance in addition to the root device volume, the block device mapping for the new AMI contains the information for these volumes as well. In addition, the block device mappings for the instances those are launched from the new AMI will automatically contain information for these volumes. QUESTION NO: 299 A user has created an EBS volume of 10 GB and attached it to a running instance. The user is trying to access EBS for first time. Which of the below mentioned options is the correct statement with respect to a first time EBS access? A. The volume will show a size of 8 GB B. The volume will show a loss of the IOPS performance the first time C. The volume will be blank D. If the EBS is mounted it will ask the user to create a file system Answer: B Explanation: A user can create an EBS volume either from a snapshot or as a blank volume. If the volume is from a snapshot it will not be blank. The volume shows the right size only as long as it is mounted. This shows that the file system is created. When the user is accessing the volume the AWS EBS will wipe out the block storage or instantiate from the snapshot. Thus, the volume will show a loss of IOPS. It is recommended that the user should pre warm the EBS before use to achieve better IO. QUESTION NO: 300 A user has enabled termination protection on an EC2 instance. The user has also set Instance initiated shutdown behaviour to terminate. When the user shuts down the instance from the OS, what will happen? A. The OS will shutdown but the instance will not be terminated due to protection B. It will terminate the instance C. It will not allow the user to shutdown the instance from the OS D. It is not possible to set the termination protection when an Instance initiated shutdown is set to Terminate Answer: B Explanation: It is always possible that someone can terminate an EC2 instance using the Amazon EC2 console, command line interface or API by mistake. If the admin wants to prevent the instance from being accidentally terminated, he can enable termination protection for that instance. The user can also setup shutdown behaviour for an EBS backed instance to guide the instance on what should be done when he initiates shutdown from the OS using Instance initiated shutdown behaviour. If the instance initiated behaviour is set to terminate and the user shuts off the OS even though termination protection is enabled, it will still terminate the instance. QUESTION NO: 301 A user has deployed an application on an EBS backed EC2 instance. For a better performance of application, it requires dedicated EC2 to EBS traffic. How can the user achieve this? A. Launch the EC2 instance as EBS dedicated with PIOPS EBS B. Launch the EC2 instance as EBS enhanced with PIOPS EBS C. Launch the EC2 instance as EBS dedicated with PIOPS EBS D. Launch the EC2 instance as EBS optimized with PIOPS EBS Answer: D Explanation: Any application which has performance sensitive workloads and requires minimal variability with dedicated EC2 to EBS traffic should use provisioned IOPS EBS volumes, which are attached to an EBS-optimized EC2 instance or it should use an instance with 10 Gigabit network connectivity. Launching an instance that is EBSoptimized provides the user with a dedicated connection between the EC2 instance and the EBS volume. QUESTION NO: 302 A user has launched a Windows based EC2 instance. However, the instance has some issues and the user wants to check the log. When the user checks the Instance console output from the AWS console, what will it display? A. All the event logs since instance boot B. The last 10 system event log error C. The Windows instance does not support the console output D. The last three system events\u2019 log errors Answer: D Explanation: The AWS EC2 console provides a useful tool called Console output for problem diagnosis. It is useful to find out any kernel issues, termination reasons or service configuration issues. For a Windows instance it lists the last three system event log errors. For Linux it displays the exact console output. You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"AWS Interview Questions"},{"location":"nightwolf-cotribution/aws/#aws-certified-sysops-administrator-questions-and-answers","text":"These are AWS interview questions for experianced professionals. You will find these questions very helpful in your AWS professional role interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. QUESTION NO: 1 You are currently hosting multiple applications in a VPC and have logged numerous port scans coming in from a specific IP address block. Your security team has requested that all access from the offending IP address block be denied tor the next 24 hours. Which of the following is the best method to quickly and temporarily deny access from the specified IP address block? A. Create an AD policy to modify Windows Firewall settings on all hosts in the VPC to deny access from the IP address block B. Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP address block C. Add a rule to all of the VPC 5 Security Groups to deny access from the IP address block D. Modify the Windows Firewall settings on all Amazon Machine Images (AMIs) that your organization uses in that VPC to deny access from the IP address block Answer: C Reference: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html QUESTION NO: 2 When preparing for a compliance assessment of your system built inside of AWS. what are three best practices for you to prepare for anaudit? Choose any 3 answers. A. Gather evidence of your IT operational controls B. Request and obtain applicable third-party audited AWS compliance reports and certifications C. Request and obtain a compliance and security tour of an AWS data center for a pre-assessment security review D. Request and obtain approval from AWS to perform relevant network scans and indepth penetration tests of your system and endpoints E. Schedule meetings with AWS third-party auditors to provide evidence of AWS compliance that maps to your control objectives Answer: B,D,E QUESTION NO: 3 You have started a new job and are reviewing your company infrastructure on AWS You notice one web application where they have an Elastic Load Balancer (&B) in front of web instances in an Auto Scaling Group. When you check the metrics for the ELB in CloudWatch you see four healthy instances In Availability Zone (AZ) A and zero in AZ B There are zero unhealthy instances. What do you need to fix to balance the instances across AZs? A. Set the ELB to only be attached to another AZ B. Make sure Auto Scaling is configured to launch in both AZs C. Make sure your AMI is available in both AZs D. Make sure the maximum size of the Auto Scaling Group is greater than 4 Answer: B QUESTION NO: 4 You have been asked to leverage Amazon VPC BC2 and SOS to implement an application that submits and receives millions of messages per second to a message queue. You want to ensure your application has sufficient bandwidth between your EC2 instances and SQS Which option will provide (he most scalable solution for bandwidth between the application and SOS? A. Ensure the application instances are properly configured with an Elastic Load Balancer. B. Ensure the application instances are launched in private subnets with the EBS-optimized option enabled. C. Ensure the application instances are launched in public subnets with the associate-publicIP-address=true option enabled D. Launch application instances in private subnets with an Auto Scaling group and Auto Scaling triggers configured to watch the SOS queue size Answer: C Reference: http://www.cardinalpath.com/autoscaling-your-website-with-amazon-web-services-part-2/ QUESTION NO: 5 You have identified network throughput as a bottleneck on your ml small EC2 instance when uploading data into Amazon S3 In the same region. How do you remedy this situation? A. Add an additional ENI B. Change to a larger Instance C. Use DirectConnect between EC2 and S3 D. Use EBS PIOPS on the local volume Answer: B Reference: https://media.amazonwebservices.com/AWS_Amazon_EMR_Best_Practices.pdf QUESTION NO: 6 When attached to an Amazon VPC which two components provide connectivity with external networks? Choose 2 answers A. Elastic IPS (EIP) B. NAT Gateway (NAT) C. Internet Gateway {IGW) D. Virtual Private Gateway (VGW) Answer: C,D QUESTION NO: 7 Your application currently leverages AWS Auto Scaling to grow and shrink as load Increases/decreases and has been performing well. Your marketing team expects a steady ramp up in traffic to follow an upcoming campaign that will result in a 20x growth in traffic over 4 weeks. Your forecast for the approximate number of Amazon EC2 instances necessary to meet the peak demand is 175. What should you do to avoid potential service disruptions during the ramp up in traffic? 456789 A. Ensure that you have pre-allocated 175 Elastic IP addresses so that each server will be able to obtain one as it launches B. Check the service limits in Trusted Advisor and adjust as necessary so the forecasted count remains within limits. C. Change your Auto Scaling configuration to set a desired capacity of 175 prior to the launch of the marketing campaign. D. Pre-warm your Elastic Load Balancer to match the requests per second anticipated during peak demand prior to the marketing campaign. Answer: D QUESTION NO: 8 You have an Auto Scaling group associated with an Elastic Load Balancer (ELB). You have noticed that instances launched via the Auto Scaling group are being marked unhealthy due to an ELB health check, but these unhealthy instances are not being terminated. What do you need to do to ensure trial instances marked unhealthy by the ELB will be terminated and replaced? A. Change the thresholds set on the Auto Scaling group health check B. Add an Elastic Load Balancing health check to your Auto Scaling group C. Increase the value for the Health check interval set on the Elastic Load Balancer D. Change the health check set on the Elastic Load Balancer to use TCP rather than HTTP checks Answer: B Reference: http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/as-add-elb-healthcheck.html Add an Elastic Load Balancing Health Check to your Auto Scaling GroupBy default, an Auto Scaling group periodically reviews the results of EC2 instance status to determine the health state of each instance. However, if you have associated your Auto Scaling group with an Elastic Load Balancing load balancer, you can choose to use the Elastic Load Balancing health check. In this case, Auto Scaling determines the health status of your instances by checking the results of both the EC2 instance status check and the Elastic Load Balancing instance health check. For information about EC2 instance status checks, see Monitor Instances With Status Checks in the Amazon EC2 User Guide for Linux Instances. For information about Elastic Load Balancing health checks, see Health Check in the Elastic Load Balancing Developer Guide. This topic shows you how to add an Elastic Load Balancing health check to your Auto Scaling group, assuming that you have created a load balancer and have registered the load balancer with your Auto Scaling group. If you have not registered the load balancer with your Auto Scaling group, see Set Up a Scaled and Load-Balanced Application. Auto Scaling marks an instance unhealthy if the calls to the Amazon EC2 action DescribeInstanceStatus return any state other than running, the system status shows impaired, or the calls to Elastic Load Balancing action DescribeInstanceHealth returns OutOfService in the instance state field. If there are multiple load balancers associated with your Auto Scaling group, Auto Scaling checks the health state of your EC2 instances by making health check calls to each load balancer. For each call, if the Elastic Load Balancing action returns any state other than InService, the instance is marked as unhealthy. After Auto Scaling marks an instance as unhealthy, it remains in that state, even if subsequent calls from other load balancers return an InService state for the same instance. QUESTION NO: 9 Which two AWS services provide out-of-the-box user configurable automatic backup-as-a-service and backup rotation options? Choose any 2 answers. A. Amazon S3 B. Amazon RDS C. Amazon EBS D. Amazon Red shift Answer: C,D QUESTION NO: 10 An organization has configured a VPC with an Internet Gateway (IGW). pairs of public and private subnets (each with one subnet per Availability Zone), and an Elastic Load Balancer (ELB) configured to use the public subnets. The application web tier leverages the ELB, Auto Scaling and a mum-AZ RDS database instance. The Organization would like to eliminate any potential single points ft failure in this design. What step should you take to achieve this organization's objective? A. Nothing, there are no single points of failure in this architecture. B. Create and attach a second IGW to provide redundant internet connectivity. C. Create and configure a second Elastic Load Balancer to provide a redundant load balancer. D. Create a second multi-AZ RDS instance in another Availability Zone and configurereplication to provide a redundant database. Answer: C QUESTION NO: 11 Which of the following are characteristics of Amazon VPC subnets? Choose any 2 answers. A. Each subnet maps to a single Availability Zone B. A CIDR block mask of /25 is the smallest range supported C. Instances in a private subnet can communicate with the internet only if they have an Elastic IP. D. By default, all subnets can route between each other, whether they are private or public E. V Each subnet spans at least 2 Availability zones to provide a high-availability environment. Answer: C,E QUESTION NO: 12 You are creating an Auto Scaling group whose Instances need to insert a custom metric into CloudWatch. Which method would be the best way to authenticate your CloudWatch PUT request? A. Create an IAM role with the Put MetricData permission and modify the Auto Scaling launch configuration to launch instances in that role. B. Create an IAM user with the PutMetricData permission and modify the Auto Scaling launch configuration to inject the userscredentials into the instance User Data. C. Modify the appropriate Cloud Watch metric policies to allow the Put MetricData permission to instances from the Auto Scaling group. D. Create an IAM user with the PutMetricData permission and put the credentials in a private repository and have applications on the server pull the credentials as needed. Answer: B QUESTION NO: 13 When an EC2 instance that is backed by an S3-based AMI Is terminated, what happens to the data on me root volume? A. Data is automatically saved as an E8S volume. B. Data is automatically saved as an ESS snapshot. C. Data is automatically deleted. D. Data is unavailable until the instance is restarted. Answer: D Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html QUESTION NO: 14 You have a web application leveraging an Elastic Load Balancer (ELB) In front of the web servers deployed using an Auto Scaling Group Your database is running on Relational Database Service (RDS) The application serves out technical articles and responses to them in general there are more views of an article than there are responses to the article. On occasion, an article on the site becomes extremely popular resulting in significant traffic Increases that causes the site to go down. What could you do to help alleviate the pressure on the infrastructure while maintaining availability during these events? Choose 3 answers A. Leverage CloudFront for the delivery of the articles. B. Add RDS read-replicas for the read traffic going to your relational database C. Leverage ElastiCache for caching the most frequently used data. D. Use SOS to queue up the requests for the technical posts and deliver them out of the queue. E. Use Route53 health checks to fail over to an S3 bucket for an error page. Answer: A,C,E QUESTION NO: 15 The majority of your Infrastructure is on premises and you have a small footprint on AWS Your company has decided to roll out a new application that is heavily dependent on low latency connectivity to LOAP for authentication Your security policy requires minimal changes to the company's existing application user management processes. What option would you implement to successfully launch this application1? A. Create a second, independent LOAP server in AWS for your application to use for authentication B. Establish a VPN connection so your applications can authenticate against your existing on-premises LDAP servers C. Establish a VPN connection between your data center and AWS create a LDAP replica on AWS and configure your application to use the LDAP replica for authentication D. Create a second LDAP domain on AWS establish a VPN connection to establish a trust relationship between your new and existing domains and use the new domain for authentication Answer: D Reference: http://msdn.microsoft.com/en-us/library/azure/jj156090.aspx QUESTION NO: 16 You need to design a VPC for a web-application consisting of an Elastic Load Balancer (ELB). a fleet of web/application servers, and an RDS database The entire Infrastructure must be distributed over 2 availability zones. Which VPC configuration works while assuring the database is not available from the Internet? A. One public subnet for ELB one public subnet for the web-servers, and one private subnet for the database B. One public subnet for ELB two private subnets for the web-servers, two private subnets for RDS C. Two public subnets for ELB two private subnets for the web-servers and two private subnets for RDS D. Two public subnets for ELB two public subnets for the web-servers, and two public subnets for RDS Answer: A QUESTION NO: 17 An application that you are managing has EC2 instances & Dynamo OB tables deployed to several AWS Regions In order to monitor the performance of the application globally, you would like to see two graphs 1) Avg CPU Utilization across all EC2 instances and 2) Number of Throttled Requests for all DynamoDB tables. How can you accomplish this? A. Tag your resources with the application name, and select the tag name as the dimension in the Cloudwatch Management console to view the respective graphs B. Use the Cloud Watch CLI tools to pull the respective metrics from each regional endpoint Aggregate the data offline & store it for graphing in CloudWatch. C. Add SNMP traps to each instance and DynamoDB table Leverage a central monitoring server to capture data from each instance and table Put the aggregate data into Cloud Watch for graphing. D. Add a CloudWatch agent to each instance and attach one to each DynamoDB table. When configuring the agent set the appropriate application name & view the graphs in CloudWatch. Answer: C QUESTION NO: 18 When assessing an organization s use of AWS API access credentials which of the following three credentials should be evaluated? Choose 3 answers A. Key pairs B. Console passwords 10 C. Access keys D. Signing certificates E. Security Group memberships Answer: A,C,D Reference: http://media.amazonwebservices.com/AWS_Operational_Checklists.pdf QUESTION NO: 19 You have a Linux EC2 web server instance running inside a VPC The instance is In a public subnet and has an EIP associated with it so you can connect to It over the Internet via HTTP or SSH The instance was also fully accessible when you last logged in via SSH. and was also serving web requests on port 80. Now you are not able to SSH into the host nor does it respond to web requests on port 80 that were working fine last time you checked You have double-checked that all networking configuration parameters (security groups route tables. IGW'EIP. NACLs etc) are properly configured {and you haven\u2019t made any changes to those anyway since you were last able to reach the Instance). You look at the EC2 console and notice that system status check shows \"impaired.\" Which should be your next step in troubleshooting and attempting to get the instance back to a healthy state so that you can log in again? A. Stop and start the instance so that it will be able to be redeployed on a healthy host system that most likely will fix the \"impaired\" system status B. Reboot your instance so that the operating system will have a chance to boot in a clean healthy state that most likely will fix the 'impaired\" system status C. Add another dynamic private IP address to me instance and try to connect via mat new path, since the networking stack of the OS may be locked up causing the \u201cimpaired\u201d system status. D. Add another Elastic Network Interface to the instance and try to connect via that new path since the networking stack of the OS may be locked up causing the \"impaired\" system status E. un-map and then re-map the EIP to the instance, since the IGWVNAT gateway may not be working properly, causing the \"impaired\" system status Answer: B QUESTION NO: 20 What is a placement group? A. A collection of Auto Scaling groups in the same Region B. Feature that enables EC2 instances to interact with each other via nigh bandwidth, low latency connections C. A collection of Elastic Load Balancers in the same Region or Availability Zone D. A collection of authorized Cloud Front edge locations for a distribution Answer: C Reference: http://aws.amazon.com/ec2/faqs/ QUESTION NO: 21 Your entire AWS infrastructure lives inside of one Amazon VPC You have an Infrastructure monitoring application running on an Amazon instance in Availability Zone (AZ) A of the region, and another application instance running in AZ B. The monitoring application needs to make use of ICMP ping to confirm network reachability of the instance hosting the application. Can you configure the security groups for these instances to only allow the ICMP ping to pass from the monitoringinstance to the application instance and nothing else\" If so how? A. No Two instances in two different AZ's can't talk directly to each other via ICMP ping as that protocol is not allowed across subnet (iebroadcast) boundaries B. Yes Both the monitoring instance and the application instance have to be a part of the same security group, and that security group needs to allow inbound ICMP C. Yes, The security group for the monitoring instance needs to allow outbound ICMP and the application instance's security group needs to allow Inbound ICMP D. Yes, Both the monitoring instance's security group and the application instance's security group need to allow both inbound and outbound ICMP ping packets since ICMP is not a connection-oriented protocol Answer: D QUESTION NO: 22 You have two Elastic Compute Cloud (EC2) instances inside a Virtual Private Cloud (VPC) in the same Availability Zone (AZ) but in different subnets.One instance is running a database and the other instance an application that will interface with the database. You want to confirm that they can talk to each other for your application to work properly. Which two things do we need to confirm in the VPC settings so that these EC2 instances can communicate inside the VPC? Choose 2 answers A. A network ACL that allows communication between the two subnets. B. Both instances are the same instance class and using the same Key-pair. C. That the default route is set to a NAT instance or internet Gateway (IGW) for them to communicate. D. Security groups are set to allow the application host to talk to the database on the right port/protocol. Answer: A,C QUESTION NO: 23 Which services allow the customer to retain full administrative privileges of the underlying EC2 instances? Choose 2 answers A. Amazon Elastic Map Reduce B. Elastic Load Balancing C. AWS Elastic Beanstalk D. I\" Amazon Elasticache E. Amazon Relational Database service Answer: B,C QUESTION NO: 24 You have a web-style application with a stateless but CPU and memory-intensive web tier running on a cc2 8xlarge EC2 instance inside of a VPC The instance when under load is having 134 problems returning requests within the SLA as defined by your business The application maintains its state in a DynamoDB table, but the data tier is properly provisioned and responses are consistently fast. How can you best resolve the issue of the application responses not meeting your SLA? A. Add another cc2 8xlarge application instance, and put both behind an Elastic Load Balancer B. Move the cc2 8xlarge to the same Availability Zone as the DynamoDB table C. Cache the database responses in ElastiCache for more rapid access D. Move the database from DynamoDB to RDS MySQL in scale-out read-replica configuration Answer: B Reference: http://aws.amazon.com/elasticmapreduce/faqs/ QUESTION NO: 25 You are managing a legacy application Inside VPC with hard coded IP addresses in its configuration. Which two mechanisms will allow the application to failover to new instances without the need for reconfiguration? Choose 2 answers A. Create an ELB to reroute traffic to a failover instance B. Create a secondary ENI that can be moved to a failover instance C. Use Route53 health checks to fail traffic over to a failover instance D. Assign a secondary private IP address to the primary ENIO that can De moved to a failover instance Answer: A,D QUESTION NO: 26 You are designing a system that has a Bastion host. This component needs to be highly available without human intervention. Which of the following approaches would you select? A. Run the bastion on two instances one in each AZ B. Run the bastion on an active Instance in one AZ and have an AMI ready to boot up in the event of failure C. Configure the bastion instance in an Auto Scaling group Specify the Auto Scaling group to include multiple AZs but have a min-size of 1 and max-size of 1 D. Configure an ELB in front of the bastion instance Answer: C QUESTION NO: 27 Which of the following statements about this S3 bucket policy is true? 15 A. Denies the server with the IP address 192 168 100 0 full access to the \"mybucket\" bucket B. Denies the server with the IP address 192 168 100 188 full access to the \"mybucket\" bucket C. Grants all the servers within the 192 168 100 0/24 subnet full access to the \"mybucket\" bucket D. Grants all the servers within the 192 168 100 188/32 subnet full access to the \"mybucket\" bucket Answer: C QUESTION NO: 28 Which of the following requires a custom CloudWatch metric to monitor? A. Data transfer of an EC2 instance B. Disk usage activity of an EC2 instance C. Memory Utilization of an EC2 instance D. CPU Utilization of an EC2mstance Answer: B Reference: http://aws.amazon.com/cloudwatch/ QUESTION NO: 29 You run a web application where web servers on EC2 Instances are In an Auto Scaling group Monitoring over the last 6 months shows that 6 web servers are necessary to handle the minimum load During the day up to 12 servers are needed Five to six days per year, the number of web servers required might go up to 15. What would you recommend to minimize costs while being able to provide hill availability? A. 6 Reserved instances (heavy utilization). 6 Reserved instances {medium utilization), rest covered by On-Demand instances B. 6 Reserved instances (heavy utilization). 6 On-Demand instances, rest covered by Spot Instances C. 6 Reserved instances (heavy utilization) 6 Spot instances, rest covered by OnDemand instances D. 6 Reserved instances (heavy utilization) 6 Reserved instances (medium utilization) rest covered by Spot instances 1678 Answer: C QUESTION NO: 30 You have been asked to propose a multi-region deployment of a web-facing application where a controlled portion of your traffic is being processed by an alternate region. Which configuration would achieve that goal? A. Route53 record sets with weighted routing policy B. Route53 record sets with latency based routing policy C. Auto Scaling with scheduled scaling actions set D. Elastic Load Balancing with health checks enabled Answer: D Reference: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/TerminologyandKeyConcepts.html QUESTION NO: 31 You have set up Individual AWS accounts for each project. You have been asked to make sure your AWS Infrastructure costs do not exceed the budget set per project for each month. Which of the following approaches can help ensure that you do not exceed the budget each month? A. Consolidate your accounts so you have a single bill for all accounts and projects B. Set up auto scaling with CloudWatch alarms using SNS to notify you when you are running too many Instances in a given account C. Set up CloudWatch billing alerts for all AWS resources used by each project, with a notification occurring when the amount for each resource tagged to a particular project matches the budget allocated to the project. D. Set up CloudWatch billing alerts for all AWS resources used by each account, with email notifications when it hits 50%. 80% and 90% of its budgeted monthly spend Answer: C QUESTION NO: 32 When creation of an EBS snapshot Is initiated but not completed the EBS volume? A. Cannot De detached or attached to an EC2 instance until me snapshot completes B. Can be used in read-only mode while me snapshot is in progress C. Can be used while me snapshot Is in progress D. Cannot be used until the snapshot completes Answer: C Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html QUESTION NO: 33 You are using ElastiCache Memcached to store session state and cache database queries in your infrastructure You notice in Cloud Watch that Evictions and GetMisses are Doth very high. What two actions could you take to rectify this? Choose 2 answers A. Increase the number of nodes in your cluster B. Tweak the max-item-size parameter C. Shrink the number of nodes in your cluster D. Increase the size of the nodes in the duster Answer: B,D QUESTION NO: 34 You are running a database on an EC2 instance, with the data stored on Elastic Block Store (EBS) for persistence At times throughout the day, you are seeing large variance in the response times of the database queries Looking into the instance with the isolate command you see a lot of wait time on the disk volume that the database's data is stored on. What two ways can you improve the performance of the database's storage while maintaining the current persistence of the data? Choose 2 answers A. Move to an SSD backed instance B. Move the database to an EBS-Optimized Instance C. T Use Provisioned IOPs EBS D. Use the ephemeral storage on an m2 4xiarge Instance Instead Answer: A,B QUESTION NO: 35 Your EC2-Based Multi-tier application includes a monitoring instance that periodically makes application -level read only requests of various application components and if any of those fail more than three times 30 seconds calls CloudWatch lo fire an alarm, and the alarm notifies your operations team by email and SMS of a possible application health problem. However, you also need to watch the watcher -the monitoring instance itself - and be notified if it becomes unhealthy. Which of the following Is a simple way to achieve that goal? A. Run another monitoring instance that pings the monitoring instance and fires a could watch alarm mat notifies your operations teamshould the primary monitoring instance become unhealthy. B. Set a Cloud Watch alarm based on EC2 system and instance status checks and have the alarm notify your operations team of anydetected problem with the monitoring instance. C. Set a Cloud Watch alarm based on the CPU utilization of the monitoring instance and nave the alarm notify your operations team if C r the CPU usage exceeds 50% few more than one minute: then have your monitoring application go into a CPU-bound loop should itDetect any application problems. D. Have the monitoring instances post messages to an SOS queue and then dequeue those messages on another instance should D c- the queue cease to have new messages, the second instance should first terminate the original monitoring instance start anotherbackup monitoring instance and assume (he role of the previous monitoring instance and beginning adding messages to the SOSqueue. 19 Answer: D QUESTION NO: 36 You have decided to change the Instance type for instances running In your application tier that are using Auto Scaling. In which area below would you change the instance type definition? A. Auto Scaling launch configuration B. Auto Scaling group C. Auto Scaling policy D. Auto Scaling tags Answer: B Reference: http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/WhatIsAutoScaling.html QUESTION NO: 37 You are attempting to connect to an instance in Amazon VPC without success You have already verified that the VPC has an Internet Gateway (IGW) the instance has an associated Elastic IP (EIP) and correct security group rules are in place. Which VPC component should you evaluate next? A. The configuration of a MAT instance B. The configuration of the Routing Table C. The configuration of the internet Gateway (IGW) D. The configuration of SRC/DST checking Answer: C Reference: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/UserScenariosForVPC.html QUESTION NO: 38 You are tasked with the migration of a highly trafficked Node JS application to AWS In order to comply with organizational standards Chef recipes must be used to configure the application servers that host this application and to support application lifecycle events. Which deployment option meets these requirements while minimizing administrative burden? A. Create a new stack within Opsworks add the appropriate layers to the stack and deploy the application B. Create a new application within Elastic Beanstalk and deploy this application to a new environment C. Launch a Mode JS server from a community AMI and manually deploy the application to the launched EC2 instance D. Launch and configure Chef Server on an EC2 instance and leverage the AWS CLI to launch application servers and configure those instances using Chef. Answer: B Reference: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deployment.html QUESTION NO: 39 You have been asked to automate many routine systems administrator backup and recovery activities Your current plan is to leverage AWS-managed solutions as much as possible and automate the rest with the AWS CU and scripts. Which task would be best accomplished with a script? A. Creating daily EBS snapshots with a monthly rotation of snapshots B. Creating daily ROS snapshots with a monthly rotation of snapshots C. Automatically detect and stop unused or underutilized EC2 instances D. Automatically add Auto Scaled EC2 instances to an Amazon Elastic Load Balancer Answer: C QUESTION NO: 40 Your organization's security policy requires that all privileged users either use frequently rotated passwords or one-time access credentials in addition to username/password. Which two of the following options would allow an organization to enforce this policy for AWS users? Choose 2 answers A. Configure multi-factor authentication for privileged 1AM users B. Create 1AM users for privileged accounts C. Implement identity federation between your organization's Identity provider leveraging the 1AM Security Token Service D. Enable the 1AM single-use password policy option for privileged users Answer: C,D QUESTION NO: 41 What are characteristics of Amazon S3? Choose 2 answers A. Objects are directly accessible via a URL B. S3 should be used to host a relational database C. S3 allows you to store objects or virtually unlimited size D. S3 allows you to store virtually unlimited amounts of data E. S3 offers Provisioned IOPS Answer: B,C QUESTION NO: 42 You receive a frantic call from a new DBA who accidentally dropped a table containing all your customers. Which Amazon RDS feature will allow you to reliably restore your database to within 5 minutes of when the mistake was made? A. Multi-AZ RDS B. RDS snapshots C. RDS read replicas D. RDS automated backup Answer: B Reference: http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.BackingUpAndRestoringAmazonRDSInstances.html QUESTION NO: 43 A media company produces new video files on-premises every day with a total size of around 100GBS after compression All files have a size of 1 -2 GB and need to be uploaded to Amazon S3 every night in a fixed time window between 3am and 5am Current upload takes almost 3 hours, although less than half of the available bandwidth is used. What step(s) would ensure that the file uploads are able to complete in the allotted time window? A. Increase your network bandwidth to provide faster throughput to S3 B. Upload the files in parallel to S3 C. Pack all files into a single archive, upload it to S3, then extract the files in AWS D. Use AWS Import/Export to transfer the video files Answer: D Reference: http://aws.amazon.com/importexport/faqs/ QUESTION NO: 44 You are running a web-application on AWS consisting of the following components an Elastic Load Balancer (ELB) an Auto-Scaling Group of EC2 instances running Linux/PHP/Apache, and Relational DataBase Service (RDS) MySQL. Which security measures fall into AWS's responsibility? A. Protect the EC2 instances against unsolicited access by enforcing the principle of least-privilege access B. Protect against IP spoofing or packet sniffing C. Assure all communication between EC2 instances and ELB is encrypted D. Install latest security patches on ELB. RDS and EC2 instances Answer: B QUESTION NO: 45 You use S3 to store critical data for your company Several users within your group currently have lull permissions to your S3 buckets You need to come up with a solution mat does not impact your users and also protect against the accidental deletion of objects. Which two options will address this issue? Choose 2 answers A. Enable versioning on your S3 Buckets B. Configure your S3 Buckets with MFA delete C. Create a Bucket policy and only allow read only permissions to all users at the bucket level D. Enable object life cycle policies and configure the data older than 3 months to be archived in Glacier Answer: B,C QUESTION NO: 46 An organization's security policy requires multiple copies of all critical data to be replicated across at least a primary and backup data center. The organization has decided to store some critical data on Amazon S3. 245 Which option should you implement to ensure this requirement is met? A. Use the S3 copy API to replicate data between two S3 buckets in different regions B. You do not need to implement anything since S3 data is automatically replicated between regions C. Use the S3 copy API to replicate data between two S3 buckets in different facilities within an AWS Region D. You do not need to implement anything since S3 data is automatically replicated between multiple facilities within an AWS Region Answer: C QUESTION NO: 47 You are tasked with setting up a cluster of EC2 Instances for a NoSOL database The database requires random read 10 disk performance up to a 100.000 IOPS at 4KB block side per node Which of the following EC2 instances will perform the best for this workload? A. A High-Memory Quadruple Extra Large (m2 4xlarge) with EBS-Optimized set to true and a PIOPs EBS volume B. A Cluster Compute Eight Extra Large (cc2 8xlarge) using instance storage C. High I/O Quadruple Extra Large (hil 4xiarge) using instance storage D. A Cluster GPU Quadruple Extra Large (cg1 4xlarge) using four separate 4000 PIOPS EBS volumes in a RAID 0 configuration Answer: B Reference: http://aws.amazon.com/ec2/instance-types/ QUESTION NO: 48 When an EC2 EBS-backed (EBS root) instance is stopped, what happens to the data on any ephemeral store volumes? A. Data will be deleted and win no longer be accessible B. Data Is automatically saved in an EBS volume. C. Data Is automatically saved as an E8S snapshot D. Data is unavailable until the instance is restarted Answer: D QUESTION NO: 49 Your team Is excited about theuse of AWS because now they have access to programmable Infrastructure\" You have been asked to manage your AWS infrastructure In a manner similar to the way you might manage application code You want to be able to deploy exact copies of different versions of your infrastructure, stage changes into different environments, revert back to previous versions, and identify what versions are running at any particular time (development test QA. production). Which approach addresses this requirement? A. Use cost allocation reports and AWS Opsworks to deploy and manage your infrastructure. B. Use AWS CloudWatch metrics and alerts along with resource tagging to deploy and manage your infrastructure. C. Use AWS Beanstalk and a version control system like GIT to deploy and manage your infrastructure. D. Use AWS CloudFormation and a version control system like GIT to deploy and manage your infrastructure. Answer: B Reference: http://aws.amazon.com/opsworks/faqs/ QUESTION NO: 50 You have a server with a 5O0GB Amazon EBS data volume. The volume is 80% full. You need to back up the volume at regular intervals and be able to re-create the volume in a new Availability Zone in the shortest time possible. All applications using the volume can be paused for a period of a few minutes with no discernible user impact. Which of the following backup methods will best fulfill your requirements? A. Take periodic snapshots of the EBS volume 2678930 B. Use a third party Incremental backup application to back up to Amazon Glacier C. Periodically back up all data to a single compressed archive and archive to Amazon S3 using a parallelized multi-part upload D. Create another EBS volume in the second Availability Zone attach it to the Amazon EC2 instance, and use a disk manager to mirror me two disks Answer: D Reference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html QUESTION NO: 51 Your company Is moving towards tracking web page users with a small tracking Image loaded on each page Currently you are serving this image out of US-East, but are starting to get concerned about the time It takes to load the image for users on the west coast. What are the two best ways to speed up serving this image? Choose 2 answers A. Use Route 53's Latency Based Routing and serve the image out of US-West-2 as well as US-East-1 B. Serve the image out through CloudFront C. Serve the image out of S3 so that it isn't being served oft of your web application tier D. Use EBS PIOPs to serve the image faster out of your EC2 instances Answer: A,B QUESTION NO: 52 If you want to launch Amazon Elastic Compute Cloud (EC2) Instances and assign each Instance a predetermined private IP address you should: A. Assign a group or sequential Elastic IP address to the instances B. Launch the instances in a Placement Group C. Launch the instances in the Amazon virtual Private Cloud (VPC). D. Use standard EC2 instances since each instance gets a private Domain Name Service (DNS) already E. Launch the Instance from a private Amazon Machine image (Mil) Answer: C Reference: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-ip-addressing.html QUESTION NO: 53 A customer has a web application that uses cookie Based sessions to track logged in users It Is deployed on AWS using ELB and Auto Scaling The customer observes that when load increases. Auto Scaling launches new Instances but the load on the easting Instances does not decrease, causing all existing users to have a sluggish experience. Which two answer choices independently describe a behavior that could be the cause of the sluggish user experience? Choose 2 answers A. ELB's normal behavior sends requests from the same user to the same backend instance B. ELB's behavior when sticky sessions are enabled causes ELB to send requests in the same session to the same backend instance C. A faulty browser is not honoring the TTL of the ELB DNS name. D. The web application uses long polling such as comet or websockets. Thereby keeping a connection open to a web server tor a long time E. The web application uses long polling such as comet or websockets. Thereby keeping a connection open to a web server for a long time. Answer: B,D QUESTION NO: 54 What would happen to an RDS (Relational Database Service) multi-Availability Zone deployment of the primary OB instance fails? A. The IP of the primary DB instance is switched to the standby OB instance B. The RDS (Relational Database Service) DB instance reboots C. A new DB instance is created in the standby availability zone D. The canonical name record (CNAME) is changed from primary to standby Answer: B QUESTION NO: 55 How can the domain's zone apex for example \"myzoneapexdomain com\" be pointed towards an Elastic Load Balancer? A. By using an AAAA record B. By using an A record C. By using an Amazon Route 53 CNAME record D. By using an Amazon Route 53 Alias record Answer: C Reference: http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosingalias-non-alias.html Topic 2, Volume B QUESTION NO: 56 An organization has created 5 IAM users. The organization wants to give them the same login ID but different passwords. How can the organization achieve this? A. The organization should create a separate login ID but give the IAM users the same alias so that each one can login with their alias B. The organization should create each user in a separate region so that they have their own URL to login C. It is not possible to have the same login ID for multiple IAM users of the same account D. The organization should create various groups and add each user with the same login ID to different groups. The user can login with their own group ID Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Whenever the organization is creating an IAM user, there should be a unique ID for each user. It is not possible to have the same login ID for multiple users. The names of users,groups, roles, instance profiles must be alphanumeric, including the following common characters: plus(+), equal(=), comma(,), period(.), at(@), and dash (-) QUESTION NO: 57 A user is planning to evaluate AWS for their internal use. The user does not want to incur any charge on his account during the evaluation. Which of the below mentioned AWS services would incur a charge if used? A. AWS S3 with 1 GB of storage B. AWS micro instance running 24 hours daily C. AWS ELB running 24 hours a day D. AWS PIOPS volume of 10 GB size Answer: D Explanation: AWS is introducing a free usage tier for one year to help the new AWS customers get started in Cloud. The free tier can be used for anything that the user wants to run in the Cloud. AWS offers a handful of AWS services as a part of this which includes 750 hours of free micro instances and 750 hours of ELB. It includes the AWS S3 of 5 GB and AWS EBS general purpose volume upto 30 GB. PIOPS is not part of free usage tier. QUESTION NO: 58 A user has developed an application which is required to send the data to a NoSQL database. The user wants to decouple the data sending such that the application keeps processing and sending data but does not wait for an acknowledgement of DB. Which of the below mentioned applications helps in this scenario? A. AWS Simple Notification Service B. AWS Simple Workflow C. AWS Simple Queue Service D. AWS Simple Query Service Answer: C Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. In this case, the user can use AWS SQS to send messages which are received from an application and sent to DB. The application can continue processing data without waiting for any acknowledgement from DB. The user can use SQS to transmit any volume of data without losing messages or requiring other services to always be available. QUESTION NO: 59 An organization has created 50 IAM users. The organization has introduced a new policy which will change the access of an IAM user. How can the organization implement this effectively so that there is no need to apply the policy at the individual user level? A. Use the IAM groups and add users as per their role to different groups and apply policy to group B. The user can create a policy and apply it to multiple users in a single go with the AWS CLI C. Add each user to the IAM role as per their organization role to achieve effective policy setup D. Use the IAM role and implement access at the role level Answer: A Explanation: With AWS IAM, a group is a collection of IAM users. A group allows the user to specify permissions for a collection of users, which can make it easier to manage the permissions for those users. A group helps an organization manage access in a better way; instead of applying at the individual level, the organization can apply at the group level which is applicable to all the users who are a part of that group. QUESTION NO: 60 A user is planning to use AWS Cloud formation for his automatic deployment requirements. Which of the below mentioned components are required as a part of the template? A. Parameters B. Outputs C. Template version D. Resources Answer: D Explanation: AWS Cloud formation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The template is a JSON-format, text-based file that describes all the AWS resources required to deploy and run an application. It can have option fields, such as Template Parameters, Output, Data tables, and Template file format version. The only mandatory value is Resource. The user can define the AWS services which will be used/ created by this template inside the Resource section QUESTION NO: 61 A user has recently started using EC2. The user launched one EC2 instance in the default subnet in EC2-VPC Which of the below mentioned options is not attached or available with the EC2 instance when it is launched? A. Public IP address B. Internet gateway C. Elastic IP D. Private IP address Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to a user\u2019s AWS account. A subnet is a range of IP addresses in the VPC. The user can launch the AWS resources into a subnet. There are two supported platforms into which a user can launch instances: EC2-Classic and EC2-VPC (default subnet.. A default VPC has all the benefits of EC2-VPC and the ease of use of EC2- Classic. Each instance that the user launches into a default subnet has a private IP address and a public IP address. These instances can communicate with the internet through an internet gateway. An internet gateway enables the EC2 instances to connect to the internet through the Amazon EC2 network edge. QUESTION NO: 62 A user has launched an EC2 instance. The user is planning to setup the CloudWatch alarm. Which of the below mentioned actions is not supported by the CloudWatch alarm? A. Notify the Auto Scaling launch config to scale up B. Send an SMS using SNS C. Notify the Auto Scaling group to scale down D. Stop the EC2 instance Answer: B Explanation: A user can create a CloudWatch alarm that takes various actions when the alarm changes state. An alarm watches a single metric over the time period that the user has specified, and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The actions could be sending a notification to an Amazon Simple Notification Service topic (SMS, Email, and HTTP end point.,notifying the Auto Scaling policy or changing the state of the instance to Stop/Terminate. QUESTION NO: 63 A user is trying to delete an Auto Scaling group from CLI. Which of the below mentioned steps are to be performed by the user? A. Terminate the instances with the ec2-terminate-instance command B. Terminate the Auto Scaling instances with the as-terminate-instance command C. Set the minimum size and desired capacity to 0 D. There is no need to change the capacity. Run the as-delete-group command and it will reset all values to 0 Answer: C Explanation: If the user wants to delete the Auto Scaling group, the user should manually set the values of the minimum and desired capacity to 0. Otherwise Auto Scaling will not allow for the deletion of the group from CLI. While trying from the AWS console, the user need not set the values to 0 as the Auto Scaling console will automatically do so. QUESTION NO: 64 An organization is planning to create 5 different AWS accounts considering various security requirements. The organization wants to use a single payee account by using the consolidated billing option. Which of the below mentioned statements is true with respect to the above information? A. Master (Payee. account will get only the total bill and cannot see the cost incurred by each account B. Master (Payee. account can view only the AWS billing details of the linked accounts C. It is not recommended to use consolidated billing since the payee account will have access to the linked accounts D. Each AWS account needs to create an AWS billing policy to provide permission to the payee account Answer: B Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. Consolidated billing enables the organization to see a combined view of the AWS charges incurred by each account as well as obtain a detailed cost report for each of the individual AWS accounts associated with the paying account. The payee account will not have any other access than billing data of linked accounts. QUESTIONNO: 64-A A user has deployed an application on his private cloud. The user is using his own monitoring tool. He wants to configure that whenever there is an error, the monitoring tool should notify him via SMS. Which of the below mentioned AWS services will help in this scenario? A. None because the user infrastructure is in the private cloud/ B. AWS SNS C. AWS SES D. AWS SMS Answer: B Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can be used to make push notifications to mobile devices. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. In this case user can use the SNS apis to send SMS. QUESTION NO: 65 A user has created a web application with Auto Scaling. The user is regularly monitoring the application and he observed that the traffic is highest on Thursday and Friday between 8 AM to 6 PM. What is the best solution to handle scaling in this case? A. Add a new instance manually by 8 AM Thursday and terminate the same by 6 PM Friday B. Schedule Auto Scaling to scale up by 8 AM Thursday and scale down after 6 PM on Friday C. Schedule a policy which may scale up every day at 8 AM and scales down by 6 PM D. Configure a batch process to add a instance by 8 AM and remove it by Friday 6 PM Answer: B Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. In this case the load increases by Thursday and decreases by Friday. Thus, the user can setup the scaling activity based on the predictable traffic patterns of the web application using Auto Scaling scale by Schedule. QUESTION NO: 66 A user has setup a CloudWatch alarm on an EC2 action when the CPU utilization is above 75%. The alarm sends a notification to SNS on the alarm state. If the user wants to simulate the alarm action how can he achieve this? A. Run activities on the CPU such that its utilization reaches above 75% B. From the AWS console change the state to \u2018Alarm\u2019 C. The user can set the alarm state to \u2018Alarm\u2019 using CLI D. Run the SNS action manually Answer: C Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods.The user can test an alarm by setting it to any state using the SetAlarmState API (mon-set-alarm-state command.. This temporary state change lasts only until the next alarm comparison occurs. QUESTION 13 A user is trying to setup a scheduled scaling activity using Auto Scaling. The user wants to setup the recurring schedule. Which of the below mentioned parameters is not required in this case? A. Maximum size B. Auto Scaling group name C. End time D. Recurrence value Answer: A Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can also configure the recurring schedule action which will follow the Linux cron format. If the user is setting a recurring event, it is required that the user specifies the Recurrence value (in a cron format., end time (not compulsory but recurrence will stop after this. and the Auto Scaling group for which the scaling activity is to be scheduled. QUESTION NO: 67 A user has setup a billing alarm using CloudWatch for $200. The usage of AWS exceeded $200 after some days. The user wants to increase the limit from $200 to $400? What should the user do? A. Create a new alarm of $400 and link it with the first alarm B. It is not possible to modify the alarm once it has crossed the usage limit C. Update the alarm to set the limit at $400 instead of $200 D. Create a new alarm for the additional $200 amount Answer: C Explanation: AWS CloudWatch supports enabling the billing alarm on the total AWS charges. The estimated charges are calculated and sent several times daily to CloudWatch in the form of metric data. This data will be stored for 14 days. This data also includes the estimated charges for every service in AWS used by the user, as well as the estimated overall AWS charges. If the user wants to increase the limit, the user can modify the alarm and specify a new threshold. QUESTION NO: 68 A sys admin has created the below mentioned policy and applied to an S3 object named aws.jpg. The aws.jpg is inside a bucket named cloudacademy. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg\"] }] A. It is not possible to define a policy at the object level B. It will make all the objects of the bucket cloudacademy as public C. It will make the bucket cloudacademy as public D. the aws.jpg object as public Answer: A Explanation: A system admin can grant permission to the S3 objects or buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. It cannot be applied at the object level. QUESTION NO: 69 A user is trying to save some cost on the AWS services. Which of the below mentioned options will not help him save cost? A. Delete the unutilized EBS volumes once the instance is terminated B. Delete the AutoScaling launch configuration after the instances are terminated C. Release the elastic IP if not required once the instance is terminated D. Delete the AWS ELB after the instances are terminated Answer: B Explanation: AWS bills the user on a as pay as you go model. AWS will charge the user once the AWS resource is allocated. Even though the user is not using the resource, AWS will charge if it is in service or allocated. Thus, it is advised that once the user\u2019s work is completed he should: Terminate the EC2 instance Delete the EBS volumes Release the unutilized Elastic IPs Delete ELB The AutoScaling launch configuration does not cost the user. Thus, it will not make any difference to the cost whether it is deleted or not. QUESTION NO: 70 A user is trying to aggregate all the CloudWatch metric data of the last 1 week. Which of the below mentioned statistics is not available for the user as a part of data aggregation? A. Aggregate B. Sum C. Sample data D. Average Answer: A Explanation: Amazon CloudWatch is basically a metrics repository. Either the user can send the custom data or an AWS product can put metrics into the repository, and the user can retrieve the statistics based on those metrics. The statistics are metric data aggregations over specified periods of time. Aggregations are made using the namespace, metric name, dimensions, and the data point unit of measure, within the time period that is specified by the user. CloudWatch supports Sum, Min, Max, Sample Data and Average statistics aggregation. QUESTION NO: 71 An organization is planning to use AWS for their production roll out. The organization wants to implement automation for deployment such that it will automatically create a LAMP stack, download the latest PHP installable from S3 and setup the ELB. Which of the below mentioned AWS services meets the quirement for making an orderly deployment of the software? A. AWS Elastic Beanstalk B. AWS Cloudfront C. AWS Cloudformation D. AWS DevOps Answer: C Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. Cloudformation provides an easy way to create and delete the collection of related AWS resources and provision them in an orderly way. AWS CloudFormation automates and simplifies the task of repeatedly and predictably creating groups of related resources that power the user\u2019s applications. AWS Cloudfront is a CDN; Elastic Beanstalk does quite a few of the required tasks. However, it is a PAAS which uses a ready AMI. AWS Elastic Beanstalk provides an environment to easily develop and run applications in the cloud. QUESTION NO: 72 A user has created a subnet with VPC and launched an EC2 instance in that subnet with only default settings.Which of the below mentioned options is ready to use on the EC2 instance as soon as it is launched? A. Elastic IP B. Private IP C. Public IP D. I nternet gateway Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to a user\u2019s AWS account. A subnet is a range of IP addresses in the VPC. The user can launch the AWS resources into a subnet. There are two supported platforms into which a user can launch instances: EC2-Classic and EC2-VPC. When the user launches an instance which is not a part of the non-default subnet, it will only have a private IP assigned to it. The instances part of a subnet can communicate with each other but cannot communicate over the internet or to the AWS services, such as RDS / S3. QUESTION NO: 73 An organization is setting up programmatic billing access for their AWS account. Which of the below mentioned services is not required or enabled when the organization wants to use programmatic access? A. Programmatic access B. AWS bucket to hold the billing report C. AWS billing alerts D. Monthly Billing report Answer: C Explanation: AWS provides an option to have programmatic access to billing. Programmatic Billing Access leverages the existing Amazon Simple Storage Service (Amazon S3. APIs. Thus, the user can build applications that reference his billing data from a CSV (comma-separated value. file stored in an Amazon S3 bucket. To enable programmatic access, the user has to first enable the monthly billing report. Then the user needs to provide an AWS bucket name where the billing CSV will be uploaded. The user should also enable the Programmatic access option. QUESTION NO: 74 A user has configured the Auto Scaling group with the minimum capacity as 3 and the maximum capacity as 5. When the user configures the AS group, how many instances will Auto Scaling launch? A. 3 B. 0 C. 5 D. 2 Answer: C Explanation: When the user configures the launch configuration and the Auto Scaling group, the Auto Scaling group will start instances by launching the minimum number (or the desired number, if specified. of EC2 instances. If there are no other scaling conditions attached to the Auto Scaling group, it will maintain the minimum number of running instances at all times. QUESTION NO: 75 An admin is planning to monitor the ELB. Which of the below mentioned services does not help the admin capture the monitoring information about the ELB activity? A. ELB Access logs B. ELB health check C. CloudWatch metrics D. ELB API calls with CloudTrail Answer: B Explanation: The admin can capture information about Elastic Load Balancer using either: CloudWatch Metrics ELB Logs files which are stored in the S3 bucket CloudTrail with API calls which can notify the user as well generate logs for each API calls The health check is internally performed by ELB and does not help the admin get the ELB activity. QUESTION NO: 76 A user is planning to use AWS Cloudformation. Which of the below mentioned functionalities does not help him to correctly understand Cloudfromation? A. Cloudformation follows the DevOps model for the creation of Dev & Test B. AWS Cloudfromation does not charge the user for its service but only charges for the AWS resources created with it C. Cloudformation works with a wide variety of AWS services, such as EC2, EBS, VPC, IAM, S3, RDS, ELB, etc D. CloudFormation provides a set of application bootstrapping scripts which enables the user to install Software Answer: A Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. It supports a wide variety of AWS services, such as EC2, EBS, AS, ELB, RDS, VPC, etc. It also provides application bootstrapping scripts which enable the user to install software packages or create folders. It is free of the cost and only charges the user for the services created with it. The only challenge is that it does not follow any model, such as DevOps; instead customers can define templates and use them to provision and manage the AWS resources in an orderly way. QUESTION NO: 77 A user has launched 10 instances from the same AMI ID using Auto Scaling. The user is trying to see the average CPU utilization across all instances of the last 2 weeks under the CloudWatch console. How can the user achieve this? A. View the Auto Scaling CPU metrics B. Aggregate the data over the instance AMI ID C. The user has to use the CloudWatchanalyser to find the average data across instances D. It is not possible to see the average CPU utilization of the same AMI ID since the instance ID is different Answer: B Explanation: Amazon CloudWatch is basically a metrics repository. Either the user can send the custom data or an AWS product can put metrics into the repository, and the user can retrieve the statistics based on those metrics. The statistics are metric data aggregations over specified periods of time. Aggregations are made using the namespace, metric name, dimensions, and the data point unit of measure, within the time period that is specified by the user. To aggregate the data across instances launched with AMI, the user should select the AMI ID under EC2 metrics and select the aggregate average to view the data. QUESTION NO: 78 A user is trying to understand AWS SNS. To which of the below mentioned end points is SNS unable to send a notification? A. Email JSON B. HTTP C. AWS SQS D. AWS SES Answer: D Explanation: Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can select one the following transports as part of the subscription requests: \u201cHTTP\u201d, \u201cHTTPS\u201d,\u201dEmail\u201d, \u201cEmailJSON\u201d, \u201cSQS\u201d, \u201cand SMS\u201d. QUESTION NO: 79 A user has configured an Auto Scaling group with ELB. The user has enabled detailed CloudWatch monitoring on Auto Scaling. Which of the below mentioned statements will help the user understand the functionality better? A. It is not possible to setup detailed monitoring for Auto Scaling B. In this case, Auto Scaling will send data every minute and will charge the user extra C. Detailed monitoring will send data every minute without additional charges D. Auto Scaling sends data every minute only and does not charge the user Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Auto Scaling includes 7 metrics and 1 dimension, and sends data to CloudWatch every 5 minutes by default. The user can enable detailed monitoring for Auto Scaling, which sends data to CloudWatch every minute. However, this will have some extra-costs. QUESTION NO: 80 A system admin is planning to setup event notifications on RDS. Which of the below mentioned services will help the admin setup notifications? A. AWS SES B. AWS Cloudtrail C. AWS Cloudwatch D. AWS SNS Answer: D Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. These notifications can be in any notification form supported by Amazon SNS for an AWS region, such as an email, a text message or a call to an HTTP endpoint QUESTION NO: 81 You are building an online store on AWS that uses SQS to process your customer orders. Your backend system needs those messages in the same sequence the customer orders have been put in. How can you achieve that? A. It is not possible to do this with SQS B. You can use sequencing information on each message C. You can do this with SQS but you also need to use SWF D. Messages will arrive in the same order by default Answer: B Explanation: Amazon SQS is engineered to always be available and deliver messages. One of the resulting tradeoffs is that SQSdoes not guarantee first in, first out delivery of messages. For many distributed applications, each message can stand on its own, and as long as all messages are delivered, the order is not important. If your system requires that order be preserved, you can place sequencing information in each message, so that you can reorder the messages when the queue returns them. QUESTION NO: 82 An organization wants to move to Cloud. They are looking for a secure encrypted database storage option. Which of the below mentioned AWS functionalities helps them to achieve this? A. AWS MFA with EBS B. AWS EBS encryption C. Multi-tier encryption with Redshift D. AWS S3 server side storage Answer: B Explanation: AWS EBS supports encryption of the volume while creating new volumes. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. The data at rest, the I/O as well as all the snapshots of EBS will be encrypted. The encryption occurs on the servers that host the EC2 instances, providing encryption of data as it moves between the EC2 instances and EBS storage. EBS encryption is based on the AES-256 cryptographic algorithm, which is the industry standard QUESTION NO: 83 A user wants to disable connection draining on an existing ELB. Which of the below mentioned statements helps the user disable connection draining on the ELB? A. The user can only disable connection draining from CLI B. It is not possible to disable the connection draining feature once enabled C. The user can disable the connection draining feature from EC2 -> ELB console or from CLI D. The user needs to stop all instances before disabling connection draining Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. The user can enable or disable connection draining from the AWS EC2 console -> ELB or using CLI. QUESTION NO: 84 A user has a refrigerator plant. The user is measuring the temperature of the plant every 15 minutes. If the user wants to send the data to CloudWatch to view the data visually, which of the below mentioned statements is true with respect to the information given above? A. The user needs to use AWS CLI or API to upload the data B. The user can use the AWS Import Export facility to import data to CloudWatch C. The user will upload data from the AWS console D. The user cannot upload data to CloudWatch since it is not an AWS service metric Answer: A Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. While sending the data the user has to include the metric name, namespace and timezone as part of the request. QUESTION NO: 85 A system admin is managing buckets, objects and folders with AWS S3. Which of the below mentioned statements is true and should be taken in consideration by the sysadmin? A. The folders support only ACL B. Both the object and bucket can have an Access Policy but folder cannot have policy C. Folders can have a policy D. Both the object and bucket can have ACL but folders cannot have ACL Answer: A Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. It cannot be applied at the object level. The folders are similar to objects with no content. Thus, folders can have only ACL and cannot have a policy. QUESTION NO: 86 A user has created an ELB with three instances. How many security groups will ELB create by default? A. 3 B. 5 C. 2 D. 1 Answer: C Explanation: Elastic Load Balancing provides a special Amazon EC2 source security group that the user can use to ensure that back-end EC2 instances receive traffic only from Elastic Load Balancing. This feature needs two security groups: the source security group and a security group that defines the ingress rules for the back-end instances. To ensure that traffic only flows between the load balancer and the back-end instances, the user can add or modify a rule to the back-end security group which can limit the ingress traffic. Thus, it can come only from the source security group provided by Elastic load Balancing. QUESTION NO: 87 An organization has created 50 IAM users. The organization wants that each user can change their password but cannot change their access keys. How can the organization achieve this? A. The organization has to create a special password policy and attach it to each user B. The root account owner has to use CLI which forces each IAM user to change their password on first login C. By default each IAM user can modify their passwords D. The root account owner can set the policy from the IAM console under the password policy screen Answer: D Explanation: With AWS IAM, organizations can use the AWS Management Console to display, create, change or delete a password policy. As a part of managing the password policy, the user can enable all users to manage their own passwords. If the user has selected the option which allows the IAM users to modify their password, he does not need to set a separate policy for the users. This option in the AWS console allows changing only the password. QUESTION NO: 88 A user has created a photo editing software and hosted it on EC2. The software accepts requests from the user about the photo format and resolution and sends a message to S3 to enhance the picture accordingly.Which of the below mentioned AWS services will help make a scalable software with the AWS infrastructure in this scenario? A. AWS Glacier B. AWS Elastic Transcoder C. AWS Simple Notification Service D. AWS Simple Queue Service Answer: D Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. The user can configure SQS, which will decouple the call between the EC2 application and S3. Thus, the application does not keep waiting for S3 to provide the data. QUESTION NO: 89 An application is generating a log file every 5 minutes. The log file is not critical but may be required only for verification in case of some major issue. The file should be accessible over the internet whenever required. Which of the below mentioned options is a best possible storage solution for it? A. AWS S3 B. AWS Glacier C. AWS RDS D. AWS RRS Answer: D Explanation: Amazon S3 stores objects according to their storage class. There are three major storage classes: Standard, Reduced Redundancy Storage and Glacier. Standard is for AWS S3 and provides very high durability. However, the costs are a little higher. Glacier is for archival and the files are not available over the internet. Reduced Redundancy Storage is for less critical files. Reduced Redundancy is little cheaper as it provides less durability in comparison to S3. In this case since the log files are not mission critical files, RRS will be a better option. QUESTION NO: 90 A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25. The user is trying to create the private subnet with CIDR 20.0.0.128/25. Which of the below mentioned statements is true in this scenario? A. It will not allow the user to create the private subnet due to a CIDR overlap B. It will allow the user to create a private subnet with CIDR as 20.0.0.128/25 C. This statement is wrong as AWS does not allow CIDR 20.0.0.0/25 D. It will not allow the user to create a private subnet due to a wrong CIDR range Answer: B Explanation: When the user creates a subnet in VPC, he specifies the CIDR block for the subnet. The CIDR block of a subnet can be the same as the CIDR block for the VPC (for a single subnet in the VPC., or a subset (to enable multiple subnets.. If the user creates more than one subnet in a VPC, the CIDR blocks of the subnets must not overlap. Thus, in this case the user has created a VPC with the CIDR block 20.0.0.0/24, which supports 256 IP addresses (20.0.0.0 to 20.0.0.255.. The user can break this CIDR block into two subnets, each supporting 128 IP addresses. One subnet uses the CIDR block 20.0.0.0/25 (for addresses 20.0.0.0 - 20.0.0.127. and the other uses the CIDR block 20.0.0.128/25 (for addresses 20.0.0.128 - 20.0.0.255.. QUESTION NO: 91 A user has created an S3 bucket which is not publicly accessible. The bucket is having thirty objects which are also private. If the user wants to make the objects public, how can he configure this with minimal efforts? A. The user should select all objects from the console and apply a single policy to mark them public B. The user can write a program which programmatically makes all objects public using S3 SDK C. Set the AWS bucket policy which marks all objects as public D. Make the bucket ACL as public so it will also mark all objects as public Answer: C Explanation: A system admin can grant permission of the S3 objects or buckets to any user or make the objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. QUESTION NO: 92 A sys admin is maintaining an application on AWS. The application is installed on EC2 and user has configured ELB and Auto Scaling. Considering future load increase, the user is planning to launch new servers proactively so that they get registered with ELB. How can the user add these instances with Auto Scaling? A. Increase the desired capacity of the Auto Scaling group B. Increase the maximum limit of the Auto Scaling group C. Launch an instance manually and register it with ELB on the fly D. Decrease the minimum limit of the Auto Scaling grou Answer: A Explanation: A user can increase the desired capacity of the Auto Scaling group and Auto Scaling will launch a new instance as per the new capacity. The newly launched instances will be registered with ELB if Auto Scaling group is configured with ELB. If the user decreases the minimum size the instances will be removed from Auto Scaling. Increasing the maximum size will not add instances but only set the maximum instance cap. QUESTION NO: 93 An organization, which has the AWS account ID as 999988887777, has created 50 IAM users. All the users are added to the same group cloudacademy. If the organization has enabled that each IAM user can login with the AWS console, which AWS login URL will the IAM users use? A. https:// 999988887777.signin.aws.amazon.com/console/ B. https:// signin.aws.amazon.com/cloudacademy/ C. https:// cloudacademy.signin.aws.amazon.com/999988887777/console/ D. https:// 999988887777.aws.amazon.com/ cloudacademy/ Answer: A Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Once the organization has created the IAM users, they will have a separate AWS console URL to login to the AWS console. The console login URL for the IAM user will be https:// AWS_Account_ID.signin.aws.amazon.com/console/. It uses only the AWS account ID and does not depend on the group or user ID. QUESTION NO: 94 A user has setup connection draining with ELB to allow in-flight requests to continue while the instance is being deregistered through Auto Scaling. If the user has not specified the draining time, how long will ELB allow inflight requests traffic to continue? A. 600 seconds B. 3600 seconds C. 300 seconds D. 0 seconds Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. The user can specify a maximum time (3600 seconds. for the load balancer to keep the connections alive before reporting the instance as deregistered. If the user does not specify the maximum timeout period, by default, the load balancer will close the connections to the deregistering instance after 300 seconds. QUESTION NO: 95 A root AWS account owner is trying to understand various options to set the permission to AWS S3. Which of the below mentioned options is not the right option to grant permission for S3? A. User Access Policy B. S3 Object Access Policy C. S3 Bucket Access Policy D. S3 ACL Answer: B Explanation: Amazon S3 provides a set of operations to work with the Amazon S3 resources. Managing S3 resource access refers to granting others permissions to work with S3. There are three ways the root account owner can define access with S3: S3 ACL: The user can use ACLs to grant basic read/write permissions to other AWS accounts. S3 Bucket Policy: The policy is used to grant other AWS accounts or IAM users permissions for the bucket and the objects in it. User Access Policy: Define an IAM user and assign him the IAM policy which grants him access to S3. QUESTION NO: 96 A sys admin has created a shopping cart application and hosted it on EC2. The EC2 instances are running behind ELB. The admin wants to ensure that the end user request will always go to the EC2 instance where the user session has been created. How can the admin configure this? A. Enable ELB cross zone load balancing B. Enable ELB cookie setup C. Enable ELB sticky session D. Enable ELB connection draining Answer: C Explanation: Generally AWS ELB routes each request to a zone with the minimum load. The Elastic Load Balancer provides a feature called sticky session which binds the user\u2019s session with a specific EC2 instance. If the sticky session is enabled the first request from the user will be redirected to any of the EC2 instances. But, henceforth, all requests from the same user will be redirected to the same EC2 instance. This ensures that all requests coming from the user during the session will be sent to the same application instance. QUESTION NO: 97 A user has configured ELB with three instances. The user wants to achieve High Availabilityi as well as redundancy with ELB. Which of the below mentioned AWS services helps the user achieve this for ELB? A. Route 53 B. AWS Mechanical Turk C. Auto Scaling D. AWS EMR Answer: A Explanation: The user can provide high availability and redundancy for applications running behind Elastic Load Balancer by enabling the Amazon Route 53 Domain Name System (DNS. failover for the load balancers. Amazon Route 53 is a DNS service that provides reliable routing to the user\u2019s infrastructure. QUESTION NO: 98 An organization is using AWS since a few months. The finance team wants to visualize the pattern of AWS spending. Which of the below AWS tool will help for this requirement? A. AWS Cost Manager B. AWS Cost Explorer C. AWS CloudWatch D. AWS Consolidated Billing Answer: B Explanation: The AWS Billing and Cost Management console includes the Cost Explorer tool for viewing AWS cost data as a graph. It does not charge extra to user for this service. With Cost Explorer the user can filter graphs using resource tags or with services in AWS. If the organization is using Consolidated Billing it helps generate report based on linked accounts. This will help organization to identify areas that require further inquiry. The organization can view trends and use that to understand spend and to predict future costs. QUESTION NO: 99 A user has launched an ELB which has 5 instances registered with it. The user deletes the ELB by mistake. What will happen to the instances? A. ELB will ask the user whether to delete the instances or not B. Instances will be terminated C. ELB cannot be deleted if it has running instances registered with it D. Instances will keep running Answer: D Explanation: When the user deletes the Elastic Load Balancer, all the registered instances will be deregistered. However, they will continue to run. The user will incur charges if he does not take any action on those instances. QUESTION NO: 100 A user is planning to setup notifications on the RDS DB for a snapshot. Which of the below mentioned event categories is not supported by RDS for this snapshot source type? A. Backup B. Creation C. Deletion D. Restoration Answer: A Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. Event categories for a snapshot source type include: Creation, Deletion, and Restoration. The Backup is a part of DB instance source type. QUESTION NO: 101 A customer is using AWS for Dev and Test. The customer wants to setup the Dev environment with Cloudformation. Which of the below mentioned steps are not required while using Cloudformation? A. Create a stack B. Configure a service C. Create and upload the template D. Provide the parameters configured as part of the template Answer: B Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. AWS CloudFormation introduces two concepts: the template and the stack. The template is a JSON-format, text-based file that describes all the AWS resources required to deploy and run an application. The stack is a collection of AWS resources which are created and managed as a single unit when AWS CloudFormation instantiates a template. While creating a stack, the user uploads the template and provides the data for the parameters if required. QUESTION NO: 102 A user has configured the AWS CloudWatch alarm for estimated usage charges in the US East region. Which of the below mentioned statements is not true with respect to the estimated charges? A. It will store the estimated charges data of the last 14 days B. It will include the estimated charges of every AWS service C. The metric data will represent the data of all the regions D. The metric data will show data specific to that region Answer: D Explanation: When the user has enabled the monitoring of estimated charges for the AWS account with AWS CloudWatch, the estimated charges are calculated and sent several times daily to CloudWatch in the form of metric data. This data will be stored for 14 days. The billing metric data is stored in the US East (Northern Virginia. Region and represents worldwide charges. This data also includes the estimated charges for every service in AWS used by the user, as well as the estimated overall AWS charges. QUESTION NO: 103 A user is accessing RDS from an application. The user has enabled the Multi AZ feature with the MS SQL RDS DB. During a planned outage how will AWS ensure that a switch from DB to a standby replica will not affect access to the application? A. RDS will have an internal IP which will redirect all requests to the new DB B. RDS uses DNS to switch over to stand by replica for seamless transition C. The switch over changes Hardware so RDS does not need to worry about access D. RDS will have both the DBs running independently and the user has to manually switch over Answer: B Explanation: In the event of a planned or unplanned outage of a DB instance, Amazon RDS automatically switches to a standby replica in another Availability Zone if the user has enabled Multi AZ. The automatic failover mechanism simply changes the DNS record of the DB instance to point to the standby DB instance. As a result, the user will need to re-establish any existing connections to the DB instance. However, as the DNS is the same, the application can access DB seamlessly. QUESTION NO: 104 An organization is generating digital policy files which are required by the admins for verification. Once the files are verified they may not be required in the future unless there is some compliance issue. If the organization wants to save them in a cost effective way, which is the best possible solution? A. AWS RRS B. AWS S3 C. AWS RDS D. AWS Glacier Answer: D Explanation: Amazon S3 stores objects according to their storage class. There are three major storage classes: Standard, Reduced Redundancy and Glacier. Standard is for AWS S3 and provides very high durability. However, the costs are a little higher. Reduced redundancy is for less critical files. Glacier is for archival and the files which are accessed infrequently. It is an extremely low-cost storage service that provides secure and durable storage for data archiving and backup. QUESTION NO: 105 A user has launched an EBS backed instance. The user started the instance at 9 AM in the morning. Between 9 AM to 10 AM, the user is testing some script. Thus, he stopped the instance twice and restarted it. In the same hour the user rebooted the instance once. For how many instance hours will AWS charge the user? A. 3 hours B. 4 hours C. 2 hours D. 1 hour Answer: A Explanation: A user can stop/start or reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. When the instance is rebooted AWS will not charge the user for the extra hours. In case the user stops the instance, AWS does not charge the running cost but charges only the EBS storage cost. If the user starts and stops the instance multiple times in a single hour, AWS will charge the user for every start and stop. In this case, since the instance was rebooted twice, it will cost the user for 3 instance hours. QUESTION NO: 106 An organization has configured the custom metric upload with CloudWatch. The organization has given permission to its employees to upload data using CLI as well SDK. How can the user track the calls made to CloudWatch? A. The user can enable logging with CloudWatch which logs all the activities B. Use CloudTrail to monitor the API calls C. Create an IAM user and allow each user to log the data using the S3 bucket D. Enable detailed monitoring with CloudWatch Answer: B Explanation: AWS CloudTrail is a web service which will allow the user to monitor the calls made to the Amazon CloudWatch API for the organization\u2019s account, including calls made by the AWS Management Console, Command Line Interface (CLI., and other services. When CloudTrail logging is turned on, CloudWatch will write log files into the Amazon S3 bucket, which is specified during the CloudTrail configuration. QUESTION NO: 107 A user has created a queue named \u201cmyqueue\u201d with SQS. There are four messages published to queue which are not received by the consumer yet. If the user tries to delete the queue, what will happen? A. A user can never delete a queue manually. AWS deletes it after 30 days of inactivity on queue B. It will delete the queue C. It will initiate the delete but wait for four days before deleting until all messages are deleted automatically. D. It will ask user to delete the messages first Answer: B Explanation: SQS allows the user to move data between distributed components of applications so they can perform different tasks without losing messages or requiring each component to be always available. The user can delete a queue at any time, whether it is empty or not. It is important to note that queues retain messages for a set period of time. By default, a queue retains messages for four days. QUESTION NO: 108 A user has launched a large EBS backed EC2 instance in the US-East-1a region. The user wants to achieve Disaster Recovery (DR. for that instance by creating another small instance in Europe. How can the user achieve DR? A. Copy the running instance using the \u201cInstance Copy\u201d command to the EU region B. Create AMI of the instance and copy the AMI to the EU region. Then launch the instance from the EU AMI. C. Copy the instance from the US East region to the EU region D. Use the \u201cLaunch more like this\u201d option to copy the instance from one region to another Answer: B Explanation: To launch an EC2 instance it is required to have an AMI in that region. If the AMI is not available in that region, then create a new AMI or use the copy command to copy the AMI from one region to the other region. QUESTION NO: 109 A user has created numerous EBS volumes. What is the general limit for each AWS account for the maximum number of EBS volumes that can be created? A. 10000 B. 5000 C. 100 D. 1000 Answer: B Explanation: A user can attach multiple EBS volumes to the same instance within the limits specified by his AWS account. Each AWS account has a limit on the number of Amazon EBS volumes that the user can create, and the total storage available. The default limit for the maximum number of volumes that can be created is 5000. QUESTION NO: 110 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24. and VPN only subnets CIDR (20.0.1.0/24. along with the VPN gateway (vgw-12345. to connect to the user\u2019s data centre. Which of the below mentioned options is a valid entry for the main route table in this scenario? A. Destination: 20.0.0.0/24 and Target: vgw-12345 B. Destination: 20.0.0.0/16 and Target: ALL C. Destination: 20.0.1.0/16 and Target: vgw-12345 D. Destination: 0.0.0.0/0 and Target: vgw-12345 Answer: D Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. Here are the valid entries for the main route table in this scenario: Destination: 0.0.0.0/0 & Target: vgw-12345 (To route all internet traffic to the VPN gateway. Destination: 20.0.0.0/16 & Target: local (To allow local routing in VPC. QUESTION NO: 111 A user has stored data on an encrypted EBS volume. The user wants to share the data with his friend\u2019s AWS account. How can user achieve this? A. Create an AMI from the volume and share the AMI B. Copy the data to an unencrypted volume and then share C. Take a snapshot and share the snapshot with a friend D. If both the accounts are using the same encryption key then the user can share the volume directly Answer: B Explanation: AWS EBS supports encryption of the volume. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. If the user is having data on an encrypted volume and is trying to share it with others, he has to copy the data from the encrypted volume to a new unencrypted volume. Only then can the user share it as an encrypted volume data. Otherwise the snapshot cannot be shared. QUESTION NO: 112 A user has enabled the Multi AZ feature with the MS SQL RDS database server. Which of the below mentioned statements will help the user understand the Multi AZ feature better? A. In a Multi AZ, AWS runs two DBs in parallel and copies the data asynchronously to the replica copy B. In a Multi AZ, AWS runs two DBs in parallel and copies the data synchronously to the replica copy C. In a Multi AZ, AWS runs just one DB but copies the data synchronously to the standby replica D. AWS MS SQL does not support the Multi AZ feature Answer: C Explanation: Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption.Note that the high-availability feature is not a scaling solution for read-only scenarios; you cannot use a standby replica to serve read traffic. To service read-only traffic, you should use a read replica. QUESTION NO: 113 An organization is using cost allocation tags to find the cost distribution of different departments and projects. One of the instances has two separate tags with the key/ value as \u201cInstanceName/ HR\u201d, \u201cCostCenter/HR\u201d. What will AWS do in this case? A. InstanceName is a reserved tag for AWS. Thus, AWS will not allow this tag B. AWS will not allow the tags as the value is the same for different keys C. AWS will allow tags but will not show correctly in the cost allocation report due to the same value of the two separate keys D. AWS will allow both the tags and show properly in the cost distribution report Answer: D Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources, AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. Each tag will have a key-value and can be applied to services, such as EC2, S3, RDS, EMR, etc. It is required that the key should be different for each tag. The value can be the same for different keys. In this case since the value is different, AWS will properly show the distribution report with the correct values. QUESTION NO: 114 A user is publishing custom metrics to CloudWatch. Which of the below mentioned statements will help the user understand the functionality better? A. The user can use the CloudWatch Import tool B. The user should be able to see the data in the console after around 15 minutes C. If the user is uploading the custom data, the user must supply the namespace, timezone, and metric name as part of the command D. The user can view as well as upload data using the console, CLI and APIs Answer: B Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user has to always include the namespace as a part of the request. However, the other parameters are optional. If the user has uploaded data using CLI, he can view it as a graph inside the console. The data will take around 2 minutes to upload but can be viewed only after around 15 minutes. QUESTION NO: 115 A user is launching an EC2 instance in the US East region. Which of the below mentioned options is recommended by AWS with respect to the selection of the availability zone? A. Always select the US-East-1-a zone for HA B. Do not select the AZ; instead let AWS select the AZ C. The user can never select the availability zone while launching an instance D. Always select the AZ while launching an instance Answer: B Explanation: When launching an instance with EC2, AWS recommends not to select the availability zone (AZ.. AWS specifies that the default Availability Zone should be accepted. This is because it enables AWS to select the best Availability Zone based on the system health and available capacity. If the user launches additional instances, only then an Availability Zone should be specified. This is to specify the same or different AZ from the running instances. QUESTION NO: 116 A user has created a VPC with CIDR 20.0.0.0/16 with only a private subnet and VPN connection using the VPC wizard. The user wants to connect to the instance in a private subnet over SSH. How should the user define the security rule for SSH? A. Allow Inbound traffic on port 22 from the user\u2019s network B. The user has to create an instance in EC2 Classic with an elastic IP and configure the security group of a private subnet to allow SSH from that elastic IP C. The user can connect to a instance in a private subnet using the NAT instance D. Allow Inbound traffic on port 80 and 22 to allow the user to connect to a private subnet over the Internet Answer: A Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, the user can setup a case with a VPN only subnet (private. which uses VPN access to connect with his data centre. When the user has configured this setup with Wizard, all network connections to the instances in the subnet will come from his data centre. The user has to configure the security group of the private subnet which allows the inbound traffic on SSH (port 22. from the data centre\u2019s network range. QUESTION NO: 117 A user has created an ELB with the availability zone US-East-1A. The user wants to add more zones to ELB to achieve High Availability. How can the user add more zones to the existing ELB? A. It is not possible to add more zones to the existing ELB B. The only option is to launch instances in different zones and add to ELB C. The user should stop the ELB and add zones and instances as required D. The user can add zones on the fly from the AWS console Answer: D Explanation: The user has created an Elastic Load Balancer with the availability zone and wants to add more zones to the existing ELB. The user can do so in two ways: From the console or CLI, add new zones to ELB; Launch instances in a separate AZ and add instances to the existing ELB. QUESTION NO: 118 A user has configured an Auto Scaling group with ELB. The user has enabled detailed CloudWatch monitoring on Elastic Load balancing. Which of the below mentioned statements will help the user understand this functionality better? A. ELB sends data to CloudWatch every minute only and does not charge the user B. ELB will send data every minute and will charge the user extra C. ELB is not supported by CloudWatch D. It is not possible to setup detailed monitoring for ELB Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Elastic Load Balancing includes 10 metrics and 2 dimensions, and sends data to CloudWatch every minute. This does not cost extra. QUESTION NO: 119 A user has configured ELB with two EBS backed EC2 instances. The user is trying to understand the DNS access and IP support for ELB. Which of the below mentioned statements may not help the user understand the IP mechanism supported by ELB? A. The client can connect over IPV4 or IPV6 using Dualstack B. ELB DNS supports both IPV4 and IPV6 C. Communication between the load balancer and back-end instances is always through IPV4 D. The ELB supports either IPV4 or IPV6 but not both Answer: D Explanation: Elastic Load Balancing supports both Internet Protocol version 6 (IPv6. and Internet Protocol version 4 (IPv4.. Clients can connect to the user\u2019s load balancer using either IPv4 or IPv6 (in EC2- Classic. DNS. However, communication between the load balancer and its back-end instances uses only IPv4. The user can use the Dualstack-prefixed DNS name to enable IPv6 support for communications between the client and the load balancers. Thus, the clients are able to access the load balancer using either IPv4 or IPv6 as their individual connectivity needs dictate. QUESTION NO: 120 A user has received a message from the support team that an issue occurred 1 week back between 3 AM to 4 AM and the EC2 server was not reachable. The user is checking the CloudWatch metrics of that instance. How can the user find the data easily using the CloudWatch console? A. The user can find the data by giving the exact values in the time Tab under CloudWatch metrics B. The user can find the data by filtering values of the last 1 week for a 1 hour period in the Relative tab under CloudWatch metrics C. It is not possible to find the exact time from the console. The user has to use CLI to provide the specific time D. The user can find the data by giving the exact values in the Absolute tab under CloudWatch metrics Answer: D Explanation: If the user is viewing the data inside the CloudWatch console, the console provides options to filter values either using the relative period, such as days /hours or using the Absolute tab where the user can provide data with a specific date and time. The console also provides the option to search using the local timezone under the time range caption in the console. QUESTION NO: 121 A user has setup Auto Scaling with ELB on the EC2 instances. The user wants to configure that whenever the CPU utilization is below 10%, Auto Scaling should remove one instance. How can the user configure this? A. The user can get an email using SNS when the CPU utilization is less than 10%. The user can use the desired capacity of Auto Scaling to remove the instance B. Use CloudWatch to monitor the data and Auto Scaling to remove the instances using scheduled actions C. Configure CloudWatch to send a notification to Auto Scaling Launch configuration when the CPU utilization is less than 10% and configure the Auto Scaling policy to remove the instance D. Configure CloudWatch to send a notification to the Auto Scaling group when the CPU Utilization is less than 10% and configure the Auto Scaling policy to remove the instance Answer: D Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup to receive a notification on the Auto Scaling group with the CloudWatch alarm when the CPU utilization is below a certain threshold. The user can configure the Auto Scaling policy to take action for removing the instance. When the CPU utilization is below 10% CloudWatch will send an alarm to the Auto Scaling group to execute the policy. QUESTION NO: 122 A user has enabled detailed CloudWatch metric monitoring on an Auto Scaling group. Which of the below mentioned metrics will help the user identify the total number of instances in an Auto Scaling group cluding pending, terminating and running instances? A. GroupTotalInstances B. GroupSumInstances C. It is not possible to get a count of all the three metrics together. The user has to find the individual number of running, terminating and pending instances and sum it D. GroupInstancesCount Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. For Auto Scaling, CloudWatch provides various metrics to get the group information, such as the Number of Pending, Running or Terminating instances at any moment. If the user wants to get the total number of Running, Pending and Terminating instances at any moment, he can use the GroupTotalInstances metric. QUESTION NO: 123 A user is trying to configure the CloudWatch billing alarm. Which of the below mentioned steps should be performed by the user for the first time alarm creation in the AWS Account Management section? A. Enable Receiving Billing Reports B. Enable Receiving Billing Alerts C. Enable AWS billing utility D. Enable CloudWatch Billing Threshold Answer: B Explanation: AWS CloudWatch supports enabling the billing alarm on the total AWS charges. Before the user can create an alarm on the estimated charges, he must enable monitoring of the estimated AWS charges, by selecting the option \u201cEnable receiving billing alerts\u201d. It takes about 15 minutes before the user can view the billing data. The user can then create the alarms. QUESTION NO: 124 A user is checking the CloudWatch metrics from the AWS console. The user notices that the CloudWatch data is coming in UTC. The user wants to convert the data to a local time zone. How can the user perform this? A. In the CloudWatch dashboard the user should set the local timezone so that CloudWatch shows the data only in the local time zone B. In the CloudWatch console select the local timezone under the Time Range tab to 712 view the data as per the local timezone C. The CloudWatch data is always in UTC; the user has to manually convert the data D. The user should have send the local timezone while uploading the data so that CloudWatch will show the data only in the local timezone Answer: B Explanation: If the user is viewing the data inside the CloudWatch console, the console provides options to filter values either using the relative period, such as days/hours or using the Absolute tab where the use can provide data with a specific date and time. The console also provides the option to search using the local timezone under the time range caption in the console because the time range tab allows the user to change the time zone. QUESTION NO: 125 An organization (Account ID 123412341234. has attached the below mentioned IAM policy to a user. What does this policy statement entitle the user to perform? \"Statement\": [ { \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*AccessKey*\", ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] } ] Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (Account ID 123412341234. wants some of their users to manage keys (access and secret access keys. of all IAM users, the organization should set the below mentioned policy which entitles the IAM user to modify keys of all IAM users with CLI, SDK or API. \"Statement\": [ { \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*AccessKey*\", ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] } ] QUESTION NO: 126 A user is trying to connect to a running EC2 instance using SSH. However, the user gets a connection time out error. Which of the below mentioned options is not a possible reason for rejection? A. The access key to connect to the instance is wrong B. The security group is not configured properly C. The private key used to launch the instance is not correct D. The instance CPU is heavily loaded Answer: A Explanation: If the user is trying to connect to a Linux EC2 instance and receives the connection time out error the probable reasons are: - Security group is not configured with the SSH port - The private key pair is not right - The user name to login is wrong - The instance CPU is heavily loaded, so it does not allow more connections QUESTION NO: 127 A user has configured Elastic Load Balancing by enabling a Secure Socket Layer (SSL) negotiation configuration known as a Security Policy. Which of the below mentioned options is not part of this secure policy while negotiating the SSL connection between the user and the client? A. SSL Protocols B. Client Order Preference C. SSL Ciphers D. Server Order Preference Answer: B Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the load balancer. A security policy is a combination of SSL Protocols, SSL Ciphers, and the Server order Preference option. QUESTION NO: 128 A user has configured CloudWatch monitoring on an EBS backed EC2 instance. If the user has not attached any additional device, which of the below mentioned metrics will always show a 0 value? A. DiskReadBytes 7456 B. NetworkIn C. NetworkOut D. CPUUtilization Answer: A Explanation: CloudWatch is used to monitor AWS as the well custom services. For EC2 when the user is monitoring the EC2 instances, it will capture the 7 Instance level and 3 system check parameters for the EC2 instance. Since this is an EBS backed instance, it will not have ephermal storage attached to it. Out of the 7 EC2 metrics, the 4 metrics DiskReadOps, DiskWriteOps, DiskReadBytes and DiskWriteBytes are disk related data and available only when there is ephermal storage attached to an instance. For an EBS backed instance without any additional device, this data will be 0. QUESTION NO: 129 A user has launched an EBS backed EC2 instance. What will be the difference while performing the restart or stop/start options on that instance? A. For restart it does not charge for an extra hour, while every stop/start it will be charged as a separate hour B. Every restart is charged by AWS as a separate hour, while multiple start/stop actions during a single hour will be counted as a single hour C. For every restart or start/stop it will be charged as a separate hour D. For restart it charges extra only once, while for every stop/start it will be charged as a separate hour Answer: A Explanation: For an EC2 instance launched with an EBS backed AMI, each time the instance state is changed from stop to start/ running, AWS charges a full instance hour, even if these transitions happen multiple times within a single hour. Anyway, rebooting an instance AWS does not charge a new instance billing hour. QUESTION NO: 130 A user has created a queue named \u201cmyqueue\u201d in US-East region with AWS SQS. The user\u2019s AWS account ID is 123456789012. If the user wants to perform some action on this queue, which of the below Queue URL should he use? A. http://sqs.us-east-1.amazonaws.com/123456789012/myqueue B. http://sqs.amazonaws.com/123456789012/myqueue C. http://sqs. 123456789012.us-east-1.amazonaws.com/myqueue D. http:// 123456789012.sqs. us-east-1.amazonaws.com/myqueue Answer: A Explanation: When creating a new queue in SQS, the user must provide a queue name that is unique within the scope of all queues of user\u2019s account. If the user creates queues using both the latest WSDL and a previous version, he will have a single namespace for all his queues. Amazon SQS assigns each queue created by user an identifier called a queue URL, which includes the queue name and other components that Amazon SQS determines. Whenever the user wants to perform an action on a queue, he must provide its queue URL. The queue URL for the account id 123456789012 & queue name \u201cmyqueue\u201d in US-East-1 region will be http:// sqs.us-east1.amazonaws.com/123456789012/myqueue. QUESTION NO: 131 A sys admin is trying to understand the Auto Scaling activities. Which of the below mentioned processes is not performed by Auto Scaling? A. Reboot Instance B. Schedule Actions C. Replace Unhealthy D. Availability Zone Balancing Answer: A Explanation: There are two primary types of Auto Scaling processes: Launch and Terminate, which launch or terminate instances, respectively. Some other actions performed by Auto Scaling are: AddToLoadbalancer, AlarmNotification, HealthCheck, AZRebalance, ReplaceUnHealthy, and ScheduledActions. QUESTION NO: 132 A sys admin is trying to understand EBS snapshots. Which of the below mentioned statements will not be useful to the admin to understand the concepts about a snapshot? A. The snapshot is synchronous B. It is recommended to stop the instance before taking a snapshot for consistent data C. The snapshot is incremental D. The snapshot captures the data that has been written to the hard disk when the snapshot command was executed Answer: A Explanation: The AWS snapshot is a point in time backup of an EBS volume. When the snapshot command is executed it will capture the current state of the data that is written on the drive and take a backup. For a better and consistent snapshot of the root EBS volume, AWS recommends stopping the instance. For additional volumes it is recommended to unmount the device. The snapshots are asynchronous and incremental. QUESTION NO: 133 A root account owner has created an S3 bucket testmycloud. The account owner wants to allow everyone to upload the objects as well as enforce that the person who uploaded the object should manage the permission of those objects. Which is the easiest way to achieve this? A. The root account owner should create a bucket policy which allows the IAM users to upload the object B. The root account owner should create the bucket policy which allows the other account owners to set the object policy of that bucket C. The root account should use ACL with the bucket to allow everyone to upload the object D. The root account should create the IAM users and provide them the permission to upload content to the bucket Answer: C Explanation: Each AWS S3 bucket and object has an ACL (Access Control List. associated with it. An ACL is a list of grants identifying the grantee and the permission granted. The user can use ACLs to grant basic read/write permissions to other AWS accounts. ACLs use an Amazon S3\u2013specific XML schema. The user cannot grant permissions to other users in his account. ACLs are suitable for specific scenarios. For example, if a bucket owner allows other AWS accounts to upload objects, permissions to these objects can only be managed using the object ACL by the AWS account that owns the object. QUESTION NO: 134 An organization has setup consolidated billing with 3 different AWS accounts. Which of the below mentioned advantages will organization receive in terms of the AWS pricing? A. The consolidated billing does not bring any cost advantage for the organization B. All AWS accounts will be charged for S3 storage by combining the total storage of each account C. The EC2 instances of each account will receive a total of 750*3 micro instance hours free D. The free usage tier for all the 3 accounts will be 3 years and not a single year Answer: B Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. For billing purposes, AWS treats all the accounts on the consolidated bill as one account. Some services, such as Amazon EC2 and Amazon S3 have volume pricing tiers across certain usage dimensions that give the user lower prices when he uses the service more. QUESTION NO: 135 A user has launched two EBS backed EC2 instances in the US-East-1a region. The user wants to change the zone of one of the instances. How can the user change it? A. Stop one of the instances and change the availability zone B. The zone can only be modified using the AWS CLI C. From the AWS EC2 console, select the Actions - > Change zones and specify new zone D. Create an AMI of the running instance and launch the instance in a separate AZ Answer: D Explanation: With AWS EC2, when a user is launching an instance he can select the availability zone (AZ. at the time of launch. If the zone is not selected, AWS selects it on behalf of the user. Once the instance is launched, the user cannot change the zone of that instance unless he creates an AMI of that instance and launches a new instance from it. QUESTION NO: 136 A user wants to make so that whenever the CPU utilization of the AWS EC2 instance is above 90%, the redlight of his bedroom turns on. Which of the below mentioned AWS services is helpful for this purpose? A. AWS CloudWatch + AWS SES B. AWS CloudWatch + AWS SNS C. None. It is not possible to configure the light with the AWS infrastructure services D. AWS CloudWatch and a dedicated software turning on the light Answer: B Explanation: Amazon Simple Notification Service (Amazon SNS. is a fast, flexible, and fully managed push messaging service. Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can configure some sensor devices at his home which receives data on the HTTP end point (REST calls. and turn on the red light. The user can configure the CloudWatch alarm to send a notification to the AWS SNS HTTP end point (the sensor device. and it will turn the light red when there is an alarm condition. QUESTION NO: 137 An organization has added 3 of his AWS accounts to consolidated billing. One of the AWS accounts has purchased a Reserved Instance (RI. of a small instance size in the US-East-1a zone. All other AWS accounts are running instances of a small size in the same zone. What will happen in this case for the RI pricing? A. Only the account that has purchased the RI will get the advantage of RI pricing B. One instance of a small size and running in the US-East-1a zone of each AWS account will get the benefit of RI pricing C. Any single instance from all the three accounts can get the benefit of AWS RI pricing if they are running in the same zone and are of the same size D. If there are more than one instances of a small size running across multiple accounts in the same zone no one will get the benefit of RI Answer: C Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. For billing purposes, consolidated billing treats all the accounts on the consolidated bill as one account. This means that all accounts on a consolidated bill can receive the hourly cost benefit of the Amazon EC2 Reserved Instances purchased by any other account. In this case only one Reserved Instance has been purchased by one account. Thus, only a single instance from any of the accounts will get the advantage of RI. AWS will implement the blended rate for each instance if more than one instance is running concurrently. QUESTION NO: 138 An organization is planning to use AWS for 5 different departments. The finance department is responsible to pay for all the accounts. However, they want the cost separation for each account to map with the right cost centre. How can the finance department achieve this? A. Create 5 separate accounts and make them a part of one consolidate billing B. Create 5 separate accounts and use the IAM cross account access with the roles for better management C. Create 5 separate IAM users and set a different policy for their access D. Create 5 separate IAM groups and add users as per the department\u2019s employees Answer: A Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. Consolidated billing enables the organization to see a combined view of the AWS charges incurred by each account as well as obtain a detailed cost report for each of the individual AWS accounts associated with the paying account. QUESTION NO: 139 A user has setup an EBS backed instance and a CloudWatch alarm when the CPU utilization is more than 65%. The user has setup the alarm to watch it for 5 periods of 5 minutes each. The CPU utilization is 60% between 9 AM to 6 PM. The user has stopped the EC2 instance for 15 minutes between 11 AM to 11:15 AM. What will be the status of the alarm at 11:30 AM? A. Alarm B. OK C. Insufficient Data D. Error Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The state of the alarm will be OK for the whole day. When the user stops the instance for three periods the alarm may not receive the data QUESTION NO: 140 A user is running one instance for only 3 hours every day. The user wants to save some cost with the instance. Which of the below mentioned Reserved Instance categories is advised in this case? A. The user should not use RI; instead only go with the on-demand pricing B. The user should use the AWS high utilized RI C. The user should use the AWS medium utilized RI D. The user should use the AWS low utilized RI Answer: A Explanation: The AWS Reserved Instance provides the user with an option to save some money by paying a one-time fixed amount and then save on the hourly rate. It is advisable that if the user is having 30% or more usage of an instance per day, he should go for a RI. If the user is going to use an EC2 instance for more than 2200-2500 hours per year, RI will help the user save some cost. Here, the instance is not going to run for less than 1500 hours. Thus, it is advisable that the user should use the on-demand pricing. QUESTION NO: 141 A user has setup an RDS DB with Oracle. The user wants to get notifications when someone modifies the security group of that DB. How can the user configure that? A. It is not possible to get the notifications on a change in the security group B. Configure SNS to monitor security group changes C. Configure event notification on the DB security group D. Configure the CloudWatch alarm on the DB for a change in the security group Answer: C Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. These events can be configured for source categories, such as DB instance, DB security group, DB snapshot and DB parameter group. If the user is subscribed to a Configuration Change category for a DB security group, he will be notified when the DB security group is changed. QUESTION NO: 142 A user is trying to setup a recurring Auto Scaling process. The user has setup one process to scale up every day at 8 am and scale down at 7 PM. The user is trying to setup another recurring process which scales up on the 1st of every month at 8 AM and scales down the same day at 7 PM. What will Auto Scaling do in this scenario? A. Auto Scaling will execute both processes but will add just one instance on the 1st B. Auto Scaling will add two instances on the 1st of the month C. Auto Scaling will schedule both the processes but execute only one process randomly D. Auto Scaling will throw an error since there is a conflict in the schedule of two separate Auto Scaling Processes Answer: D Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can also configure the recurring schedule action which will follow the Linux cron format. As per Auto Scaling, a scheduled action must have a unique time value. If the user attempts to schedule an activity at a time when another existing activity is already scheduled, the call will be rejected with an error message noting the conflict. QUESTION NO: 143 A user is planning to setup infrastructure on AWS for the Christmas sales. The user is planning to use Auto Scaling based on the schedule for proactive scaling. What advise would you give to the user? A. It is good to schedule now because if the user forgets later on it will not scale up B. The scaling should be setup only one week before Christmas C. Wait till end of November before scheduling the activity D. It is not advisable to use scheduled based scaling Answer: C Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. The user can specify any date in the future to scale up or down during that period. As per Auto Scaling the user can schedule an action for up to a month in the future. Thus, it is recommended to wait until end of November before scheduling for Christmas. QUESTION NO: 144 A user is trying to understand the ACL and policy for an S3 bucket. Which of the below mentioned policy permissions is equivalent to the WRITE ACL on a bucket? A. s3:GetObjectAcl B. s3:GetObjectVersion C. s3:ListBucketVersions D. s3:DeleteObject Answer: D Explanation: Amazon S3 provides a set of operations to work with the Amazon S3 resources. Each AWS S3 bucket can have an ACL (Access Control List. or bucket policy associated with it. The WRITE ACL list allows the other AWS accounts to write/modify to that bucket. The equivalent S3 bucket policy permission for it is s3:DeleteObject. QUESTION NO: 145 A user has created an ELB with Auto Scaling. Which of the below mentioned offerings from ELB helps the user to stop sending new requests traffic from the load balancer to the EC2 instance when the instance is being deregistered while continuing in-flight requests? A. ELB sticky session B. ELB deregistration check C. ELB connection draining D. ELB auto registration Off Answer: C Explanation: The Elastic Load Balancer connection draining feature causes the load balancer to stop sending new requests to the back-end instances when the instances are deregistering or become unhealthy, while ensuring that inflight requests continue to be served. QUESTION NO: 146 A user has launched an EC2 instance from an instance store backed AMI. The infrastructure team wants to create an AMI from the running instance. Which of the below mentioned steps will not be performed while creating the AMI? A. Define the AMI launch permissions B. Upload the bundled volume C. Register the AMI D. Bundle the volume Answer: A Explanation: When the user has launched an EC2 instance from an instance store backed AMI, it will need to follow certain steps, such as \u201cBundling the root volume\u201d, \u201cUploading the bundled volume\u201d and \u201cRegister the AMI\u201d. Once the AMI is created the user can setup the launch permission. However, it is not required to setup during the launch. QUESTION NO: 147 You are managing the AWS account of a big organization. The organization has more than 1000+ employees and they want to provide access to the various services to most of the employees. Which of the below mentioned options is the best possible solution in this case? A. The user should create a separate IAM user for each employee and provide access to them as per the policy B. The user should create an IAM role and attach STS with the role. The user should attach that role to the EC2 instance and setup AWS authentication on that server C. The user should create IAM groups as per the organization\u2019s departments and add each user to the group for better access control D. Attach an IAM role with the organization\u2019s authentication service to authorize each user for various AWS services Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user is managing an AWS account for an organization that already has an identity system, such as the login system for the corporate network (SSO.. In this case, instead of creating individual IAM users or groups for each user who need AWS access, it may be more practical to use a proxy server to translate the user identities from the organization network into the temporary AWS security credentials. This proxy server will attach an IAM role to the user after authentication. QUESTION NO: 148 A user has configured a VPC with a new subnet. The user has created a security group. The user wants to configure that instances of the same subnet communicate with each other. How can the user configure this with the security group? A. There is no need for a security group modification as all the instances can communicate with each other inside the same subnet B. Configure the subnet as the source in the security group and allow traffic on all the protocols and ports C. Configure the security group itself as the source and allow traffic on all the protocols and ports D. The user has to use VPC peering to configure this Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features that the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level. If the user is using the default security group it will have a rule which allows the instances to communicate with other. For a new security group the user has to specify the rule, add it to define the source as the security group itself, and select all the protocols and ports for that source. QUESTION NO: 149 A user is launching an instance. He is on the \u201cTag the instance\u201d screen. Which of the below mentioned information will not help the user understand the functionality of an AWS tag? A. Each tag will have a key and value B. The user can apply tags to the S3 bucket C. The maximum value of the tag key length is 64 unicode characters D. AWS tags are used to find the cost distribution of various resources Answer: C Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources, AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. Each tag will have a key-value and can be applied to services, such as EC2, S3, RDS, EMR, etc. The maximum size of a tag key is 128 unicode characters. QUESTION NO: 150 A user has created a VPC with CIDR 20.0.0.0/16. The user has created public and VPN only subnets along with hardware VPN access to connect to the user\u2019s datacenter. The user wants to make so that all traffic coming to the public subnet follows the organization\u2019s proxy policy. How can the user make this happen? A. Setting up a NAT with the proxy protocol and configure that the public subnet receives traffic from NAT B. Settin up a proxy policy in the internet gateway connected with the public subnet C. It is not possible to setup the proxy policy for a public subnet D. Setting the route table and security group of the public subnet which receives traffic from a virtual private gateway Answer: D Explanation: The user can create subnets within a VPC. If the user wants to connect to VPC from his own data centre, he can setup public and VPN only subnets which uses hardware VPN access to connect with his data centre. When the user has configured this setup, it will update the main route table used with the VPN-only subnet, create a custom route table and associate it with the public subnet. It also creates an internet gateway for the public subnet. By default the internet traffic of the VPN subnet is routed to a virtual private gateway while the internet traffic of the public subnet is routed through the internet gateway. The user can set up the route and security group rules. These rules enable the traffic to come from the organization\u2019s network over the virtual private gateway to the public subnet to allow proxy settings on that public subnet. QUESTION NO: 151 A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25 and a private subnet with CIDR 20.0.0.128/25. The user has launched one instance each in the private and public subnets. Which of the below mentioned options cannot be the correct IP address (private IP. assigned to an instance in the public or private subnet? A. 20.0.0.255 B. 20.0.0.132 C. 20.0.0.122 D. 20.0.0.55 Answer: A Explanation: When the user creates a subnet in VPC, he specifies the CIDR block for the subnet. In this case the user has created a VPC with the CIDR block 20.0.0.0/24, which supports 256 IP addresses (20.0.0.0 to 20.0.0.255.. The public subnet will have IP addresses between 20.0.0.0 - 20.0.0.127 and the private subnet will have IP addresses between 20.0.0.128 - 20.0.0.255. AWS reserves the first four IP addresses and the last IP address in each subnet\u2019s CIDR block. These are not available for the user to use. Thus, the instance cannot have an IP address of 20.0.0.255 QUESTION NO: 152 A user has launched an EBS backed EC2 instance. The user has rebooted the instance. Which of the below mentioned statements is not true with respect to the reboot action? A. The private and public address remains the same B. The Elastic IP remains associated with the instance C. The volume is preserved D. The instance runs on a new host computer Answer: D Explanation: A user can reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. However, it is recommended that the user use the Amazon EC2 to reboot the instance instead of running the operating system reboot command from the instance. The instance remains on the same host computer and maintains its public DNS name, private IP address, and any data on its instance store volumes. It typically takes a few minutes for the reboot to complete, but the time it takes to reboot depends on the instance configuration. QUESTION NO: 153 A user has setup a web application on EC2. The user is generating a log of the application performance at every second. There are multiple entries for each second. If the user wants to send that data to CloudWatch every minute, what should he do? A. The user should send only the data of the 60th second as CloudWatch will map the receive data timezone with the sent data timezone B. It is not possible to send the custom metric to CloudWatch every minute C. Give CloudWatch the Min, Max, Sum, and SampleCount of a number of every minute D. Calculate the average of one minute and send the data to CloudWatch Answer: C Explanation: Amazon CloudWatch aggregates statistics according to the period length that the user has specified while getting data from CloudWatch. The user can publish as many data points as he wants with the same or similartime stamps. CloudWatch aggregates them by the period length when the user calls get statistics about those data points. CloudWatch records the average (sum of all items divided by the number of items. of the values received for every 1-minute period, as well as the number of samples, maximum value, and minimum value for the same time period. CloudWatch will aggregate all the data which have time stamps within a one-minute period. QUESTION NO: 154 An AWS root account owner is trying to create a policy to access RDS. Which of the below mentioned statements is true with respect to the above information? A. Create a policy which allows the users to access RDS and apply it to the RDS instances B. The user cannot access the RDS database if he is not assigned the correct IAM policy C. The root account owner should create a policy for the IAM user and give him access to the RDS services D. The policy should be created for the user and provide access for RDS Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the account owner wants to create a policy for RDS, the owner has to create an IAM user and define the policy which entitles the IAM user with various RDS services such as Launch Instance, Manage security group, Manage parameter group etc. QUESTION NO: 155 A user is using a small MySQL RDS DB. The user is experiencing high latency due to the Multi AZ feature.Which of the below mentioned options may not help the user in this situation? A. Schedule the automated back up in non-working hours B. Use a large or higher size instance C. Use PIOPS D. Take a snapshot from standby Replica Answer: D Explanation: An RDS DB instance which has enabled Multi AZ deployments may experience increased write and commit latency compared to a Single AZ deployment, due to synchronous data replication. The user may also face changes in latency if deployment fails over to the standby replica. For production workloads, AWS recommends the user to use provisioned IOPS and DB instance classes (m1.large and larger. as they are optimized for provisioned IOPS to give a fast, and consistent performance. With Multi AZ feature, the user can not have option to take snapshot from replica. QUESTION NO: 156 A user is displaying the CPU utilization, and Network in and Network out CloudWatch metrics data of a single instance on the same graph. The graph uses one Y-axis for CPU utilization and Network in and another Y-axis for Network out. Since Network in is too high, the CPU utilization data is not visible clearly on graph to the user. How can the data be viewed better on the same graph? A. It is not possible to show multiple metrics with the different units on the same graph B. Add a third Y-axis with the console to show all the data in proportion C. Change the axis of Network by using the Switch command from the graph D. Change the units of CPU utilization so it can be shown in proportion with Network Answer: C Explanation: Amazon CloudWatch provides the functionality to graph the metric data generated either by the AWS services or the custom metric to make it easier for the user to analyse. It is possible to show the multiple metrics with different units on the same graph. If the graph is not plotted properly due to a difference in the unit data over two metrics, the user can change the Y-axis of one of the graph by selecting that graph and clicking on the Switch option. QUESTION NO: 157 A user is planning to use AWS services for his web application. If the user is trying to set up his own billing management system for AWS, how can he configure it? A. Set up programmatic billing access. Download and parse the bill as per the requirement B. It is not possible for the user to create his own billing management service with AWS C. Enable the AWS CloudWatch alarm which will provide APIs to download the alarm data D. Use AWS billing APIs to download the usage report of each service from the AWS billing console Answer: A Explanation: AWS provides an option to have programmatic access to billing. Programmatic Billing Access leverages the existing Amazon Simple Storage Service (Amazon S3. APIs. Thus, the user can build applications that reference his billing data from a CSV (comma-separated value. file stored in an Amazon S3 bucket. AWS will upload the bill to the bucket every few hours and the user can download the bill CSV from the bucket, parse itand create a billing system as per the requirement. QUESTION NO: 158 A user is planning to schedule a backup for an EBS volume. The user wants security of the snapshot data. How can the user achieve data encryption with a snapshot? A. Use encrypted EBS volumes so that the snapshot will be encrypted by AWS B. While creating a snapshot select the snapshot with encryption C. By default the snapshot is encrypted by AWS D. Enable server side encryption for the snapshot using S3 Answer: A Explanation: AWS EBS supports encryption of the volume. It also supports creating volumes from existing snapshots provided the snapshots are created from encrypted volumes. The data at rest, the I/O as well as all the snapshots of the encrypted EBS will also be encrypted. EBS encryption is based on the AES-256 cryptographic algorithm, which is the industry standard. QUESTION NO: 159 A user has created a public subnet with VPC and launched an EC2 instance within it. The user is trying to delete the subnet. What will happen in this scenario? A. It will delete the subnet and make the EC2 instance as a part of the default subnet B. It will not allow the user to delete the subnet until the instances are terminated C. It will delete the subnet as well as terminate the instances D. The subnet can never be deleted independently, but the user has to delete the VPC first Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When an instance is launched it will have a network interface attached with it. The user cannot delete the subnet until he terminates the instance and deletes the network interface. QUESTION NO: 160 A user has setup an EBS backed instance and attached 2 EBS volumes to it. The user has setup a CloudWatch alarm on each volume for the disk data. The user has stopped the EC2 instance and detached the EBS volumes. What will be the status of the alarms on the EBS volume? A. OK B. Insufficient Data C. Alarm D. The EBS cannot be detached until all the alarms are removed Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. Alarms invoke actions only for sustained state changes. There are three states of the alarm: OK, Alarm and Insufficient data. In this case since the EBS is detached and inactive the state will be Insufficient. QUESTION NO: 161 A user has launched an EC2 instance from an instance store backed AMI. The infrastructure team wants to create an AMI from the running instance. Which of the below mentioned credentials is not required while creating the AMI? A. AWS account ID B. X.509 certificate and private key C. AWS login ID to login to the console D. Access key and secret access key Answer: C Explanation: When the user has launched an EC2 instance from an instance store backed AMI and the admin team wants to create an AMI from it, the user needs to setup the AWS AMI or the API tools first. Once the tool is setup the user will need the following credentials: AWS account ID; AWS access and secret access key; X.509 certificate with private key. QUESTION NO: 162 A user has configured an SSL listener at ELB as well as on the back-end instances. Which of the below mentioned statements helps the user understand ELB traffic handling with respect to the SSL listener? A. It is not possible to have the SSL listener both at ELB and back-end instances B. ELB will modify headers to add requestor details C. ELB will intercept the request to add the cookie details if sticky session is enabled D. ELB will not modify the headers Answer: D Explanation: When the user has configured Transmission Control Protocol (TCP. or Secure Sockets Layer (SSL. for both front-end and back-end connections of the Elastic Load Balancer, the load balancer forwards the request to the back-end instances without modifying the request headers unless the proxy header is enabled. SSL does not support sticky sessions. If the user has enabled a proxy protocol it adds the source and destination IP to the header. QUESTION NO: 163 A user has created a Cloudformation stack. The stack creates AWS services, such as EC2 instances, ELB, AutoScaling, and RDS. While creating the stack it created EC2, ELB and AutoScaling but failed to create RDS. What will Cloudformation do in this scenario? A. Cloudformation can never throw an error after launching a few services since it verifies all the steps before launching. B. It will warn the user about the error and ask the user to manually create RDS C. Rollback all the changes and terminate all the created services D. It will wait for the user\u2019s input about the error and correct the mistake after the input Answer: C Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The AWS Cloudformation stack is a collection of AWS resources which are created and managed as a single unit when AWS CloudFormation instantiates a template. If any of the services fails to launch, Cloudformation will rollback all the changes and terminate or delete all the created services. QUESTION NO: 164 A user is trying to launch an EBS backed EC2 instance under free usage. The user wants to achieve encryption of the EBS volume. How can the user encrypt the data at rest? A. Use AWS EBS encryption to encrypt the data at rest B. The user cannot use EBS encryption and has to encrypt the data manually or using a third party tool C. The user has to select the encryption enabled flag while launching the EC2 instance D. Encryption of volume is not available as a part of the free usage tier Answer: B Explanation: AWS EBS supports encryption of the volume while creating new volumes. It supports encryption of the data at rest, the I/O as well as all the snapshots of the EBS volume. The EBS supports encryption for the selected instance type and the newer generation instances, such as m3, c3, cr1, r3, g2. It is not supported with a micro instance. QUESTION NO: 165 A user has created a VPC with public and private subnets using the VPC wizard. The user has not launched any instance manually and is trying to delete the VPC. What will happen in this scenario? A. It will not allow to delete the VPC as it has subnets with route tables B. It will not allow to delete the VPC since it has a running route instance C. It will terminate the VPC along with all the instances launched by the wizard D. It will not allow to delete the VPC since it has a running NAT instance Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance with an elastic IP. If the user is trying to delete the VPC it will not allow as the NAT instance is still running. QUESTION NO: 166 An organization is measuring the latency of an application every minute and storing data inside a file in the JSON format. The organization wants to send all latency data to AWS CloudWatch. How can the organization achieve this? A. The user has to parse the file before uploading data to CloudWatch B. It is not possible to upload the custom data to CloudWatch C. The user can supply the file as an input to the CloudWatch command D. The user can use the CloudWatch Import command to import data from the file to CloudWatch Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user has to always include the namespace as part of the request. If the user wants to upload the custom data from a file, he can supply file name along with the parameter -- metric-data to command put-metric-data. QUESTION NO: 167 A user has launched an EBS backed instance with EC2-Classic. The user stops and starts the instance. Which of the below mentioned statements is not true with respect to the stop/start action? A. The instance gets new private and public IP addresses B. The volume is preserved C. The Elastic IP remains associated with the instance D. The instance may run on a anew host computer Answer: C Explanation: A user can always stop/start an EBS backed EC2 instance. When the user stops the instance, it first enters the stopping state, and then the stopped state. AWS does not charge the running cost but charges only for the EBS storage cost. If the instance is running in EC2-Classic, it receives a new private IP address; as the Elastic IP address (EIP. associated with the instance is no longer associated with that instance. QUESTION NO: 168 A user has launched an RDS postgreSQL DB with AWS. The user did not specify the maintenance window during creation. The user has configured RDS to update the DB instance type from micro to large. If the user wants to have it during the maintenance window, what will AWS do? A. AWS will not allow to update the DB until the maintenance window is configured B. AWS will select the default maintenance window if the user has not provided it C. AWS will ask the user to specify the maintenance window during the update D. It is not possible to change the DB size from micro to large with RDS Answer: B Explanation: AWS RDS has a compulsory maintenance window which by default is 30 minutes. If the user does not specify the maintenance window during the creation of RDS then AWS will select a 30-minute maintenance window randomly from an 8-hour block of time per region. In this case, Amazon RDS assigns a 30-minute maintenance window on a randomly selected day of the week. QUESTION NO: 169 A user has created a subnet in VPC and launched an EC2 instance within it. The user has not selected the option to assign the IP address while launching the instance. The user has 3 elastic IPs and is trying to assign one of the Elastic IPs to the VPC instance from the console. The console does not show any instance in the IP assignment screen. What is a possible reason that the instance is unavailable in the assigned IP console? A. The IP address may be attached to one of the instances B. The IP address belongs to a different zone than the subnet zone C. The user has not created an internet gateway D. The IP addresses belong to EC2 Classic; so they cannot be assigned to VPC Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When the user is launching an instance he needs toselect an option which attaches a public IP to the instance. If the user has not selected the option to attach the public IP then it will only have a private IP when launched. If the user wants to connect to an instance from the internet he should create an elastic IP with VPC. If the elastic IP is a part of EC2 Classic it cannot be assigned to a VPC instance. QUESTION NO: 170 A user has launched multiple EC2 instances for the purpose of development and testing in the same region. The user wants to find the separate cost for the production and development instances. How can the user find the cost distribution? A. The user should download the activity report of the EC2 services as it has the instance ID wise data B. It is not possible to get the AWS cost usage data of single region instances separately C. The user should use Cost Distribution Metadata and AWS detailed billing D. The user should use Cost Allocation Tags and AWS billing reports Answer: D Explanation: AWS provides cost allocation tags to categorize and track the AWS costs. When the user applies tags to his AWS resources (such as Amazon EC2 instances or Amazon S3 buckets., AWS generates a cost allocation report as a comma-separated value (CSV file. with the usage and costs aggregated by those tags. The user can apply tags which represent business categories (such as cost centres, application names, or instance type \u2013 Production/Dev. to organize usage costs across multiple services. QUESTION NO: 171 A user has created a VPC with CIDR 20.0.0.0/16 using VPC Wizard. The user has created a public CIDR (20.0.0.0/24) and a VPN only subnet CIDR (20.0.1.0/24) along with the hardware VPN access to connect to the user\u2019s data centre. Which of the below mentioned components is not present when the VPC is setup with the wizard? A. Main route table attached with a VPN only subnet B. A NAT instance configured to allow the VPN subnet instances to connect with the internet C. Custom route table attached with a public subnet D. An internet gateway for a public subnet Answer: B Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will update the main route table used with the VPN-only subnet, create a custom route table and associate it with the public subnet. It also creates an internet gateway for the public subnet. The wizard does not create a NAT instance by default. The user can create it manually and attach it with a VPN only subnet. QUESTION NO: 172 A user has created a VPC with the public subnet. The user has created a security group for that VPC. Which of the below mentioned statements is true when a security group is created? A. It can connect to the AWS services, such as S3 and RDS by default B. It will have all the inbound traffic by default C. It will have all the outbound traffic by default D. It will by default allow traffic to the internet gateway Answer: C Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level while ACLs work at the subnet level. When a user creates a security group with AWS VPC, by default it will allow all the outbound traffic but block all inbound traffic. QUESTION NO: 173 A user has setup an Auto Scaling group. The group has failed to launch a single instance for more than 24 hours. What will happen to Auto Scaling in this condition? A. Auto Scaling will keep trying to launch the instance for 72 hours B. Auto Scaling will suspend the scaling process C. Auto Scaling will start an instance in a separate region D. The Auto Scaling group will be terminated automatically Answer: B Explanation: If Auto Scaling is trying to launch an instance and if the launching of the instance fails continuously, it will suspend the processes for the Auto Scaling groups since it repeatedly failed to launch an instance. This is known as an administrative suspension. It commonly applies to the Auto Scaling group that has no running instances which is trying to launch instances for more than 24 hours, and has not succeeded in that to do so. QUESTION NO: 174 A user is planning to set up the Multi AZ feature of RDS. Which of the below mentioned conditions won't take advantage of the Multi AZ feature? A. Availability zone outage B. A manual failover of the DB instance using Reboot with failover option C. Region outage D. When the user changes the DB instance\u2019s server type Answer: C Explanation: Amazon RDS when enabled with Multi AZ will handle failovers automatically. Thus, the user can resume database operations as quickly as possible without administrative intervention. The primary DB instance switches over automatically to the standby replica if any of the following conditions occur: An Availability Zone outage The primary DB instance fails The DB instance's server type is changed The DB instance is undergoing software patching A manual failover of the DB instance was initiated using Reboot with failover QUESTION NO: 175 An organization has configured Auto Scaling with ELB. One of the instance health check returns the status as Impaired to Auto Scaling. What will Auto Scaling do in this scenario? A. Perform a health check until cool down before declaring that the instance has failed B. Terminate the instance and launch a new instance C. Notify the user using SNS for the failed state D. Notify ELB to stop sending traffic to the impaired instance Answer: B Explanation: The Auto Scaling group determines the health state of each instance periodically by checking the results of the Amazon EC2 instance status checks. If the instance status description shows any other state other than \u201crunning\u201d or the system status description shows impaired, Auto Scaling considers the instance to be unhealthy. Thus, it terminates the instance and launches a replacement. QUESTION NO: 176 A user is using Cloudformation to launch an EC2 instance and then configure an application after the instance is launched. The user wants the stack creation of ELB and AutoScaling to wait until the EC2 instance is launched and configured properly. How can the user configure this? A. It is not possible that the stack creation will wait until one service is created and launched B. The user can use the HoldCondition resource to wait for the creation of the other dependent resources C. The user can use the DependentCondition resource to hold the creation of the other dependent resources D. The user can use the WaitCondition resource to hold the creation of the other 1034 dependent resources Answer: D Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. AWS CloudFormation provides a WaitCondition resource which acts as a barrier and blocks the creation of other resources until a completion signal is received from an external source, such as a user application or management system. QUESTION NO: 177 An organization has configured two single availability zones. The Auto Scaling groups are configured in separate zones. The user wants to merge the groups such that one group spans across multiple zones. How can the user configure this? A. Run the command as-join-auto-scaling-group to join the two groups B. Run the command as-update-auto-scaling-group to configure one group to span across zones and delete the other group C. Run the command as-copy-auto-scaling-group to join the two groups D. Run the command as-merge-auto-scaling-group to merge the groups Answer: B Explanation: If the user has configured two separate single availability zone Auto Scaling groups and wants to merge them then he should update one of the groups and delete the other one. While updating the first group it is recommended that the user should increase the size of the minimum, maximum and desired capacity as a summation of both the groups. QUESTION NO: 178 An AWS account wants to be part of the consolidated billing of his organization\u2019s payee account. How can the owner of that account achieve this? A. The payee account has to request AWS support to link the other accounts with his account B. The owner of the linked account should add the payee account to his master account list from the billing console C. The payee account will send a request to the linked account to be a part of consolidated billing D. The owner of the linked account requests the payee account to add his account to consolidated billing Answer: C Explanation: AWS consolidated billing enables the organization to consolidate payments for multiple Amazon Web Services (AWS. accounts within a single organization by making a single paying account. To add a particular account (linked. to the master (payee. account, the payee account has to request the linked account to join consolidated billing. Once the linked account accepts the request henceforth all charges incurred by the linked account will be paid by the payee account. QUESTION NO: 179 A sysadmin has created the below mentioned policy on an S3 bucket named cloudacademy. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy] }] A. It will make the cloudacademy bucket as well as all its objects as public B. It will allow everyone to view the ACL of the bucket C. It will give an error as no object is defined as part of the policy while the action defines the rule about the object D. It will make the cloudacademy bucket as public Answer: D Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if the user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. In the sample policy the action says \u201cS3:ListBucket\u201d for effect Allow on Resource arn:aws:s3:::cloudacademy. This will make the cloudacademy bucket public. \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy] }] QUESTION NO: 180 A user has launched two EBS backed EC2 instances in the US-East-1a region. The user wants to change the zone of one of the instances. How can the user change it? A. The zone can only be modified using the AWS CLI B. It is not possible to change the zone of an instance after it is launched C. Stop one of the instances and change the availability zone D. From the AWS EC2 console, select the Actions - > Change zones and specify the new zone Answer: B Explanation: With AWS EC2, when a user is launching an instance he can select the availability zone (AZ. at the time of launch. If the zone is not selected, AWS selects it on behalf of the user. Once the instance is launched, the user cannot change the zone of that instance unless he creates an AMI of that instance and launches a new instance from it. QUESTION NO: 181 An organization (account ID 123412341234. has configured the IAM policy to allow the user to modify his credentials. What will the below mentioned statement allow the user to perform? { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"iam:AddUserToGroup\", \"iam:RemoveUserFromGroup\", \"iam:GetGroup\" ], \"Resource\": \"arn:aws:iam:: 123412341234:group/TestingGroup\" }] A. The IAM policy will throw an error due to an invalid resource name B. The IAM policy will allow the user to subscribe to any IAM group C. Allow the IAM user to update the membership of the group called TestingGroup D. Allow the IAM user to delete the TestingGroup Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (account ID 123412341234. wants their users to manage their subscription to the groups, they should create a relevant policy for that. The below mentioned policy allows the respective IAM user to update the membership of the group called MarketingGroup. { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"iam:AddUserToGroup\", \"iam:RemoveUserFromGroup\", \"iam:GetGroup\" ], \"Resource\": \"arn:aws:iam:: 123412341234:group/ TestingGroup \" }] QUESTION NO: 182 A user has configured ELB with two EBS backed instances. The user has stopped the instances for 1 week to save costs. The user restarts the instances after 1 week. Which of the below mentioned statements will help the user to understand the ELB and instance registration better? A. There is no way to register the stopped instances with ELB B. The user cannot stop the instances if they are registered with ELB C. If the instances have the same Elastic IP assigned after reboot they will be registered with ELB D. The instances will automatically get registered with ELB Answer: C Explanation: Elastic Load Balancing registers the user\u2019s load balancer with his EC2 instance using the associated IP address. When the instances are stopped and started back they will have a different IP address. Thus, they will not get registered with ELB unless the user manually registers them. If the instances are assigned the same Elastic IP after reboot they will automatically get registered with ELB. QUESTION NO: 183 A user is trying to connect to a running EC2 instance using SSH. However, the user gets a Host key not found error. Which of the below mentioned options is a possible reason for rejection? A. The user has provided the wrong user name for the OS login B. The instance CPU is heavily loaded C. The security group is not configured properly D. The access key to connect to the instance is wrong Answer: A Explanation: If the user is trying to connect to a Linux EC2 instance and receives the Host Key not found error the probable reasons are: The private key pair is not right The user name to login is wrong QUESTION NO: 184 A user has hosted an application on EC2 instances. The EC2 instances are configured with ELB and Auto Scaling. The application server session time out is 2 hours. The user wants to configure connection draining to ensure that all in-flight requests are supported by ELB even though the instance is being deregistered. What time out period should the user specify for connection draining? A. 5 minutes B. 1 hour C. 30 minutes D. 2 hours Answer: B QUESTION NO: 185 A user is using the AWS EC2. The user wants to make so that when there is an issue in the EC2 server, such as instance status failed, it should start a new instance in the user\u2019s private cloud. Which AWS service helps to achieve this automation? A. AWS CloudWatch + Cloudformation B. AWS CloudWatch + AWS AutoScaling + AWS ELB C. AWS CloudWatch + AWS VPC D. AWS CloudWatch + AWS SNS Answer: D Explanation: Amazon SNS can deliver notifications by SMS text message or email to the Amazon Simple Queue Service (SQS. queues or to any HTTP endpoint. The user can configure a web service (HTTP End point. in his data centre which receives data and launches an instance in the private cloud. The user should configure the CloudWatch alarm to send a notification to SNS when the \u201cStatusCheckFailed\u201d metric is true for the EC2 instance. The SNS topic can be configured to send a notification to the user\u2019s HTTP end point which launches an instance in the private cloud. QUESTION NO: 186 A sys admin has enabled logging on ELB. Which of the below mentioned fields will not be a part of the log file name? A. Load Balancer IP B. EC2 instance IP C. S3 bucket name D. Random string Answer: B Explanation: Elastic Load Balancing access logs capture detailed information for all the requests made to the load balancer. Elastic Load Balancing publishes a log file from each load balancer node at the interval that the user has specified. The load balancer can deliver multiple logs for the same period. Elastic Load Balancing creates log file names in the following format: \u201c{Bucket}/{Prefix}/ AWSLogs/{AWS AccountID}/elasticloadbalancing/{Region}/{Year}/{Month}/{Day}/{AWS Account ID}_elasticloadbalancing_{Region}_{Load Balancer Name}_{End Time}_{Load Balancer IP}_{Random String}.log\u201c QUESTION NO: 187 A user has created a queue named \u201cawsmodule\u201d with SQS. One of the consumers of queue is down for 3 days and then becomes available. Will that component receive message from queue? A. Yes, since SQS by default stores message for 4 days B. No, since SQS by default stores message for 1 day only C. No, since SQS sends message to consumers who are available that time D. Yes, since SQS will not delete message until it is delivered to all consumers Answer: A Explanation: SQS allows the user to move data between distributed components of applications so they can perform different tasks without losing messages or requiring each component to be always available. Queues retain messages for a set period of time. By default, a queue retains messages for four days. However, the user can configure a queue to retain messages for up to 14 days after the message has been sent. QUESTION NO: 188 An organization has setup multiple IAM users. The organization wants that each IAM user accesses the IAM console only within the organization and not from outside. How can it achieve this? A. Create an IAM policy with the security group and use that security group for AWS console login B. Create an IAM policy with a condition which denies access when the IP address range is not from the organization C. Configure the EC2 instance security group which allows traffic only from the organization\u2019s IP range D. Create an IAM policy with VPC and allow a secure gateway between the organization and AWS Console Answer: B Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user can add conditions as a part of the IAM policies. The condition can be set on AWS Tags, Time, and Client IP as well as on many other parameters. If the organization wants the user to access only from a specific IP range, they should set an IAM policy condition which denies access when the IP is not in a certain range. E.g. The sample policy given below denies all traffic when the IP is not in a certain range. \"Statement\": [{ \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\", \"Condition\": { \"NotIpAddress\": { \"aws:SourceIp\": [\"10.10.10.0/24\", \"20.20.30.0/24\"] } } }] QUESTION NO: 189 An organization has created one IAM user and applied the below mentioned policy to the user. What entitlements do the IAM users avail with this policy? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\" \"Action\": [ \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:Describe*\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"autoscaling:Describe*\", \"Resource\": \"*\" } ] } A. The policy will allow the user to perform all read only activities on the EC2 services B. The policy will allow the user to list all the EC2 resources except EBS C. The policy will allow the user to perform all read and write activities on the EC2 services D. The policy will allow the user to perform all read only activities on the EC2 services except load Balancing Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If an organization wants to setup read only access to EC2 for a particular user, they should mention the action in the IAM policy which entitles the user for Describe rights for EC2, CloudWatch, Auto Scaling and ELB. In the policy shown below, the user will have read only access for EC2 and EBS, CloudWatch and Auto Scaling. Since ELB is not mentioned as a part of the list, the user will not have access to ELB. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:Describe*\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"autoscaling:Describe*\", \"Resource\": \"*\" } ] } QUESTION NO: 190 A user has enabled session stickiness with ELB. The user does not want ELB to manage the cookie; instead he wants the application to manage the cookie. What will happen when the server instance, which is bound to a cookie, crashes? A. The response will have a cookie but stickiness will be deleted B. The session will not be sticky until a new cookie is inserted C. ELB will throw an error due to cookie unavailability D. The session will be sticky and ELB will route requests to another server as ELB keeps replicating the Cookie Answer: B Explanation: With Elastic Load Balancer, if the admin has enabled a sticky session with application controlled stickiness, the load balancer uses a special cookie generated by the application to associate the session with the original server which handles the request. ELB follows the lifetime of the application-generated cookie corresponding to the cookie name specified in the ELB policy configuration. The load balancer only inserts a new stickiness cookie if the application response includes a new application cookie. The load balancer stickiness cookie does not update with each request. If the application cookie is explicitly removed or expires, the session stops being sticky until a new application cookie is issued. QUESTION NO: 191 A user is observing the EC2 CPU utilization metric on CloudWatch. The user has observed some interesting patterns while filtering over the 1 week period for a particular hour. The user wants to zoom that data point to a more granular period. How can the user do that easily with CloudWatch? A. The user can zoom a particular period by selecting that period with the mouse and then releasing the mouse B. The user can zoom a particular period by double clicking on that period with the mouse C. The user can zoom a particular period by specifying the aggregation data for that period D. The user can zoom a particular period by specifying the period in the Time Range Answer: A QUESTION NO: 192 A user has created an Auto Scaling group with default configurations from CLI. The user wants to setup the CloudWatch alarm on the EC2 instances, which are launched by the Auto Scaling group. The user has setup an alarm to monitor the CPU utilization every minute. Which of the below mentioned statements is true? A. It will fetch the data at every minute but the four data points [corresponding to 4 minutes] will not have value since the EC2 basic monitoring metrics are collected every five minutes B. It will fetch the data at every minute as detailed monitoring on EC2 will be enabled by the default launch configuration of Auto Scaling C. The alarm creation will fail since the user has not enabled detailed monitoring on the EC2 instances D . The user has to first enable detailed monitoring on the EC2 instances to support alarm monitoring at every minute Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates an Auto Scaling launch config using CLI, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. Thus, by default detailed monitoring will be enabled for Auto Scaling as well as for all the instances launched by that Auto Scaling group. QUESTION NO: 193 A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is not true in this scenario? A. The VPC will create a routing instance and attach it with a public subnet B. The VPC will create two subnets 116789 C. The VPC will create one internet gateway and attach it to VPC D. The VPC will launch one NAT instance with an elastic IP Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance with an elastic IP. Wizard will also create two subnets with route tables. It will also create an internet gateway and attach it to the VPC. QUESTION NO: 194 A user has configured ELB with a TCP listener at ELB as well as on the back-end instances. The user wants to enable a proxy protocol to capture the source and destination IP information in the header. Which of the below mentioned statements helps the user understand a proxy protocol with TCP configuration? A. If the end user is requesting behind a proxy server then the user should not enable a proxy protocol on ELB B. ELB does not support a proxy protocol when it is listening on both the load balancer and the backend instances C. Whether the end user is requesting from a proxy server or directly, it does not make a difference for the proxy protocol D. If the end user is requesting behind the proxy then the user should add the \u201cisproxy\u201d flag to the ELB Configuration Answer: A Explanation: When the user has configured Transmission Control Protocol (TCP. or Secure Sockets Layer (SSL. for both front-end and back-end connections of the Elastic Load Balancer, the load balancer forwards the request to the back-end instances without modifying the request headers unless the proxy header is enabled. If the end user is requesting from a Proxy Protocol enabled proxy server, then the ELB admin should not enable the Proxy Protocol on the load balancer. If the Proxy Protocol is enabled on both the proxy server and the load balancer, the load balancer will add another header to the request which already has a header from the proxy server. This duplication may result in errors. QUESTION NO: 195 A user has launched 5 instances in EC2-CLASSIC and attached 5 elastic IPs to the five different instances in the US East region. The user is creating a VPC in the same region. The user wants to assign an elastic IP to the VPC instance. How can the user achieve this? A. The user has to request AWS to increase the number of elastic IPs associated with the account B. AWS allows 10 EC2 Classic IPs per region ; so it will allow to allocate new Elastic IPs to the same region C. The AWS will not allow to create a new elastic IP in VPC; it will throw an error D. The user can allocate a new IP address in VPC as it has a different limit than EC2 Answer: D Explanation: Section: (none) A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. A user can have 5 IP addresses per region with EC2 Classic. The user can have 5 separate IPs with VPC in the same region as it has a separate limit than EC2 Classic. QUESTION NO: 196 A user has created a subnet in VPC and launched an EC2 instance within it. The user has not selected the option to assign the IP address while launching the instance. Which of the below mentioned statements is true with respect to this scenario? A. The instance will always have a public DNS attached to the instance by default B. The user can directly attach an elastic IP to the instance C. The instance will never launch if the public IP is not assigned D. The user would need to create an internet gateway and then attach an elastic IP to the instance to connect from internet Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When the user is launching an instance he needs to select an option which attaches a public IP to the instance. If the user has not selected the option to attach the public IP then it will only have a private IP when launched. The user cannot connect to the instance from the internet. If the user wants an elastic IP to connect to the instance from the internet he should create an internet gateway and assign an elastic IP to instance. QUESTION NO: 197 An organization has applied the below mentioned policy on an IAM group which has selected the IAM users. What entitlements do the IAM users avail with this policy? { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" } ] } A. The policy is not created correctly. It will throw an error for wrong resource name B. The policy is for the group. Thus, the IAM user cannot have any entitlement to this C. It allows full access to all AWS services for the IAM users who are a part of this group D. If this policy is applied to the EC2 resource, the users of the group will have full access to the EC2 Resources Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The IAM group allows the organization to specify permissions for a collection of users. With the below mentioned policy, it will allow the group full access (Admin. to all AWS services. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" } ] } QUESTION NO: 198 A user is configuring a CloudWatch alarm on RDS to receive a notification when the CPU utilization of RDS is higher than 50%. The user has setup an alarm when there is some iinactivity on RDS, such as RDS unavailability. How can the user configure this? A. Setup the notification when the CPU is more than 75% on RDS B. Setup the notification when the state is Insufficient Data C. Setup the notification when the CPU utilization is less than 10% D. It is not possible to setup the alarm on RDS Answer: B Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The alarm has three states: Alarm, OK and Insufficient data. The Alarm will change to Insufficient Data when any of the three situations arise: when the alarm has just started, when the metric is not available or when enough data is not available for the metric to determine the alarm state. If the user wants to find that RDS is not available, he can setup to receive the notification when the state is in Insufficient data. QUESTION NO: 199 George has shared an EC2 AMI created in the US East region from his AWS account with Stefano. George copies the same AMI to the US West region. Can Stefano access the copied AMI of George\u2019s account from the US West region? A. No, copy AMI does not copy the permission B. It is not possible to share the AMI with a specific account C. Yes, since copy AMI copies all private account sharing permissions D. Yes, since copy AMI copies all the permissions attached with the AMI Answer: A Explanation: Within EC2, when the user copies an AMI, the new AMI is fully independent of the source AMI; there is no link to the original (source. AMI. AWS does not copy launch the permissions, userdefined tags or the Amazon S3 bucket permissions from the source AMI to the new AMI. Thus, in this case by default Stefano will not have access to the AMI in the US West region. QUESTION NO: 200 A user has created a VPC with a subnet and a security group. The user has launched an instance in that subnet and attached a public IP. The user is still unable to connect to the instance. The internet gateway has also been created. What can be the reason for the error? A. The internet gateway is not configured with the route table B. The private IP is not present C. The outbound traffic on the security group is disabled D. The internet gateway is not configured with the security group Answer: A Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. AWS provides two features the user can use to increase security in VPC: security groups and network ACLs. Security groups work at the instance level. When a user launches an instance and wants to connect to an instance, he needs an internet gateway. The internet gateway should be configured with the route table to allow traffic from the internet. QUESTION NO: 201 A user is trying to setup a security policy for ELB. The user wants ELB to meet the cipher supported by the client by configuring the server order preference in ELB security policy. Which of the below mentioned preconfigured policies supports this feature? A. ELBSecurity Policy-2014-01 B. ELBSecurity Policy-2011-08 C. ELBDefault Negotiation Policy D. ELBSample- OpenSSLDefault Cipher Policy Answer: A Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. If the load balancer is configured to support the Server Order Preference, then load balancer gets to select the first cipher in its list that matches any one of the ciphers in client's list. When the user verifies the preconfigured policies supported by ELB, the policy \u201cELBSecurity Policy-2014-01\u201d supports server order preference. QUESTION NO: 202 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling AlarmNotification (which notifies Auto Scaling for CloudWatch alarms. process for a while. What will Auto Scaling do during this period? A. AWS will not receive the alarms from CloudWatch B. AWS will receive the alarms but will not execute the Auto Scaling policy C. Auto Scaling will execute the policy but it will not launch the instances until the process is resumed D. It is not possible to suspend the AlarmNotification process Answer: B Explanation: Auto Scaling performs various processes, such as Launch, Terminate Alarm Notification etc. The user can also suspend individual process. The AlarmNotification process type accepts notifications from the Amazon CloudWatch alarms that are associated with the Auto Scaling group. If the user suspends this process type, Auto Scaling will not automatically execute the scaling policies that would be triggered by the alarms. QUESTION NO: 203 George has launched three EC2 instances inside the US-East-1a zone with his AWS account. Ray has launched two EC2 instances in the US-East-1a zone with his AWS account. Which of the below entioned statements will help George and Ray understand the availability zone (AZ. concept better? A. The instances of George and Ray will be running in the same data centre B. All the instances of George and Ray can communicate over a private IP with a minimal cost C. All the instances of George and Ray can communicate over a private IP without any cost D. The US-East-1a region of George and Ray can be different availability zones Answer: D Explanation: Each AWS region has multiple, isolated locations known as Availability Zones. To ensure that the AWS resources are distributed across the Availability Zones for a region, AWS independently maps the Availability Zones to identifiers for each account. In this case the Availability Zone US-East-1a where George\u2019s EC2 instances are running might not be the same location as the US-East-1a zone of Ray\u2019s EC2 instances. There is no way for the user to coordinate the Availability Zones between accounts. QUESTION NO: 204 A user had aggregated the CloudWatch metric data on the AMI ID. The user observed some abnormal behaviour of the CPU utilization metric while viewing the last 2 weeks of data. The user wants to share that data with his manager. How can the user achieve this easily with the AWS console? A. The user can use the copy URL functionality of CloudWatch to share the exact details B. The user can use the export data option from the CloudWatch console to export the current data point C. The user has to find the period and data and provide all the aggregation information to the manager D. The user can use the CloudWatch data copy functionality to copy the current data points Answer: A Explanation: Amazon CloudWatch provides the functionality to graph the metric data generated either by the AWS services or the custom metric to make it easier for the user to analyse. The console provides the option to save the URL or bookmark it so that it can be used in the future by typing the same URL. The Copy URL functionality is available under the console when the user selects any metric to view. QUESTION NO: 205 A user has setup a CloudWatch alarm on the EC2 instance for CPU utilization. The user has setup to receive a notification on email when the CPU utilization is higher than 60%. The user is running a virus scan on the same instance at a particular time. The user wants to avoid receiving an email at this time. What should the user do? A. Remove the alarm B. Disable the alarm for a while using CLI C. Modify the CPU utilization by removing the email alert D. Disable the alarm for a while using the console Answer: B Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. When the user has setup an alarm and it is know that for some unavoidable event the status may change to Alarm, the user can disable the alarm using the DisableAlarmActions API or from the command line mon-disable-alarm-actions. QUESTION NO: 206 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned SSL protocols is not supported by the security policy? A. TLS 1.3 B. TLS 1.2 C. SSL 2.0 D. SSL 3.0 Answer: A Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and loadbalancer. Elastic Load Balancing supports the following versions of the SSL protocol: TLS 1.2 TLS 1.1 TLS 1.0 SSL 3.0 SSL 2.0 QUESTION NO: 207 A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group for the public subnet (WebSecGrp. and the private subnet(DBSecGrp). Which of the below mentioned entries is required in the private subnet database security group (DBSecGrp)? A. Allow Inbound on port 3306 for Source Web Server Security Group (WebSecGrp. B. Allow Inbound on port 3306 from source 20.0.0.0/16 C. Allow Outbound on port 3306 for Destination Web Server Security Group (WebSecGrp. D. Allow Outbound on port 80 for Destination NAT Instance IP Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the private subnet can receive inbound traffic from the public subnet on the DB port. Thus, configure port 3306 in Inbound with the source as the Web Server Security Group (WebSecGrp.. The user should configure ports 80 and 443 for Destination 0.0.0.0/0 as the route table directs traffic to the NAT instance from the private subnet. QUESTION NO: 208 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created public and VPN only subnets along with hardware VPN access to connect to the user\u2019s data centre. The user has not yet launched any instance as well as modified or deleted any setup. He wants to delete this VPC from the console. Will the console allow the user to delete the VPC? A. Yes, the console will delete all the setups and also delete the virtual private gateway B. No, the console will ask the user to manually detach the virtual private gateway first and then allow deleting the VPC C. Yes, the console will delete all the setups and detach the virtual private gateway D. No, since the NAT instance is running Answer: C Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. If the virtual private gateway is attached with VPC and the user deletes the VPC from the console it will first detach the gateway automatically and only then delete the VPC. QUESTION NO: 209 A user is trying to create a PIOPS EBS volume with 4000 IOPS and 100 GB size. AWS does not allow the user to create this volume. What is the possible root cause for this? A. The ratio between IOPS and the EBS volume is higher than 30 B. The maximum IOPS supported by EBS is 3000 C. The ratio between IOPS and the EBS volume is lower than 50 D. PIOPS is supported for EBS higher than 500 GB size Answer: A Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30; for example, a volume with 3000 IOPS must be at least 100 GB. QUESTION NO: 210 A user has setup a custom application which generates a number in decimals. The user wants to track that number and setup the alarm whenever the number is above a certain limit. The application is sending the data to CloudWatch at regular intervals for this purpose. Which of the below mentioned statements is not true with respect to the above scenario? A. The user can get the aggregate data of the numbers generated over a minute and send it to CloudWatch B. The user has to supply the timezone with each data point C. CloudWatch will not truncate the number until it has an exponent larger than 126 (i.e. (1 x 10^126) ). D. The user can create a file in the JSON format with the metric name and value and supply it to CloudWatch Answer: B QUESTION NO: 211 A user has launched an EC2 Windows instance from an instance store backed AMI. The user has also set the Instance initiated shutdown behavior to stop. What will happen when the user shuts down the OS? A. It will not allow the user to shutdown the OS when the shutdown behaviour is set to Stop B. It is not possible to set the termination behaviour to Stop for an Instance store backed AMI instance C. The instance will stay running but the OS will be shutdown D. The instance will be terminated Answer: B Explanation: When the EC2 instance is launched from an instance store backed AMI, it will not allow the user to configure the shutdown behaviour to \u201cStop\u201d. It gives a warning that the instance does not have the EBS root volume. QUESTION NO: 212 A user has enabled versioning on an S3 bucket. The user is using server side encryption for data at Rest. If the user is supplying his own keys for encryption (SSE-C., which of the below mentioned statements is true? A. The user should use the same encryption key for all versions of the same object B. It is possible to have different encryption keys for different versions of the same object C. AWS S3 does not allow the user to upload his own keys for server side encryption D. The SSE-C does not work when versioning is enabled Answer: B Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. If the bucket is versioningenabled, each object version uploaded by the user using the SSE-C feature can have its own encryption key. The user is responsible for tracking which encryption key was used for which object's version QUESTION NO: 213 A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 in this VPC. The user is trying to create another subnet with the same VPC for CIDR 20.0.0.1/24. What will happen in this scenario? A. The VPC will modify the first subnet CIDR automatically to allow the second subnet IP range B. It is not possible to create a subnet with the same CIDR as VPC C. The second subnet will be created D. It will throw a CIDR overlaps error Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. The user can create a subnet with the same size of VPC. However, he cannot create any other subnet since the CIDR of the second subnet will conflict with the first subnet. QUESTION NO: 214 A user has launched an RDS MySQL DB with the Multi AZ feature. The user has scheduled the scaling of instance storage during maintenance window. What is the correct order of events during maintenance window? Perform maintenance on standby Promote standby to primary Perform maintenance on original primary Promote original master back as primary A. 1, 2, 3, 4 B. 1, 2, 3 C. 2, 3, 1, 4 Answer: B Explanation: Running MySQL on the RDS DB instance as a Multi-AZ deployment can help the user reduce the impact of a maintenance event, as the Amazon will conduct maintenance by following the steps in the below mentioned order: Perform maintenance on standby Promote standby to primary Perform maintenance on original primary, which becomes the new standby. QUESTION NO: 215 A sys admin is using server side encryption with AWS S3. Which of the below mentioned statements helps the user understand the S3 encryption functionality? A. The server side encryption with the user supplied key works when versioning is enabled B. The user can use the AWS console, SDK and APIs to encrypt or decrypt the content for server side encryption with the user supplied key. C. The user must send an AES-128 encrypted key D. The user can upload his own encryption key to the S3 console Answer: A Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key. The encryption with the user supplied key (SSE-C. does not work with the AWS console. The S3 does not store the keys and the user has to send a key with each request. The SSE-C works when the user has enabled versioning. QUESTION NO: 216 A root account owner is trying to understand the S3 bucket ACL. Which of the below mentioned options cannot be used to grant ACL on the object using the authorized predefined group? A. Authenticated user group B. All users group C. Log Delivery Group D. Canonical user group Answer: D Explanation: An S3 bucket ACL grantee can be an AWS account or one of the predefined Amazon S3 groups. Amazon S3 has a set of predefined groups. When granting account access to a group, the user can specify one of the URLs of that group instead of a canonical user ID. AWS S3 has the following predefined groups: Authenticated Users group: It represents all AWS accounts. All Users group: Access permission to this group allows anyone to access the resource. Log Delivery group: WRITE permission on a bucket enables this group to write server access logs to the bucket. QUESTION NO: 217 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24. and VPN only subnets CIDR (20.0.1.0/24. along with the VPN gateway (vgw-12345. to connect to the user\u2019s data centre. The user\u2019s data centre has CIDR 172.28.0.0/12. The user has also setup a NAT instance (i-123456. to allow traffic to the internet from the VPN subnet. Which of the below mentioned options is not a valid entry for the main route table in this scenario? A. Destination: 20.0.1.0/24 and Target: i-12345 B. Destination: 0.0.0.0/0 and Target: i-12345 C. Destination: 172.28.0.0/12 and Target: vgw-12345 D. Destination: 20.0.0.0/16 and Target: local Answer: A Explanation: The user can create subnets as per the requirement within a VPC. If the user wants to connect VPC from his own data centre, he can setup a public and VPN only subnet which uses hardware VPN access to connect with his data centre. When the user has configured this setup with Wizard, it will create a virtual private gateway to route all traffic of the VPN subnet. If the user has setup a NAT instance to route all the internet requests then all requests to the internet should be routed to it. All requests to the organization\u2019s DC will be routed to the VPN gateway. Here are the valid entries for the main route table in this scenario: Destination: 0.0.0.0/0 & Target: i-12345 (To route all internet traffic to the NAT Instance. Destination: 172.28.0.0/12 & Target: vgw-12345 (To route all the organization\u2019s data centre traffic to the VPN gateway. Destination: 20.0.0.0/16 & Target: local (To allow local routing in VPC. QUESTION NO: 218 A user has created a VPC with public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24 . The NAT instance ID is i-a12345. Which of the below mentioned entries are required in the main route table attached with the private subnet to allow instances to connect with the internet? A. Destination: 0.0.0.0/0 and Target: i-a12345 B. Destination: 20.0.0.0/0 and Target: 80 C. Destination: 20.0.0.0/0 and Target: i-a12345 D. Destination: 20.0.0.0/24 and Target: i-a12345 Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create two route tables and attach to the subnets. The main route table will have the entry \u201cDestination: 0.0.0.0/0 and Target: ia12345\u201d, which allows all the instances in the private subnet to connect to the internet using NAT. QUESTION NO: 219 A root account owner has given full access of his S3 bucket to one of the IAM users using the bucket ACL. When the IAM user logs in to the S3 console, which actions can he perform? A. He can just view the content of the bucket B. He can do all the operations on the bucket C. It is not possible to give access to an IAM user using ACL D. The IAM user can perform all operations on the bucket using only API/SDK Answer: C Explanation: Each AWS S3 bucket and object has an ACL (Access Control List. associated with it. An ACL is a list of grants identifying the grantee and the permission granted. The user can use ACLs to grant basic read/write permissions to other AWS accounts. ACLs use an Amazon S3\u2013specific XML schema. The user cannot grant permissions to other users (IAM users. in his account. QUESTION NO: 220 An organization has configured Auto Scaling with ELB. There is a memory issue in the application which is causing CPU utilization to go above 90%. The higher CPU usage triggers an event for Auto Scaling as per the scaling policy. If the user wants to find the root cause inside the application without triggering a scaling activity, how can he achieve this? A. Stop the scaling process until research is completed B. It is not possible to find the root cause from that instance without triggering scaling C. Delete Auto Scaling until research is completed D. Suspend the scaling process until research is completed Answer: D Explanation: Auto Scaling allows the user to suspend and then resume one or more of the Auto Scaling processes in the Auto Scaling group. This is very useful when the user wants to investigate a configuration problem or some other issue, such as a memory leak with the web application and then make changes to the application, without triggering the Auto Scaling process. QUESTION NO: 221 A sys admin is planning to subscribe to the RDS event notifications. For which of the below mentioned source categories the subscription cannot be configured? A. DB security group B. DB snapshot C. DB options group D. DB parameter group Answer: C Explanation: Amazon RDS uses the Amazon Simple Notification Service (SNS. to provide a notification when an Amazon RDS event occurs. These events can be configured for source categories, such as DB instance, DB security group, DB snapshot and DB parameter group. QUESTION NO: 222 A user has launched an EC2 instance. The instance got terminated as soon as it was launched. Which of the below mentioned options is not a possible reason for this? A. The user account has reached the maximum EC2 instance limit B. The snapshot is corrupt C. The AMI is missing. It is the required part D. The user account has reached the maximum volume limit Answer: A Explanation: When the user account has reached the maximum number of EC2 instances, it will not be allowed to launch an instance. AWS will throw an \u2018InstanceLimitExceeded\u2019 error. For all other reasons, such as \u201cAMI is missing part\u201d, \u201cCorrupt Snapshot\u201d or \u201dVolume limit has reached\u201d it will launch an EC2 instance and then terminate it. QUESTION NO: 223 A user is trying to understand the detailed CloudWatch monitoring concept. Which of the below mentioned services does not provide detailed monitoring with CloudWatch? A. AWS EMR B. AWS RDS C. AWS ELB D. AWS Route53 Answer: A Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Services, such as RDS, EC2, Auto Scaling, ELB, and Route 53 can provide the monitoring data every minute. QUESTION NO: 224 A user is measuring the CPU utilization of a private data centre machine every minute. The machine provides the aggregate of data every hour, such as Sum of data\u201d, \u201cMin value\u201d, \u201cMax value, and \u201cNumber of Data points\u201d. The user wants to send these values to CloudWatch. How can the user achieve this? A. Send the data using the put-metric-data command with the aggregate-values parameter B. Send the data using the put-metric-data command with the average-values parameter C. Send the data using the put-metric-data command with the statistic-values parameter D. Send the data using the put-metric-data command with the aggregate \u2013data parameter Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish the data to CloudWatch as single data points or as an aggregated set of data points called a statistic set using the command put-metric-data. When sending the aggregate data, the user needs to send it with the parameter statistic-values: awscloudwatch put-metric-data --metric-name <Name> --namespace <Custom namespace> -- timestamp <UTC Format> --statistic-values Sum=XX,Minimum=YY,Maximum=AA,SampleCount=BB --unit Milliseconds QUESTION NO: 225 A user has enabled detailed CloudWatch monitoring with the AWS Simple Notification Service. Which of the below mentioned statements helps the user understand detailed monitoring better? A. SNS will send data every minute after configuration B. There is no need to enable since SNS provides data every minute C. AWS CloudWatch does not support monitoring for SNS D. SNS cannot provide data every minute Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. The AWS SNS service sends data every 5 minutes. Thus, it supports only the basic monitoring. The user cannot enable detailed monitoring with SNS. QUESTION NO: 226 A user has setup a VPC with CIDR 20.0.0.0/16. The VPC has a private subnet (20.0.1.0/24) and a public subnet (20.0.0.0/240). The user\u2019s data centre has CIDR of 20.0.54.0/24 and 20.1.0.0/24 If the private subnet wants to communicate with the data centre, what will happen? A. It will allow traffic communication on both the CIDRs of the data centre B. It will not allow traffic with data centre on CIDR 20.1.0.0/24 but allows traffic communication on 20.0.54.0/24 C. It will not allow traffic communication on any of the data centre CIDRs D. It will allow traffic with data centre on CIDR 20.1.0.0/24 but does not allow on 20.0.54.0/24 Answer: D Explanation: VPC allows the user to set up a connection between his VPC and corporate or home network data centre. If the user has an IP address prefix in the VPC that overlaps with one of the networks' prefixes, any traffic to the network's prefix is dropped. In this case CIDR 20.0.54.0/24 falls in the VPC\u2019s CIDR range of 20.0.0.0/16. Thus, it will not allow traffic on that IP. In the case of 20.1.0.0/24, it does not fall in the VPC\u2019s CIDR range. Thus, traffic will be allowed on it. QUESTION NO: 227 A user wants to find the particular error that occurred on a certain date in the AWS MySQL RDS DB. Which of the below mentioned activities may help the user to get the data easily? A. It is not possible to get the log files for MySQL RDS B. Find all the transaction logs and query on those records C. Direct the logs to the DB table and then query that table D. Download the log file to DynamoDB and search for the record Answer: C Explanation: The user can view, download, and watch the database logs using the Amazon RDS console, the Command Line Interface (CLI. or the Amazon RDS API. For the MySQL RDS, the user can view the error log, slow query log, and general logs. The user can also view the MySQL logs easily by directing the logs to a database table in the main database and querying that table. QUESTION NO: 228 A user is trying to send custom metrics to CloudWatch using the PutMetricData APIs. Which of the below mentioned points should the user needs to take care while sending the data to CloudWatch? A. The size of a request is limited to 8KB for HTTP GET requests and 40KB for HTTP POST requests B. The size of a request is limited to 128KB for HTTP GET requests and 64KB for HTTP POST requests C. The size of a request is limited to 40KB for HTTP GET requests and 8KB for HTTP POST requests D. The size of a request is limited to 16KB for HTTP GET requests and 80KB for HTTP POST requests Answer: A Explanation: With AWS CloudWatch, the user can publish data points for a metric that share not only the same time stamp, but also the same namespace and dimensions. CloudWatch can accept multiple data points in the same PutMetricData call with the same time stamp. The only thing that the user needs to take care of is that the size of a PutMetricData request is limited to 8KB for HTTP GET requests and 40KB for HTTP POST requests. QUESTION NO: 229 An AWS account owner has setup multiple IAM users. One IAM user only has CloudWatch access. He has setup the alarm action which stops the EC2 instances when the CPU utilization is below the threshold limit. What will happen in this case? A. It is not possible to stop the instance using the CloudWatch alarm B. CloudWatch will stop the instance when the action is executed C. The user cannot set an alarm on EC2 since he does not have the permission D. The user can setup the action but it will not be executed if the user does not have EC2 rights Answer: D Explanation: Amazon CloudWatch alarms watch a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup an action which stops the instances when their CPU utilization is below a certain threshold for a certain period of time. The EC2 action can either terminate or stop the instance as part of the EC2 action. If the IAM user has read/write permissions for Amazon CloudWatch but not for Amazon EC2, he can still create an alarm. However, the stop or terminate actions will not be performed on the Amazon EC2 instance. QUESTION NO: 230 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling terminate process only for a while. What will happen to the availability zone rebalancing process (AZRebalance. during this period? A. Auto Scaling will not launch or terminate any instances B. Auto Scaling will allow the instances to grow more than the maximum size C. Auto Scaling will keep launching instances till the maximum instance size D. It is not possible to suspend the terminate process while keeping the launch active Answer: B Explanation: Auto Scaling performs various processes, such as Launch, Terminate, Availability Zone Rebalance (AZRebalance. etc. The AZRebalance process type seeks to maintain a balanced number of instances across Availability Zones within a region. If the user suspends the Terminate process, the AZRebalance process can cause the Auto Scaling group to grow up to ten percent larger than the maximum size. This is because Auto Scaling allows groups to temporarily grow larger than the maximum size during rebalancing activities. If Auto Scaling cannot terminate instances, the Auto Scaling group could remain up to ten percent larger than the maximum size until the user resumes the Terminate process type. QUESTION NO: 231 A user has created a mobile application which makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK and root account access/secret access key to connect to DynamoDB from mobile. Which of the below mentioned statements is true with respect to the best practice for security in this scenario? A. The user should create a separate IAM user for each mobile application and provide DynamoDB access with it. B. The user should create an IAM role with DynamoDB and EC2 access. Attach the role with EC2 and route all calls from the mobile through EC2. C. The application should use an IAM role with web identity federation which validates calls to DynamoDB with identity providers, such as Google, Amazon, and Facebook. D. Create an IAM Role with DynamoDB access and attach it with the mobile application Answer: C Explanation: With AWS IAM a user is creating an application which runs on an EC2 instance and makes requests to AWS, such as DynamoDB or S3 calls. Here it is recommended that the user should not create an IAM user and pass the user's credentials to the application or embed those credentials inside the application. If the user is creating an app that runs on a mobile phone and makes requests to AWS, the user should not create an IAMuser and distribute the user's access key with the app. Instead, he should use an identity provider, such as Login with Amazon, Facebook, or Google to authenticate the users, and then use that identity to get temporary security credentials. QUESTION NO: 232 A user is configuring the Multi AZ feature of an RDS DB. The user came to know that this RDS DB does not use the AWS technology, but uses server mirroring to achieve HA. Which DB is the user using right now? A. My SQL B. Oracle C. MS SQL D. PostgreSQL Answer: C Explanation: Amazon RDS provides high availability and failover support for DB instances using Multi AZ deployments. In a Multi AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Multi AZ deployments for Oracle, PostgreSQL, and MySQL DB instances use Amazon technology, while SQL Server (MS SQL. DB instances use SQL Server Mirroring. QUESTION NO: 233 A user is receiving a notification from the RDS DB whenever there is a change in the DB security group. The user does not want to receive these notifications for only a month. Thus, he does not want to delete the notification. How can the user configure this? A. Change the Disable button for notification to \u201cYes\u201d in the RDS console B. Set the send mail flag to false in the DB event notification console C. The only option is to delete the notification from the console D. Change the Enable button for notification to \u201cNo\u201d in the RDS console Answer: D Explanation: Amazon RDS uses the Amazon Simple Notification Service to provide a notification when an Amazon RDS event occurs. Event notifications are sent to the addresses that the user has provided while creating the subscription. The user can easily turn off the notification without deleting a subscription by setting the Enabled radio button to No in the Amazon RDS console or by setting the Enabled parameter to false using the CLI or Amazon RDS API. QUESTION NO: 234 A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 by mistake. The user is trying to create another subnet of CIDR 20.0.0.1/24. How can the user create the second subnet? A. There is no need to update the subnet as VPC automatically adjusts the CIDR of the first subnet based on the second subnet\u2019s CIDR B. The user can modify the first subnet CIDR from the console C. It is not possible to create a second subnet as one subnet with the same CIDR as the VPC has been created D. The user can modify the first subnet CIDR with AWS CLI Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside the subnet. The user can create a subnet with the same size of VPC. However, he cannot create any other subnet since the CIDR of the second subnet will conflict with the first subnet. The user cannot modify the CIDR of a subnet once it is created. Thus, in this case if required, the user has to delete the subnet and create new subnets. QUESTION NO: 235 A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group for the public subnet (WebSecGrp. and the private subnet (DBSecGrp.. Which of the below mentioned entries is required in the web server security group (WebSecGrp.? A. Configure Destination as DB Security group ID (DbSecGrp. for port 3306 Outbound B. 80 for Destination 0.0.0.0/0 Outbound C. Configure port 3306 for source 20.0.0.0/24 InBound D. Configure port 80 InBound for source 20.0.0.0/16 Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the public subnet can receive inbound traffic directly from the internet. Thus, the user should configure port 80 with source 0.0.0.0/0 in InBound. The user should configure that the instance in the public subnet can send traffic to the private subnet instances on the DB port. Thus, the user should configure the DB security group of the private subnet (DbSecGrp. as the destination for port 3306 in Outbound. QUESTION NO: 236 A user is trying to understand the detailed CloudWatch monitoring concept. Which of the below mentioned services provides detailed monitoring with CloudWatch without charging the user extra? A. AWS Auto Scaling B. AWS Route 53 C. AWS EMR D. AWS SNS Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. Services, such as RDS, ELB, OpsWorks, and Route 53 can provide the monitoring data every minute without charging the user. QUESTION NO: 237 A user is trying to understand the CloudWatch metrics for the AWS services. It is required that the user should first understand the namespace for the AWS services. Which of the below mentioned is not a valid namespace for the AWS services? A. AWS/StorageGateway B. AWS/CloudTrail C. AWS/ElastiCache D. AWS/SWF Answer: B Explanation: Amazon CloudWatch is basically a metrics repository. The AWS product puts metrics into this repository, and the user can retrieve the data or statistics based on those metrics. To distinguish the data for each service, the CloudWatch metric has a namespace. Namespaces are containers for metrics. All AWS services that provide the Amazon CloudWatch data use a namespace string, beginning with \"AWS/\". All the services which are supported by CloudWatch will have some namespace. CloudWatch does not monitor CloudTrail. Thus, the namespace \u201cAWS/CloudTrail\u201d is incorrect. QUESTION NO: 238 A system admin is planning to encrypt all objects being uploaded to S3 from an application. The system admin does not want to implement his own encryption algorithm; instead he is planning to use server side encryption by supplying his own key (SSE-C.. Which parameter is not required while making a call for SSE-C? A. x-amz-server-side-encryption-customer-key-AES-256 B. x-amz-server-side-encryption-customer-key C. x-amz-server-side-encryption-customer-algorithm D. x-amz-server-side-encryption-customer-key-MD5 Answer: A Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. When the user is supplying his own encryption key, the user has to send the below mentioned parameters as a part of the API calls: x-amz-server-side-encryption-customer-algorithm: Specifies the encryption algorithm x-amzserver-side-encryption-customer-key: To provide the base64-encoded encryption key x-amzserver-side-encryption-customer-key-MD5: To provide the base64-encoded 128-bit MD5 digest of the encryption key QUESTION NO: 239 A user is using the AWS SQS to decouple the services. Which of the below mentioned operations is not supported by SQS? A. SendMessageBatch B. DeleteMessageBatch C. CreateQueue D. DeleteMessageQueue Answer: D Explanation: Amazon Simple Queue Service (SQS. is a fast, reliable, scalable, and fully managed message queuing service. SQS provides a simple and cost-effective way to decouple the components of an application. The user can perform the following set of operations using the Amazon SQS: CreateQueue, ListQueues, DeleteQueue, SendMessage, SendMessageBatch, ReceiveMessage, DeleteMessage, DeleteMessageBatch, ChangeMessageVisibility, ChangeMessageVisibilityBatch, SetQueueAttributes, GetQueueAttributes, GetQueueUrl, AddPermission and RemovePermission. Operations can be performed only by the AWS account owner or an AWS account that the account owner has delegated to. QUESTION NO: 240 A user has configured Auto Scaling with 3 instances. The user had created a new AMI after updating one of the instances. If the user wants to terminate two specific instances to ensure that Auto Scaling launches an instances with the new launch configuration, which command should he run? A. as-delete-instance-in-auto-scaling-group <Instance ID> --no-decrement-desired-capacity B. as-terminate-instance-in-auto-scaling-group <Instance ID> --update-desired-capacity C. as-terminate-instance-in-auto-scaling-group <Instance ID> --decrement-desired-capacity D. as-terminate-instance-in-auto-scaling-group <Instance ID> --no-decrement-desired-capacity Answer: D Explanation: The Auto Scaling command as-terminate-instance-in-auto-scaling-group <Instance ID> will terminate the specific instance ID. The user is required to specify the parameter as \u2013no-decrement-desiredcapacity to ensure that it launches a new instance from the launch config after terminating the instance. If the user specifies the parameter --decrement-desired-capacity then Auto Scaling will terminate the instance and decrease the desired capacity by 1. QUESTION NO: 241 A user has launched an EC2 instance from an instance store backed AMI. If the user restarts the instance, what will happen to the ephermal storage data? A. All the data will be erased but the ephermal storage will stay connected B. All data will be erased and the ephermal storage is released C. It is not possible to restart an instance launched from an instance store backed AMI D. The data is preserved Answer: D Explanation: A user can reboot an EC2 instance using the AWS console, the Amazon EC2 CLI or the Amazon EC2 API. Rebooting an instance is equivalent to rebooting an operating system. However, it is recommended that the user use Amazon EC2 to reboot the instance instead of running the operating system reboot command from the instance. When an instance launched from an instance store backed AMI is rebooted all the ephermal storage data is still preserved. QUESTION NO: 242 A user has launched an EC2 instance. However, due to some reason the instance was terminated. If the user wants to find out the reason for termination, where can he find the details? A. It is not possible to find the details after the instance is terminated B. The user can get information from the AWS console, by checking the Instance description under the State transition reason label C. The user can get information from the AWS console, by checking the Instance description under the Instance Status Change reason label D. The user can get information from the AWS console, by checking the Instance description under the Instance Termination reason label Answer: D Explanation: An EC2 instance, once terminated, may be available in the AWS console for a while after termination. The user can find the details about the termination from the description tab under the label State transition reason. If the instance is still running, there will be no reason listed. If the user has explicitly stopped or terminated the instance, the reason will be \u201cUser initiated shutdown\u201d. QUESTION NO: 243 A user has created a VPC with CIDR 20.0.0.0/24. The user has used all the IPs of CIDR and wants to increase the size of the VPC. The user has two subnets: public (20.0.0.0/28. and private (20.0.1.0/28). How can the user change the size of the VPC? A. The user can delete all the instances of the subnet. Change the size of the subnets to 20.0.0.0/32 and 20.0.1.0/32, respectively. Then the user can increase the size of the VPC using CLI B. It is not possible to change the size of the VPC once it has been created C. The user can add a subnet with a higher range so that it will automatically increase the size of the VPC. D. The user can delete the subnets first and then modify the size of the VPC Answer: B Explanation: Once the user has created a VPC, he cannot change the CIDR of that VPC. The user has to terminate all the instances, delete the subnets and then delete the VPC. Create a new VPC with a higher size and launch instances with the newly created VPC and subnets. QUESTION NO: 244 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned security policies is supported by ELB? A. Dynamic Security Policy B. All the other options C. Predefined Security Policy D. Default Security Policy Answer: C Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. ELB supports two policies: Predefined Security Policy: which comes with predefined cipher and SSL protocols; Custom Security Policy: which allows the user to configure a policy. QUESTION NO: 245 A user has granted read/write permission of his S3 bucket using ACL. Which of the below mentioned options is a valid ID to grant permission to other AWS accounts (grantee. using ACL? A. IAM User ID B. S3 Secure ID C. Access ID D. Canonical user ID Answer: D Explanation: An S3 bucket ACL grantee can be an AWS account or one of the predefined Amazon S3 groups. The user can grant permission to an AWS account by the email address of that account or by the canonical user ID. If the user provides an email in the grant request, Amazon S3 finds the canonical user ID for that account and adds it to the ACL. The resulting ACL will always contain the canonical user ID for the AWS account, and not the AWS account's email address. QUESTION NO: 246 A user has configured an ELB to distribute the traffic among multiple instances. The user instances are facing some issues due to the back-end servers. Which of the below mentioned CloudWatch metrics helps the user understand the issue with the instances? A. HTTPCode_Backend_3XX B. HTTPCode_Backend_4XX C. HTTPCode_Backend_2XX D. HTTPCode_Backend_5XX Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. For ELB, CloudWatch provides various metrics including error code by ELB as well as by back-end servers (instances.. It gives data for the count of the number of HTTP response codes generated by the back-end instances. This metric does not include any response codes generated by the load balancer. These metrics are: The 2XX class status codes represents successful actions The 3XX class status code indicates that the user agent requires action The 4XX class status code represents client errors The 5XX class status code represents back-end server errors QUESTION NO: 247 A user has launched an EC2 instance store backed instance in the US-East-1a zone. The user created AMI #1 and copied it to the Europe region. After that, the user made a few updates to the application running in the US-East-1a zone. The user makes an AMI#2 after the changes. If the user launches a new instance in Europe from the AMI #1 copy, which of the below mentioned statements is true? A. The new instance will have the changes made after the AMI copy as AWS just copies the reference of the original AMI during the copying. Thus, the copied AMI will have all the updated data. B. The new instance will have the changes made after the AMI copy since AWS keeps updating the AMI. C. It is not possible to copy the instance store backed AMI from one region to another. D. The new instance in the EU region will not have the changes made after the AMI copy. Answer: D Explanation: Within EC2, when the user copies an AMI, the new AMI is fully independent of the source AMI; there is no link to the original (source. AMI. The user can modify the source AMI without affecting the new AMI and vice a versa. Therefore, in this case even if the source AMI is modified, the copied AMI of the EU region will not have the changes. Thus, after copy the user needs to copy the new source AMI to the destination region to get those changes. QUESTION NO: 248 A user runs the command \u201cdd if=/dev/zero of=/dev/xvdfbs=1M\u201d on a fresh blank EBS volume attached to a Linux instance. Which of the below mentioned activities is the user performing with the command given above? A. Creating a file system on the EBS volume B. Mounting the device to the instance C. Pre warming the EBS volume D. Formatting the EBS volume Answer: C Explanation: When the user creates a new EBS volume and is trying to access it for the first time it will encounter reduced IOPS due to wiping or initiating of the block storage. To avoid this as well as achieve the best performance it is required to pre warm the EBS volume. For a blank volume attached with a Linux OS, the \u201cdd\u201d command is used to write to all the blocks on the device. In the command \u201cdd if=/dev/zero of=/dev/xvdfbs=1M\u201d the parameter \u201cif =import file\u201d should be set to one of the Linux virtual devices, such as /dev/zero. The \u201cof=output file\u201d parameter should be set to the drive that the user wishes to warm. The \u201cbs\u201d parameter sets the block size of the write operation; for optimal performance, this should be set to 1 MB. QUESTION NO: 249 A user has created an Auto Scaling group using CLI. The user wants to enable CloudWatch detailed monitoring for that group. How can the user configure this? A. When the user sets an alarm on the Auto Scaling group, it automatically enables detail monitoring B. By default detailed monitoring is enabled for Auto Scaling C. Auto Scaling does not support detailed monitoring D. Enable detail monitoring from the AWS console Answer: B Explanation: CloudWatch is used to monitor AWS as well as the custom services. It provides either basic or detailed monitoring for the supported AWS products. In basic monitoring, a service sends data points to CloudWatch every five minutes, while in detailed monitoring a service sends data points to CloudWatch every minute. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates an Auto Scaling launch config as the first step for creating an Auto Scaling group, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. Thus, the user does not need to set this flag if he wants detailed monitoring. QUESTION NO: 250 A user has created a VPC with a public subnet. The user has terminated all the instances which are part of the subnet. Which of the below mentioned statements is true with respect to this scenario? A. The user cannot delete the VPC since the subnet is not deleted B. All network interface attached with the instances will be deleted C. When the user launches a new instance it cannot use the same subnet D. The subnet to which the instances were launched with will be deleted Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. When an instance is launched it will have a network interface attached with it. The user cannot delete the subnet until he terminates the instance and deletes the network interface. When the user terminates the instance all the network interfaces attached with it are also deleted. QUESTION NO: 251 A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. The ELB security policy supports various ciphers. Which of the below mentioned options helps identify the matching cipher at the client side to the ELB cipher list when client is requesting ELB DNS over SSL? A. Cipher Protocol B. Client Configuration Preference C. Server Order Preference D. Load Balancer Preference Answer: C Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. When client is requesting ELB DNS over SSL and if the load balancer is configured to support the Server Order Preference, then the load balancer gets to select the first cipher in its list that matches any one of the ciphers in the client's list. Server Order Preference ensures that the load balancer determines which cipher is used for the SSL connection. QUESTION NO: 252 A user has created a VPC with public and private subnets. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.1.0/24 and the public subnet uses CIDR 20.0.0.0/24. The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group of the NAT instance. Which of the below mentioned entries is not required for the NAT security group? A. For Inbound allow Source: 20.0.1.0/24 on port 80 B. For Outbound allow Destination: 0.0.0.0/0 on port 80 C. For Inbound allow Source: 20.0.0.0/24 on port 80 D. For Outbound allow Destination: 0.0.0.0/0 on port 443 Answer: C Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet to host the web server and DB server respectively, the user should configure that the instances in the private subnet can connect to the internet using the NAT instances. The user should first configure that NAT can receive traffic on ports 80 and 443 from the private subnet. Thus, allow ports 80 and 443 in Inbound for the private subnet 20.0.1.0/24. Now to route this traffic to the internet configure ports 80 and 443 in Outbound with destination 0.0.0.0/0. The NAT should not have an entry for the public subnet CIDR. QUESTION NO: 253 A user has created an application which will be hosted on EC2. The application makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK to connect with from the EC2 instance. Which of the below mentioned statements is true with respect to the best practice for security in this scenario? A. The user should attach an IAM role with DynamoDB access to the EC2 instance B. The user should create an IAM user with DynamoDB access and use its credentials within the application to connect with DynamoDB C. The user should create an IAM role, which has EC2 access so that it will allow deploying the application D. The user should create an IAM user with DynamoDB and EC2 access. Attach the user with the application so that it does not use the root account credentials Answer: A Explanation: With AWS IAM a user is creating an application which runs on an EC2 instance and makes requests to AWS, such as DynamoDB or S3 calls. Here it is recommended that the user should not create an IAM user and pass the user's credentials to the application or embed those credentials inside the application. Instead, the user should use roles for EC2 and give that role access to DynamoDB /S3. When the roles are attached to EC2, it will give temporary security credentials to the application hosted on that EC2, to connect with DynamoDB / S3. QUESTION NO: 254 An organization (Account ID 123412341234. has attached the below mentioned IAM policy to a user. What does this policy statement entitle the user to perform? { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\", \"Action\": [ \"iam:*LoginProfile\", \"iam:*AccessKey*\", \"iam:*SigningCertificate*\" ], \"Resource\": [\"arn:aws:iam:: 123412341234:user/${aws:username}\"] }] } A. The policy allows the IAM user to modify all IAM user\u2019s credentials using the console, SDK, CLI or APIs B. The policy will give an invalid resource error C. The policy allows the IAM user to modify all credentials using only the console D. The policy allows the user to modify all IAM user\u2019s password, sign in certificates and access keys using only CLI, SDK or APIs Answer: D Explanation: WS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the organization (Account ID 123412341234. wants some of their users to manage credentials (access keys, password, and sing in certificates. of all IAM users, they should set an applicable policy to that user or group of users. The below mentioned policy allows the IAM user to modify the credentials of all IAM user\u2019s using only CLI, SDK or APIs. The user cannot use the AWS console for this activity since he does not have list permission for the IAM users. { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Sid\": \"AllowUsersAllActionsForCredentials\", \"Effect\": \"Allow\" \"Action\": [ \"iam:*LoginProfile\", \"iam:*AccessKey*\", \"iam:*SigningCertificate*\" ], \"Resource\": [\"arn:aws:iam::123412341234:user/${aws:username}\"] }] } QUESTION NO: 255 A sys admin is trying to understand the sticky session algorithm. Please select the correct sequence of steps, both when the cookie is present and when it is not, to help the admin understand the implementation of the sticky session: ELB inserts the cookie in the response ELB chooses the instance based on the load balancing algorithm Check the cookie in the service request The cookie is found in the request The cookie is not found in the request A. 3,1,4,2 [Cookie is not Present] & 3,1,5,2 [Cookie is Present] B. 3,4,1,2 [Cookie is not Present] & 3,5,1,2 [Cookie is Present] C. 3,5,2,1 [Cookie is not Present] & 3,4,2,1 [Cookie is Present] D. 3,2,5,4 [Cookie is not Present] & 3,2,4,5 [Cookie is Present] Answer: C Explanation: Generally AWS ELB routes each request to a zone with the minimum load. The Elastic Load Balancer provides a feature called sticky session which binds the user\u2019s session with a specific EC2 instance. The load balancer uses a special load-balancer-generated cookie to track the application instance for each request. When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the request is sent to the application instance specified in the cookie. If there is no cookie, the load balancer chooses an application instance based on the existing load balancing algorithm. A cookie is inserted into the response for binding subsequent requests from the same user to that application instance. QUESTION NO: 256 A user has a weighing plant. The user measures the weight of some goods every 5 minutes and sends data to AWS CloudWatch for monitoring and tracking. Which of the below mentioned parameters is mandatory for the user to include in the request list? A. Value B. Namespace C. Metric Name D. Timezone Answer: B Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish the data to CloudWatch as single data points or as an aggregated set of data points called a statistic set. The user has to always include the namespace as part of the request. The user can supply a file instead of the metric name. If the user does not supply the timezone, it accepts the current time. If the user is sending the data as a single data point it will have parameters, such as value. However, if the user is sending as an aggregate it will have parameters, such as statistic-values. QUESTION NO: 257 An organization has configured Auto Scaling for hosting their application. The system admin wants to understand the Auto Scaling health check process. If the instance is unhealthy, Auto Scaling launches an instance and terminates the unhealthy instance. What is the order execution? A. Auto Scaling launches a new instance first and then terminates the unhealthy instance B. Auto Scaling performs the launch and terminate processes in a random order C. Auto Scaling launches and terminates the instances simultaneously D. Auto Scaling terminates the instance first and then launches a new instance Answer: D Explanation: Auto Scaling keeps checking the health of the instances at regular intervals and marks the instance for replacement when it is unhealthy. The ReplaceUnhealthy process terminates instances which are marked as unhealthy and subsequently creates new instances to replace them. This process first terminates the instance and then launches a new instance. QUESTION NO: 258 A user is trying to connect to a running EC2 instance using SSH. However, the user gets an Unprotected Private Key File error. Which of the below mentioned options can be a possible reason for rejection? A. The private key file has the wrong file permission B. The ppk file used for SSH is read only C. The public key file has the wrong permission D. The user has provided the wrong user name for the OS login Answer: A Explanation: While doing SSH to an EC2 instance, if you get an Unprotected Private Key File error it means that the private key file's permissions on your computer are too open. Ideally the private key should have the Unix permission of 0400. To fix that, run the command: # chmod 0400 /path/to/private.key QUESTION NO: 259 A user has provisioned 2000 IOPS to the EBS volume. The application hosted on that EBS is experiencing less IOPS than provisioned. Which of the below mentioned options does not affect the IOPS of the volume? A. The application does not have enough IO for the volume B. The instance is EBS optimized C. The EC2 instance has 10 Gigabit Network connectivity D. The volume size is too large Answer: D Explanation: When the application does not experience the expected IOPS or throughput of the PIOPS EBS volume that was provisioned, the possible root cause could be that the EC2 bandwidth is the limiting factor and the instance might not be either EBS-optimized or might not have 10 Gigabit network connectivity. Another possible cause for not experiencing the expected IOPS could also be that the user is not driving enough I/O to the EBS volumes. The size of the volume may not affect IOPS. QUESTION NO: 260 A storage admin wants to encrypt all the objects stored in S3 using server side encryption. The user does not want to use the AES 256 encryption key provided by S3. How can the user achieve this? A. The admin should upload his secret key to the AWS console and let S3 decrypt the objects B. The admin should use CLI or API to upload the encryption key to the S3 bucket. When making a call to the S3 API mention the encryption key URL in each request C. S3 does not support client supplied encryption keys for server side encryption D. The admin should send the keys and encryption algorithm with each API call Answer: D Explanation: AWS S3 supports client side or server side encryption to encrypt all data at rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API callto supply his own encryption key. Amazon S3 never stores the user\u2019s encryption key. The user has to supply it for each encryption or decryption call. QUESTION NO: 261 A user is trying to create a PIOPS EBS volume with 8 GB size and 200 IOPS. Will AWS create the volume? A. Yes, since the ratio between EBS and IOPS is less than 30 B. No, since the PIOPS and EBS size ratio is less than 30 C. No, the EBS size is less than 10 GB D. Yes, since PIOPS is higher than 100 Answer: C Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30; for example, a volume with 3000 IOPS must be at least 100 GB. QUESTION NO: 262 A user has scheduled the maintenance window of an RDS DB on Monday at 3 AM. Which of the below mentioned events may force to take the DB instance offline during the maintenance window? A. Enabling Read Replica B. Making the DB Multi AZ C. DB password change D. Security patching Answer: D Explanation: Amazon RDS performs maintenance on the DB instance during a user-definable maintenance window. The system may be offline or experience lower performance during that window. The only maintenance events that may require RDS to make the DB instance offline are: Scaling compute operations Software patching. Required software patching is automatically scheduled only for patches that are security and durability related. Such patching occurs infrequently (typically once every few months. and seldom requires more than a fraction of the maintenance window. QUESTION NO: 263 An organization has launched 5 instances: 2 for production and 3 for testing. The organization wants that one particular group of IAM users should only access the test instances and not the production ones. How can the organization set that as a part of the policy? A. Launch the test and production instances in separate regions and allow region wise access to the group B. Define the IAM policy which allows access based on the instance ID C. Create an IAM policy with a condition which allows access to only small instances D. Define the tags on the test and production servers and add a condition to the IAM policy which allows access to specific tags Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The user can add conditions as a part of the IAM policies. The condition can be set on AWS Tags, Time, and Client IP as well as on various parameters. If the organization wants the user to access only specific instances he should define proper tags and add to the IAM policy condition. The sample policy is shown below. \"Statement\": [ { \"Action\": \"ec2:*\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/InstanceType\": \"Production\" } } } ] QUESTION NO: 264 A user has configured Auto Scaling with the minimum capacity as 2 and the desired capacity as 2. The user is trying to terminate one of the existing instance with the command: as-terminate-instance-in-auto-scaling-group<Instance ID> --decrement-desired-capacity What will Auto Scaling do in this scenario? A. Terminates the instance and does not launch a new instance B. Terminates the instance and updates the desired capacity to 1 C. Terminates the instance and updates the desired capacity and minimum size to 1 D. Throws an error Answer: D Explanation: The Auto Scaling command as-terminate-instance-in-auto-scaling-group <Instance ID> will terminate the specific instance ID. The user is required to specify the parameter as --decrement-desiredcapacity. Then Auto Scaling will terminate the instance and decrease the desired capacity by 1. In this case since the minimum size is 2, Auto Scaling will not allow the desired capacity to go below 2. Thus, it will throw an error. QUESTION NO: 265 A user is collecting 1000 records per second. The user wants to send the data to CloudWatch using the custom namespace. Which of the below mentioned options is recommended for this activity? A. Aggregate the data with statistics, such as Min, max, Average, Sum and Sample data and send the data to CloudWatch B. Send all the data values to CloudWatch in a single command by separating them with a comma. CloudWatch will parse automatically C. Create one csv file of all the data and send a single file to CloudWatch D. It is not possible to send all the data in one call. Thus, it should be sent one by one. CloudWatch will aggregate the data automatically Answer: A Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. The user can publish data to CloudWatch as single data points or as an aggregated set of data points called a statistic set using the command put-metric-data. It is recommended that when the user is having multiple data points per minute, he should aggregate the data so that it will minimize the number of calls to put-metric-data. In this case it will be single call to CloudWatch instead of 1000 calls if the data is aggregated. QUESTION NO: 266 A user is trying to create an EBS volume with the highest PIOPS supported by EBS. What is the minimum size of EBS required to have the maximum IOPS? A. 124 B. 150 C. 134 D. 128 Answer: C Explanation: A provisioned IOPS EBS volume can range in size from 10 GB to 1 TB and the user can provision up to 4000 IOPS per volume. The ratio of IOPS provisioned to the volume size requested should be a maximum of 30. QUESTION NO: 267 An organization is trying to create various IAM users. Which of the below mentioned options is not a valid IAM username? A. John.cloud B. john@cloud C. John=cloud D. john#cloud Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. Whenever the organization is creating an IAM user, there should be a unique ID for each user. The names of users, groups, roles, instance profiles must be alphanumeric, including the following common characters: plus (+., equal (=., comma (,., period (.., at (@., and dash (-.. QUESTION NO: 268 A user is having data generated randomly based on a certain event. The user wants to upload that data to CloudWatch. It may happen that event may not have data generated for some period due to andomness. Which of the below mentioned options is a recommended option for this case? A. For the period when there is no data, the user should not send the data at all B. For the period when there is no data the user should send a blank value C. For the period when there is no data the user should send the value as 0 D. The user must upload the data to CloudWatch as having no data for some period will cause an error at CloudWatch monitoring Answer: C Explanation: AWS CloudWatch supports the custom metrics. The user can always capture the custom data and upload the data to CloudWatch using CLI or APIs. When the user data is more random and not generated at regular intervals, there can be a period which has no associated data. The user can either publish the zero (0. Value for that period or not publish the data at all. It is recommended that the user should publish zero instead of no value to monitor the health of the application. This is helpful in an alarm as well as in the generation of the sample data count. QUESTION NO: 269 A user is sending the data to CloudWatch using the CloudWatch API. The user is sending data 90 minutes in the future. What will CloudWatch do in this case? A. CloudWatch will accept the data B. It is not possible to send data of the future C. It is not possible to send the data manually to CloudWatch D. The user cannot send data for more than 60 minutes in the future Answer: A Explanation: With Amazon CloudWatch, each metric data point must be marked with a time stamp. The user can send the data using CLI but the time has to be in the UTC format. If the user does not provide the time, CloudWatch will take the data received time in the UTC timezone. The time stamp sent by the user can be up to two weeks in the past and up to two hours into the future. QUESTION NO: 270 A user wants to upload a complete folder to AWS S3 using the S3 Management console. How can the user perform this activity? A. Just drag and drop the folder using the flash tool provided by S3 B. Use the Enable Enhanced Folder option from the S3 console while uploading objects C. The user cannot upload the whole folder in one go with the S3 management console D. Use the Enable Enhanced Uploader option from the S3 console while uploading objects Answer: D Explanation: AWS S3 provides a console to upload objects to a bucket. The user can use the file upload screen to upload the whole folder in one go by clicking on the Enable Enhanced Uploader option. When the user uploads afolder, Amazon S3 uploads all the files and subfolders from the specified folder to the user\u2019s bucket. It then assigns a key value that is a combination of the uploaded file name and the folder name. QUESTION NO: 271 Which of the below mentioned AWS RDS logs cannot be viewed from the console for MySQL? A. Error Log B. Slow Query Log C. Transaction Log D. General Log Answer: C Explanation: The user can view, download, and watch the database logs using the Amazon RDS console, the Command Line Interface (CLI., or the Amazon RDS API. For the MySQL RDS, the user can view the error log, slow querylog, and general logs. RDS does not support viewing the transaction logs. QUESTION NO: 272 A user has launched an EBS backed EC2 instance in the US-East-1a region. The user stopped the instance and started it back after 20 days. AWS throws up an \u2018InsufficientInstanceCapacity\u2019 error. What can be the possible reason for this? A. AWS does not have sufficient capacity in that availability zone B. AWS zone mapping is changed for that user account C. There is some issue with the host capacity on which the instance is launched D. The user account has reached the maximum EC2 instance limit Answer: A Explanation: When the user gets an \u2018InsufficientInstanceCapacity\u2019 error while launching or starting an EC2 instance, it means that AWS does not currently have enough available capacity to service the user request. If the user is requesting a large number of instances, there might not be enough server capacity to host them. The user can either try again later, by specifying a smaller number of instances or changing the availability zone if launching a fresh instance. QUESTION NO: 273 A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is true in this scenario? A. The AWS VPC will automatically create a NAT instance with the micro size B. VPC bounds the main route table with a private subnet and a custom route table with a public subnet C. The user has to manually create a NAT instance D. VPC bounds the main route table with a public subnet and a custom route table with a private subnet Answer: B Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create a NAT instance of a smaller or higher size, respectively. The VPC has an implied router and the VPC wizard updates the main route table used with the private subnet, creates a custom route table and associates it with the public subnet. QUESTION NO: 274 The CFO of a company wants to allow one of his employees to view only the AWS usage report page. Which of the below mentioned IAM policy statements allows the user to have access to the AWS usage report page? A. \"Effect\": \"Allow\", \"Action\": [\u201cDescribe\u201d], \"Resource\": \"Billing\" B. \"Effect\": \"Allow\", \"Action\": [\"AccountUsage], \"Resource\": \"*\" C. \"Effect\": \"Allow\", \"Action\": [\"aws-portal:ViewUsage\"], \"Resource\": \"*\" D. \"Effect\": \"Allow\", \"Action\": [\"aws-portal: ViewBilling\"], \"Resource\": \"*\" Answer: C Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. If the CFO wants to allow only AWS usage report page access, the policy for that IAM user will be as given below: { \"Version\": \"2012-10-17\", \"Statement\": [ 168 { \"Effect\": \"Allow\", \"Action\": [ \"aws-portal:ViewUsage\" ], \"Resource\": \"*\" } ] } QUESTION NO: 275 An organization has created 10 IAM users. The organization wants each of the IAM users to have access to a separate DyanmoDB table. All the users are added to the same group and the organization wants to setup a group level policy for this. How can the organization achieve this? A. Define the group policy and add a condition which allows the access based on the IAM name B. Create a DynamoDB table with the same name as the IAM user name and define the policy rule which grants access based on the DynamoDB ARN using a variable C. Create a separate DynamoDB database for each user and configure a policy in the group based on the DB variable. D. It is not possible to have a group level policy which allows different IAM users to different DynamoDB Tables. Answer: D Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. AWS DynamoDB has only tables and the organization cannot makeseparate databases. The organization should create a table with the same name as the IAM user name and use the ARN of DynamoDB as part of the group policy. The sample policy is shown below: { \"Version\": \"2012-10-17\", \"Statement\": [{ 169 \"Effect\": \"Allow\", \"Action\": [\"dynamodb:*\"], \"Resource\": \"arn:aws:dynamodb:region:account-number-without-hyphens:table/ ${aws:username}\" } ] } QUESTION NO: 276 A user has configured an HTTPS listener on an ELB. The user has not configured any security policy which can help to negotiate SSL between the client and ELB. What will ELB do in this scenario? A. By default ELB will select the first version of the security policy B. By default ELB will select the latest version of the policy C. ELB creation will fail without a security policy D. It is not required to have a security policy since SSL is already installed Answer: B Explanation: Elastic Load Balancing uses a Secure Socket Layer (SSL. negotiation configuration which is known as a Security Policy. It is used to negotiate the SSL connections between a client and the loadbalancer. If the user has created an HTTPS/SSL listener without associating any security policy, Elastic Load Balancing will, bydefault, associate the latest version of the ELBSecurityPolicyYYYY-MM with the load balancer. QUESTION NO: 277 A user is creating a Cloudformation stack. Which of the below mentioned limitations does not hold true for Cloudformation? A. One account by default is limited to 100 templates B. The user can use 60 parameters and 60 outputs in a single template C. The template, parameter, output, and resource description fields are limited to 4096 characters D. One account by default is limited to 20 stacks Answer: A Explanation: AWS Cloudformation is an application management tool which provides application modelling, deployment, configuration, management and related activities. The limitations given below apply to the Cloudformation template and stack. There are no limits to the number of templates but each AWS CloudFormation account is limited to a maximum of 20 stacks by default. The Template, Parameter, Output, and Resource description fields are limited to 4096 characters. The user can include up to 60 parameters and 60 outputs in a template. QUESTION NO: 278 A user has two EC2 instances running in two separate regions. The user is running an internal memory management tool, which captures the data and sends it to CloudWatch in US East, using a CLI with the same namespace and metric. Which of the below mentioned options is true with respect to the above statement? A. The setup will not work as CloudWatch cannot receive data across regions B. CloudWatch will receive and aggregate the data based on the namespace and metric C. CloudWatch will give an error since the data will conflict due to two sources D. CloudWatch will take the data of the server, which sends the data first Answer: B Explanation: Amazon CloudWatch does not differentiate the source of a metric when receiving custom data. If the user is publishing a metric with the same namespace and dimensions from different sources, CloudWatch will treat them as a single metric. If the data is coming with the same timezone within a minute, CloudWatch will aggregate the data. It treats these as a single metric, allowing the user to get the statistics, such as minimum, maximum, average, and the sum of all across all servers. QUESTION NO: 279 An organization has created a Queue named \u201cmodularqueue\u201d with SQS. The organization is not performing any operations such as SendMessage, ReceiveMessage, DeleteMessage, GetQueueAttributes, SetQueueAttributes, AddPermission, and RemovePermission on the queue. What can happen in this scenario? A. AWS SQS sends notification after 15 days for inactivity on queue B. AWS SQS can delete queue after 30 days without notification C. AWS SQS marks queue inactive after 30 days D. AWS SQS notifies the user after 2 weeks and deletes the queue after 3 weeks. Answer: B Explanation: Amazon SQS can delete a queue without notification if one of the following actions hasn't been performed on it for 30 consecutive days: SendMessage, ReceiveMessage, DeleteMessage, GetQueueAttributes, SetQueueAttributes, AddPermission, and RemovePermission. QUESTION NO: 280 An organization has setup Auto Scaling with ELB. Due to some manual error, one of the instances got rebooted. Thus, it failed the Auto Scaling health check. Auto Scaling has marked it for replacement. How can the system admin ensure that the instance does not get terminated? A. Update the Auto Scaling group to ignore the instance reboot event B. It is not possible to change the status once it is marked for replacement C. Manually add that instance to the Auto Scaling group after reboot to avoid replacement D. Change the health of the instance to healthy using the Auto Scaling commands Answer: D Explanation: After an instance has been marked unhealthy by Auto Scaling, as a result of an Amazon EC2 or ELB health check, it is almost immediately scheduled for replacement as it will never automatically recover its health. If the user knows that the instance is healthy then he can manually call the SetInstanceHealth action (or the as-setinstance- health command from CLI. to set the instance's health status back to healthy. Auto Scaling will throw an error if the instance is already terminating or else it will mark it healthy. QUESTION NO: 281 A system admin wants to add more zones to the existing ELB. The system admin wants to perform this activity from CLI. Which of the below mentioned command helps the system admin to add new zones to the existing ELB? A. elb-enable-zones-for-lb B. elb-add-zones-for-lb C. It is not possible to add more zones to the existing ELB D. elb-configure-zones-for-lb Answer: A Explanation: The user has created an Elastic Load Balancer with the availability zone and wants to add more zones to the existing ELB. The user can do so in two ways: From the console or CLI, add new zones to ELB; QUESTION NO: 282 An organization is planning to create a user with IAM. They are trying to understand the limitations of IAM so that they can plan accordingly. Which of the below mentioned statements is not true with respect to the limitations of IAM? A. One IAM user can be a part of a maximum of 5 groups B. The organization can create 100 groups per AWS account C. One AWS account can have a maximum of 5000 IAM users D. One AWS account can have 250 roles Answer: A Explanation: AWS Identity and Access Management is a web service which allows organizations to manage users and user permissions for various AWS services. The default maximums for each of the IAM entities is given below: Groups per AWS account: 100 Users per AWS account: 5000 Roles per AWS account: 250 Number of groups per user: 10 (that is, one user can be part of these many groups. QUESTION NO: 283 A user is planning to scale up an application by 8 AM and scale down by 7 PM daily using Auto Scaling. What should the user do in this case? A. Setup the scaling policy to scale up and down based on the CloudWatch alarms B. The user should increase the desired capacity at 8 AM and decrease it by 7 PM manually C. The user should setup a batch process which launches the EC2 instance at a specific time D. Setup scheduled actions to scale up or down at a specific time Answer: A Explanation: Auto Scaling based on a schedule allows the user to scale the application in response to predictable load changes. To configure the Auto Scaling group to scale based on a schedule, the user needs to create scheduled actions. A scheduled action tells Auto Scaling to perform a scaling action at a certain time in the future. QUESTION NO: 284 A user has created a VPC with two subnets: one public and one private. The user is planning to run the patch update for the instances in the private subnet. How can the instances in the private subnet connect to theinternet? A. Use the internet gateway with a private IP B. Allow outbound traffic in the security group for port 80 to allow internet updates C. The private subnet can never connect to the internet D. Use NAT with an elastic IP Answer: D Explanation: A Virtual Private Cloud (VPC. is a virtual network dedicated to the user\u2019s AWS account. A user can create a subnet with VPC and launch instances inside that subnet. If the user has created two subnets (one private and one public., he would need a Network Address Translation (NAT. instance with the elastic IP address. This enables the instances in the private subnet to send requests to the internet (for example, to perform software updates.. QUESTION NO: 285 A user has configured an EC2 instance in the US-East-1a zone. The user has enabled detailed monitoring of the instance. The user is trying to get the data from CloudWatch using a CLI. Which of the below mentioned CloudWatch endpoint URLs should the user use? A. monitoring.us-east-1.amazonaws.com B. monitoring.us-east-1-a.amazonaws.com C. monitoring.us-east-1a.amazonaws.com D. cloudwatch.us-east-1a.amazonaws.com Answer: A Explanation: The CloudWatch resources are always region specific and they will have the end point as region specific. If the user is trying to access the metric in the US-East-1 region, the endpoint URL will be: monitoring.us-east- 1.amazonaws.com QUESTION NO: 286 A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling AddToLoadBalancer (which adds instances to the load balancer. process for a while. What will happen to the instances launched during the suspension period? A. The instances will not be registered with ELB and the user has to manually register when the process is resumed B. The instances will be registered with ELB only once the process has resumed C. Auto Scaling will not launch the instance during this period due to process suspension D. It is not possible to suspend only the AddToLoadBalancer process Answer: A Explanation: Auto Scaling performs various processes, such as Launch, Terminate, add to Load Balancer etc. The user can also suspend the individual process. The AddToLoadBalancer process type adds instances to the load balancer when the instances are launched. If this process is suspended, Auto Scaling will launch the instances but will not add them to the load balancer. When the user resumes this process, Auto Scaling will resume adding new instances launched after resumption to the load balancer. However, it will not add running instances that were launched while the process was suspended; those instances must be added manually. QUESTION NO: 287 A sys admin has enabled a log on ELB. Which of the below mentioned activities are not captured by the log? A. Response processing time B. Front end processing time C. Backend processing time D. Request processing time Answer: B Explanation: Elastic Load Balancing access logs capture detailed information for all the requests made to the load balancer. Each request will have details, such as client IP, request path, ELB IP, time, and latencies. The time will have information, such as Request Processing time, Backend Processing time and Response Processing time. QUESTION NO: 288 A user has moved an object to Glacier using the life cycle rules. The user requests to restore the archive after 6 months. When the restore request is completed the user accesses that archive. Which of the below mentioned statements is not true in this condition? A. The archive will be available as an object for the duration specified by the user during the restoration request B. The restored object\u2019s storage class will be RRS C. The user can modify the restoration period only by issuing a new restore request with the updated period D. The user needs to pay storage for both RRS (restored) and Glacier (Archive) Rates. Answer: B Explanation: AWS Glacier is an archival service offered by AWS. AWS S3 provides lifecycle rules to archive and restore objects from S3 to Glacier. Once the object is archived their storage class will change to Glacier. If the user sends a request for restore, the storage class will still be Glacier for the restored object. The user will be paying for both the archived copy as well as for the restored object. The object is available only for the duration specified in the restore request and if the user wants to modify that period, he has to raise another restore request with the updated duration. QUESTION NO: 289 A user is running a batch process on EBS backed EC2 instances. The batch process starts a few instances to process hadoop Map reduce jobs which can run between 50 \u2013 600 minutes or sometimes for more time. The user wants to configure that the instance gets terminated only when the process is completed. How can the user configure this with CloudWatch? A. Setup the CloudWatch action to terminate the instance when the CPU utilization is less than 5% B. Setup the CloudWatch with Auto Scaling to terminate all the instances C. Setup a job which terminates all instances after 600 minutes D. It is not possible to terminate instances automatically Answer: D Explanation: Amazon CloudWatch alarm watches a single metric over a time period that the user specifies and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. The user can setup an action which terminates the instances when their CPU utilization is below a certain threshold for a certain period of time. The EC2 action can either terminate or stop the instance as part of the EC2 action. QUESTION NO: 290 A user has enabled versioning on an S3 bucket. The user is using server side encryption for data at rest. If the user is supplying his own keys for encryption (SSE-C., what is recommended to the user for the purpose of security? A. The user should not use his own security key as it is not secure B. Configure S3 to rotate the user\u2019s encryption key at regular intervals C. Configure S3 to store the user\u2019s keys securely with SSL D. Keep rotating the encryption key manually at the client side Answer: D Explanation: AWS S3 supports client side or server side encryption to encrypt all data at Rest. The server side encryption can either have the S3 supplied AES-256 encryption key or the user can send the key along with each API call to supply his own encryption key (SSE-C.. Since S3 does not store the encryption keys in SSE-C, it is recommended that the user should manage keys securely and keep rotating them regularly at the client side version. QUESTION NO: 291 A user runs the command \u201cdd if=/dev/xvdf of=/dev/null bs=1M\u201d on an EBS volume created from a snapshot and attached to a Linux instance. Which of the below mentioned activities is the user performing with the step given above? A. Pre warming the EBS volume B. Initiating the device to mount on the EBS volume C. Formatting the volume D. Copying the data from a snapshot to the device Answer: A Explanation: When the user creates an EBS volume and is trying to access it for the first time it will encounter reduced IOPS due to wiping or initiating of the block storage. To avoid this as well as achieve the best performance it is required to pre warm the EBS volume. For a volume created from a snapshot and attached with a Linux OS, the \u201cdd\u201d command pre warms the existing data on EBS and any restored snapshots of volumes that have been previously fully pre warmed. This command maintains incremental snapshots; however, because this operation is read-only, it does not pre warm unused space that has never been written to on the original volume. In the command \u201cdd if=/dev/xvdf of=/dev/null bs=1M\u201d , the parameter \u201cif=input file\u201d should be set to the drive that the user wishes to warm. The \u201cof=output file\u201d parameter should be set to the Linux null virtual device, /dev/null. The \u201cbs\u201d parameter sets the block size of the read operation; for optimal performance, this should be set to 1 MB. QUESTION NO: 292 A user has launched an EC2 Windows instance from an instance store backed AMI. The user wants to convert the AMI to an EBS backed AMI. How can the user convert it? A. Attach an EBS volume to the instance and unbundle all the AMI bundled data inside the EBS B. A Windows based instance store backed AMI cannot be converted to an EBS backed AMI C. It is not possible to convert an instance store backed AMI to an EBS backed AMI D. Attach an EBS volume and use the copy command to copy all the ephermal content to the EBS Volume Answer: B Explanation: Generally when a user has launched an EC2 instance from an instance store backed AMI, it can be converted to an EBS backed AMI provided the user has attached the EBS volume to the instance and unbundles the AMI data to it. However, if the instance is a Windows instance, AWS does not allow this. In this case, since the instance is a Windows instance, the user cannot convert it to an EBS backed AMI. QUESTION NO: 293 A user has created a VPC with public and private subnets using the VPC Wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24. Which of the below mentioned entries are required in the main route table to allow the instances in VPC to communicate with each other? A. Destination : 20.0.0.0/24 and Target : VPC B. Destination : 20.0.0.0/16 and Target : ALL C. Destination : 20.0.0.0/0 and Target : ALL D. Destination : 20.0.0.0/16 and Target : Local Answer: A Explanation: A user can create a subnet with VPC and launch instances inside that subnet. If the user has created a public private subnet, the instances in the public subnet can receive inbound traffic directly from the Internet, whereas the instances in the private subnet cannot. If these subnets are created with Wizard, AWS will create two route tables and attach to the subnets. The main route table will have the entry \u201cDestination: 20.0.0.0/24 and Target: Local\u201d, which allows all instances in the VPC to communicate with each other. QUESTION NO: 294 A sysadmin has created the below mentioned policy on an S3 bucket named cloudacademy. The bucket has both AWS.jpg and index.html objects. What does this policy define? \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg] }] A. It will make all the objects as well as the bucket public B. It will throw an error for the wrong action and does not allow to save the policy C. It will make the AWS.jpg object as public D. It will make the AWS.jpg as well as the cloudacademy bucket as public Answer: B Explanation: A sysadmin can grant permission to the S3 objects or the buckets to any user or make objects public using the bucket policy and user policy. Both use the JSON-based access policy language. Generally if user is defining the ACL on the bucket, the objects in the bucket do not inherit it and vice a versa. The bucket policy can be defined at the bucket level which allows the objects as well as the bucket to be public with a single policy applied to that bucket. In the below policy the action says \u201cS3:ListBucket\u201d for effect Allow and when there is no bucket name mentioned as a part of the resource, it will throw an error and not save the policy. \"Statement\": [{ \"Sid\": \"Stmt1388811069831\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\"}, \"Action\": [ \"s3:GetObjectAcl\", \"s3:ListBucket\", \"s3:GetObject\"], \"Resource\": [ \"arn:aws:s3:::cloudacademy/*.jpg] }] QUESTION NO: 295 A user has launched an EC2 instance and deployed a production application in it. The user wants to prohibit any mistakes from the production team to avoid accidental termination. How can the user achieve this? A. The user can the set DisableApiTermination attribute to avoid accidental termination B. It is not possible to avoid accidental termination C. The user can set the Deletion termination flag to avoid accidental termination D. The user can set the InstanceInitiatedShutdownBehavior flag to avoid accidental termination Answer: A Explanation: It is always possible that someone can terminate an EC2 instance using the Amazon EC2 console, command line interface or API by mistake. If the admin wants to prevent the instance from being accidentally terminated, he can enable termination protection for that instance. The DisableApiTermination attribute controls whether the instance can be terminated using the console, CLI or API. By default, termination protection is disabled for an EC2 instance. When it is set it will not allow the user to terminate the instance from CLI, API or the console. QUESTION NO: 296 A user has created a launch configuration for Auto Scaling where CloudWatch detailed monitoring is disabled. The user wants to now enable detailed monitoring. How can the user achieve this? A. Update the Launch config with CLI to set InstanceMonitoringDisabled = false B. The user should change the Auto Scaling group from the AWS console to enable detailed monitoring C. Update the Launch config with CLI to set InstanceMonitoring.Enabled = true D. Create a new Launch Config with detail monitoring enabled and update the Auto Scaling group Answer: D Explanation: CloudWatch is used to monitor AWS as well as the custom services. To enable detailed instance monitoring for a new Auto Scaling group, the user does not need to take any extra steps. When the user creates the AutoScaling launch config as the first step for creating an Auto Scaling group, each launch configuration contains a flag named InstanceMonitoring.Enabled. The default value of this flag is true. When the user has created a launch configuration with InstanceMonitoring.Enabled = false it will involve multiple steps to enable detail monitoring. The steps are: Create a new Launch config with detailed monitoring enabled Update the Auto Scaling group with a new launch config Enable detail monitoring on each EC2 instance QUESTION NO: 297 A user is trying to pre-warm a blank EBS volume attached to a Linux instance. Which of the below mentioned steps should be performed by the user? A. There is no need to pre-warm an EBS volume B. Contact AWS support to pre-warm C. Unmount the volume before pre-warming D. Format the device Answer: C Explanation: When the user creates a new EBS volume or restores a volume from the snapshot, the backend storage blocks are immediately allocated to the user EBS. However, the first time when the user is trying to access a block of the storage, it is recommended to either be wiped from the new volumes or instantiated from the snapshot (for restored volumes. before the user can access the block. This preliminary action takes time and can cause a 5 to 50 percent loss of IOPS for the volume when the block is accessed for the first time. To avoid this it is required to pre warm the volume. Prewarming an EBS volume on a Linux instance requires that the user should unmount the blank device first and then write all the blocks on the device using a command, such as \u201cdd\u201d. QUESTION NO: 298 A user has launched an EC2 instance from an instance store backed AMI. The user has attached an additional instance store volume to the instance. The user wants to create an AMI from the running instance. Will the AMI have the additional instance store volume data? A. Yes, the block device mapping will have information about the additional instance store volume B. No, since the instance store backed AMI can have only the root volume bundled C. It is not possible to attach an additional instance store volume to the existing instance store backed AMI instance D. No, since this is ephermal storage it will not be a part of the AMI Answer: A Explanation: When the user has launched an EC2 instance from an instance store backed AMI and added an instance store volume to the instance in addition to the root device volume, the block device mapping for the new AMI contains the information for these volumes as well. In addition, the block device mappings for the instances those are launched from the new AMI will automatically contain information for these volumes. QUESTION NO: 299 A user has created an EBS volume of 10 GB and attached it to a running instance. The user is trying to access EBS for first time. Which of the below mentioned options is the correct statement with respect to a first time EBS access? A. The volume will show a size of 8 GB B. The volume will show a loss of the IOPS performance the first time C. The volume will be blank D. If the EBS is mounted it will ask the user to create a file system Answer: B Explanation: A user can create an EBS volume either from a snapshot or as a blank volume. If the volume is from a snapshot it will not be blank. The volume shows the right size only as long as it is mounted. This shows that the file system is created. When the user is accessing the volume the AWS EBS will wipe out the block storage or instantiate from the snapshot. Thus, the volume will show a loss of IOPS. It is recommended that the user should pre warm the EBS before use to achieve better IO. QUESTION NO: 300 A user has enabled termination protection on an EC2 instance. The user has also set Instance initiated shutdown behaviour to terminate. When the user shuts down the instance from the OS, what will happen? A. The OS will shutdown but the instance will not be terminated due to protection B. It will terminate the instance C. It will not allow the user to shutdown the instance from the OS D. It is not possible to set the termination protection when an Instance initiated shutdown is set to Terminate Answer: B Explanation: It is always possible that someone can terminate an EC2 instance using the Amazon EC2 console, command line interface or API by mistake. If the admin wants to prevent the instance from being accidentally terminated, he can enable termination protection for that instance. The user can also setup shutdown behaviour for an EBS backed instance to guide the instance on what should be done when he initiates shutdown from the OS using Instance initiated shutdown behaviour. If the instance initiated behaviour is set to terminate and the user shuts off the OS even though termination protection is enabled, it will still terminate the instance. QUESTION NO: 301 A user has deployed an application on an EBS backed EC2 instance. For a better performance of application, it requires dedicated EC2 to EBS traffic. How can the user achieve this? A. Launch the EC2 instance as EBS dedicated with PIOPS EBS B. Launch the EC2 instance as EBS enhanced with PIOPS EBS C. Launch the EC2 instance as EBS dedicated with PIOPS EBS D. Launch the EC2 instance as EBS optimized with PIOPS EBS Answer: D Explanation: Any application which has performance sensitive workloads and requires minimal variability with dedicated EC2 to EBS traffic should use provisioned IOPS EBS volumes, which are attached to an EBS-optimized EC2 instance or it should use an instance with 10 Gigabit network connectivity. Launching an instance that is EBSoptimized provides the user with a dedicated connection between the EC2 instance and the EBS volume. QUESTION NO: 302 A user has launched a Windows based EC2 instance. However, the instance has some issues and the user wants to check the log. When the user checks the Instance console output from the AWS console, what will it display? A. All the event logs since instance boot B. The last 10 system event log error C. The Windows instance does not support the console output D. The last three system events\u2019 log errors Answer: D Explanation: The AWS EC2 console provides a useful tool called Console output for problem diagnosis. It is useful to find out any kernel issues, termination reasons or service configuration issues. For a Windows instance it lists the last three system event log errors. For Linux it displays the exact console output. You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"AWS Certified SysOps Administrator - Questions and answers"},{"location":"nightwolf-cotribution/azure_interview_questions/","text":"Microsoft Azure Administrator \uf0c1 QUESTION 1: You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Dev, you assign the Contributor role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: A Explanation: The Contributor role can manage all resources (and add resources) in a Resource Group. QUESTION 2 You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Subscription1, you assign the Logic App Operator role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: B Explanation: You would need the Logic App Contributor role. References: - https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles - https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 3 You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Dev, you assign the Logic App Contributor role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: A Explanation: The Logic App Contributor role lets you manage logic app, but not access to them. It provides access to view, edit, and update a logic app. References: - https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles - https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 4 You have an Azure subscription named Subscription1 that contains an Azure Log Analytics workspace named Workspace1. You need to view the error events from a table named Event. Which query should you run in Workspace1? A. Get-Event Event | where ($_.EventType \u2013eq \"error\") B. Get-Event Event | where ($_.EventType == \"error\") C. search in (Event) * | where EventType \u2013eq \"error\" D. search in (Event) \"error\" E. select *from Event where EventType == \"error\" F. Event | where EventType is \"error\" Correct Answer: D Explanation: To search a term in a specific table, add in (table-name) just after the search operator References: - https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/search-queries - https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-portal QUESTION 5 You have an Azure subscription named Subscription1. Subscription1 contains the resource groups in the following table. RG1 has a web app named WebApp1. WebApp1 is located in West Europe. You move WebApp1 to RG2. What is the effect of the move? A. The App Service plan for WebApp1 moves to North Europe. Policy2 applies to WebApp1. B. The App Service plan for WebApp1 remains in West Europe. Policy2 applies to WebApp1. C. The App Service plan for WebApp1 moves to North Europe. Policy1 applies to WebApp1. D. The App Service plan for WebApp1 remains in West Europe. Policy1 applies to WebApp1. Correct Answer: B Section: [none] Explanation Explanation: You can move an app to another App Service plan, as long as the source plan and the target plan are in the same resource group and geographical region. The region in which your app runs is the region of the App Service plan it's in. However, you cannot change an App Service plan's region. References: - https://docs.microsoft.com/en-us/azure/app-service/app-service-plan-manage QUESTION 6 You have an Azure subscription that contains a resource group named RG1. RG1 contains 100 virtual machines. Your company has three cost centers named Manufacturing, Sales, and Finance. You need to associate each virtual machine to a specific cost center. What should you do? A. Configure locks for the virtual machine. B. Add an extension to the virtual machines. C. Assign tags to the virtual machines. D. Modify the inventory settings of the virtual machine. Correct Answer: C Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/billing/billing-getting-started https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-using-tags QUESTION 7 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the Subscriptions blade, you select the subscription, and then click Programmatic deployment. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 8 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the Subscriptions blade, you select the subscription, and then click Resource providers. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 9 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the RG1 blade, you click Automation script. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 10 HOTSPOT You have an Azure subscription. You plan to use Azure Resource Manager templates to deploy 50 Azure virtual machines that will be part of the same availability set. You need to ensure that as many virtual machines as possible are available if the fabric fails or during servicing. How should you configure the template? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Use two fault domains. 2 or 3 is max value, depending on which region you are in. Use 20 for platformUpdateDomainCount Increasing the update domain (platformUpdateDomainCount) helps with capacity and availability planning when the platform reboots nodes. A higher number for the pool (20 is max) means that fewer of their nodes in any given availability set would be rebooted at once. References: https://www.itprotoday.com/microsoft-azure/check-if-azure-region-supports-2-or-3-fault-domains-managed- disks https://github.com/Azure/acs-engine/issues/1030 QUESTION 11 HOTSPOT You have an Azure subscription named Subscription1 that has a subscription ID of c276fc76-9cd4-44c9- 99a7-4fd71546436e. You need to create a custom RBAC role named CR1 that meets the following requirements: Can be assigned only to the resource groups in Subscription1 Prevents the management of the access permissions for the resource groups Allows the viewing, creating, modifying, and deleting of resource within the resource groups What should you specify in the assignable scopes and the permission elements of the definition of CR1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/role-based-access-control/custom-roles https://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider- operations#microsoftresources QUESTION 12 HOTSPOT You have an Azure Active Directory (Azure AD) tenant that contains three global administrators named Admin1, Admin2, and Admin3. The tenant is associated to an Azure subscription. Access control for the subscription is configured as shown in the Access control exhibit. (Click the Exhibit tab.) You sign in to the Azure portal as Admin1 and configure the tenant as shown in the Tenant exhibit. (Click the Exhibit tab.) For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 13 You have an Azure policy as shown in the following exhibit. What is the effect of the policy? A. You are prevented from creating Azure SQL Servers in ContosoRG1 only. B. You can create Azure SQL servers in ContosoRG1 only. C. You can create Azure SQL servers in any resource group within Subscription1. D. You are prevented from creating Azure SQL servers anywhere in Subscription1. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You are prevented from creating Azure SQL servers anywhere in Subscription 1 with the exception of ContosoRG1 QUESTION 14 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Subscription1, you assign the DevTest Labs User role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: DevTest Labs User role only lets you connect, start, restart, and shutdown virtual machines in your Azure DevTest Labs. The Logic App Contributor role lets you manage logic app, but not access to them. It provides access to view, edit, and update a logic app. References: https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 15 You have an Azure Active Directory (Azure AD) tenant that contains 5,000 user accounts. You create a new user account named AdminUser1. You need to assign the User administrator administrative role to AdminUser1. What should you do from the user account properties? A. From the Directory role blade, modify the directory role. B. From the Licenses blade, assign a new license. C. From the Groups blade, invite the user account to a new group. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Assign a role to a user 1. Sign in to the Azure portal with an account that's a global admin or privileged role admin for the directory. 2. Select Azure Active Directory, select Users, and then select a specific user from the list. 3. For the selected user, select Directory role, select Add role, and then pick the appropriate admin roles from the Directory roles list, such as Conditional access administrator. 4. Press Select to save. References: https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-users-assign-role- azure-portal QUESTION 16 HOTSPOT You have an Azure subscription named Subscription1. You plan to deploy an Ubuntu Server virtual machine named VM1 to Subscription1. You need to perform a custom deployment of the virtual machine. A specific trusted root certification authority (CA) must be added during the deployment. What should you do? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Cloud-init.txt Cloud-init.txt is used to customize a Linux VM on first boot up. It can be used to install packages and write files, or to configure users and security. No additional steps or agents are required to apply your configuration. Box 2: The az vm create command Once Cloud-init.txt has been created, you can deploy the VM with az vm create cmdlet, sing the --custom- data parameter to provide the full path to the cloud-init.txt file. References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/tutorial-automate-vm-deployment QUESTION 17 You have an Azure subscription named Subscription1. In Subscription1, you create an alert rule named Alert1. The Alert1 action group is configured as shown in the following exhibit. Alert1 alert criteria is triggered every minute. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: 60 One alert per minute will trigger one email per minute. Box 2: 12 No more than 1 SMS every 5 minutes can be send, which equals 12 per hour. Note: Rate limiting is a suspension of notifications that occurs when too many are sent to a particular phone number, email address or device. Rate limiting ensures that alerts are manageable and actionable. The rate limit thresholds are: SMS: No more than 1 SMS every 5 minutes. Voice: No more than 1 Voice call every 5 minutes. Email: No more than 100 emails in an hour. Other actions are not rate limited. References: https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/monitoring-and-diagnostics/monitoring- overview-alerts.md Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 You need to resolve the licensing issue before you attempt to assign the license again. What should you do? A. From the Groups blade, invite the user accounts to a new group. B. From the Profile blade, modify the usage location. C. From the Directory role blade, modify the directory role. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: License cannot be assigned to a user without a usage location specified. Scenario: Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License agreement failed for one user.\" You verify that the Azure subscription has the available licenses. QUESTION 2 You need to resolve the Active Directory issue. What should you do? A. Run the IdFix tool then use the Update actions. B. From Active Directory Domains and Trusts, modify the list of UPN suffixes. C. From Azure AD Connect, modify the outbound synchronization rule. D. From Active Directory Users and Computers, select the user accounts and then modify the UPN suffix value. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: IdFix is used to perform discovery and remediation of identity objects and their attributes in an on-premises Active Directory environment in preparation for migration to Azure Active Directory. IdFix is intended for the Active Directory administrators responsible for directory synchronization with Azure Active Directory. Scenario: Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. References: https://www.microsoft.com/en-us/download/details.aspx?id=36832 QUESTION 3 You need to define a custom domain name for Azure AD to support the planned infrastructure. Which domain name should you use? A. ad.humongousinsurance.com B. humingousinsurance.onmicrosoft.com C. humongousinsurance.com D. humongousinsurance.local Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: Every Azure AD directory comes with an initial domain name in the form of domainname.onmicrosoft.com. The initial domain name cannot be changed or deleted, but you can add your corporate domain name to Azure AD as well. For example, your organization probably has other domain names used to do business and users who sign in using your corporate domain name. Adding custom domain names to Azure AD allows you to assign user names in the directory that are familiar to your users, such as \u2018alice@contoso.com.\u2019 instead of 'alice@domain name.onmicrosoft.com'. Scenario: Network Infrastructure: Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com Planned Azure AD Infrastructure: The on-premises Active Directory domain will be synchronized to Azure AD. References: https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/add-custom-domain Question Set 1 QUESTION 1 You plan to use the Azure Import/Export service to copy files to a storage account. Which two files should you create before you prepare the drives for the import job? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. a driveset CSV file B. a JSON configuration file C. a PowerShell PS1 file D. an XML manifest file E. a dataset CSV file Correct Answer: AE Section: [none] Explanation Explanation/Reference: Explanation: A: Modify the driveset.csv file in the root folder where the tool resides. E: Modify the dataset.csv file in the root folder where the tool resides. Depending on whether you want to import a file or folder or both, add entries in the dataset.csv file References: https://docs.microsoft.com/en-us/azure/storage/common/storage-import-export-data-to-files QUESTION 2 DRAG DROP You have an on-premises file server named Server1 that runs Windows Server 2016. You have an Azure subscription that contains an Azure file share. You deploy an Azure File Sync Storage Sync Service, and you create a sync group. You need to synchronize files from Server1 to Azure. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Install the Azure File Sync agent on Server1 The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share Step 2: Register Server1. Register Windows Server with Storage Sync Service Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server (or cluster) and the Storage Sync Service. Step 3: Add a server endpoint Create a sync group and a cloud endpoint. A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud endpoint, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-sync-files-deployment-guide QUESTION 3 You create an Azure Storage account named contosostorage. You plan to create a file share named data. Users need to map a drive to the data file share from home computers that run Windows 10. Which outbound port should you open between the home computers and the data file share? A. 80 B. 443 C. 445 D. 3389 Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: Ensure port 445 is open: The SMB protocol requires TCP port 445 to be open; connections will fail if port 445 is blocked. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows QUESTION 4 HOTSPOT You have several Azure virtual machines on a virtual network named VNet1. You configure an Azure Storage account as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: always Endpoint status is enabled. Box 2: Never After you configure firewall and virtual network settings for your storage account, select Allow trusted Microsoft services to access this storage account as an exception to enable Azure Backup service to access the network restricted storage account. Reference: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows https://azure.microsoft.com/en-us/blog/azure-backup-now-supports-storage-accounts-secured-with-azure- storage-firewalls-and-virtual-networks/ QUESTION 5 HOTSPOT You have an Azure subscription named Subscription1 that contains the resources shown in the following table. The status of VM1 is Running. You assign an Azure policy as shown in the exhibit. (Click the Exhibit tab.) You assign the policy by using the following parameters: For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 6 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 2 You plan to store media files in the rg1lod9172796 storage account. You need to configure the storage account to store the media files. The solution must ensure that only users who have access keys can download the media files and that the files are accessible only over HTTPS. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: We should create an Azure file share. Step 1: In the Azure portal, select All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. On the Storage Accounts window that appears. Step 2: Locate the rg1lod9172796 storage account. Step 3: On the storage account page, in the Services section, select Files. Step 4: On the menu at the top of the File service page, click + File share. The New file share page drops down. Step 5: In Name type myshare. Click OK to create the Azure file share. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-portal QUESTION 7 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 1 You plan to migrate a large amount of corporate data to Azure Storage and to back up files stored on old hardware to Azure Storage. You need to create a storage account named corpdata9172795n1 in the corpdatalod9172795 resource group. The solution must meet the following requirements: Corpdata9172795n1 must be able to host the virtual disk files for Azure virtual machines. The cost of accessing the files must be minimized. Replication costs must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: In the Azure portal, click All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. Step 2: On the Storage Accounts window that appears, choose Add. Step 3: Select the subscription in which to create the storage account. Step 4: Under the Resource group field, select corpdatalod9172795. Step 5: Enter a name for your storage account: corpdata9172795n1 Step 6: For Account kind select: General-purpose v2 accounts (recommended for most scenarios) General-purpose v2 accounts is recommended for most scenarios. . General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. Step 7: For replication select: Read-access geo-redundant storage (RA-GRS) Read-access geo-redundant storage (RA-GRS) maximizes availability for your storage account. RA-GRS provides read-only access to the data in the secondary location, in addition to geo-replication across two regions. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview QUESTION 8 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 2 You plan to move backup files and documents from an on-premises Windows file server to Azure Storage. The backup files will be stored as blobs. You need to create a storage account named corpdata9172795n2. The solution must meet the following requirements: Ensure that the documents are accessible via drive mappings from Azure virtual machines that run Windows Server 2016. Provide the highest possible redundancy for the documents. Minimize storage access costs. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: In the Azure portal, click All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. Step 2: On the Storage Accounts window that appears, choose Add. Step 3: Select the subscription in which to create the storage account. Step 4: Under the Resource group field, select Create New. Create a new Resource Step 5: Enter a name for your storage account: corpdata9172795n2 Step 6: For Account kind select: General-purpose v2 accounts (recommended for most scenarios) General-purpose v2 accounts is recommended for most scenarios. General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. Step 7: For replication select: Read-access geo-redundant storage (RA-GRS) Read-access geo-redundant storage (RA-GRS) maximizes availability for your storage account. RA-GRS provides read-only access to the data in the secondary location, in addition to geo-replication across two regions. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview QUESTION 9 You have an Azure subscription that contains the resources in the following table. Store1 contains a file share named Data. Data contains 5,000 files. You need to synchronize the files in Data to an on-premises server named Server1. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Download an automation script. B. Register Server1. C. Create a sync group. D. Create a container instance. E. Install the Azure File Sync agent on Server1. Correct Answer: BCE Section: [none] Explanation Explanation/Reference: Explanation: Step 1 (E): Install the Azure File Sync agent on Server1 The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share Step 2 (B): Register Server1. Register Windows Server with Storage Sync Service Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server (or cluster) and the Storage Sync Service. Step 3 (C): Create a sync group and a cloud endpoint. A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud endpoint, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-sync-files-deployment-guide QUESTION 10 DRAG DROP You have an Azure subscription named Subscription1. You create an Azure Storage account named contosostorage, and then you create a file share named data. Which UNC path should you include in a script that references files from the data file share? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: contosostorage The name of account Box 2: file.core.windows.net Box 3: data The name of the file share is data. Example: References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows QUESTION 11 DRAG DROP You have an Azure subscription that contains a storage account. You have an on-premises server named Server1 that runs Windows Server 2016. Server1 has 2 TB of data. You need to transfer the data to the storage account by using the Azure Import/Export service. In which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order. NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: At a high level, an import job involves the following steps: Step 1: Attach an external disk to Server1 and then run waimportexport.exe Determine data to be imported, number of drives you need, destination blob location for your data in Azure storage. Use the WAImportExport tool to copy data to disk drives. Encrypt the disk drives with BitLocker. Step 2: From the Azure portal, create an import job. Create an import job in your target storage account in Azure portal. Upload the drive journal files. Step 3: Detach the external disks from Server1 and ship the disks to an Azure data center. Provide the return address and carrier account number for shipping the drives back to you. Ship the disk drives to the shipping address provided during job creation. Step 4: From the Azure portal, update the import job Update the delivery tracking number in the import job details and submit the import job. The drives are received and processed at the Azure data center. The drives are shipped using your carrier account to the return address provided in the import job. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-import-export-service QUESTION 12 You have the Azure virtual machines shown in the following table. You have a Recovery Services vault that protects VM1 and VM2. You need to protect VM3 and VM4 by using Recovery Services. What should you do first? A. Create a new backup policy. B. Configure the extensions for VM3 and VM4. C. Create a storage account. D. Create a new Recovery Services vault. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: A Recovery Services vault is a storage entity in Azure that houses data. The data is typically copies of data, or configuration information for virtual machines (VMs), workloads, servers, or workstations. You can use Recovery Services vaults to hold backup data for various Azure services References: https://docs.microsoft.com/en-us/azure/site-recovery/azure-to-azure-tutorial-enable-replication QUESTION 13 HOTSPOT You have an Azure subscription named Subscription1 that is associated to an Azure Active Directory (Azure AD) tenant named AAD1. Subscription1 contains the objects in the following table. You plan to create a single backup policy for Vault1. To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Share1 only Box 2: 99 years With the latest update to Azure Backup, customers can retain their data for up to 99 years in Azure. Note: A backup policy defines a matrix of when the data snapshots are taken, and how long those snapshots are retained. The backup policy interface looks like this: References: https://docs.microsoft.com/en-us/azure/backup/backup-azure-files https://docs.microsoft.com/en-us/azure/backup/backup-azure-vms-first-look-arm#defining-a-backup-policy https://blogs.microsoft.com/firehose/2015/02/16/february-update-to-azure-backup-includes-data-retention- up-to-99-years-offline-backup-and-more/ QUESTION 14 HOTSPOT You have an Azure subscription named Subscription1. In Subscription1, you create an Azure file share named share1. You create a shared access signature (SAS) named SAS1 as shown in the following exhibit. To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Will have no access The IP 193.77.134.1 does not have access on the SAS. Box 2: Will have read, write, and list access The net use command is used to connect to file shares. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1 https://docs.microsoft.com/en-us/azure/vs-azure-tools-storage-manage-with-storage-explorer? tabs=windows QUESTION 15 HOTSPOT You have Azure Storage accounts as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: storageaccount1 and storageaccount2 only Box 2: All the storage accounts Note: The three different storage account options are: General-purpose v2 (GPv2) accounts, General- purpose v1 (GPv1) accounts, and Blob storage accounts. General-purpose v2 (GPv2) accounts are storage accounts that support all of the latest features for blobs, files, queues, and tables. Blob storage accounts support all the same block blob features as GPv2, but are limited to supporting only block blobs. General-purpose v1 (GPv1) accounts provide access to all Azure Storage services, but may not have the latest features or the lowest per gigabyte pricing. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-account-options QUESTION 16 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 1 You plan to prevent users from accidentally deleting blob data from Azure. You need to ensure that administrators can recover any blob data that is deleted accidentally from the storagelod9272261 storage account for 14 days after the deletion occurred. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Task A: Create a Recovery Services vault (if a vault already exists skip this task, go to Task B below) A1. From Azure Portal, On the Hub menu, click All services and in the list of resources, type Recovery Services and click Recovery Services vaults. If there are recovery services vaults in the subscription, the vaults are listed. A2. On the Recovery Services vaults menu, click Add. A3. The Recovery Services vault blade opens, prompting you to provide a Name, Subscription, Resource group, and Location Task B. Create a backup goal B1. On the Recovery Services vault blade (for the vault you just created), in the Getting Started section, click Backup, then on the Getting Started with Backup blade, select Backup goal. The Backup Goal blade opens. If the Recovery Services vault has been previously configured, then the Backup Goal blades opens when you click Backup on the Recovery Services vault blade. B2. From the Where is your workload running? drop-down menu, select Azure. B3. From the What do you want to backup? menu, select Blob Storage, and click OK. B4. Finish the Wizard. Task C. create a backup schedule C1. Open the Microsoft Azure Backup agent. You can find it by searching your machine for Microsoft Azure Backup. C2. In the Backup agent's Actions pane, click Schedule Backup to launch the Schedule Backup Wizard. C3. On the Getting started page of the Schedule Backup Wizard, click Next. C4. On the Select Items to Backup page, click Add Items. The Select Items dialog opens. C5. Select Blob Storage you want to protect, and then click OK. C6. In the Select Items to Backup page, click Next. On the Specify Backup Schedule page, specify Schedule a backup every day, and click Next. C7. On the Select Retention Policy page, set it to 14 days, and click Next. C8. Finish the Wizard. References: https://docs.microsoft.com/en-us/azure/backup/backup-configure-vault QUESTION 17 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 2 Your company plans to store several documents on a public website. You need to create a container named bios that will host the documents in the storagelod9272261 storage account. The solution must ensure anonymous access and must ensure that users can browse folders in the container. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Azure portal create public container To create a container in the Azure portal, follow these steps: Step 1: Navigate to your new storage account in the Azure portal. Step 2: In the left menu for the storage account, scroll to the lob service section, then select Blobs. Select the + Container button. Type a name for your new container: bios Set the level of public access to the container: Select anonymous access. Step 3: Select OK to create the container. References: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal QUESTION 18 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 3 Your company plans to host in Azure the source files of several line-of-business applications. You need to create an Azure file share named corpsoftware in the storagelod9272261 storage account. The solution must ensure that corpsoftware can store only up to 250 GB of data. What should you do from the Azure portal? Correct Answer: See explanation below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Go to the Storage Account blade on the Azure portal: Step 2: Click on add File Share button: Step 3: Provide Name (storagelod9272261) and Quota (250 GB). References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-create-file-share QUESTION 19 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 6 You recently created a virtual machine named Web01. You need to attach a new 80-GB standard data disk named Web01-Disk1 to Web01. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Add a data disk Step 1: In the Azure portal, from the menu on the left, select Virtual machines. Step 2: Select the Web01 virtual machine from the list. Step 3: On the Virtual machine page, , in Essentials, select Disks. Step 4: On the Disks page, select the Web01-Disk1 from the list of existing disks. Step 5: In the Disks pane, click + Add data disk. Step 6: Click the drop-down menu for Name to view a list of existing managed disks accessible to your Azure subscription. Select the managed disk Web01-Disk1 to attach: References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/attach-disk-portal Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a manufacturing company that has offices worldwide. Contoso works with partner organizations to bring products to market. Contoso products are manufactured by using blueprint files that the company authors and maintains. Existing Environment Currently, Contoso uses multiple types of servers for business operations, including the following: File servers Domain controllers Microsoft SQL Server servers Your network contains an Active Directory forest named contoso.com. All servers and client computers are joined to Active Directory. You have a public-facing application named App1. App1 is comprised of the following three tiers: A SQL database A web front end A processing middle tier Each tier is comprised of five virtual machines. Users access the web front end by using HTTPS only. Requirements Planned Changes Contoso plans to implement the following changes to the infrastructure: Move all the tiers of App1 to Azure. Move the existing product blueprint files to Azure Blob storage. Create a hybrid directory to support an upcoming Microsoft Office 365 migration project. Technical Requirements Contoso must meet the following technical requirements: Move all the virtual machines for App1 to Azure. Minimize the number of open ports between the App1 tiers. Ensure that all the virtual machines for App1 are protected by backups. Copy the blueprint files to Azure over the Internet. Ensure that the blueprint files are stored in the archive storage tier. Ensure that partner access to the blueprint files is secured and temporary. Prevent user passwords or hashes of passwprds from being stored in Azure. Use unmanaged standard storage for the hard disks of the virtual machines. Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity Minimize administrative effort whenever possible User Requirements Contoso identifies the following requirements for users: Ensure that only users who are part of a group named Pilot can join devices to Azure AD. Designate a new user named Admin1 as the service admin for the Azure subscription. Admin1 must receive email alerts regarding service outages. Ensure that a new user named User3 can create network objects for the Azure subscription. QUESTION 1 HOTSPOT You need to identify the storage requirements for Contoso. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes Contoso is moving the existing product blueprint files to Azure Blob storage. Use unmanaged standard storage for the hard disks of the virtual machines. We use Page Blobs for these. Box 2: No Box 3: No QUESTION 2 You need to move the blueprint files to Azure. What should you do? A. Use Azure Storage Explorer to copy the files. B. Use the Azure Import/Export service. C. Generate a shared access signature (SAS). Map a drive, and then copy the files by using File Explorer. D. Generate an access key. Map a drive, and then copy the files by using File Explorer. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Azure Storage Explorer is a free tool from Microsoft that allows you to work with Azure Storage data on Windows, macOS, and Linux. You can use it to upload and download data from Azure blob storage. Scenario: Planned Changes include: move the existing product blueprint files to Azure Blob storage. Technical Requirements include: Copy the blueprint files to Azure over the Internet. References: https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-data-to-azure- blob-using-azure-storage-explorer QUESTION 3 You need to implement a backup solution for App1 after the application is moved. What should you create first? A. a recovery plan B. a Recovery Services vault C. an Azure Backup Server D. a backup policy Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: A Recovery Services vault is a logical container that stores the backup data for each protected resource, such as Azure VMs. When the backup job for a protected resource runs, it creates a recovery point inside the Recovery Services vault. Scenario: There are three application tiers, each with five virtual machines. Move all the virtual machines for App1 to Azure. Ensure that all the virtual machines for App1 are protected by backups. References: https://docs.microsoft.com/en-us/azure/backup/quick-backup-vm-portal Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 DRAG DROP You need to prepare the environment to ensure that the web administrators can deploy the web apps as quickly as possible. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: First you create a storage account using the Azure portal. Step 2: Select Automation options at the bottom of the screen. The portal shows the template on the Template tab. Add the storage account to the library. Step 3: Share the template. Scenario: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. References: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-quickstart-create- templates-use-the-portal Question Set 1 QUESTION 1 You plan to automate the deployment of a virtual machine scale set that uses the Windows Server 2016 Datacenter image. You need to ensure that when the scale set virtual machines are provisioned, they have web server components installed. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Modify the extensionProfile section of the Azure Resource Manager template. B. Create an automation account. C. Upload a configuration script. D. Create a new virtual machine scale set in the Azure portal. E. Create an Azure policy. Correct Answer: AD Section: [none] Explanation Explanation/Reference: Explanation: Virtual Machine Scale Sets can be used with the Azure Desired State Configuration (DSC) extension handler. Virtual machine scale sets provide a way to deploy and manage large numbers of virtual machines, and can elastically scale in and out in response to load. DSC is used to configure the VMs as they come online so they are running the production software. References: https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-dsc QUESTION 2 DRAG DROP You have two Azure virtual machines named VM1 and VM2. VM1 has a single data disk named Disk1. You need to attach Disk1 to VM2. The solution must minimize downtime for both virtual machines. Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Stop VM1. Step 2: Detach Disk1 from VM1. Step 3: Attach Disk1 to VM2 Attach an existing disk Follow these steps to reattach an existing available data disk to a running VM. 1. Select a running VM for which you want to reattach a data disk. 2. From the menu on the left, select Disks. 3. Select Attach existing to attach an available data disk to the VM. 4. From the Attach existing disk pane, select OK. Step 4: Start VM1. Detach a data disk using the portal 1. In the left menu, select Virtual Machines. 2. Select the virtual machine that has the data disk you want to detach and click Stop to deallocate the VM. 3. In the virtual machine pane, select Disks. 4. At the top of the Disks pane, select Edit. 5. In the Disks pane, to the far right of the data disk that you would like to detach, click the Detach button image detach button. 6. After the disk has been removed, click Save on the top of the pane. 7. In the virtual machine pane, click Overview and then click the Start button at the top of the pane to restart the VM. 8. The disk stays in storage but is no longer attached to a virtual machine. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/detach-disk https://docs.microsoft.com/en-us/azure/lab-services/devtest-lab-attach-detach-data-disk QUESTION 3 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains a virtual machine named VM1. You install and configure a web server and a DNS server on VM1. VM1 has the effective network security rules shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Rule2 blocks ports 50-60, which includes port 53, the DNS port. Internet users can reach the Web server, since it uses port 80. Box 2: If Rule2 is removed internet users can reach the DNS server as well. Note: Rules are processed in priority order, with lower numbers processed before higher numbers, because lower numbers have higher priority. Once traffic matches a rule, processing stops. As a result, any rules that exist with lower priorities (higher numbers) that have the same attributes as rules with higher priorities are not processed. References: https://docs.microsoft.com/en-us/azure/virtual-network/security-overview QUESTION 4 DRAG DROP You have an Azure Linux virtual machine that is protected by Azure Backup. One week ago, two files were deleted from the virtual machine. You need to restore the deleted files to an on-premises computer as quickly as possible. Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: To restore files or folders from the recovery point, go to the virtual machine and choose the desired recovery point. Step 0. In the virtual machine's menu, click Backup to open the Backup dashboard. Step 1. In the Backup dashboard menu, click File Recovery. Step 2. From the Select recovery point drop-down menu, select the recovery point that holds the files you want. By default, the latest recovery point is already selected. Step 3: To download the software used to copy files from the recovery point, click Download Executable (for Windows Azure VM) or Download Script (for Linux Azure VM, a python script is generated). Step 4: Copy the files by using AzCopy AzCopy is a command-line utility designed for copying data to/from Microsoft Azure Blob, File, and Table storage, using simple commands designed for optimal performance. You can copy data between a file system and a storage account, or between storage accounts. References: https://docs.microsoft.com/en-us/azure/backup/backup-azure-restore-files-from-vm https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy QUESTION 5 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 3 You plan to protect on-premises virtual machines and Azure virtual machines by using Azure Backup. You need to prepare the backup infrastructure in Azure. The solution must minimize the cost of storing the backups in Azure. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: First, create Recovery Services vault. Step 1: On the left-hand menu, select All services and in the services list, type Recovery Services. As you type, the list of resources filters. When you see Recovery Services vaults in the list, select it to open the Recovery Services vaults menu. Step 2: In the Recovery Services vaults menu, click Add to open the Recovery Services vault menu. Step 3: In the Recovery Services vault menu, for example, Type myRecoveryServicesVault in Name. The current subscription ID appears in Subscription. If you have additional subscriptions, you could choose another subscription for the new vault. For Resource group select Use existing and choose myResourceGroup. If myResourceGroup doesn't exist, select Create new and type myResourceGroup. From the Location drop-down menu, choose West Europe. Click Create to create your Recovery Services vault. References: https://docs.microsoft.com/en-us/azure/backup/tutorial-backup-vm-at-scale QUESTION 6 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 3 You need to deploy two Azure virtual machines named VM1003a and VM1003b based on an Ubuntu Server image. The deployment must meet the following requirements: Provide a Service Level Agreement (SLA) of 99.95 percent availability. Use managed disks. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Open the Azure portal. Step 2: On the left menu, select All resources. You can sort the resources by Type to easily find your images. Step 3: Select the image you want to use from the list. The image Overview page opens. Step 4: Select Create VM from the menu. Step 5: Enter the virtual machine information. Select VM1003a as the name for the first Virtual machine. The user name and password entered here will be used to log in to the virtual machine. When complete, select OK. You can create the new VM in an existing resource group, or choose Create new to create a new resource group to store the VM. Step 6: Select a size for the VM. To see more sizes, select View all or change the Supported disk type filter. Step 7: Under Settings, make changes as necessary and select OK. Step 8: On the summary page, you should see your image name listed as a Private image. Select Ok to start the virtual machine deployment. Repeat the procedure for the second VM and name it VM1003b. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/create-vm-generalized-managed QUESTION 7 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 4 You need to deploy an Azure virtual machine named VM1004a based on an Ubuntu Server image, and then configure VM1004a to meet the following requirements: The virtual machine must contain data disks that can store at least 15 TB of data. The data disks must be able to provide at least 2.000 IOPS. Storage costs must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Open the Azure portal. Step 2: On the left menu, select All resources. You can sort the resources by Type to easily find your images. Step 3: Select the image you want to use from the list. The image Overview page opens. Step 4: Select Create VM from the menu. Step 5: Enter the virtual machine information. Select VM1004a as the name for the first Virtual machine. The user name and password entered here will be used to log in to the virtual machine. When complete, select OK. You can create the new VM in an existing resource group, or choose Create new to create a new resource group to store the VM. Step 6: Select a size for the VM. To see more sizes, select View all or change the Supported disk type filter. To support 15 TB of data you would need a Premium disk. Step 7: Under Settings, make changes as necessary and select OK. Step 8: On the summary page, you should see your image name listed as a Private image. Select Ok to start the virtual machine deployment. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/create-vm-generalized-managed QUESTION 8 You have an Azure subscription that contains a virtual machine named VM1. VM1 hosts a line-of-business application that is available 24 hours a day. VM1 has one network interface and one managed disk. VM1 uses the D4s v3 size. You plan to make the following changes to VM1: Change the size to D8s v3. Add a 500-GB managed disk. Add the Puppet Agent extension. Attach an additional network interface. Which change will cause downtime for VM1? A. Add the Puppet Agent extension. B. Change the size to D8s v3. C. Add a 500-GB managed disk. D. Attach an additional network interface. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: While resizing the VM it must be in a stopped state. References: https://azure.microsoft.com/en-us/blog/resize-virtual-machines/ QUESTION 9 You have an Azure virtual machine named VM1 that you use for testing. VM1 is protected by Azure Backup. You delete VM1. You need to remove the backup data stored for VM1. What should you do first? A. Delete the Recovery Services vault. B. Delete the storage account. C. Stop the backup D. Modify the backup policy. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Azure Backup provides backup for virtual machines \u2014 created through both the classic deployment model and the Azure Resource Manager deployment model \u2014 by using custom-defined backup policies in a Recovery Services vault. With the release of backup policy management, customers can manage backup policies and model them to meet their changing requirements from a single window. Customers can edit a policy, associate more virtual machines to a policy, and delete unnecessary policies to meet their compliance requirements. Incorrect Answers: B: You can't delete a Recovery Services vault if it is registered to a server and holds backup data. If you try to delete a vault, but can't, the vault is still configured to receive backup data. References: https://azure.microsoft.com/en-in/updates/azure-vm-backup-policy-management/ QUESTION 10 You have an Azure subscription named Subscription1. You deploy a Linux virtual machine named VM1 to Subscription1. You need to monitor the metrics and the logs of VM1. What should you use? A. the AzurePerformanceDiagnostics extension B. Azure HDInsight C. Linux Diagnostic Extension (LAD) 3.0 D. Azure Analysis Services Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: You can use extensions to configure diagnostics on your VMs to collect additional metric data. The basic host metrics are available, but to see more granular and VM-specific metrics, you need to install the Azure diagnostics extension on the VM. The Azure diagnostics extension allows additional monitoring and diagnostics data to be retrieved from the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/tutorial-monitoring QUESTION 11 DRAG DROP You have an availability set named AS1 that contains three virtual machines named VM1, VM2, and VM3. You attempt to reconfigure VM1 to use a larger size. The operation fails and you receive an allocation failure message. You need to ensure that the resize operation succeeds. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Stop VM1, VM, and VM3. If the VM you wish to resize is part of an availability set, then you must stop all VMs in the availability set before changing the size of any VM in the availability set. The reason all VMs in the availability set must be stopped before performing the resize operation to a size that requires different hardware is that all running VMs in the availability set must be using the same physical hardware cluster. Therefore, if a change of physical hardware cluster is required to change the VM size then all VMs must be first stopped and then restarted one-by-one to a different physical hardware clusters. Step 2: Resize VM1. Step 3: Start VM1, VM2, and VM3. References: https://azure.microsoft.com/es-es/blog/resize-virtual-machines/ QUESTION 12 You plan to back up an Azure virtual machine named VM1. You discover that the Backup Pre-Check status displays a status of Warning. What is a possible cause of the Warning status? A. VM1 is stopped. B. VM1 does not have the latest version of WaAppAgent.exe installed. C. VM1 has an unmanaged disk. D. A Recovery Services vault is unavailable. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Warning state indicates one or more issues in VM\u2019s configuration that might lead to backup failures and provides recommended steps to ensure successful backups. Not having the latest VM Agent installed, for example, can cause backups to fail intermittently and falls in this class of issues. References: https://azure.microsoft.com/en-us/blog/azure-vm-backup-pre-checks/ QUESTION 13 You have an Azure subscription named Subscription1 that is used by several departments at your company. Subscription1 contains the resources in the following table. Another administrator deploys a virtual machine named VM1 and an Azure Storage account named Storage2 by using a single Azure Resource Manager template. You need to view the template used for the deployment. From which blade can you view the template that was used for the deployment? A. Container1 B. RG1 C. VM1 D. Storage2 Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: View template from deployment history 1. Go to the resource group for your new resource group. Notice that the portal shows the result of the last deployment. Select this link. 2. You see a history of deployments for the group. In your case, the portal probably lists only one deployment. Select this deployment. 3. The portal displays a summary of the deployment. The summary includes the status of the deployment and its operations and the values that you provided for parameters. To see the template that you used for the deployment, select View template. References: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-export-template QUESTION 14 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Overview blade, you move the virtual machine to a different subscription. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You would need to redeploy the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 15 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Redeploy blade, you click Redeploy. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: When you redeploy a VM, it moves the VM to a new node within the Azure infrastructure and then powers it back on, retaining all your configuration options and associated resources. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 16 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Update management blade, you click Enable. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You would need to redeploy the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 17 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains two Azure virtual machines named VM1 and VM2. VM1 and VM2 run Windows Server 2016. VM1 is backed up daily by Azure Backup without using the Azure Backup agent. VM1 is affected by ransomware that encrypts data. You need to restore the latest backup of VM1. To which location can you restore the backup? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 18 You download an Azure Resource Manager template based on an existing virtual machine. The template will be used to deploy 100 virtual machines. You need to modify the template to reference an administrative password. You must prevent the password from being stored in plain text. What should you create to store the password? A. an Azure Key Vault and an access policy B. a Recovery Services vault and a backup policy C. Azure Active Directory (AD) Identity Protection and an Azure policy D. an Azure Storage account and an access policy Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: You can use a template that allows you to deploy a simple Windows VM by retrieving the password that is stored in a Key Vault. Therefore, the password is never put in plain text in the template parameter file. References: https://azure.microsoft.com/en-us/resources/templates/101-vm-secure-password/ QUESTION 19 HOTSPOT You create a virtual machine scale set named Scale1. Scale1 is configured as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: The Autoscale scale out rule increases the number of VMs by 2 if the CPU threshold is 80% or higher. The initial instance count is 4 and rises to 6 when the 2 extra instances of VMs are added. Box 2: The Autoscale scale in rule decreases the number of VMs by 4 if the CPU threshold is 30% or lower. The initial instance count is 4 and thus cannot be reduced to 0 as the minimum instances is set to 2. Instances are only added when the CPU threshold reaches 80%. References: https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-overview https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-best-practices https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-common-scale-patterns QUESTION 20 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 4 You plan to back up all the Azure virtual machines in your Azure subscription at 02:00 Coordinated Universal Time (UTC) daily. You need to prepare the Azure environment to ensure that any new virtual machines can be configured quickly for backup. The solution must ensure that all the daily backups performed at 02:00 UTC are stored for only 90 days. What should you do from your Recovery Services vault on the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Task A: Create a Recovery Services vault (if a vault already exists skip this task, go to Task B below) A1. From Azure Portal, On the Hub menu, click All services and in the list of resources, type Recovery Services and click Recovery Services vaults. If there are recovery services vaults in the subscription, the vaults are listed. A2. On the Recovery Services vaults menu, click Add. A3. The Recovery Services vault blade opens, prompting you to provide a Name, Subscription, Resource group, and Location Task B. B1. On the Recovery Services vault blade (for the vault you just created), in the Getting Started section, click Backup, then on the Getting Started with Backup blade, select Backup goal. The Backup Goal blade opens. If the Recovery Services vault has been previously configured, then the Backup Goal blades opens when you click Backup on the Recovery Services vault blade. B2. From the Where is your workload running? drop-down menu, select Azure. B3. From the What do you want to backup? menu, select Virtual Machine, and click OK. B4. Finish the Wizard. Task C. create a backup schedule C1. Open the Microsoft Azure Backup agent. You can find it by searching your machine for Microsoft Azure Backup. C2. In the Backup agent's Actions pane, click Schedule Backup to launch the Schedule Backup Wizard. C3. On the Getting started page of the Schedule Backup Wizard, click Next. C4. On the Select Items to Backup page, click Add Items. The Select Items dialog opens. C5. Select Blob Storage you want to protect, and then click OK. C6. In the Select Items to Backup page, click Next. On the Specify Backup Schedule page, specify Schedule a backup every: day At the following times: 2.00 AM C7. On the Select Retention Policy page, set it to 90 days, and click Next. C8. Finish the Wizard. References: https://docs.microsoft.com/en-us/azure/backup/backup-configure-vault QUESTION 21 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task5 You plan to connect several virtual machines to the VNET01-USEA2 virtual network. In the Web-RGlod9272261 resource group, you need to create a virtual machine that uses the Standard_B2ms size named Web01 that runs Windows Server 2016. Web01 must be added to an availability set. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Choose Create a resource in the upper left-hand corner of the Azure portal. Step 2: In the Basics tab, under Project details, make sure the correct subscription is selected and then choose Web-RGlod9272261 resource group Step 3: Under Instance details type/select: Virtual machine name: Web01 Image: Windows Server 2016 Size: Standard_B2ms size Leave the other defaults. Step 4: Finish the Wizard Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a consulting company that has a main office in Montreal and two branch offices in Seattle and New York. The Montreal office has 2,000 employees. The Seattle office has 1,000 employees. The New York office has 200 employees. All the resources used by Contoso are hosted on-premises. Contoso creates a new Azure subscription. The Azure Active Directory (Azure AD) tenant uses a domain named contoso.onmicrosoft.com. The tenant uses the P1 pricing tier. Existing Environment The network contains an Active Directory forest named contoso.com. All domain controllers are configured as DNS servers and host the contoso.com DNS zone. Contoso has finance, human resources, sales, research, and information technology departments. Each department has an organizational unit (OU) that contains all the accounts of that respective department. All the user accounts have the department attribute set to their respective department. New users are added frequently. Contoso.com contains a user named User1. All the offices connect by using private links. Contoso has data centers in the Montreal and Seattle offices. Each data center has a firewall that can be configured as a VPN device. All infrastructure servers are virtualized. The virtualization environment contains the servers in the following table. Contoso uses two web applications named App1 and App2. Each instance on each web application requires 1GB of memory. The Azure subscription contains the resources in the following table. The network security team implements several network security groups (NSGs). Planned Changes Contoso plans to implement the following changes: Deploy Azure ExpressRoute to the Montreal office. Migrate the virtual machines hosted on Server1 and Server2 to Azure. Synchronize on-premises Active Directory to Azure Active Directory (Azure AD). Migrate App1 and App2 to two Azure web apps named WebApp1 and WebApp2. Technical requirements Contoso must meet the following technical requirements: Ensure that WebApp1 can adjust the number of instances automatically based on the load and can scale up to five instances. Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. Ensure that routing information is exchanged automatically between Azure and the routers in the Montreal office. Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. Ensure that webapp2.azurewebsites.net can be accessed by using the name app2.contoso.com Connect the New York office to VNet1 over the Internet by using an encrypted connection. Create a workflow to send an email message when the settings of VM4 are modified. Create a custom Azure role named Role1 that is based on the Reader role. Minimize costs whenever possible. QUESTION 1 You discover that VM3 does NOT meet the technical requirements. You need to verify whether the issue relates to the NSGs. What should you use? A. Diagram in VNet1 B. the security recommendations in Azure Advisor C. Diagnostic settings in Azure Monitor D. Diagnose and solve problems in Traffic Manager profiles E. IP flow verify in Azure Network Watcher Correct Answer: E Section: [none] Explanation Explanation/Reference: Explanation: Scenario: Contoso must meet technical requirements including: Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. IP flow verify checks if a packet is allowed or denied to or from a virtual machine. The information consists of direction, protocol, local IP, remote IP, local port, and remote port. If the packet is denied by a security group, the name of the rule that denied the packet is returned. While any source or destination IP can be chosen, IP flow verify helps administrators quickly diagnose connectivity issues from or to the internet and from or to the on-premises environment. References: https://docs.microsoft.com/en-us/azure/network-watcher/network-watcher-ip-flow-verify-overview Question Set 1 QUESTION 1 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You modify the name servers at the domain registrar. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Modify the Name Server (NS) record. References: https://docs.microsoft.com/en-us/azure/dns/dns-delegate-domain-azure-dns QUESTION 2 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You modify the SOA record in the contoso.com zone. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Modify the NS record, not the SOA record. Note: The SOA record stores information about the name of the server that supplied the data for the zone; the administrator of the zone; the current version of the data file; the number of seconds a secondary name server should wait before checking for updates; the number of seconds a secondary name server should wait before retrying a failed zone transfer; the maximum number of seconds that a secondary name server can use data before it must either be refreshed or expire; and a default number of seconds for the time-to- live file on resource records. References: https://searchnetworking.techtarget.com/definition/start-of-authority-record QUESTION 3 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You add an NS record to the contoso.com Azure DNS zone. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Before you can delegate your DNS zone to Azure DNS, you need to know the name servers for your zone. The NS record set contains the names of the Azure DNS name servers assigned to the zone. References: https://docs.microsoft.com/en-us/azure/dns/dns-delegate-domain-azure-dns QUESTION 4 You are troubleshooting a performance issue for an Azure Application Gateway. You need to compare the total requests to the failed requests during the past six hours. What should you use? A. NSG flow logs in Azure Network Watcher B. Metrics in Application Gateway C. Connection monitor in Azure Network Watcher D. Diagnostics logs in Application Gateway Correct Answer: B Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-diagnostics#metrics QUESTION 5 You have two subscriptions named Subscription1 and Subscription2. Each subscription is associated to a different Azure AD tenant. Subscription1 contains a virtual network named VNet1. VNet1 contains an Azure virtual machine named VM1 and has an IP address space of 10.0.0.0/16. Subscription2 contains a virtual network named VNet2. VNet2 contains an Azure virtual machine named VM2 and has an IP address space of 10.10.0.0/24. You need to connect VNet1 to VNet2. What should you do first? A. Move VM1 to Subscription2. B. Modify the IP address space of VNet2. C. Provision virtual network gateways. D. Move VNet1 to Subscription2. Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: The virtual networks can be in the same or different regions, and from the same or different subscriptions. When connecting VNets from different subscriptions, the subscriptions do not need to be associated with the same Active Directory tenant. Configuring a VNet-to-VNet connection is a good way to easily connect VNets. Connecting a virtual network to another virtual network using the VNet-to-VNet connection type (VNet2VNet) is similar to creating a Site- to-Site IPsec connection to an on-premises location. Both connectivity types use a VPN gateway to provide a secure tunnel using IPsec/IKE, and both function the same way when communicating. The local network gateway for each VNet treats the other VNet as a local site. This lets you specify additional address space for the local network gateway in order to route traffic. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager- portal QUESTION 6 DRAG DROP You have an Azure subscription that contains two virtual networks named VNet1 and VNet2. Virtual machines connect to the virtual networks. The virtual networks have the address spaces and the subnets configured as shown in the following table. You need to add the address space of 10.33.0.0/16 to VNet1. The solution must ensure that the hosts on VNet1 and VNet2 can communicate. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Remove peering between Vnet1 and VNet2. You can't add address ranges to, or delete address ranges from a virtual network's address space once a virtual network is peered with another virtual network. To add or remove address ranges, delete the peering, add or remove the address ranges, then re-create the peering. Step 2: Add the 10.44.0.0/16 address space to VNet1. Step 3: Recreate peering between VNet1 and VNet2 References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-manage-peering QUESTION 7 You have an Azure subscription that contains the resources in the following table. To which subnets can you apply NSG1? A. the subnets on VNet2 only B. the subnets on VNet2 and VNet3 only C. the subnets on VNet1, VNet2, and VNet3 D. the subnets on VNet1 only E. the subnets on VNet3 only Correct Answer: E Section: [none] Explanation Explanation/Reference: Explanation: All Azure resources are created in an Azure region and subscription. A resource can only be created in a virtual network that exists in the same region and subscription as the resource. References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-vnet-plan-design-arm QUESTION 8 HOTSPOT You have an Azure virtual machine named VM1 that connects to a virtual network named VNet1. VM1 has the following configurations: Subnet 10.0.0.0/24 Availability set: AVSet Network security group (NSG): None Private IP address: 10.0.0.4 (dynamic) Public IP address: 40.90.219.6 (dynamic) You deploy a standard, Internet-facing load balancer named slb1. You need to configure slb1 to allow connectivity to VM1. Which changes should you apply to VM1 as you configure slb1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 9 You have five Azure virtual machines that run Windows Server 2016. The virtual machines are configured as web servers. You have an Azure load balancer named LB1 that provides load balancing services for the virtual machines. You need to ensure that visitors are serviced by the same web server for each request. What should you configure? A. Protocol to UDP B. Session persistence to None C. Session persistence to Client IP D. Idle Time-out (minutes) to 20 Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: You can set the sticky session in load balancer rules with setting the session persistence as the client IP. References: https://cloudopszone.com/configure-azure-load-balancer-for-sticky-sessions/ QUESTION 10 You have the Azure virtual networks shown in the following table. To which virtual networks can you establish a peering connection from VNet1? A. VNet2 and VNet3 only B. VNet2 only C. VNet3 and VNet4 only D. VNet2, VNet3, and VNet4 Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-connect-virtual-networks-portal QUESTION 11 You have an Azure subscription that contains a policy-based virtual network gateway named GW1 and a virtual network named VNet1. You need to ensure that you can configure a point-to-site connection from VNet1 to an on-premises computer. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Reset GW1. B. Create a route-based virtual network gateway. C. Delete GW1. D. Add a public IP address space to VNet1. E. Add a connection to GW1. F. Add a service endpoint to VNet1. Correct Answer: BC Section: [none] Explanation Explanation/Reference: Explanation: B: A VPN gateway is used when creating a VPN connection to your on-premises network. Route-based VPN devices use any-to-any (wildcard) traffic selectors, and let routing/forwarding tables direct traffic to different IPsec tunnels. It is typically built on router platforms where each IPsec tunnel is modeled as a network interface or VTI (virtual tunnel interface). C: Policy-based VPN devices use the combinations of prefixes from both networks to define how traffic is encrypted/decrypted through IPsec tunnels. It is typically built on firewall devices that perform packet filtering. IPsec tunnel encryption and decryption are added to the packet filtering and processing engine. Incorrect Answers: D: Point-to-Site connections do not require a VPN device or a public-facing IP address. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/create-routebased-vpn-gateway-portal https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-connect-multiple-policybased-rm-ps QUESTION 12 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains the virtual networks in the following table. Subscription1 contains the virtual machines in the following table. The firewalls on all the virtual machines are configured to allow all ICMP traffic. You add the peerings in the following table. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes Vnet1 and Vnet3 are peers. Box 2: Yes Vnet2 and Vnet3 are peers. Box 3: No Peering connections are non-transitive. References: https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/hybrid-networking/hub-spoke QUESTION 13 You have an Azure subscription named Subscription1 that contains the resource groups shown in the following table. In RG1, you create a virtual machine named VM1 in the East Asia location. You plan to create a virtual network named VNET1. You need to create VNET1, and then connect VM1 to VNET1. What are two possible ways to achieve this goal? Each correct answer presents a complete solution. NOTE: Each correct selection is worth one point. A. Create VNET1 in RG2, and then set East Asia as the location. B. Create VNET1 in a new resource group in the West US location, and then set West US as the location. C. Create VNET1 in RG1, and then set East US as the location. D. Create VNET1 in RG2, and then set East US as the location. E. Create VNET1 in RG1, and then set East Asia as the location. Correct Answer: AE Section: [none] Explanation Explanation/Reference: QUESTION 14 You have an Azure subscription that contains a virtual network named VNet1. VNet1 contains four subnets named Gateway, Perimeter, NVA, and Production. The NVA subnet contains two network virtual appliances (NVAs) that will perform network traffic inspection between the Perimeter subnet and the Production subnet. You need to implement an Azure load balancer for the NVAs. The solution must meet the following requirements: The NVAs must run in an active-active configuration that uses automatic failover. The NVAs must load balance traffic to two services on the Profuction subnet. The services have different IP addresses. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Add two load balancing rules that have HA Ports enabled and Floating IP disabled. B. Add a frontend IP configuration, two backend pools, and a health probe. C. Add two load balancing rules that have HA Ports and Floating IP enabled. D. Deploy a standard load balancer. E. Deploy a basic load balancer. F. Add a frontend IP configuration a backend pool, and a health probe. Correct Answer: BCD Section: [none] Explanation Explanation/Reference: Explanation: A standard load balancer is required for the HA ports. Two backend pools are needed as there are two services with different IP addresses. Floating IP rule is used where backend ports are reused. Incorrect Answers: F: HA Ports are not available for the basic load balancer. References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-overview https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-multivip-overview QUESTION 15 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 1 Your on-premises network uses an IP address range of 131.107.2.0 to 131.107.2.255. You need to ensure that only device from the on-premises network can connect to the rg1lod9172796n1 storage account. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Navigate to the rg1lod9172796n1 storage account. Step 2: Click on the settings menu called Firewalls and virtual networks. Step 3: Ensure that you have elected to allow access from 'Selected networks'. Step 4: To grant access to an internet IP range, enter the address range of 131.107.2.0 to 131.107.2.255 (in CIDR format) under Firewall, Address Ranges. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security QUESTION 16 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 4 Another administrator attempts to establish connectivity between two virtual networks named VNET1 and VNET2. The administrator reports that connections across the virtual networks fail. You need to ensure that network connections can be established successfully between VNET1 and VNET2 as quickly as possible. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can connect one VNet to another VNet using either a Virtual network peering, or an Azure VPN Gateway. To create a virtual network gateway Step 1: In the portal, on the left side, click +Create a resource and type 'virtual network gateway' in search. Locate Virtual network gateway in the search return and click the entry. On the Virtual network gateway page, click Create at the bottom of the page to open the Create virtual network gateway page. Step 2: On the Create virtual network gateway page, fill in the values for your virtual network gateway. Name: Name your gateway. This is not the same as naming a gateway subnet. It's the name of the gateway object you are creating. Gateway type: Select VPN. VPN gateways use the virtual network gateway type VPN. Virtual network: Choose the virtual network to which you want to add this gateway. Click Virtual network to open the 'Choose a virtual network' page. Select the VNet. If you don't see your VNet, make sure the Location field is pointing to the region in which your virtual network is located. Gateway subnet address range: You will only see this setting if you did not previously create a gateway subnet for your virtual network. If you previously created a valid gateway subnet, this setting will not appear. Step 4: Select Create New to create a Gateway subnet. Step 5: Click Create to begin creating the VPN gateway. The settings are validated and you'll see the \"Deploying Virtual network gateway\" tile on the dashboard. Creating a gateway can take up to 45 minutes. You may need to refresh your portal page to see the completed status. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager- portal? QUESTION 17 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 5 You plan to configure VM1 to be accessible from the Internet. You need to add a public IP address to the network interface used by VM1. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can add private and public IP addresses to an Azure network interface by completing the steps that follow. Step 1: In Azure portal, click More services > type virtual machines in the filter box, and then click Virtual machines. Step 2: In the Virtual machines pane, click the VM you want to add IP addresses to. Click Network interfaces in the virtual machine pane that appears, and then select the network interface you want to add the IP addresses to. In the example shown in the following picture, the NIC named myNIC from the VM named myVM is selected: Step 3: In the pane that appears for the NIC you selected, click IP configurations. Step 4: Click Create public IP address. Step 5: In the Create public IP address pane that appears, enter a Name, select an IP address assignment type, a Subscription, a Resource group, and a Location, then click Create, as shown in the following picture: References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-multiple-ip-addresses-portal QUESTION 18 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 6 You need to allow RDP connections over TCP port 3389 to VM1 from the Internet. The solution must prevent connections from the Internet over all other TCP ports. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Create a new network security group Step 2: Select your new network security group. Step 3: Select Inbound security rules. Under Add inbound security rule, enter the following Destination: Select Network security group, and then select the security group you created previously. Destination port ranges: 3389 Protocol: Select TCP References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-filter-network-traffic QUESTION 19 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 5 You plan to create 100 Azure virtual machines on each of the following three virtual networks: VNET1005a VNET1005b VNET1005c All the network traffic between the three virtual networks will be routed through VNET1005a. You need to create the virtual networks, and then to ensure that all the Azure virtual machines can connect to other virtual machines by using their private IP address. The solution must NOT require any virtual network gateways and must minimize the number of peerings. What should you do from the Azure portal before you configure IP routing? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Click Create a resource in the portal. Step 2: Enter Virtual network in the Search the Marketplace box at the top of the New pane that appears. Click Virtual network when it appears in the search results. Step 3: Select Classic in the Select a deployment model box in the Virtual Network pane that appears, then click Create. Step 4: Enter the following values on the Create virtual network (classic) pane and then click Create: Name: VNET1005a Address space: 10.0.0.0/16 Subnet name: subnet0 Resource group: Create new Subnet address range: 10.0.0.0/24 Subscription and location: Select your subscription and location. Step 5: Repeat steps 3-5 for VNET1005b (10.1.0.0/16, 10.1.0.0/24), and for VNET1005c 10.2.0.0/16, 10.2.0.0/24). References: https://docs.microsoft.com/en-us/azure/virtual-network/create-virtual-network-classic QUESTION 20 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 6 You plan to create several virtual machines in different availability zones, and then to configure the virtual machines for load balanced connections from the Internet. You need to create an IP address resource named ip1006 to support the planned load balancing solution. The solution must minimize costs. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: We should create a public IP address. Step 1: At the top, left corner of the portal, select + Create a resource. Step 2: Enter public ip address in the Search the Marketplace box. When Public IP address appears in the search results, select it. Step 3: Under Public IP address, select Create. Step 4: Enter, or select values for the following settings, under Create public IP address, then select Create: Name: ip1006 SKU: Basic SKU IP Version: IPv6 IP address assignment: Dynamic Subscription: Select appropriate Resource group: Select appropriate References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-public-ip-address QUESTION 21 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 8 You need to create a virtual network named VNET1008 that contains three subnets named subnet0, subnet1, and subnet2. The solution must meet the following requirements: Connections from any of the subnets to the Internet must be blocked. Connections from the Internet to any of the subnets must be blocked. The number of network security groups (NSGs) and NSG rules must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Click Create a resource in the portal. Step 2: Enter Virtual network in the Search the Marketplace box at the top of the New pane that appears. Click Virtual network when it appears in the search results. Step 3: Select Classic in the Select a deployment model box in the Virtual Network pane that appears, then click Create. Step 4: Enter the following values on the Create virtual network (classic) pane and then click Create: Name: VNET1008 Address space: 10.0.0.0/16 Subnet name: subnet0 Resource group: Create new Subnet address range: 10.0.0.0/24 Subscription and location: Select your subscription and location. Step 5: In the portal, you can create only one subnet when you create a virtual network. Click Subnets (in the SETTINGS section) on the Create virtual network (classic) pane that appears. Click +Add on the VNET1008 - Subnets pane that appears. Step 6: Enter subnet1 for Name on the Add subnet pane. Enter 10.0.1.0/24 for Address range. Click OK. Step 7: Create the third subnet: Click +Add on the VNET1008 - Subnets pane that appears. Enter subnet2 for Name on the Add subnet pane. Enter 10.0.2.0/24 for Address range. Click OK. References: https://docs.microsoft.com/en-us/azure/virtual-network/create-virtual-network-classic QUESTION 22 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Azure Network Watcher, you create a connection monitor. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: References: https://azure.microsoft.com/en-us/updates/general-availability-azure-network-watcher-connection-monitor- in-all-public-regions/ QUESTION 23 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Azure Network Watcher, you create a packet capture. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Use the Connection Monitor feature of Azure Network Watcher. References: https://azure.microsoft.com/en-us/updates/general-availability-azure-network-watcher-connection-monitor- in-all-public-regions/ QUESTION 24 HOTSPOT Subscription1 contains the virtual machines in the following table. In Subscription1, you create a load balancer that has the following configurations: Name: LB1 SKU: Basic Type: Internal Subnet: Subnet12 Virtual network: VNET1 For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-overview QUESTION 25 You have an Azure subscription named Subscription1 that contains two Azure virtual networks named VNet1 and VNet2. VNet1 contains a VPN gateway named VPNGW1 that uses static routing. There is a site- to-site VPN connection between your on-premises network and VNet1. On a computer named Client1 that runs Windows 10, you configure a point-to-site VPN connection to VNet1. You configure virtual network peering between VNet1 and VNet2. You verify that you can connect to VNet2 from the on-premises network. Client1 is unable to connect to VNet2. You need to ensure that you can connect Client1 to VNet2. What should you do? A. Select Allow gateway transit on VNet2. B. Enable BGP on VPNGW1. C. Select Allow gateway transit on VNet1. D. Download and re-install the VPN client configuration package on Client1. Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-point-to-site-routing QUESTION 26 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Performance Monitor, you create a Data Collector Set (DCS). Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Use the Connection Monitor feature of Azure Network Watcher. References: https://docs.microsoft.com/en-us/azure/network-watcher/network-watcher-monitoring-overview QUESTION 27 HOTSPOT You have a virtual network named VNet1 that has the configuration shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: add an address space Your IaaS virtual machines (VMs) and PaaS role instances in a virtual network automatically receive a private IP address from a range that you specify, based on the address space of the subnet they are connected to. We need to add the 192.168.1.0/24 address space. Box 2: add a network interface The 10.2.1.0/24 network exists. We need to add a network interface. References: https://docs.microsoft.com/en-us/office365/enterprise/designing-networking-for-microsoft-azure-iaas https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-static-private-ip-arm-pportal QUESTION 28 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains the virtual machines in the following table. Subscription1 contains a virtual network named VNet1 that has the subnets in the following table. VM3 has multiple network adapters, including a network adapter named NIC3. IP forwarding is enabled on NIC3. Routing is enabled on VM3. You create a route table named RT1 that contains the routes in the following table. You apply RT1 to Subnet1 and Subnet2. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: IP forwarding enables the virtual machine a network interface is attached to: Receive network traffic not destined for one of the IP addresses assigned to any of the IP configurations assigned to the network interface. Send network traffic with a different source IP address than the one assigned to one of a network interface's IP configurations. The setting must be enabled for every network interface that is attached to the virtual machine that receives traffic that the virtual machine needs to forward. A virtual machine can forward traffic whether it has multiple network interfaces or a single network interface attached to it. Box 1: Yes The routing table allows connections from VM3 to VM1 and VM2. And as IP forwarding is enabled on VM3, VM3 can connect to VM1. Box 2: No VM3, which has IP forwarding, must be turned on, in order for VM2 to connect to VM1. Box 3: Yes The routing table allows connections from VM1 and VM2 to VM3. IP forwarding on VM3 allows VM1 to connect to VM2 via VM3. References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-udr-overview https://www.quora.com/What-is-IP-forwarding QUESTION 29 You have an Azure subscription that contains the resources in the following table. VM1 and VM2 are deployed from the same template and host line-of-business applications accessed by using Remote Desktop. You configure the network security group (NSG) shown in the exhibit. (Click the Exhibit tab.) You need to prevent users of VM2 and VM2 from accessing websites on the Internet over TCP port 80. What should you do? A. Change the DenyWebSites outbound security rule. B. Change the Port_80 inbound security rule. C. Disassociate the NSG from a network interface. D. Associate the NSG to Subnet1. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: You can associate or dissociate a network security group from a network interface or subnet. The NSG has the appropriate rule to block users from accessing the Internet. We just need to associate it with Subnet1. References: https://docs.microsoft.com/en-us/azure/virtual-network/manage-network-security-group QUESTION 30 HOTSPOT You are creating an Azure load balancer. You need to add an IPv6 load balancing rule to the load balancer. How should you complete the Azure PowerShell script? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-ipv6-internet-ps QUESTION 31 HOTSPOT You have an Azure subscription named Subscription1 that contains a resource group named RG1. In RG1, you create an internal load balancer named LB1 and a public load balancer named LB2. You need to ensure that an administrator named Admin1 can manage LB1 and LB2. The solution must follow the principle of least privilege. Which role should you assign to Admin1 for each task? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 32 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 7 You plan to allow connections between the VNET01-USEA2 and VNET01-USWE2 virtual networks. You need to ensure that virtual machines can communicate across both virtual networks by using their private IP address. The solution must NOT require any virtual network gateways. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Virtual network peering enables you to seamlessly connect two Azure virtual networks. Once peered, the virtual networks appear as one, for connectivity purposes. Peer virtual networks Step 1. In the Search box at the top of the Azure portal, begin typing VNET01-USEA2. When VNET01- USEA2 appears in the search results, select it. Step 2. Select Peerings, under SETTINGS, and then select + Add, as shown in the following picture: Step 3. Enter, or select, the following information, accept the defaults for the remaining settings, and then select OK. Name: myVirtualNetwork1-myVirtualNetwork2 (for example) Subscription: elect your subscription. Virtual network: VNET01-USWE2 - To select the VNET01-USWE2 virtual network, select Virtual network, then select VNET01-USWE2. You can select a virtual network in the same region or in a different region. Now we need to repeat steps 1-3 for the other network VNET01-USWE2: Step 4. In the Search box at the top of the Azure portal, begin typing VNET01- USEA2. When VNET01- USEA2 appears in the search results, select it. Step 5. Select Peerings, under SETTINGS, and then select + Add. References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-connect-virtual-networks-portal QUESTION 33 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 8 You plan to host several secured websites on Web01. You need to allow HTTPS over TCP port 443 to Web01 and to prevent HTTP over TCP port 80 to Web01. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can filter network traffic to and from Azure resources in an Azure virtual network with a network security group. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. Step A: Create a network security group A1. Search for and select the resource group for the VM, choose Add, then search for and select Network security group. A2. Select Create. The Create network security group window opens. A3. Create a network security group Enter a name for your network security group. Select or create a resource group, then select a location. A4. Select Create to create the network security group. Step B: Create an inbound security rule to allows HTTPS over TCP port 443 B1. Select your new network security group. B2. Select Inbound security rules, then select Add. B3. Add inbound rule B4. Select Advanced. From the drop-down menu, select HTTPS. You can also verify by clicking Custom and selecting TCP port, and 443. B5. Select Add to create the rule. Repeat step B2-B5 to deny TCP port 80 B6. Select Inbound security rules, then select Add. B7. Add inbound rule B8. Select Advanced. Clicking Custom and selecting TCP port, and 80. B9. Select Deny. Step C: Associate your network security group with a subnet Your final step is to associate your network security group with a subnet or a specific network interface. C1. In the Search resources, services, and docs box at the top of the portal, begin typing Web01. When the Web01 VM appears in the search results, select it. C2. Under SETTINGS, select Networking. Select Configure the application security groups, select the Security Group you created in Step A, and then select Save, as shown in the following picture: References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-filter-network-traffic Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 HOTSPOT You are evaluating the name resolution for the virtual machines after the planned implementation of the Azure networking infrastructure. For each of the following statements, select Yes if the statement is true. Otherwise, select No. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes All client computers in the Paris office will be joined to an Azure AD domain. A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 Box 2: Yes A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Box 3: No Only VMs in the registration network, here the ClientResources-VNet, will be able to register hostname records. References: https://docs.microsoft.com/en-us/azure/dns/private-dns-overview Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a consulting company that has a main office in Montreal and two branch offices in Seattle and New York. The Montreal office has 2,000 employees. The Seattle office has 1,000 employees. The New York office has 200 employees. All the resources used by Contoso are hosted on-premises. Contoso creates a new Azure subscription. The Azure Active Directory (Azure AD) tenant uses a domain named contoso.onmicrosoft.com. The tenant uses the P1 pricing tier. Existing Environment The network contains an Active Directory forest named contoso.com. All domain controllers are configured as DNS servers and host the contoso.com DNS zone. Contoso has finance, human resources, sales, research, and information technology departments. Each department has an organizational unit (OU) that contains all the accounts of that respective department. All the user accounts have the department attribute set to their respective department. New users are added frequently. Contoso.com contains a user named User1. All the offices connect by using private links. Contoso has data centers in the Montreal and Seattle offices. Each data center has a firewall that can be configured as a VPN device. All infrastructure servers are virtualized. The virtualization environment contains the servers in the following table. Contoso uses two web applications named App1 and App2. Each instance on each web application requires 1GB of memory. The Azure subscription contains the resources in the following table. The network security team implements several network security groups (NSGs). Planned Changes Contoso plans to implement the following changes: Deploy Azure ExpressRoute to the Montreal office. Migrate the virtual machines hosted on Server1 and Server2 to Azure. Synchronize on-premises Active Directory to Azure Active Directory (Azure AD). Migrate App1 and App2 to two Azure web apps named WebApp1 and WebApp2. Technical requirements Contoso must meet the following technical requirements: Ensure that WebApp1 can adjust the number of instances automatically based on the load and can scale up to five instances. Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. Ensure that routing information is exchanged automatically between Azure and the routers in the Montreal office. Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. Ensure that webapp2.azurewebsites.net can be accessed by using the name app2.contoso.com Connect the New York office to VNet1 over the Internet by using an encrypted connection. Create a workflow to send an email message when the settings of VM4 are modified. Create a custom Azure role named Role1 that is based on the Reader role. Minimize costs whenever possible. QUESTION 1 HOTSPOT You need to meet the connection requirements for the New York office. What should you do? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Create a virtual network gateway and a local network gateway. Azure VPN gateway. The VPN gateway service enables you to connect the VNet to the on-premises network through a VPN appliance. For more information, see Connect an on-premises network to a Microsoft Azure virtual network. The VPN gateway includes the following elements: Virtual network gateway. A resource that provides a virtual VPN appliance for the VNet. It is responsible for routing traffic from the on-premises network to the VNet. Local network gateway. An abstraction of the on-premises VPN appliance. Network traffic from the cloud application to the on-premises network is routed through this gateway. Connection. The connection has properties that specify the connection type (IPSec) and the key shared with the on-premises VPN appliance to encrypt traffic. Gateway subnet. The virtual network gateway is held in its own subnet, which is subject to various requirements, described in the Recommendations section below. Box 2: Configure a site-to-site VPN connection On premises create a site-to-site connection for the virtual network gateway and the local network gateway. Scenario: Connect the New York office to VNet1 over the Internet by using an encrypted connection. Incorrect Answers: Azure ExpressRoute: Established between your network and Azure, through an ExpressRoute partner. This connection is private. Traffic does not go over the internet. References: https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/hybrid-networking/vpn Question Set 1 QUESTION 1 You have an Azure Active Directory (Azure AD) tenant named contoso.onmicrosoft.com. The User administrator role is assigned to a user named Admin1. An external partner has a Microsoft account that uses the user1@outlook.com sign in. Admin1 attempts to invite the external partner to sign in to the Azure AD tenant and receives the following error message: \u201cUnable to invite user user1@outlook.com \u2013 Generic authorization exception.\u201d You need to ensure that Admin1 can invite the external partner to sign in to the Azure AD tenant. What should you do? A. From the Roles and administrators blade, assign the Security administrator role to Admin1. B. From the Organizational relationships blade, add an identity provider. C. From the Custom domain names blade, add a custom domain. D. From the Users blade, modify the External collaboration settings. Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://techcommunity.microsoft.com/t5/Azure-Active-Directory/Generic-authorization-exception-inviting- Azure-AD-gests/td-p/274742 QUESTION 2 Your company has an Azure Active Directory (Azure AD) tenant named contoso.com that is configured for hybrid coexistence with the on-premises Active Directory domain. The tenant contains the users shown in the following table. Whenever possible, you need to enable Azure Multi-Factor Authentication (MFA) for the users in contoso.com. Which users should you enable for Azure MFA? A. User1 only B. User1, User2, and User3 only C. User1 and User2 only D. User1, User2, User3, and User4 E. User2 only Correct Answer: D Section: [none] Explanation Explanation/Reference: QUESTION 3 You have two Azure Active Directory (Azure AD) tenants named contoso.com and fabrikam.com. You have a Microsoft account that you use to sign in to both tenants. You need to configure the default sign-in tenant for the Azure portal. What should you do? A. From Azure Cloud Shell, run Set-AzureRmSubscription. B. From Azure Cloud Shell, run Set-AzureRmContext. C. From the Azure portal, configure the portal settings. D. From the Azure portal, change the directory. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Set-AzureRmContext cmdlet sets authentication information for cmdlets that you run in the current session. The context includes tenant, subscription, and environment information. References: https://docs.microsoft.com/en-us/powershell/module/azurerm.profile/set-azurermcontext QUESTION 4 You have an Azure Active Directory (Azure AD) tenant. All administrators must enter a verification code to access the Azure portal. You need to ensure that the administrators can access the Azure portal only from your on-premises network. What should you configure? A. an Azure AD Identity Protection user risk policy. B. the multi-factor authentication service settings. C. the default for all the roles in Azure AD Privileged Identity Management D. an Azure AD Identity Protection sign-in risk policy Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 5 You have an Active Directory forest named contoso.com. You install and configure Azure AD Connect to use password hash synchronization as the single sign-on (SSO) method. Staging mode is enabled. You review the synchronization results and discover that the Synchronization Service Manager does not display any sync jobs. You need to ensure that the synchronization completes successfully. What should you do? A. Run Azure AD Connect and set the SSO method to Pass-through Authentication. B. From Synchronization Service Manager, run a full import. C. From Azure PowerShell, run Start-AdSyncSyncCycle \u2013PolicyType Initial. D. Run Azure AD Connect and disable staging mode. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Staging mode must be disabled. If the Azure AD Connect server is in staging mode, password hash synchronization is temporarily disabled. References: https://docs.microsoft.com/en-us/azure/active-directory/connect/active-directory-aadconnectsync- troubleshoot-password-hash-synchronization#no-passwords-are-synchronized-troubleshoot-by-using-the- troubleshooting-task QUESTION 6 You have an Azure Active Directory (Azure AD) tenant named contoso.onmicrosoft.com. You hire a temporary vendor. The vendor uses a Microsoft account that has a sign-in of user1@outlook.com. You need to ensure that the vendor can authenticate to the tenant by using user1@outlook.com. What should you do? A. From the Azure portal, add a custom domain name, create a new Azure AD user, and then specify user1@outlook.com as the username. B. From Azure Cloud Shell, run the New-AzureADUser cmdlet and specify the \u2013UserPrincipalName user1@outlook.com parameter. C. From the Azure portal, add a new guest user, and then specify user1@outlook.com as the email address. D. From Windows PowerShell, run the New-AzureADUser cmdlet and specify the \u2013UserPrincipalName user1@outlook.com parameter. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: UserPrincipalName - contains the UserPrincipalName (UPN) of this user. The UPN is what the user will use when they sign in into Azure AD. The common structure is @, so for Abby Brown in Contoso.com, the UPN would be AbbyB@contoso.com Example: To create the user, call the New-AzureADUser cmdlet with the parameter values: powershell New-AzureADUser -AccountEnabled $True -DisplayName \"Abby Brown\" -PasswordProfile $PasswordProfile -MailNickName \"AbbyB\" -UserPrincipalName \"AbbyB@contoso.com\" References: https://docs.microsoft.com/bs-cyrl-ba/powershell/azure/active-directory/new-user-sample?view=azureadps- 2.0 QUESTION 7 You have an Azure Active Directory (Azure AD) tenant named contosocloud.onmicrosoft.com. Your company has a public DNS zone for contoso.com. You add contoso.com as a custom domain name to Azure AD. You need to ensure that Azure can verify the domain name. Which type of DNS record should you create? A. MX B. SRV C. DNSKEY D. NSEC Correct Answer: A Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/dns/dns-web-sites-custom-domain QUESTION 8 You set the multi-factor authentication status for a user named admin1@contoso.com to Enabled. Admin1 accesses the Azure portal by using a web browser. Which additional security verifications can Admin1 use when accessing the Azure portal? A. a phone call, a text message that contains a verification code, and a notification or a verification code sent from the Microsoft Authenticator app B. an app password, a text message that contains a verification code, and a notification sent from the Microsoft Authenticator app C. an app password, a text message that contains a verification code, and a verification code sent from the Microsoft Authenticator app D. a phone call, an email message that contains a verification code, and a text message that contains an app password Correct Answer: A Section: [none] Explanation Explanation/Reference: QUESTION 9 DRAG DROP You have an Azure Active Directory (Azure AD) tenant that has the initial domain name. You have a domain name of contoso.com registered at a third-party registrar. You need to ensure that you can create Azure AD users that have names containing a suffix of @contoso.com. Which three actions should you perform in sequence? To answer, move the appropriate cmdlets from the list of cmdlets to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/dns/dns-web-sites-custom-domain QUESTION 10 Your company has a main office in London that contains 100 client computers. Three years ago, you migrated to Azure Active Directory (Azure AD). The company\u2019s security policy states that all personal devices and corporate-owned devices must be registered or joined to Azure AD. A remote user named User1 is unable to join a personal device to Azure AD from a home network. You verify that other users can join their devices to Azure AD. You need to ensure that User1 can join the device to Azure AD. What should you do? A. From the Device settings blade, modify the Users may join devices to Azure AD setting. B. From the Device settings blade, modify the Maximum number of devices per user setting. C. Create a point-to-site VPN from the home network of User1 to Azure. D. Assign the User administrator role to User1. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Maximum number of devices setting enables you to select the maximum number of devices that a user can have in Azure AD. If a user reaches this quota, they will not be able to add additional devices until one or more of the existing devices are removed. Incorrect Answers: A: The Users may join devices to Azure AD setting enables you to select the users who can join devices to Azure AD. Options are All, Selected and None. The default is All. C: Azure AD Join enables users to join their devices to Active Directory from anywhere as long as they have connectivity with the Internet. References: https://docs.microsoft.com/en-us/azure/active-directory/devices/device-management-azure-portal http://techgenix.com/pros-and-cons-azure-ad-join/ QUESTION 11 You have an Azure DNS zone named adatum.com. You need to delegate a subdomain named research.adatum.com to a different DNS server in Azure. What should you do? A. Create an A record named *.research in the adatum.com zone. B. Create a PTR record named research in the adatum.com zone. C. Modify the SOA record of adatum.com. D. Create an NS record named research in the adatum.com zone. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: You need to create a name server (NS) record for the zone. References: https://docs.microsoft.com/en-us/azure/dns/delegate-subdomain QUESTION 12 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 7 You plan to deploy several Azure virtual machines and to connect them to a virtual network named VNET1007. You need to ensure that future virtual machines on VNET1007 can register their name in an internal DNS zone named corp9172795.com. The zone must NOT be hosted on a virtual machine. What should you do from Azure Cloud Shell? To complete this task, start Azure Cloud Shell and select PowerShell (Linux). Click Show Advanced Settings, and then enter corp9172795n1 in the Storage account text box and File1 in the File share text box. Click Create storage, and then complete the task. Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: New-AzureRMResourceGroup -name MyResourceGroup Before you create the DNS zone, create a resource group to contain the DNS zone. Step 2: New-AzureRmDnsZone -Name corp9172795.com -ResourceGroupName MyResourceGroup A DNS zone is created by using the New-AzureRmDnsZone cmdlet. This creates a DNS zone called corp9172795.com in the resource group called MyResourceGroup. References: https://docs.microsoft.com/en-us/azure/dns/dns-getstarted-powershell QUESTION 13 HOTSPOT You have an Azure Active Directory (Azure AD) tenant named adatum.com. Adatum.com contains the groups in the following table: You create two user accounts that are configured as shown in the following table. To which groups do User1 and User2 belong? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Group 1 only First rule applies Box 2: Group1 and Group2 only Both membership rules apply. References: https://docs.microsoft.com/en-us/sccm/core/clients/manage/collections/create-collections Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a manufacturing company that has offices worldwide. Contoso works with partner organizations to bring products to market. Contoso products are manufactured by using blueprint files that the company authors and maintains. Existing Environment Currently, Contoso uses multiple types of servers for business operations, including the following: File servers Domain controllers Microsoft SQL Server servers Your network contains an Active Directory forest named contoso.com. All servers and client computers are joined to Active Directory. You have a public-facing application named App1. App1 is comprised of the following three tiers: A SQL database A web front end A processing middle tier Each tier is comprised of five virtual machines. Users access the web front end by using HTTPS only. Requirements Planned Changes Contoso plans to implement the following changes to the infrastructure: Move all the tiers of App1 to Azure. Move the existing product blueprint files to Azure Blob storage. Create a hybrid directory to support an upcoming Microsoft Office 365 migration project. Technical Requirements Contoso must meet the following technical requirements: Move all the virtual machines for App1 to Azure. Minimize the number of open ports between the App1 tiers. Ensure that all the virtual machines for App1 are protected by backups. Copy the blueprint files to Azure over the Internet. Ensure that the blueprint files are stored in the archive storage tier. Ensure that partner access to the blueprint files is secured and temporary. Prevent user passwords or hashes of passwprds from being stored in Azure. Use unmanaged standard storage for the hard disks of the virtual machines. Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity Minimize administrative effort whenever possible User Requirements Contoso identifies the following requirements for users: Ensure that only users who are part of a group named Pilot can join devices to Azure AD. Designate a new user named Admin1 as the service admin for the Azure subscription. Admin1 must receive email alerts regarding service outages. Ensure that a new user named User3 can create network objects for the Azure subscription. QUESTION 1 HOTSPOT You need to configure the Device settings to meet the technical requirements and the user requirements. Which two settings should you modify? To answer, select the appropriate settings in the answer area. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Selected Only selected users should be able to join devices Box 2: Yes Require Multi-Factor Auth to join devices. From scenario: Ensure that only users who are part of a group named Pilot can join devices to Azure AD Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity. QUESTION 2 You need to recommend a solution to automate the configuration for the finance department users. The solution must meet the technical requirements. What should you include in the recommendation? A. Azure AD B2C B. Azure AD Identity Protection C. an Azure logic app and the Microsoft Identity Management (MIM) client D. dynamic groups and conditional access policies Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Scenario: Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. The recommendation is to use conditional access policies that can then be targeted to groups of users, specific applications, or other conditions. References: https://docs.microsoft.com/en-us/azure/active-directory/authentication/howto-mfa-userstates QUESTION 3 HOTSPOT You need to implement Role1. Which command should you run before you create Role1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 You need to prepare the environment to meet the authentication requirements. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Install the Active Directory Federation Services (AD FS) role on a domain controller in the Miami office. B. Allow inbound TCP port 8080 to the domain controllers in the Miami office. C. Join the client computers in the Miami office to Azure AD. D. Add http://autologon.microsoftazuread-sso.com to the intranet zone of each client computer in the Miami office. E. Install Azure AD Connect on a server in the Miami office and enable Pass-through Authentication. Correct Answer: DE Section: [none] Explanation Explanation/Reference: Explanation: D: You can gradually roll out Seamless SSO to your users. You start by adding the following Azure AD URL to all or selected users' Intranet zone settings by using Group Policy in Active Directory: https:// autologon.microsoftazuread-sso.com E: Seamless SSO works with any method of cloud authentication - Password Hash Synchronization or Pass-through Authentication, and can be enabled via Azure AD Connect. Incorrect Answers: A: Seamless SSO is not applicable to Active Directory Federation Services (ADFS). B: Azure AD connect does not port 8080. It uses port 443. C: Seamless SSO needs the user's device to be domain-joined, but doesn't need for the device to be Azure AD Joined. Scenario: Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. Planned Azure AD Infrastructure include: The on-premises Active Directory domain will be synchronized to Azure AD. References: https://docs.microsoft.com/en-us/azure/active-directory/connect/active-directory-aadconnect-sso-quick-start","title":"Azure interview questions"},{"location":"nightwolf-cotribution/azure_interview_questions/#microsoft-azure-administrator","text":"QUESTION 1: You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Dev, you assign the Contributor role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: A Explanation: The Contributor role can manage all resources (and add resources) in a Resource Group. QUESTION 2 You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Subscription1, you assign the Logic App Operator role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: B Explanation: You would need the Logic App Contributor role. References: - https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles - https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 3 You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Dev, you assign the Logic App Contributor role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: A Explanation: The Logic App Contributor role lets you manage logic app, but not access to them. It provides access to view, edit, and update a logic app. References: - https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles - https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 4 You have an Azure subscription named Subscription1 that contains an Azure Log Analytics workspace named Workspace1. You need to view the error events from a table named Event. Which query should you run in Workspace1? A. Get-Event Event | where ($_.EventType \u2013eq \"error\") B. Get-Event Event | where ($_.EventType == \"error\") C. search in (Event) * | where EventType \u2013eq \"error\" D. search in (Event) \"error\" E. select *from Event where EventType == \"error\" F. Event | where EventType is \"error\" Correct Answer: D Explanation: To search a term in a specific table, add in (table-name) just after the search operator References: - https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/search-queries - https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-portal QUESTION 5 You have an Azure subscription named Subscription1. Subscription1 contains the resource groups in the following table. RG1 has a web app named WebApp1. WebApp1 is located in West Europe. You move WebApp1 to RG2. What is the effect of the move? A. The App Service plan for WebApp1 moves to North Europe. Policy2 applies to WebApp1. B. The App Service plan for WebApp1 remains in West Europe. Policy2 applies to WebApp1. C. The App Service plan for WebApp1 moves to North Europe. Policy1 applies to WebApp1. D. The App Service plan for WebApp1 remains in West Europe. Policy1 applies to WebApp1. Correct Answer: B Section: [none] Explanation Explanation: You can move an app to another App Service plan, as long as the source plan and the target plan are in the same resource group and geographical region. The region in which your app runs is the region of the App Service plan it's in. However, you cannot change an App Service plan's region. References: - https://docs.microsoft.com/en-us/azure/app-service/app-service-plan-manage QUESTION 6 You have an Azure subscription that contains a resource group named RG1. RG1 contains 100 virtual machines. Your company has three cost centers named Manufacturing, Sales, and Finance. You need to associate each virtual machine to a specific cost center. What should you do? A. Configure locks for the virtual machine. B. Add an extension to the virtual machines. C. Assign tags to the virtual machines. D. Modify the inventory settings of the virtual machine. Correct Answer: C Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/billing/billing-getting-started https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-using-tags QUESTION 7 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the Subscriptions blade, you select the subscription, and then click Programmatic deployment. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 8 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the Subscriptions blade, you select the subscription, and then click Resource providers. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 9 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure subscription named Subscription1. Subscription1 contains a resource group named RG1. RG1 contains resources that were deployed by using templates. You need to view the date and time when the resources were created in RG1. Solution: From the RG1 blade, you click Automation script. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 10 HOTSPOT You have an Azure subscription. You plan to use Azure Resource Manager templates to deploy 50 Azure virtual machines that will be part of the same availability set. You need to ensure that as many virtual machines as possible are available if the fabric fails or during servicing. How should you configure the template? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Use two fault domains. 2 or 3 is max value, depending on which region you are in. Use 20 for platformUpdateDomainCount Increasing the update domain (platformUpdateDomainCount) helps with capacity and availability planning when the platform reboots nodes. A higher number for the pool (20 is max) means that fewer of their nodes in any given availability set would be rebooted at once. References: https://www.itprotoday.com/microsoft-azure/check-if-azure-region-supports-2-or-3-fault-domains-managed- disks https://github.com/Azure/acs-engine/issues/1030 QUESTION 11 HOTSPOT You have an Azure subscription named Subscription1 that has a subscription ID of c276fc76-9cd4-44c9- 99a7-4fd71546436e. You need to create a custom RBAC role named CR1 that meets the following requirements: Can be assigned only to the resource groups in Subscription1 Prevents the management of the access permissions for the resource groups Allows the viewing, creating, modifying, and deleting of resource within the resource groups What should you specify in the assignable scopes and the permission elements of the definition of CR1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/role-based-access-control/custom-roles https://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider- operations#microsoftresources QUESTION 12 HOTSPOT You have an Azure Active Directory (Azure AD) tenant that contains three global administrators named Admin1, Admin2, and Admin3. The tenant is associated to an Azure subscription. Access control for the subscription is configured as shown in the Access control exhibit. (Click the Exhibit tab.) You sign in to the Azure portal as Admin1 and configure the tenant as shown in the Tenant exhibit. (Click the Exhibit tab.) For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 13 You have an Azure policy as shown in the following exhibit. What is the effect of the policy? A. You are prevented from creating Azure SQL Servers in ContosoRG1 only. B. You can create Azure SQL servers in ContosoRG1 only. C. You can create Azure SQL servers in any resource group within Subscription1. D. You are prevented from creating Azure SQL servers anywhere in Subscription1. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You are prevented from creating Azure SQL servers anywhere in Subscription 1 with the exception of ContosoRG1 QUESTION 14 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure Active Directory (Azure AD) tenant named Adatum and an Azure Subscription named Subscription1. Adatum contains a group named Developers. Subscription1 contains a resource group named Dev. You need to provide the Developers group with the ability to create Azure logic apps in the Dev resource group. Solution: On Subscription1, you assign the DevTest Labs User role to the Developers group. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: DevTest Labs User role only lets you connect, start, restart, and shutdown virtual machines in your Azure DevTest Labs. The Logic App Contributor role lets you manage logic app, but not access to them. It provides access to view, edit, and update a logic app. References: https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-securing-a-logic-app QUESTION 15 You have an Azure Active Directory (Azure AD) tenant that contains 5,000 user accounts. You create a new user account named AdminUser1. You need to assign the User administrator administrative role to AdminUser1. What should you do from the user account properties? A. From the Directory role blade, modify the directory role. B. From the Licenses blade, assign a new license. C. From the Groups blade, invite the user account to a new group. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Assign a role to a user 1. Sign in to the Azure portal with an account that's a global admin or privileged role admin for the directory. 2. Select Azure Active Directory, select Users, and then select a specific user from the list. 3. For the selected user, select Directory role, select Add role, and then pick the appropriate admin roles from the Directory roles list, such as Conditional access administrator. 4. Press Select to save. References: https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-users-assign-role- azure-portal QUESTION 16 HOTSPOT You have an Azure subscription named Subscription1. You plan to deploy an Ubuntu Server virtual machine named VM1 to Subscription1. You need to perform a custom deployment of the virtual machine. A specific trusted root certification authority (CA) must be added during the deployment. What should you do? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Cloud-init.txt Cloud-init.txt is used to customize a Linux VM on first boot up. It can be used to install packages and write files, or to configure users and security. No additional steps or agents are required to apply your configuration. Box 2: The az vm create command Once Cloud-init.txt has been created, you can deploy the VM with az vm create cmdlet, sing the --custom- data parameter to provide the full path to the cloud-init.txt file. References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/tutorial-automate-vm-deployment QUESTION 17 You have an Azure subscription named Subscription1. In Subscription1, you create an alert rule named Alert1. The Alert1 action group is configured as shown in the following exhibit. Alert1 alert criteria is triggered every minute. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: 60 One alert per minute will trigger one email per minute. Box 2: 12 No more than 1 SMS every 5 minutes can be send, which equals 12 per hour. Note: Rate limiting is a suspension of notifications that occurs when too many are sent to a particular phone number, email address or device. Rate limiting ensures that alerts are manageable and actionable. The rate limit thresholds are: SMS: No more than 1 SMS every 5 minutes. Voice: No more than 1 Voice call every 5 minutes. Email: No more than 100 emails in an hour. Other actions are not rate limited. References: https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/monitoring-and-diagnostics/monitoring- overview-alerts.md Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 You need to resolve the licensing issue before you attempt to assign the license again. What should you do? A. From the Groups blade, invite the user accounts to a new group. B. From the Profile blade, modify the usage location. C. From the Directory role blade, modify the directory role. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: License cannot be assigned to a user without a usage location specified. Scenario: Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License agreement failed for one user.\" You verify that the Azure subscription has the available licenses. QUESTION 2 You need to resolve the Active Directory issue. What should you do? A. Run the IdFix tool then use the Update actions. B. From Active Directory Domains and Trusts, modify the list of UPN suffixes. C. From Azure AD Connect, modify the outbound synchronization rule. D. From Active Directory Users and Computers, select the user accounts and then modify the UPN suffix value. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: IdFix is used to perform discovery and remediation of identity objects and their attributes in an on-premises Active Directory environment in preparation for migration to Azure Active Directory. IdFix is intended for the Active Directory administrators responsible for directory synchronization with Azure Active Directory. Scenario: Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. References: https://www.microsoft.com/en-us/download/details.aspx?id=36832 QUESTION 3 You need to define a custom domain name for Azure AD to support the planned infrastructure. Which domain name should you use? A. ad.humongousinsurance.com B. humingousinsurance.onmicrosoft.com C. humongousinsurance.com D. humongousinsurance.local Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: Every Azure AD directory comes with an initial domain name in the form of domainname.onmicrosoft.com. The initial domain name cannot be changed or deleted, but you can add your corporate domain name to Azure AD as well. For example, your organization probably has other domain names used to do business and users who sign in using your corporate domain name. Adding custom domain names to Azure AD allows you to assign user names in the directory that are familiar to your users, such as \u2018alice@contoso.com.\u2019 instead of 'alice@domain name.onmicrosoft.com'. Scenario: Network Infrastructure: Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com Planned Azure AD Infrastructure: The on-premises Active Directory domain will be synchronized to Azure AD. References: https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/add-custom-domain Question Set 1 QUESTION 1 You plan to use the Azure Import/Export service to copy files to a storage account. Which two files should you create before you prepare the drives for the import job? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. a driveset CSV file B. a JSON configuration file C. a PowerShell PS1 file D. an XML manifest file E. a dataset CSV file Correct Answer: AE Section: [none] Explanation Explanation/Reference: Explanation: A: Modify the driveset.csv file in the root folder where the tool resides. E: Modify the dataset.csv file in the root folder where the tool resides. Depending on whether you want to import a file or folder or both, add entries in the dataset.csv file References: https://docs.microsoft.com/en-us/azure/storage/common/storage-import-export-data-to-files QUESTION 2 DRAG DROP You have an on-premises file server named Server1 that runs Windows Server 2016. You have an Azure subscription that contains an Azure file share. You deploy an Azure File Sync Storage Sync Service, and you create a sync group. You need to synchronize files from Server1 to Azure. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Install the Azure File Sync agent on Server1 The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share Step 2: Register Server1. Register Windows Server with Storage Sync Service Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server (or cluster) and the Storage Sync Service. Step 3: Add a server endpoint Create a sync group and a cloud endpoint. A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud endpoint, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-sync-files-deployment-guide QUESTION 3 You create an Azure Storage account named contosostorage. You plan to create a file share named data. Users need to map a drive to the data file share from home computers that run Windows 10. Which outbound port should you open between the home computers and the data file share? A. 80 B. 443 C. 445 D. 3389 Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: Ensure port 445 is open: The SMB protocol requires TCP port 445 to be open; connections will fail if port 445 is blocked. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows QUESTION 4 HOTSPOT You have several Azure virtual machines on a virtual network named VNet1. You configure an Azure Storage account as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: always Endpoint status is enabled. Box 2: Never After you configure firewall and virtual network settings for your storage account, select Allow trusted Microsoft services to access this storage account as an exception to enable Azure Backup service to access the network restricted storage account. Reference: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows https://azure.microsoft.com/en-us/blog/azure-backup-now-supports-storage-accounts-secured-with-azure- storage-firewalls-and-virtual-networks/ QUESTION 5 HOTSPOT You have an Azure subscription named Subscription1 that contains the resources shown in the following table. The status of VM1 is Running. You assign an Azure policy as shown in the exhibit. (Click the Exhibit tab.) You assign the policy by using the following parameters: For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 6 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 2 You plan to store media files in the rg1lod9172796 storage account. You need to configure the storage account to store the media files. The solution must ensure that only users who have access keys can download the media files and that the files are accessible only over HTTPS. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: We should create an Azure file share. Step 1: In the Azure portal, select All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. On the Storage Accounts window that appears. Step 2: Locate the rg1lod9172796 storage account. Step 3: On the storage account page, in the Services section, select Files. Step 4: On the menu at the top of the File service page, click + File share. The New file share page drops down. Step 5: In Name type myshare. Click OK to create the Azure file share. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-portal QUESTION 7 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 1 You plan to migrate a large amount of corporate data to Azure Storage and to back up files stored on old hardware to Azure Storage. You need to create a storage account named corpdata9172795n1 in the corpdatalod9172795 resource group. The solution must meet the following requirements: Corpdata9172795n1 must be able to host the virtual disk files for Azure virtual machines. The cost of accessing the files must be minimized. Replication costs must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: In the Azure portal, click All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. Step 2: On the Storage Accounts window that appears, choose Add. Step 3: Select the subscription in which to create the storage account. Step 4: Under the Resource group field, select corpdatalod9172795. Step 5: Enter a name for your storage account: corpdata9172795n1 Step 6: For Account kind select: General-purpose v2 accounts (recommended for most scenarios) General-purpose v2 accounts is recommended for most scenarios. . General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. Step 7: For replication select: Read-access geo-redundant storage (RA-GRS) Read-access geo-redundant storage (RA-GRS) maximizes availability for your storage account. RA-GRS provides read-only access to the data in the secondary location, in addition to geo-replication across two regions. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview QUESTION 8 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 2 You plan to move backup files and documents from an on-premises Windows file server to Azure Storage. The backup files will be stored as blobs. You need to create a storage account named corpdata9172795n2. The solution must meet the following requirements: Ensure that the documents are accessible via drive mappings from Azure virtual machines that run Windows Server 2016. Provide the highest possible redundancy for the documents. Minimize storage access costs. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: In the Azure portal, click All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select Storage Accounts. Step 2: On the Storage Accounts window that appears, choose Add. Step 3: Select the subscription in which to create the storage account. Step 4: Under the Resource group field, select Create New. Create a new Resource Step 5: Enter a name for your storage account: corpdata9172795n2 Step 6: For Account kind select: General-purpose v2 accounts (recommended for most scenarios) General-purpose v2 accounts is recommended for most scenarios. General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. Step 7: For replication select: Read-access geo-redundant storage (RA-GRS) Read-access geo-redundant storage (RA-GRS) maximizes availability for your storage account. RA-GRS provides read-only access to the data in the secondary location, in addition to geo-replication across two regions. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview QUESTION 9 You have an Azure subscription that contains the resources in the following table. Store1 contains a file share named Data. Data contains 5,000 files. You need to synchronize the files in Data to an on-premises server named Server1. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Download an automation script. B. Register Server1. C. Create a sync group. D. Create a container instance. E. Install the Azure File Sync agent on Server1. Correct Answer: BCE Section: [none] Explanation Explanation/Reference: Explanation: Step 1 (E): Install the Azure File Sync agent on Server1 The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share Step 2 (B): Register Server1. Register Windows Server with Storage Sync Service Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server (or cluster) and the Storage Sync Service. Step 3 (C): Create a sync group and a cloud endpoint. A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud endpoint, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server. References: https://docs.microsoft.com/en-us/azure/storage/files/storage-sync-files-deployment-guide QUESTION 10 DRAG DROP You have an Azure subscription named Subscription1. You create an Azure Storage account named contosostorage, and then you create a file share named data. Which UNC path should you include in a script that references files from the data file share? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: contosostorage The name of account Box 2: file.core.windows.net Box 3: data The name of the file share is data. Example: References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows QUESTION 11 DRAG DROP You have an Azure subscription that contains a storage account. You have an on-premises server named Server1 that runs Windows Server 2016. Server1 has 2 TB of data. You need to transfer the data to the storage account by using the Azure Import/Export service. In which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order. NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: At a high level, an import job involves the following steps: Step 1: Attach an external disk to Server1 and then run waimportexport.exe Determine data to be imported, number of drives you need, destination blob location for your data in Azure storage. Use the WAImportExport tool to copy data to disk drives. Encrypt the disk drives with BitLocker. Step 2: From the Azure portal, create an import job. Create an import job in your target storage account in Azure portal. Upload the drive journal files. Step 3: Detach the external disks from Server1 and ship the disks to an Azure data center. Provide the return address and carrier account number for shipping the drives back to you. Ship the disk drives to the shipping address provided during job creation. Step 4: From the Azure portal, update the import job Update the delivery tracking number in the import job details and submit the import job. The drives are received and processed at the Azure data center. The drives are shipped using your carrier account to the return address provided in the import job. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-import-export-service QUESTION 12 You have the Azure virtual machines shown in the following table. You have a Recovery Services vault that protects VM1 and VM2. You need to protect VM3 and VM4 by using Recovery Services. What should you do first? A. Create a new backup policy. B. Configure the extensions for VM3 and VM4. C. Create a storage account. D. Create a new Recovery Services vault. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: A Recovery Services vault is a storage entity in Azure that houses data. The data is typically copies of data, or configuration information for virtual machines (VMs), workloads, servers, or workstations. You can use Recovery Services vaults to hold backup data for various Azure services References: https://docs.microsoft.com/en-us/azure/site-recovery/azure-to-azure-tutorial-enable-replication QUESTION 13 HOTSPOT You have an Azure subscription named Subscription1 that is associated to an Azure Active Directory (Azure AD) tenant named AAD1. Subscription1 contains the objects in the following table. You plan to create a single backup policy for Vault1. To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Share1 only Box 2: 99 years With the latest update to Azure Backup, customers can retain their data for up to 99 years in Azure. Note: A backup policy defines a matrix of when the data snapshots are taken, and how long those snapshots are retained. The backup policy interface looks like this: References: https://docs.microsoft.com/en-us/azure/backup/backup-azure-files https://docs.microsoft.com/en-us/azure/backup/backup-azure-vms-first-look-arm#defining-a-backup-policy https://blogs.microsoft.com/firehose/2015/02/16/february-update-to-azure-backup-includes-data-retention- up-to-99-years-offline-backup-and-more/ QUESTION 14 HOTSPOT You have an Azure subscription named Subscription1. In Subscription1, you create an Azure file share named share1. You create a shared access signature (SAS) named SAS1 as shown in the following exhibit. To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Will have no access The IP 193.77.134.1 does not have access on the SAS. Box 2: Will have read, write, and list access The net use command is used to connect to file shares. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1 https://docs.microsoft.com/en-us/azure/vs-azure-tools-storage-manage-with-storage-explorer? tabs=windows QUESTION 15 HOTSPOT You have Azure Storage accounts as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: storageaccount1 and storageaccount2 only Box 2: All the storage accounts Note: The three different storage account options are: General-purpose v2 (GPv2) accounts, General- purpose v1 (GPv1) accounts, and Blob storage accounts. General-purpose v2 (GPv2) accounts are storage accounts that support all of the latest features for blobs, files, queues, and tables. Blob storage accounts support all the same block blob features as GPv2, but are limited to supporting only block blobs. General-purpose v1 (GPv1) accounts provide access to all Azure Storage services, but may not have the latest features or the lowest per gigabyte pricing. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-account-options QUESTION 16 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 1 You plan to prevent users from accidentally deleting blob data from Azure. You need to ensure that administrators can recover any blob data that is deleted accidentally from the storagelod9272261 storage account for 14 days after the deletion occurred. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Task A: Create a Recovery Services vault (if a vault already exists skip this task, go to Task B below) A1. From Azure Portal, On the Hub menu, click All services and in the list of resources, type Recovery Services and click Recovery Services vaults. If there are recovery services vaults in the subscription, the vaults are listed. A2. On the Recovery Services vaults menu, click Add. A3. The Recovery Services vault blade opens, prompting you to provide a Name, Subscription, Resource group, and Location Task B. Create a backup goal B1. On the Recovery Services vault blade (for the vault you just created), in the Getting Started section, click Backup, then on the Getting Started with Backup blade, select Backup goal. The Backup Goal blade opens. If the Recovery Services vault has been previously configured, then the Backup Goal blades opens when you click Backup on the Recovery Services vault blade. B2. From the Where is your workload running? drop-down menu, select Azure. B3. From the What do you want to backup? menu, select Blob Storage, and click OK. B4. Finish the Wizard. Task C. create a backup schedule C1. Open the Microsoft Azure Backup agent. You can find it by searching your machine for Microsoft Azure Backup. C2. In the Backup agent's Actions pane, click Schedule Backup to launch the Schedule Backup Wizard. C3. On the Getting started page of the Schedule Backup Wizard, click Next. C4. On the Select Items to Backup page, click Add Items. The Select Items dialog opens. C5. Select Blob Storage you want to protect, and then click OK. C6. In the Select Items to Backup page, click Next. On the Specify Backup Schedule page, specify Schedule a backup every day, and click Next. C7. On the Select Retention Policy page, set it to 14 days, and click Next. C8. Finish the Wizard. References: https://docs.microsoft.com/en-us/azure/backup/backup-configure-vault QUESTION 17 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 2 Your company plans to store several documents on a public website. You need to create a container named bios that will host the documents in the storagelod9272261 storage account. The solution must ensure anonymous access and must ensure that users can browse folders in the container. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Azure portal create public container To create a container in the Azure portal, follow these steps: Step 1: Navigate to your new storage account in the Azure portal. Step 2: In the left menu for the storage account, scroll to the lob service section, then select Blobs. Select the + Container button. Type a name for your new container: bios Set the level of public access to the container: Select anonymous access. Step 3: Select OK to create the container. References: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal QUESTION 18 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 3 Your company plans to host in Azure the source files of several line-of-business applications. You need to create an Azure file share named corpsoftware in the storagelod9272261 storage account. The solution must ensure that corpsoftware can store only up to 250 GB of data. What should you do from the Azure portal? Correct Answer: See explanation below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Go to the Storage Account blade on the Azure portal: Step 2: Click on add File Share button: Step 3: Provide Name (storagelod9272261) and Quota (250 GB). References: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-create-file-share QUESTION 19 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 6 You recently created a virtual machine named Web01. You need to attach a new 80-GB standard data disk named Web01-Disk1 to Web01. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Add a data disk Step 1: In the Azure portal, from the menu on the left, select Virtual machines. Step 2: Select the Web01 virtual machine from the list. Step 3: On the Virtual machine page, , in Essentials, select Disks. Step 4: On the Disks page, select the Web01-Disk1 from the list of existing disks. Step 5: In the Disks pane, click + Add data disk. Step 6: Click the drop-down menu for Name to view a list of existing managed disks accessible to your Azure subscription. Select the managed disk Web01-Disk1 to attach: References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/attach-disk-portal Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a manufacturing company that has offices worldwide. Contoso works with partner organizations to bring products to market. Contoso products are manufactured by using blueprint files that the company authors and maintains. Existing Environment Currently, Contoso uses multiple types of servers for business operations, including the following: File servers Domain controllers Microsoft SQL Server servers Your network contains an Active Directory forest named contoso.com. All servers and client computers are joined to Active Directory. You have a public-facing application named App1. App1 is comprised of the following three tiers: A SQL database A web front end A processing middle tier Each tier is comprised of five virtual machines. Users access the web front end by using HTTPS only. Requirements Planned Changes Contoso plans to implement the following changes to the infrastructure: Move all the tiers of App1 to Azure. Move the existing product blueprint files to Azure Blob storage. Create a hybrid directory to support an upcoming Microsoft Office 365 migration project. Technical Requirements Contoso must meet the following technical requirements: Move all the virtual machines for App1 to Azure. Minimize the number of open ports between the App1 tiers. Ensure that all the virtual machines for App1 are protected by backups. Copy the blueprint files to Azure over the Internet. Ensure that the blueprint files are stored in the archive storage tier. Ensure that partner access to the blueprint files is secured and temporary. Prevent user passwords or hashes of passwprds from being stored in Azure. Use unmanaged standard storage for the hard disks of the virtual machines. Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity Minimize administrative effort whenever possible User Requirements Contoso identifies the following requirements for users: Ensure that only users who are part of a group named Pilot can join devices to Azure AD. Designate a new user named Admin1 as the service admin for the Azure subscription. Admin1 must receive email alerts regarding service outages. Ensure that a new user named User3 can create network objects for the Azure subscription. QUESTION 1 HOTSPOT You need to identify the storage requirements for Contoso. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes Contoso is moving the existing product blueprint files to Azure Blob storage. Use unmanaged standard storage for the hard disks of the virtual machines. We use Page Blobs for these. Box 2: No Box 3: No QUESTION 2 You need to move the blueprint files to Azure. What should you do? A. Use Azure Storage Explorer to copy the files. B. Use the Azure Import/Export service. C. Generate a shared access signature (SAS). Map a drive, and then copy the files by using File Explorer. D. Generate an access key. Map a drive, and then copy the files by using File Explorer. Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Azure Storage Explorer is a free tool from Microsoft that allows you to work with Azure Storage data on Windows, macOS, and Linux. You can use it to upload and download data from Azure blob storage. Scenario: Planned Changes include: move the existing product blueprint files to Azure Blob storage. Technical Requirements include: Copy the blueprint files to Azure over the Internet. References: https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-data-to-azure- blob-using-azure-storage-explorer QUESTION 3 You need to implement a backup solution for App1 after the application is moved. What should you create first? A. a recovery plan B. a Recovery Services vault C. an Azure Backup Server D. a backup policy Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: A Recovery Services vault is a logical container that stores the backup data for each protected resource, such as Azure VMs. When the backup job for a protected resource runs, it creates a recovery point inside the Recovery Services vault. Scenario: There are three application tiers, each with five virtual machines. Move all the virtual machines for App1 to Azure. Ensure that all the virtual machines for App1 are protected by backups. References: https://docs.microsoft.com/en-us/azure/backup/quick-backup-vm-portal Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 DRAG DROP You need to prepare the environment to ensure that the web administrators can deploy the web apps as quickly as possible. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: First you create a storage account using the Azure portal. Step 2: Select Automation options at the bottom of the screen. The portal shows the template on the Template tab. Add the storage account to the library. Step 3: Share the template. Scenario: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. References: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-quickstart-create- templates-use-the-portal Question Set 1 QUESTION 1 You plan to automate the deployment of a virtual machine scale set that uses the Windows Server 2016 Datacenter image. You need to ensure that when the scale set virtual machines are provisioned, they have web server components installed. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Modify the extensionProfile section of the Azure Resource Manager template. B. Create an automation account. C. Upload a configuration script. D. Create a new virtual machine scale set in the Azure portal. E. Create an Azure policy. Correct Answer: AD Section: [none] Explanation Explanation/Reference: Explanation: Virtual Machine Scale Sets can be used with the Azure Desired State Configuration (DSC) extension handler. Virtual machine scale sets provide a way to deploy and manage large numbers of virtual machines, and can elastically scale in and out in response to load. DSC is used to configure the VMs as they come online so they are running the production software. References: https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-dsc QUESTION 2 DRAG DROP You have two Azure virtual machines named VM1 and VM2. VM1 has a single data disk named Disk1. You need to attach Disk1 to VM2. The solution must minimize downtime for both virtual machines. Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Stop VM1. Step 2: Detach Disk1 from VM1. Step 3: Attach Disk1 to VM2 Attach an existing disk Follow these steps to reattach an existing available data disk to a running VM. 1. Select a running VM for which you want to reattach a data disk. 2. From the menu on the left, select Disks. 3. Select Attach existing to attach an available data disk to the VM. 4. From the Attach existing disk pane, select OK. Step 4: Start VM1. Detach a data disk using the portal 1. In the left menu, select Virtual Machines. 2. Select the virtual machine that has the data disk you want to detach and click Stop to deallocate the VM. 3. In the virtual machine pane, select Disks. 4. At the top of the Disks pane, select Edit. 5. In the Disks pane, to the far right of the data disk that you would like to detach, click the Detach button image detach button. 6. After the disk has been removed, click Save on the top of the pane. 7. In the virtual machine pane, click Overview and then click the Start button at the top of the pane to restart the VM. 8. The disk stays in storage but is no longer attached to a virtual machine. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/detach-disk https://docs.microsoft.com/en-us/azure/lab-services/devtest-lab-attach-detach-data-disk QUESTION 3 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains a virtual machine named VM1. You install and configure a web server and a DNS server on VM1. VM1 has the effective network security rules shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Rule2 blocks ports 50-60, which includes port 53, the DNS port. Internet users can reach the Web server, since it uses port 80. Box 2: If Rule2 is removed internet users can reach the DNS server as well. Note: Rules are processed in priority order, with lower numbers processed before higher numbers, because lower numbers have higher priority. Once traffic matches a rule, processing stops. As a result, any rules that exist with lower priorities (higher numbers) that have the same attributes as rules with higher priorities are not processed. References: https://docs.microsoft.com/en-us/azure/virtual-network/security-overview QUESTION 4 DRAG DROP You have an Azure Linux virtual machine that is protected by Azure Backup. One week ago, two files were deleted from the virtual machine. You need to restore the deleted files to an on-premises computer as quickly as possible. Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: To restore files or folders from the recovery point, go to the virtual machine and choose the desired recovery point. Step 0. In the virtual machine's menu, click Backup to open the Backup dashboard. Step 1. In the Backup dashboard menu, click File Recovery. Step 2. From the Select recovery point drop-down menu, select the recovery point that holds the files you want. By default, the latest recovery point is already selected. Step 3: To download the software used to copy files from the recovery point, click Download Executable (for Windows Azure VM) or Download Script (for Linux Azure VM, a python script is generated). Step 4: Copy the files by using AzCopy AzCopy is a command-line utility designed for copying data to/from Microsoft Azure Blob, File, and Table storage, using simple commands designed for optimal performance. You can copy data between a file system and a storage account, or between storage accounts. References: https://docs.microsoft.com/en-us/azure/backup/backup-azure-restore-files-from-vm https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy QUESTION 5 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 3 You plan to protect on-premises virtual machines and Azure virtual machines by using Azure Backup. You need to prepare the backup infrastructure in Azure. The solution must minimize the cost of storing the backups in Azure. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: First, create Recovery Services vault. Step 1: On the left-hand menu, select All services and in the services list, type Recovery Services. As you type, the list of resources filters. When you see Recovery Services vaults in the list, select it to open the Recovery Services vaults menu. Step 2: In the Recovery Services vaults menu, click Add to open the Recovery Services vault menu. Step 3: In the Recovery Services vault menu, for example, Type myRecoveryServicesVault in Name. The current subscription ID appears in Subscription. If you have additional subscriptions, you could choose another subscription for the new vault. For Resource group select Use existing and choose myResourceGroup. If myResourceGroup doesn't exist, select Create new and type myResourceGroup. From the Location drop-down menu, choose West Europe. Click Create to create your Recovery Services vault. References: https://docs.microsoft.com/en-us/azure/backup/tutorial-backup-vm-at-scale QUESTION 6 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 3 You need to deploy two Azure virtual machines named VM1003a and VM1003b based on an Ubuntu Server image. The deployment must meet the following requirements: Provide a Service Level Agreement (SLA) of 99.95 percent availability. Use managed disks. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Open the Azure portal. Step 2: On the left menu, select All resources. You can sort the resources by Type to easily find your images. Step 3: Select the image you want to use from the list. The image Overview page opens. Step 4: Select Create VM from the menu. Step 5: Enter the virtual machine information. Select VM1003a as the name for the first Virtual machine. The user name and password entered here will be used to log in to the virtual machine. When complete, select OK. You can create the new VM in an existing resource group, or choose Create new to create a new resource group to store the VM. Step 6: Select a size for the VM. To see more sizes, select View all or change the Supported disk type filter. Step 7: Under Settings, make changes as necessary and select OK. Step 8: On the summary page, you should see your image name listed as a Private image. Select Ok to start the virtual machine deployment. Repeat the procedure for the second VM and name it VM1003b. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/create-vm-generalized-managed QUESTION 7 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 4 You need to deploy an Azure virtual machine named VM1004a based on an Ubuntu Server image, and then configure VM1004a to meet the following requirements: The virtual machine must contain data disks that can store at least 15 TB of data. The data disks must be able to provide at least 2.000 IOPS. Storage costs must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Open the Azure portal. Step 2: On the left menu, select All resources. You can sort the resources by Type to easily find your images. Step 3: Select the image you want to use from the list. The image Overview page opens. Step 4: Select Create VM from the menu. Step 5: Enter the virtual machine information. Select VM1004a as the name for the first Virtual machine. The user name and password entered here will be used to log in to the virtual machine. When complete, select OK. You can create the new VM in an existing resource group, or choose Create new to create a new resource group to store the VM. Step 6: Select a size for the VM. To see more sizes, select View all or change the Supported disk type filter. To support 15 TB of data you would need a Premium disk. Step 7: Under Settings, make changes as necessary and select OK. Step 8: On the summary page, you should see your image name listed as a Private image. Select Ok to start the virtual machine deployment. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/create-vm-generalized-managed QUESTION 8 You have an Azure subscription that contains a virtual machine named VM1. VM1 hosts a line-of-business application that is available 24 hours a day. VM1 has one network interface and one managed disk. VM1 uses the D4s v3 size. You plan to make the following changes to VM1: Change the size to D8s v3. Add a 500-GB managed disk. Add the Puppet Agent extension. Attach an additional network interface. Which change will cause downtime for VM1? A. Add the Puppet Agent extension. B. Change the size to D8s v3. C. Add a 500-GB managed disk. D. Attach an additional network interface. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: While resizing the VM it must be in a stopped state. References: https://azure.microsoft.com/en-us/blog/resize-virtual-machines/ QUESTION 9 You have an Azure virtual machine named VM1 that you use for testing. VM1 is protected by Azure Backup. You delete VM1. You need to remove the backup data stored for VM1. What should you do first? A. Delete the Recovery Services vault. B. Delete the storage account. C. Stop the backup D. Modify the backup policy. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Azure Backup provides backup for virtual machines \u2014 created through both the classic deployment model and the Azure Resource Manager deployment model \u2014 by using custom-defined backup policies in a Recovery Services vault. With the release of backup policy management, customers can manage backup policies and model them to meet their changing requirements from a single window. Customers can edit a policy, associate more virtual machines to a policy, and delete unnecessary policies to meet their compliance requirements. Incorrect Answers: B: You can't delete a Recovery Services vault if it is registered to a server and holds backup data. If you try to delete a vault, but can't, the vault is still configured to receive backup data. References: https://azure.microsoft.com/en-in/updates/azure-vm-backup-policy-management/ QUESTION 10 You have an Azure subscription named Subscription1. You deploy a Linux virtual machine named VM1 to Subscription1. You need to monitor the metrics and the logs of VM1. What should you use? A. the AzurePerformanceDiagnostics extension B. Azure HDInsight C. Linux Diagnostic Extension (LAD) 3.0 D. Azure Analysis Services Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: You can use extensions to configure diagnostics on your VMs to collect additional metric data. The basic host metrics are available, but to see more granular and VM-specific metrics, you need to install the Azure diagnostics extension on the VM. The Azure diagnostics extension allows additional monitoring and diagnostics data to be retrieved from the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/linux/tutorial-monitoring QUESTION 11 DRAG DROP You have an availability set named AS1 that contains three virtual machines named VM1, VM2, and VM3. You attempt to reconfigure VM1 to use a larger size. The operation fails and you receive an allocation failure message. You need to ensure that the resize operation succeeds. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Stop VM1, VM, and VM3. If the VM you wish to resize is part of an availability set, then you must stop all VMs in the availability set before changing the size of any VM in the availability set. The reason all VMs in the availability set must be stopped before performing the resize operation to a size that requires different hardware is that all running VMs in the availability set must be using the same physical hardware cluster. Therefore, if a change of physical hardware cluster is required to change the VM size then all VMs must be first stopped and then restarted one-by-one to a different physical hardware clusters. Step 2: Resize VM1. Step 3: Start VM1, VM2, and VM3. References: https://azure.microsoft.com/es-es/blog/resize-virtual-machines/ QUESTION 12 You plan to back up an Azure virtual machine named VM1. You discover that the Backup Pre-Check status displays a status of Warning. What is a possible cause of the Warning status? A. VM1 is stopped. B. VM1 does not have the latest version of WaAppAgent.exe installed. C. VM1 has an unmanaged disk. D. A Recovery Services vault is unavailable. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Warning state indicates one or more issues in VM\u2019s configuration that might lead to backup failures and provides recommended steps to ensure successful backups. Not having the latest VM Agent installed, for example, can cause backups to fail intermittently and falls in this class of issues. References: https://azure.microsoft.com/en-us/blog/azure-vm-backup-pre-checks/ QUESTION 13 You have an Azure subscription named Subscription1 that is used by several departments at your company. Subscription1 contains the resources in the following table. Another administrator deploys a virtual machine named VM1 and an Azure Storage account named Storage2 by using a single Azure Resource Manager template. You need to view the template used for the deployment. From which blade can you view the template that was used for the deployment? A. Container1 B. RG1 C. VM1 D. Storage2 Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: View template from deployment history 1. Go to the resource group for your new resource group. Notice that the portal shows the result of the last deployment. Select this link. 2. You see a history of deployments for the group. In your case, the portal probably lists only one deployment. Select this deployment. 3. The portal displays a summary of the deployment. The summary includes the status of the deployment and its operations and the values that you provided for parameters. To see the template that you used for the deployment, select View template. References: https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-export-template QUESTION 14 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Overview blade, you move the virtual machine to a different subscription. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You would need to redeploy the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 15 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Redeploy blade, you click Redeploy. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: When you redeploy a VM, it moves the VM to a new node within the Azure infrastructure and then powers it back on, retaining all your configuration options and associated resources. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 16 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You have an Azure virtual machine named VM1. VM1 was deployed by using a custom Azure Resource Manager template named ARM1.json. You receive a notification that VM1 will be affected by maintenance. You need to move VM1 to a different host immediately. Solution: From the Update management blade, you click Enable. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: You would need to redeploy the VM. References: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node QUESTION 17 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains two Azure virtual machines named VM1 and VM2. VM1 and VM2 run Windows Server 2016. VM1 is backed up daily by Azure Backup without using the Azure Backup agent. VM1 is affected by ransomware that encrypts data. You need to restore the latest backup of VM1. To which location can you restore the backup? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 18 You download an Azure Resource Manager template based on an existing virtual machine. The template will be used to deploy 100 virtual machines. You need to modify the template to reference an administrative password. You must prevent the password from being stored in plain text. What should you create to store the password? A. an Azure Key Vault and an access policy B. a Recovery Services vault and a backup policy C. Azure Active Directory (AD) Identity Protection and an Azure policy D. an Azure Storage account and an access policy Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: You can use a template that allows you to deploy a simple Windows VM by retrieving the password that is stored in a Key Vault. Therefore, the password is never put in plain text in the template parameter file. References: https://azure.microsoft.com/en-us/resources/templates/101-vm-secure-password/ QUESTION 19 HOTSPOT You create a virtual machine scale set named Scale1. Scale1 is configured as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: The Autoscale scale out rule increases the number of VMs by 2 if the CPU threshold is 80% or higher. The initial instance count is 4 and rises to 6 when the 2 extra instances of VMs are added. Box 2: The Autoscale scale in rule decreases the number of VMs by 4 if the CPU threshold is 30% or lower. The initial instance count is 4 and thus cannot be reduced to 0 as the minimum instances is set to 2. Instances are only added when the CPU threshold reaches 80%. References: https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-overview https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-best-practices https://docs.microsoft.com/en-us/azure/azure-monitor/platform/autoscale-common-scale-patterns QUESTION 20 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 4 You plan to back up all the Azure virtual machines in your Azure subscription at 02:00 Coordinated Universal Time (UTC) daily. You need to prepare the Azure environment to ensure that any new virtual machines can be configured quickly for backup. The solution must ensure that all the daily backups performed at 02:00 UTC are stored for only 90 days. What should you do from your Recovery Services vault on the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Task A: Create a Recovery Services vault (if a vault already exists skip this task, go to Task B below) A1. From Azure Portal, On the Hub menu, click All services and in the list of resources, type Recovery Services and click Recovery Services vaults. If there are recovery services vaults in the subscription, the vaults are listed. A2. On the Recovery Services vaults menu, click Add. A3. The Recovery Services vault blade opens, prompting you to provide a Name, Subscription, Resource group, and Location Task B. B1. On the Recovery Services vault blade (for the vault you just created), in the Getting Started section, click Backup, then on the Getting Started with Backup blade, select Backup goal. The Backup Goal blade opens. If the Recovery Services vault has been previously configured, then the Backup Goal blades opens when you click Backup on the Recovery Services vault blade. B2. From the Where is your workload running? drop-down menu, select Azure. B3. From the What do you want to backup? menu, select Virtual Machine, and click OK. B4. Finish the Wizard. Task C. create a backup schedule C1. Open the Microsoft Azure Backup agent. You can find it by searching your machine for Microsoft Azure Backup. C2. In the Backup agent's Actions pane, click Schedule Backup to launch the Schedule Backup Wizard. C3. On the Getting started page of the Schedule Backup Wizard, click Next. C4. On the Select Items to Backup page, click Add Items. The Select Items dialog opens. C5. Select Blob Storage you want to protect, and then click OK. C6. In the Select Items to Backup page, click Next. On the Specify Backup Schedule page, specify Schedule a backup every: day At the following times: 2.00 AM C7. On the Select Retention Policy page, set it to 90 days, and click Next. C8. Finish the Wizard. References: https://docs.microsoft.com/en-us/azure/backup/backup-configure-vault QUESTION 21 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task5 You plan to connect several virtual machines to the VNET01-USEA2 virtual network. In the Web-RGlod9272261 resource group, you need to create a virtual machine that uses the Standard_B2ms size named Web01 that runs Windows Server 2016. Web01 must be added to an availability set. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Choose Create a resource in the upper left-hand corner of the Azure portal. Step 2: In the Basics tab, under Project details, make sure the correct subscription is selected and then choose Web-RGlod9272261 resource group Step 3: Under Instance details type/select: Virtual machine name: Web01 Image: Windows Server 2016 Size: Standard_B2ms size Leave the other defaults. Step 4: Finish the Wizard Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a consulting company that has a main office in Montreal and two branch offices in Seattle and New York. The Montreal office has 2,000 employees. The Seattle office has 1,000 employees. The New York office has 200 employees. All the resources used by Contoso are hosted on-premises. Contoso creates a new Azure subscription. The Azure Active Directory (Azure AD) tenant uses a domain named contoso.onmicrosoft.com. The tenant uses the P1 pricing tier. Existing Environment The network contains an Active Directory forest named contoso.com. All domain controllers are configured as DNS servers and host the contoso.com DNS zone. Contoso has finance, human resources, sales, research, and information technology departments. Each department has an organizational unit (OU) that contains all the accounts of that respective department. All the user accounts have the department attribute set to their respective department. New users are added frequently. Contoso.com contains a user named User1. All the offices connect by using private links. Contoso has data centers in the Montreal and Seattle offices. Each data center has a firewall that can be configured as a VPN device. All infrastructure servers are virtualized. The virtualization environment contains the servers in the following table. Contoso uses two web applications named App1 and App2. Each instance on each web application requires 1GB of memory. The Azure subscription contains the resources in the following table. The network security team implements several network security groups (NSGs). Planned Changes Contoso plans to implement the following changes: Deploy Azure ExpressRoute to the Montreal office. Migrate the virtual machines hosted on Server1 and Server2 to Azure. Synchronize on-premises Active Directory to Azure Active Directory (Azure AD). Migrate App1 and App2 to two Azure web apps named WebApp1 and WebApp2. Technical requirements Contoso must meet the following technical requirements: Ensure that WebApp1 can adjust the number of instances automatically based on the load and can scale up to five instances. Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. Ensure that routing information is exchanged automatically between Azure and the routers in the Montreal office. Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. Ensure that webapp2.azurewebsites.net can be accessed by using the name app2.contoso.com Connect the New York office to VNet1 over the Internet by using an encrypted connection. Create a workflow to send an email message when the settings of VM4 are modified. Create a custom Azure role named Role1 that is based on the Reader role. Minimize costs whenever possible. QUESTION 1 You discover that VM3 does NOT meet the technical requirements. You need to verify whether the issue relates to the NSGs. What should you use? A. Diagram in VNet1 B. the security recommendations in Azure Advisor C. Diagnostic settings in Azure Monitor D. Diagnose and solve problems in Traffic Manager profiles E. IP flow verify in Azure Network Watcher Correct Answer: E Section: [none] Explanation Explanation/Reference: Explanation: Scenario: Contoso must meet technical requirements including: Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. IP flow verify checks if a packet is allowed or denied to or from a virtual machine. The information consists of direction, protocol, local IP, remote IP, local port, and remote port. If the packet is denied by a security group, the name of the rule that denied the packet is returned. While any source or destination IP can be chosen, IP flow verify helps administrators quickly diagnose connectivity issues from or to the internet and from or to the on-premises environment. References: https://docs.microsoft.com/en-us/azure/network-watcher/network-watcher-ip-flow-verify-overview Question Set 1 QUESTION 1 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You modify the name servers at the domain registrar. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Modify the Name Server (NS) record. References: https://docs.microsoft.com/en-us/azure/dns/dns-delegate-domain-azure-dns QUESTION 2 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You modify the SOA record in the contoso.com zone. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Modify the NS record, not the SOA record. Note: The SOA record stores information about the name of the server that supplied the data for the zone; the administrator of the zone; the current version of the data file; the number of seconds a secondary name server should wait before checking for updates; the number of seconds a secondary name server should wait before retrying a failed zone transfer; the maximum number of seconds that a secondary name server can use data before it must either be refreshed or expire; and a default number of seconds for the time-to- live file on resource records. References: https://searchnetworking.techtarget.com/definition/start-of-authority-record QUESTION 3 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. Your company registers a domain name of contoso.com. You create an Azure DNS zone named contoso.com, and then you add an A record to the zone for a host named www that has an IP address of 131.107.1.10. You discover that Internet hosts are unable to resolve www.contoso.com to the 131.107.1.10 IP address. You need to resolve the name resolution issue. Solution: You add an NS record to the contoso.com Azure DNS zone. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: Explanation: Before you can delegate your DNS zone to Azure DNS, you need to know the name servers for your zone. The NS record set contains the names of the Azure DNS name servers assigned to the zone. References: https://docs.microsoft.com/en-us/azure/dns/dns-delegate-domain-azure-dns QUESTION 4 You are troubleshooting a performance issue for an Azure Application Gateway. You need to compare the total requests to the failed requests during the past six hours. What should you use? A. NSG flow logs in Azure Network Watcher B. Metrics in Application Gateway C. Connection monitor in Azure Network Watcher D. Diagnostics logs in Application Gateway Correct Answer: B Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-diagnostics#metrics QUESTION 5 You have two subscriptions named Subscription1 and Subscription2. Each subscription is associated to a different Azure AD tenant. Subscription1 contains a virtual network named VNet1. VNet1 contains an Azure virtual machine named VM1 and has an IP address space of 10.0.0.0/16. Subscription2 contains a virtual network named VNet2. VNet2 contains an Azure virtual machine named VM2 and has an IP address space of 10.10.0.0/24. You need to connect VNet1 to VNet2. What should you do first? A. Move VM1 to Subscription2. B. Modify the IP address space of VNet2. C. Provision virtual network gateways. D. Move VNet1 to Subscription2. Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: The virtual networks can be in the same or different regions, and from the same or different subscriptions. When connecting VNets from different subscriptions, the subscriptions do not need to be associated with the same Active Directory tenant. Configuring a VNet-to-VNet connection is a good way to easily connect VNets. Connecting a virtual network to another virtual network using the VNet-to-VNet connection type (VNet2VNet) is similar to creating a Site- to-Site IPsec connection to an on-premises location. Both connectivity types use a VPN gateway to provide a secure tunnel using IPsec/IKE, and both function the same way when communicating. The local network gateway for each VNet treats the other VNet as a local site. This lets you specify additional address space for the local network gateway in order to route traffic. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager- portal QUESTION 6 DRAG DROP You have an Azure subscription that contains two virtual networks named VNet1 and VNet2. Virtual machines connect to the virtual networks. The virtual networks have the address spaces and the subnets configured as shown in the following table. You need to add the address space of 10.33.0.0/16 to VNet1. The solution must ensure that the hosts on VNet1 and VNet2 can communicate. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Remove peering between Vnet1 and VNet2. You can't add address ranges to, or delete address ranges from a virtual network's address space once a virtual network is peered with another virtual network. To add or remove address ranges, delete the peering, add or remove the address ranges, then re-create the peering. Step 2: Add the 10.44.0.0/16 address space to VNet1. Step 3: Recreate peering between VNet1 and VNet2 References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-manage-peering QUESTION 7 You have an Azure subscription that contains the resources in the following table. To which subnets can you apply NSG1? A. the subnets on VNet2 only B. the subnets on VNet2 and VNet3 only C. the subnets on VNet1, VNet2, and VNet3 D. the subnets on VNet1 only E. the subnets on VNet3 only Correct Answer: E Section: [none] Explanation Explanation/Reference: Explanation: All Azure resources are created in an Azure region and subscription. A resource can only be created in a virtual network that exists in the same region and subscription as the resource. References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-vnet-plan-design-arm QUESTION 8 HOTSPOT You have an Azure virtual machine named VM1 that connects to a virtual network named VNet1. VM1 has the following configurations: Subnet 10.0.0.0/24 Availability set: AVSet Network security group (NSG): None Private IP address: 10.0.0.4 (dynamic) Public IP address: 40.90.219.6 (dynamic) You deploy a standard, Internet-facing load balancer named slb1. You need to configure slb1 to allow connectivity to VM1. Which changes should you apply to VM1 as you configure slb1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 9 You have five Azure virtual machines that run Windows Server 2016. The virtual machines are configured as web servers. You have an Azure load balancer named LB1 that provides load balancing services for the virtual machines. You need to ensure that visitors are serviced by the same web server for each request. What should you configure? A. Protocol to UDP B. Session persistence to None C. Session persistence to Client IP D. Idle Time-out (minutes) to 20 Correct Answer: C Section: [none] Explanation Explanation/Reference: Explanation: You can set the sticky session in load balancer rules with setting the session persistence as the client IP. References: https://cloudopszone.com/configure-azure-load-balancer-for-sticky-sessions/ QUESTION 10 You have the Azure virtual networks shown in the following table. To which virtual networks can you establish a peering connection from VNet1? A. VNet2 and VNet3 only B. VNet2 only C. VNet3 and VNet4 only D. VNet2, VNet3, and VNet4 Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-connect-virtual-networks-portal QUESTION 11 You have an Azure subscription that contains a policy-based virtual network gateway named GW1 and a virtual network named VNet1. You need to ensure that you can configure a point-to-site connection from VNet1 to an on-premises computer. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Reset GW1. B. Create a route-based virtual network gateway. C. Delete GW1. D. Add a public IP address space to VNet1. E. Add a connection to GW1. F. Add a service endpoint to VNet1. Correct Answer: BC Section: [none] Explanation Explanation/Reference: Explanation: B: A VPN gateway is used when creating a VPN connection to your on-premises network. Route-based VPN devices use any-to-any (wildcard) traffic selectors, and let routing/forwarding tables direct traffic to different IPsec tunnels. It is typically built on router platforms where each IPsec tunnel is modeled as a network interface or VTI (virtual tunnel interface). C: Policy-based VPN devices use the combinations of prefixes from both networks to define how traffic is encrypted/decrypted through IPsec tunnels. It is typically built on firewall devices that perform packet filtering. IPsec tunnel encryption and decryption are added to the packet filtering and processing engine. Incorrect Answers: D: Point-to-Site connections do not require a VPN device or a public-facing IP address. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/create-routebased-vpn-gateway-portal https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-connect-multiple-policybased-rm-ps QUESTION 12 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains the virtual networks in the following table. Subscription1 contains the virtual machines in the following table. The firewalls on all the virtual machines are configured to allow all ICMP traffic. You add the peerings in the following table. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes Vnet1 and Vnet3 are peers. Box 2: Yes Vnet2 and Vnet3 are peers. Box 3: No Peering connections are non-transitive. References: https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/hybrid-networking/hub-spoke QUESTION 13 You have an Azure subscription named Subscription1 that contains the resource groups shown in the following table. In RG1, you create a virtual machine named VM1 in the East Asia location. You plan to create a virtual network named VNET1. You need to create VNET1, and then connect VM1 to VNET1. What are two possible ways to achieve this goal? Each correct answer presents a complete solution. NOTE: Each correct selection is worth one point. A. Create VNET1 in RG2, and then set East Asia as the location. B. Create VNET1 in a new resource group in the West US location, and then set West US as the location. C. Create VNET1 in RG1, and then set East US as the location. D. Create VNET1 in RG2, and then set East US as the location. E. Create VNET1 in RG1, and then set East Asia as the location. Correct Answer: AE Section: [none] Explanation Explanation/Reference: QUESTION 14 You have an Azure subscription that contains a virtual network named VNet1. VNet1 contains four subnets named Gateway, Perimeter, NVA, and Production. The NVA subnet contains two network virtual appliances (NVAs) that will perform network traffic inspection between the Perimeter subnet and the Production subnet. You need to implement an Azure load balancer for the NVAs. The solution must meet the following requirements: The NVAs must run in an active-active configuration that uses automatic failover. The NVAs must load balance traffic to two services on the Profuction subnet. The services have different IP addresses. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Add two load balancing rules that have HA Ports enabled and Floating IP disabled. B. Add a frontend IP configuration, two backend pools, and a health probe. C. Add two load balancing rules that have HA Ports and Floating IP enabled. D. Deploy a standard load balancer. E. Deploy a basic load balancer. F. Add a frontend IP configuration a backend pool, and a health probe. Correct Answer: BCD Section: [none] Explanation Explanation/Reference: Explanation: A standard load balancer is required for the HA ports. Two backend pools are needed as there are two services with different IP addresses. Floating IP rule is used where backend ports are reused. Incorrect Answers: F: HA Ports are not available for the basic load balancer. References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-overview https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-multivip-overview QUESTION 15 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 1 Your on-premises network uses an IP address range of 131.107.2.0 to 131.107.2.255. You need to ensure that only device from the on-premises network can connect to the rg1lod9172796n1 storage account. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Navigate to the rg1lod9172796n1 storage account. Step 2: Click on the settings menu called Firewalls and virtual networks. Step 3: Ensure that you have elected to allow access from 'Selected networks'. Step 4: To grant access to an internet IP range, enter the address range of 131.107.2.0 to 131.107.2.255 (in CIDR format) under Firewall, Address Ranges. References: https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security QUESTION 16 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 4 Another administrator attempts to establish connectivity between two virtual networks named VNET1 and VNET2. The administrator reports that connections across the virtual networks fail. You need to ensure that network connections can be established successfully between VNET1 and VNET2 as quickly as possible. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can connect one VNet to another VNet using either a Virtual network peering, or an Azure VPN Gateway. To create a virtual network gateway Step 1: In the portal, on the left side, click +Create a resource and type 'virtual network gateway' in search. Locate Virtual network gateway in the search return and click the entry. On the Virtual network gateway page, click Create at the bottom of the page to open the Create virtual network gateway page. Step 2: On the Create virtual network gateway page, fill in the values for your virtual network gateway. Name: Name your gateway. This is not the same as naming a gateway subnet. It's the name of the gateway object you are creating. Gateway type: Select VPN. VPN gateways use the virtual network gateway type VPN. Virtual network: Choose the virtual network to which you want to add this gateway. Click Virtual network to open the 'Choose a virtual network' page. Select the VNet. If you don't see your VNet, make sure the Location field is pointing to the region in which your virtual network is located. Gateway subnet address range: You will only see this setting if you did not previously create a gateway subnet for your virtual network. If you previously created a valid gateway subnet, this setting will not appear. Step 4: Select Create New to create a Gateway subnet. Step 5: Click Create to begin creating the VPN gateway. The settings are validated and you'll see the \"Deploying Virtual network gateway\" tile on the dashboard. Creating a gateway can take up to 45 minutes. You may need to refresh your portal page to see the completed status. References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-howto-vnet-vnet-resource-manager- portal? QUESTION 17 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 5 You plan to configure VM1 to be accessible from the Internet. You need to add a public IP address to the network interface used by VM1. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can add private and public IP addresses to an Azure network interface by completing the steps that follow. Step 1: In Azure portal, click More services > type virtual machines in the filter box, and then click Virtual machines. Step 2: In the Virtual machines pane, click the VM you want to add IP addresses to. Click Network interfaces in the virtual machine pane that appears, and then select the network interface you want to add the IP addresses to. In the example shown in the following picture, the NIC named myNIC from the VM named myVM is selected: Step 3: In the pane that appears for the NIC you selected, click IP configurations. Step 4: Click Create public IP address. Step 5: In the Create public IP address pane that appears, enter a Name, select an IP address assignment type, a Subscription, a Resource group, and a Location, then click Create, as shown in the following picture: References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-multiple-ip-addresses-portal QUESTION 18 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 6 You need to allow RDP connections over TCP port 3389 to VM1 from the Internet. The solution must prevent connections from the Internet over all other TCP ports. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Create a new network security group Step 2: Select your new network security group. Step 3: Select Inbound security rules. Under Add inbound security rule, enter the following Destination: Select Network security group, and then select the security group you created previously. Destination port ranges: 3389 Protocol: Select TCP References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-filter-network-traffic QUESTION 19 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 5 You plan to create 100 Azure virtual machines on each of the following three virtual networks: VNET1005a VNET1005b VNET1005c All the network traffic between the three virtual networks will be routed through VNET1005a. You need to create the virtual networks, and then to ensure that all the Azure virtual machines can connect to other virtual machines by using their private IP address. The solution must NOT require any virtual network gateways and must minimize the number of peerings. What should you do from the Azure portal before you configure IP routing? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Click Create a resource in the portal. Step 2: Enter Virtual network in the Search the Marketplace box at the top of the New pane that appears. Click Virtual network when it appears in the search results. Step 3: Select Classic in the Select a deployment model box in the Virtual Network pane that appears, then click Create. Step 4: Enter the following values on the Create virtual network (classic) pane and then click Create: Name: VNET1005a Address space: 10.0.0.0/16 Subnet name: subnet0 Resource group: Create new Subnet address range: 10.0.0.0/24 Subscription and location: Select your subscription and location. Step 5: Repeat steps 3-5 for VNET1005b (10.1.0.0/16, 10.1.0.0/24), and for VNET1005c 10.2.0.0/16, 10.2.0.0/24). References: https://docs.microsoft.com/en-us/azure/virtual-network/create-virtual-network-classic QUESTION 20 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 6 You plan to create several virtual machines in different availability zones, and then to configure the virtual machines for load balanced connections from the Internet. You need to create an IP address resource named ip1006 to support the planned load balancing solution. The solution must minimize costs. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: We should create a public IP address. Step 1: At the top, left corner of the portal, select + Create a resource. Step 2: Enter public ip address in the Search the Marketplace box. When Public IP address appears in the search results, select it. Step 3: Under Public IP address, select Create. Step 4: Enter, or select values for the following settings, under Create public IP address, then select Create: Name: ip1006 SKU: Basic SKU IP Version: IPv6 IP address assignment: Dynamic Subscription: Select appropriate Resource group: Select appropriate References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-public-ip-address QUESTION 21 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 8 You need to create a virtual network named VNET1008 that contains three subnets named subnet0, subnet1, and subnet2. The solution must meet the following requirements: Connections from any of the subnets to the Internet must be blocked. Connections from the Internet to any of the subnets must be blocked. The number of network security groups (NSGs) and NSG rules must be minimized. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: Click Create a resource in the portal. Step 2: Enter Virtual network in the Search the Marketplace box at the top of the New pane that appears. Click Virtual network when it appears in the search results. Step 3: Select Classic in the Select a deployment model box in the Virtual Network pane that appears, then click Create. Step 4: Enter the following values on the Create virtual network (classic) pane and then click Create: Name: VNET1008 Address space: 10.0.0.0/16 Subnet name: subnet0 Resource group: Create new Subnet address range: 10.0.0.0/24 Subscription and location: Select your subscription and location. Step 5: In the portal, you can create only one subnet when you create a virtual network. Click Subnets (in the SETTINGS section) on the Create virtual network (classic) pane that appears. Click +Add on the VNET1008 - Subnets pane that appears. Step 6: Enter subnet1 for Name on the Add subnet pane. Enter 10.0.1.0/24 for Address range. Click OK. Step 7: Create the third subnet: Click +Add on the VNET1008 - Subnets pane that appears. Enter subnet2 for Name on the Add subnet pane. Enter 10.0.2.0/24 for Address range. Click OK. References: https://docs.microsoft.com/en-us/azure/virtual-network/create-virtual-network-classic QUESTION 22 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Azure Network Watcher, you create a connection monitor. Does this meet the goal? A. Yes B. No Correct Answer: A Section: [none] Explanation Explanation/Reference: References: https://azure.microsoft.com/en-us/updates/general-availability-azure-network-watcher-connection-monitor- in-all-public-regions/ QUESTION 23 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Azure Network Watcher, you create a packet capture. Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Use the Connection Monitor feature of Azure Network Watcher. References: https://azure.microsoft.com/en-us/updates/general-availability-azure-network-watcher-connection-monitor- in-all-public-regions/ QUESTION 24 HOTSPOT Subscription1 contains the virtual machines in the following table. In Subscription1, you create a load balancer that has the following configurations: Name: LB1 SKU: Basic Type: Internal Subnet: Subnet12 Virtual network: VNET1 For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-overview QUESTION 25 You have an Azure subscription named Subscription1 that contains two Azure virtual networks named VNet1 and VNet2. VNet1 contains a VPN gateway named VPNGW1 that uses static routing. There is a site- to-site VPN connection between your on-premises network and VNet1. On a computer named Client1 that runs Windows 10, you configure a point-to-site VPN connection to VNet1. You configure virtual network peering between VNet1 and VNet2. You verify that you can connect to VNet2 from the on-premises network. Client1 is unable to connect to VNet2. You need to ensure that you can connect Client1 to VNet2. What should you do? A. Select Allow gateway transit on VNet2. B. Enable BGP on VPNGW1. C. Select Allow gateway transit on VNet1. D. Download and re-install the VPN client configuration package on Client1. Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-point-to-site-routing QUESTION 26 Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You manage a virtual network named VNet1 that is hosted in the West US Azure region. VNet1 hosts two virtual machines named VM1 and VM2 that run Windows Server. You need to inspect all the network traffic from VM1 to VM2 for a period of three hours. Solution: From Performance Monitor, you create a Data Collector Set (DCS). Does this meet the goal? A. Yes B. No Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: Use the Connection Monitor feature of Azure Network Watcher. References: https://docs.microsoft.com/en-us/azure/network-watcher/network-watcher-monitoring-overview QUESTION 27 HOTSPOT You have a virtual network named VNet1 that has the configuration shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: add an address space Your IaaS virtual machines (VMs) and PaaS role instances in a virtual network automatically receive a private IP address from a range that you specify, based on the address space of the subnet they are connected to. We need to add the 192.168.1.0/24 address space. Box 2: add a network interface The 10.2.1.0/24 network exists. We need to add a network interface. References: https://docs.microsoft.com/en-us/office365/enterprise/designing-networking-for-microsoft-azure-iaas https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-static-private-ip-arm-pportal QUESTION 28 HOTSPOT You have an Azure subscription named Subscription1. Subscription1 contains the virtual machines in the following table. Subscription1 contains a virtual network named VNet1 that has the subnets in the following table. VM3 has multiple network adapters, including a network adapter named NIC3. IP forwarding is enabled on NIC3. Routing is enabled on VM3. You create a route table named RT1 that contains the routes in the following table. You apply RT1 to Subnet1 and Subnet2. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: IP forwarding enables the virtual machine a network interface is attached to: Receive network traffic not destined for one of the IP addresses assigned to any of the IP configurations assigned to the network interface. Send network traffic with a different source IP address than the one assigned to one of a network interface's IP configurations. The setting must be enabled for every network interface that is attached to the virtual machine that receives traffic that the virtual machine needs to forward. A virtual machine can forward traffic whether it has multiple network interfaces or a single network interface attached to it. Box 1: Yes The routing table allows connections from VM3 to VM1 and VM2. And as IP forwarding is enabled on VM3, VM3 can connect to VM1. Box 2: No VM3, which has IP forwarding, must be turned on, in order for VM2 to connect to VM1. Box 3: Yes The routing table allows connections from VM1 and VM2 to VM3. IP forwarding on VM3 allows VM1 to connect to VM2 via VM3. References: https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-udr-overview https://www.quora.com/What-is-IP-forwarding QUESTION 29 You have an Azure subscription that contains the resources in the following table. VM1 and VM2 are deployed from the same template and host line-of-business applications accessed by using Remote Desktop. You configure the network security group (NSG) shown in the exhibit. (Click the Exhibit tab.) You need to prevent users of VM2 and VM2 from accessing websites on the Internet over TCP port 80. What should you do? A. Change the DenyWebSites outbound security rule. B. Change the Port_80 inbound security rule. C. Disassociate the NSG from a network interface. D. Associate the NSG to Subnet1. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: You can associate or dissociate a network security group from a network interface or subnet. The NSG has the appropriate rule to block users from accessing the Internet. We just need to associate it with Subnet1. References: https://docs.microsoft.com/en-us/azure/virtual-network/manage-network-security-group QUESTION 30 HOTSPOT You are creating an Azure load balancer. You need to add an IPv6 load balancing rule to the load balancer. How should you complete the Azure PowerShell script? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-ipv6-internet-ps QUESTION 31 HOTSPOT You have an Azure subscription named Subscription1 that contains a resource group named RG1. In RG1, you create an internal load balancer named LB1 and a public load balancer named LB2. You need to ensure that an administrator named Admin1 can manage LB1 and LB2. The solution must follow the principle of least privilege. Which role should you assign to Admin1 for each task? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: QUESTION 32 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 7 You plan to allow connections between the VNET01-USEA2 and VNET01-USWE2 virtual networks. You need to ensure that virtual machines can communicate across both virtual networks by using their private IP address. The solution must NOT require any virtual network gateways. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Virtual network peering enables you to seamlessly connect two Azure virtual networks. Once peered, the virtual networks appear as one, for connectivity purposes. Peer virtual networks Step 1. In the Search box at the top of the Azure portal, begin typing VNET01-USEA2. When VNET01- USEA2 appears in the search results, select it. Step 2. Select Peerings, under SETTINGS, and then select + Add, as shown in the following picture: Step 3. Enter, or select, the following information, accept the defaults for the remaining settings, and then select OK. Name: myVirtualNetwork1-myVirtualNetwork2 (for example) Subscription: elect your subscription. Virtual network: VNET01-USWE2 - To select the VNET01-USWE2 virtual network, select Virtual network, then select VNET01-USWE2. You can select a virtual network in the same region or in a different region. Now we need to repeat steps 1-3 for the other network VNET01-USWE2: Step 4. In the Search box at the top of the Azure portal, begin typing VNET01- USEA2. When VNET01- USEA2 appears in the search results, select it. Step 5. Select Peerings, under SETTINGS, and then select + Add. References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-connect-virtual-networks-portal QUESTION 33 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Use the following login credentials as needed: Azure Username: XXXXXXX Azure Password: XXXXXXX The following information is for technical support purposes only: Lab Instance: 9172796 Task 8 You plan to host several secured websites on Web01. You need to allow HTTPS over TCP port 443 to Web01 and to prevent HTTP over TCP port 80 to Web01. What should you do from the Azure portal? Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: You can filter network traffic to and from Azure resources in an Azure virtual network with a network security group. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. Step A: Create a network security group A1. Search for and select the resource group for the VM, choose Add, then search for and select Network security group. A2. Select Create. The Create network security group window opens. A3. Create a network security group Enter a name for your network security group. Select or create a resource group, then select a location. A4. Select Create to create the network security group. Step B: Create an inbound security rule to allows HTTPS over TCP port 443 B1. Select your new network security group. B2. Select Inbound security rules, then select Add. B3. Add inbound rule B4. Select Advanced. From the drop-down menu, select HTTPS. You can also verify by clicking Custom and selecting TCP port, and 443. B5. Select Add to create the rule. Repeat step B2-B5 to deny TCP port 80 B6. Select Inbound security rules, then select Add. B7. Add inbound rule B8. Select Advanced. Clicking Custom and selecting TCP port, and 80. B9. Select Deny. Step C: Associate your network security group with a subnet Your final step is to associate your network security group with a subnet or a specific network interface. C1. In the Search resources, services, and docs box at the top of the portal, begin typing Web01. When the Web01 VM appears in the search results, select it. C2. Under SETTINGS, select Networking. Select Configure the application security groups, select the Security Group you created in Step A, and then select Save, as shown in the following picture: References: https://docs.microsoft.com/en-us/azure/virtual-network/tutorial-filter-network-traffic Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 HOTSPOT You are evaluating the name resolution for the virtual machines after the planned implementation of the Azure networking infrastructure. For each of the following statements, select Yes if the statement is true. Otherwise, select No. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Yes All client computers in the Paris office will be joined to an Azure AD domain. A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 Box 2: Yes A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Box 3: No Only VMs in the registration network, here the ClientResources-VNet, will be able to register hostname records. References: https://docs.microsoft.com/en-us/azure/dns/private-dns-overview Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a consulting company that has a main office in Montreal and two branch offices in Seattle and New York. The Montreal office has 2,000 employees. The Seattle office has 1,000 employees. The New York office has 200 employees. All the resources used by Contoso are hosted on-premises. Contoso creates a new Azure subscription. The Azure Active Directory (Azure AD) tenant uses a domain named contoso.onmicrosoft.com. The tenant uses the P1 pricing tier. Existing Environment The network contains an Active Directory forest named contoso.com. All domain controllers are configured as DNS servers and host the contoso.com DNS zone. Contoso has finance, human resources, sales, research, and information technology departments. Each department has an organizational unit (OU) that contains all the accounts of that respective department. All the user accounts have the department attribute set to their respective department. New users are added frequently. Contoso.com contains a user named User1. All the offices connect by using private links. Contoso has data centers in the Montreal and Seattle offices. Each data center has a firewall that can be configured as a VPN device. All infrastructure servers are virtualized. The virtualization environment contains the servers in the following table. Contoso uses two web applications named App1 and App2. Each instance on each web application requires 1GB of memory. The Azure subscription contains the resources in the following table. The network security team implements several network security groups (NSGs). Planned Changes Contoso plans to implement the following changes: Deploy Azure ExpressRoute to the Montreal office. Migrate the virtual machines hosted on Server1 and Server2 to Azure. Synchronize on-premises Active Directory to Azure Active Directory (Azure AD). Migrate App1 and App2 to two Azure web apps named WebApp1 and WebApp2. Technical requirements Contoso must meet the following technical requirements: Ensure that WebApp1 can adjust the number of instances automatically based on the load and can scale up to five instances. Ensure that VM3 can establish outbound connections over TCP port 8080 to the applications servers in the Montreal office. Ensure that routing information is exchanged automatically between Azure and the routers in the Montreal office. Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. Ensure that webapp2.azurewebsites.net can be accessed by using the name app2.contoso.com Connect the New York office to VNet1 over the Internet by using an encrypted connection. Create a workflow to send an email message when the settings of VM4 are modified. Create a custom Azure role named Role1 that is based on the Reader role. Minimize costs whenever possible. QUESTION 1 HOTSPOT You need to meet the connection requirements for the New York office. What should you do? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Create a virtual network gateway and a local network gateway. Azure VPN gateway. The VPN gateway service enables you to connect the VNet to the on-premises network through a VPN appliance. For more information, see Connect an on-premises network to a Microsoft Azure virtual network. The VPN gateway includes the following elements: Virtual network gateway. A resource that provides a virtual VPN appliance for the VNet. It is responsible for routing traffic from the on-premises network to the VNet. Local network gateway. An abstraction of the on-premises VPN appliance. Network traffic from the cloud application to the on-premises network is routed through this gateway. Connection. The connection has properties that specify the connection type (IPSec) and the key shared with the on-premises VPN appliance to encrypt traffic. Gateway subnet. The virtual network gateway is held in its own subnet, which is subject to various requirements, described in the Recommendations section below. Box 2: Configure a site-to-site VPN connection On premises create a site-to-site connection for the virtual network gateway and the local network gateway. Scenario: Connect the New York office to VNet1 over the Internet by using an encrypted connection. Incorrect Answers: Azure ExpressRoute: Established between your network and Azure, through an ExpressRoute partner. This connection is private. Traffic does not go over the internet. References: https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/hybrid-networking/vpn Question Set 1 QUESTION 1 You have an Azure Active Directory (Azure AD) tenant named contoso.onmicrosoft.com. The User administrator role is assigned to a user named Admin1. An external partner has a Microsoft account that uses the user1@outlook.com sign in. Admin1 attempts to invite the external partner to sign in to the Azure AD tenant and receives the following error message: \u201cUnable to invite user user1@outlook.com \u2013 Generic authorization exception.\u201d You need to ensure that Admin1 can invite the external partner to sign in to the Azure AD tenant. What should you do? A. From the Roles and administrators blade, assign the Security administrator role to Admin1. B. From the Organizational relationships blade, add an identity provider. C. From the Custom domain names blade, add a custom domain. D. From the Users blade, modify the External collaboration settings. Correct Answer: D Section: [none] Explanation Explanation/Reference: References: https://techcommunity.microsoft.com/t5/Azure-Active-Directory/Generic-authorization-exception-inviting- Azure-AD-gests/td-p/274742 QUESTION 2 Your company has an Azure Active Directory (Azure AD) tenant named contoso.com that is configured for hybrid coexistence with the on-premises Active Directory domain. The tenant contains the users shown in the following table. Whenever possible, you need to enable Azure Multi-Factor Authentication (MFA) for the users in contoso.com. Which users should you enable for Azure MFA? A. User1 only B. User1, User2, and User3 only C. User1 and User2 only D. User1, User2, User3, and User4 E. User2 only Correct Answer: D Section: [none] Explanation Explanation/Reference: QUESTION 3 You have two Azure Active Directory (Azure AD) tenants named contoso.com and fabrikam.com. You have a Microsoft account that you use to sign in to both tenants. You need to configure the default sign-in tenant for the Azure portal. What should you do? A. From Azure Cloud Shell, run Set-AzureRmSubscription. B. From Azure Cloud Shell, run Set-AzureRmContext. C. From the Azure portal, configure the portal settings. D. From the Azure portal, change the directory. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Set-AzureRmContext cmdlet sets authentication information for cmdlets that you run in the current session. The context includes tenant, subscription, and environment information. References: https://docs.microsoft.com/en-us/powershell/module/azurerm.profile/set-azurermcontext QUESTION 4 You have an Azure Active Directory (Azure AD) tenant. All administrators must enter a verification code to access the Azure portal. You need to ensure that the administrators can access the Azure portal only from your on-premises network. What should you configure? A. an Azure AD Identity Protection user risk policy. B. the multi-factor authentication service settings. C. the default for all the roles in Azure AD Privileged Identity Management D. an Azure AD Identity Protection sign-in risk policy Correct Answer: B Section: [none] Explanation Explanation/Reference: QUESTION 5 You have an Active Directory forest named contoso.com. You install and configure Azure AD Connect to use password hash synchronization as the single sign-on (SSO) method. Staging mode is enabled. You review the synchronization results and discover that the Synchronization Service Manager does not display any sync jobs. You need to ensure that the synchronization completes successfully. What should you do? A. Run Azure AD Connect and set the SSO method to Pass-through Authentication. B. From Synchronization Service Manager, run a full import. C. From Azure PowerShell, run Start-AdSyncSyncCycle \u2013PolicyType Initial. D. Run Azure AD Connect and disable staging mode. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Staging mode must be disabled. If the Azure AD Connect server is in staging mode, password hash synchronization is temporarily disabled. References: https://docs.microsoft.com/en-us/azure/active-directory/connect/active-directory-aadconnectsync- troubleshoot-password-hash-synchronization#no-passwords-are-synchronized-troubleshoot-by-using-the- troubleshooting-task QUESTION 6 You have an Azure Active Directory (Azure AD) tenant named contoso.onmicrosoft.com. You hire a temporary vendor. The vendor uses a Microsoft account that has a sign-in of user1@outlook.com. You need to ensure that the vendor can authenticate to the tenant by using user1@outlook.com. What should you do? A. From the Azure portal, add a custom domain name, create a new Azure AD user, and then specify user1@outlook.com as the username. B. From Azure Cloud Shell, run the New-AzureADUser cmdlet and specify the \u2013UserPrincipalName user1@outlook.com parameter. C. From the Azure portal, add a new guest user, and then specify user1@outlook.com as the email address. D. From Windows PowerShell, run the New-AzureADUser cmdlet and specify the \u2013UserPrincipalName user1@outlook.com parameter. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: UserPrincipalName - contains the UserPrincipalName (UPN) of this user. The UPN is what the user will use when they sign in into Azure AD. The common structure is @, so for Abby Brown in Contoso.com, the UPN would be AbbyB@contoso.com Example: To create the user, call the New-AzureADUser cmdlet with the parameter values: powershell New-AzureADUser -AccountEnabled $True -DisplayName \"Abby Brown\" -PasswordProfile $PasswordProfile -MailNickName \"AbbyB\" -UserPrincipalName \"AbbyB@contoso.com\" References: https://docs.microsoft.com/bs-cyrl-ba/powershell/azure/active-directory/new-user-sample?view=azureadps- 2.0 QUESTION 7 You have an Azure Active Directory (Azure AD) tenant named contosocloud.onmicrosoft.com. Your company has a public DNS zone for contoso.com. You add contoso.com as a custom domain name to Azure AD. You need to ensure that Azure can verify the domain name. Which type of DNS record should you create? A. MX B. SRV C. DNSKEY D. NSEC Correct Answer: A Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/dns/dns-web-sites-custom-domain QUESTION 8 You set the multi-factor authentication status for a user named admin1@contoso.com to Enabled. Admin1 accesses the Azure portal by using a web browser. Which additional security verifications can Admin1 use when accessing the Azure portal? A. a phone call, a text message that contains a verification code, and a notification or a verification code sent from the Microsoft Authenticator app B. an app password, a text message that contains a verification code, and a notification sent from the Microsoft Authenticator app C. an app password, a text message that contains a verification code, and a verification code sent from the Microsoft Authenticator app D. a phone call, an email message that contains a verification code, and a text message that contains an app password Correct Answer: A Section: [none] Explanation Explanation/Reference: QUESTION 9 DRAG DROP You have an Azure Active Directory (Azure AD) tenant that has the initial domain name. You have a domain name of contoso.com registered at a third-party registrar. You need to ensure that you can create Azure AD users that have names containing a suffix of @contoso.com. Which three actions should you perform in sequence? To answer, move the appropriate cmdlets from the list of cmdlets to the answer area and arrange them in the correct order. Select and Place: Correct Answer: Section: [none] Explanation Explanation/Reference: References: https://docs.microsoft.com/en-us/azure/dns/dns-web-sites-custom-domain QUESTION 10 Your company has a main office in London that contains 100 client computers. Three years ago, you migrated to Azure Active Directory (Azure AD). The company\u2019s security policy states that all personal devices and corporate-owned devices must be registered or joined to Azure AD. A remote user named User1 is unable to join a personal device to Azure AD from a home network. You verify that other users can join their devices to Azure AD. You need to ensure that User1 can join the device to Azure AD. What should you do? A. From the Device settings blade, modify the Users may join devices to Azure AD setting. B. From the Device settings blade, modify the Maximum number of devices per user setting. C. Create a point-to-site VPN from the home network of User1 to Azure. D. Assign the User administrator role to User1. Correct Answer: B Section: [none] Explanation Explanation/Reference: Explanation: The Maximum number of devices setting enables you to select the maximum number of devices that a user can have in Azure AD. If a user reaches this quota, they will not be able to add additional devices until one or more of the existing devices are removed. Incorrect Answers: A: The Users may join devices to Azure AD setting enables you to select the users who can join devices to Azure AD. Options are All, Selected and None. The default is All. C: Azure AD Join enables users to join their devices to Active Directory from anywhere as long as they have connectivity with the Internet. References: https://docs.microsoft.com/en-us/azure/active-directory/devices/device-management-azure-portal http://techgenix.com/pros-and-cons-azure-ad-join/ QUESTION 11 You have an Azure DNS zone named adatum.com. You need to delegate a subdomain named research.adatum.com to a different DNS server in Azure. What should you do? A. Create an A record named *.research in the adatum.com zone. B. Create a PTR record named research in the adatum.com zone. C. Modify the SOA record of adatum.com. D. Create an NS record named research in the adatum.com zone. Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: You need to create a name server (NS) record for the zone. References: https://docs.microsoft.com/en-us/azure/dns/delegate-subdomain QUESTION 12 SIMULATION Please wait while the virtual machine loads. Once loaded, you may proceed to the lab section. This may take a few minutes, and the wait time will not be deducted from your overall test time. When the Next button is available, click it to access the lab section. In this section, you will perform a set of tasks in a live environment. While most functionality will be available to you as it would be in a live environment, some functionality (e.g, copy and paste, ability to navigate to external websites) will not be possible by design. Scoring is based on the outcome of performing the tasks stated in the lab. In other words, it doesn\u2019t matter how you accomplish the task, if you successfully perform it, you will earn credit for that task. Labs are not timed separately, and this exam may have more than one lab that you must complete. You can use as much time as you would like to complete each lab. But, you should manage your time appropriately to ensure that you are able to complete the lab(s) and all other sections of the exam in the time provided. Please note that once you submit your work by clicking the Next button within a lab, you will NOT be able to return to the lab. You may now click next to proceed to the lab. Task 7 You plan to deploy several Azure virtual machines and to connect them to a virtual network named VNET1007. You need to ensure that future virtual machines on VNET1007 can register their name in an internal DNS zone named corp9172795.com. The zone must NOT be hosted on a virtual machine. What should you do from Azure Cloud Shell? To complete this task, start Azure Cloud Shell and select PowerShell (Linux). Click Show Advanced Settings, and then enter corp9172795n1 in the Storage account text box and File1 in the File share text box. Click Create storage, and then complete the task. Correct Answer: See solution below. Section: [none] Explanation Explanation/Reference: Explanation: Step 1: New-AzureRMResourceGroup -name MyResourceGroup Before you create the DNS zone, create a resource group to contain the DNS zone. Step 2: New-AzureRmDnsZone -Name corp9172795.com -ResourceGroupName MyResourceGroup A DNS zone is created by using the New-AzureRmDnsZone cmdlet. This creates a DNS zone called corp9172795.com in the resource group called MyResourceGroup. References: https://docs.microsoft.com/en-us/azure/dns/dns-getstarted-powershell QUESTION 13 HOTSPOT You have an Azure Active Directory (Azure AD) tenant named adatum.com. Adatum.com contains the groups in the following table: You create two user accounts that are configured as shown in the following table. To which groups do User1 and User2 belong? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Group 1 only First rule applies Box 2: Group1 and Group2 only Both membership rules apply. References: https://docs.microsoft.com/en-us/sccm/core/clients/manage/collections/create-collections Testlet 2 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Contoso, Ltd. is a manufacturing company that has offices worldwide. Contoso works with partner organizations to bring products to market. Contoso products are manufactured by using blueprint files that the company authors and maintains. Existing Environment Currently, Contoso uses multiple types of servers for business operations, including the following: File servers Domain controllers Microsoft SQL Server servers Your network contains an Active Directory forest named contoso.com. All servers and client computers are joined to Active Directory. You have a public-facing application named App1. App1 is comprised of the following three tiers: A SQL database A web front end A processing middle tier Each tier is comprised of five virtual machines. Users access the web front end by using HTTPS only. Requirements Planned Changes Contoso plans to implement the following changes to the infrastructure: Move all the tiers of App1 to Azure. Move the existing product blueprint files to Azure Blob storage. Create a hybrid directory to support an upcoming Microsoft Office 365 migration project. Technical Requirements Contoso must meet the following technical requirements: Move all the virtual machines for App1 to Azure. Minimize the number of open ports between the App1 tiers. Ensure that all the virtual machines for App1 are protected by backups. Copy the blueprint files to Azure over the Internet. Ensure that the blueprint files are stored in the archive storage tier. Ensure that partner access to the blueprint files is secured and temporary. Prevent user passwords or hashes of passwprds from being stored in Azure. Use unmanaged standard storage for the hard disks of the virtual machines. Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity Minimize administrative effort whenever possible User Requirements Contoso identifies the following requirements for users: Ensure that only users who are part of a group named Pilot can join devices to Azure AD. Designate a new user named Admin1 as the service admin for the Azure subscription. Admin1 must receive email alerts regarding service outages. Ensure that a new user named User3 can create network objects for the Azure subscription. QUESTION 1 HOTSPOT You need to configure the Device settings to meet the technical requirements and the user requirements. Which two settings should you modify? To answer, select the appropriate settings in the answer area. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Explanation: Box 1: Selected Only selected users should be able to join devices Box 2: Yes Require Multi-Factor Auth to join devices. From scenario: Ensure that only users who are part of a group named Pilot can join devices to Azure AD Ensure that when users join devices to Azure Active Directory (Azure AD), the users use a mobile phone to verify their identity. QUESTION 2 You need to recommend a solution to automate the configuration for the finance department users. The solution must meet the technical requirements. What should you include in the recommendation? A. Azure AD B2C B. Azure AD Identity Protection C. an Azure logic app and the Microsoft Identity Management (MIM) client D. dynamic groups and conditional access policies Correct Answer: D Section: [none] Explanation Explanation/Reference: Explanation: Scenario: Ensure Azure Multi-Factor Authentication (MFA) for the users in the finance department only. The recommendation is to use conditional access policies that can then be targeted to groups of users, specific applications, or other conditions. References: https://docs.microsoft.com/en-us/azure/active-directory/authentication/howto-mfa-userstates QUESTION 3 HOTSPOT You need to implement Role1. Which command should you run before you create Role1? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Hot Area: Correct Answer: Section: [none] Explanation Explanation/Reference: Testlet 3 Case study This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Overview Humongous Insurance is an insurance company that has three offices in Miami, Tokyo and Bangkok. Each office has 5.000 users. Existing Environment Active Directory Environment Humongous Insurance has a single-domain Active Directory forest named humongousinsurance.com. The functional level of the forest is Windows Server 2012. You recently provisioned an Azure Active Directory (Azure AD) tenant. Network Infrastructure Each office has a local data center that contains all the servers for that office. Each office has a dedicated connection to the Internet. Each office has several link load balancers that provide access to the servers. Active Directory Issue Several users in humongousinsurance.com have UPNs that contain special characters. You suspect that some of the characters are unsupported in Azure AD. Licensing Issue You attempt to assign a license in Azure to several users and receive the following error message: \"Licenses not assigned. License assignment failed for one user.\" You verify that the Azure subscription has the available licenses. Requirements Planned Changes Humongous Insurance plans to open a new office in Paris. The Paris office will contain 1,000 users who will be hired during the next 12 months. All the resources used by the Paris office users will be hosted in Azure. Planned Azure AD Infrastructure The on-premises Active Directory domain will be synchronized to Azure AD. All client computers in the Paris office will be joined to an Azure AD domain. Planned Azure Networking Infrastructure You plan to create the following networking resources in a resource group named All_Resources: Default Azure system routes that will be the only routes used to route traffic A virtual network named Paris-VNet that will contain two subnets named Subnet1 and Subnet2 A virtual network named ClientResources-VNet that will contain one subnet named ClientSubnet A virtual network named AllOffices-VNet that will contain two subnets named Subnet3 and Subnet4 You plan to enable peering between Paris-VNet and AllOffices-VNet. You will enable the Use remote gateways setting for the Paris-VNet peerings. You plan to create a private DNS zone named humongousinsurance.local and set the registration network to the ClientResources-VNet virtual network. Planned Azure Computer Infrastructure Each subnet will contain several virtual machines that will run either Windows Server 2012 R2, Windows Server 2016, or Red Hat Linux. Department Requirements Humongous Insurance identifies the following requirements for the company's departments: Web administrators will deploy Azure web apps for the marketing department. Each web app will be added to a separate resource group. The initial configuration of the web apps will be identical. The web administrators have permission to deploy web apps to resource groups. During the testing phase, auditors in the finance department must be able to review all Azure costs from the past week. Authentication Requirements Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. QUESTION 1 You need to prepare the environment to meet the authentication requirements. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Install the Active Directory Federation Services (AD FS) role on a domain controller in the Miami office. B. Allow inbound TCP port 8080 to the domain controllers in the Miami office. C. Join the client computers in the Miami office to Azure AD. D. Add http://autologon.microsoftazuread-sso.com to the intranet zone of each client computer in the Miami office. E. Install Azure AD Connect on a server in the Miami office and enable Pass-through Authentication. Correct Answer: DE Section: [none] Explanation Explanation/Reference: Explanation: D: You can gradually roll out Seamless SSO to your users. You start by adding the following Azure AD URL to all or selected users' Intranet zone settings by using Group Policy in Active Directory: https:// autologon.microsoftazuread-sso.com E: Seamless SSO works with any method of cloud authentication - Password Hash Synchronization or Pass-through Authentication, and can be enabled via Azure AD Connect. Incorrect Answers: A: Seamless SSO is not applicable to Active Directory Federation Services (ADFS). B: Azure AD connect does not port 8080. It uses port 443. C: Seamless SSO needs the user's device to be domain-joined, but doesn't need for the device to be Azure AD Joined. Scenario: Users in the Miami office must use Azure Active Directory Seamless Single Sign-on (Azure AD Seamless SSO) when accessing resources in Azure. Planned Azure AD Infrastructure include: The on-premises Active Directory domain will be synchronized to Azure AD. References: https://docs.microsoft.com/en-us/azure/active-directory/connect/active-directory-aadconnect-sso-quick-start","title":"Microsoft Azure Administrator"},{"location":"nightwolf-cotribution/devops_interview_questions-2/","text":"DevOps Interview Question and Answered for Freshers and Experianced - 2 \uf0c1 We have consolidated a list of frequently asked DevOps interview questions for Freshers and Experianced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. List of Docker components? 1. Docker image: \u2013 Contains OS (very small) (almost negligible) + soft wares 2. Docker Container: \u2013 Container like a machine which is created from Docker image. 3. Docker file: \u2013 Describes steps to create a docker image. 4. Docker hub/registry: \u2013 Stores all docker images publicly. 5. Docker daemon: \u2013 Docker service runs at back end Above five components we call as Docker components What is Docker workflow? First we create Docker file by mentioning instructions to build docker image. Form this Docker image, we are going to create Docker container. This Docker image we can push to docker hub as well. This image can be pulled by others to create docker containers. We can create docker images from docker containers. Like this we can create Docker images form either docker file or docker containers. We can create docker containers from docker images. This is the work flow of docker. Sample Docker file instructions? FROM ubuntu WORKDIR /tmp RUN echo \u201cHello\u201d > /tmp/testfile ENV myname user1 COPY testfile1 /tmp ADD test.tar.gz /tmp What is the importance of volumes in Docker? 1. Volume is a directory inside your container \u2000 2. First declare directory as a volume and then share volume \u2000 3. Even if we stop container, still we can access volume \u2000 4. Volume will be created in one container \u2000 5. You can share one volume across any no of containers \u2000 6. Volume will not be included when you update an image \u2000 7. Map volumes in two ways \u2000 8. Share host \u2013 container \u2000 9. Share container \u2013 container What do you mean by port mapping in Docker? Suppose if you want to make any container as web server by installing web package in it, you need to provide containers IP address to public in order to access website which is running inside docker container. But Docker containers don\u2019t have an IP address. So, to address this issue, we have a concept called Docker port mapping. We map host port with container port and customers use public IP of host machine. Then their request will be routed from host port to container\u2019s port and will be loaded webpage which is running inside docker container. This is how we can access website which is running inside container through port mapping. What is Registry server in Docker? Registry server is our own docker hub created to store private docker images instead of storing in public Docker hub. Registry server is one of the docker containers. We create this Registry server from \u201cregistry\u201d image, especially provided by docker to create private docker hub. We can store any no of private docker images in this Registry server. We can give access to others, so that, they also can store their docker images whomever you provide access. Whenever we want, we can pull these images and can create containers out of these images. Important docker commands? 1. Docker ps (to see list of running containers) 2. Docker ps -a (to see list of all containers) 3. Docker images (to see list of all images) 4. Docker run (to create docker container) 5. Docker attach (to go inside container) 6. Docker stop (to stop container) 7. Docker start (to start container) 8. Docker commit (to create image out of docker file) 9. Docker rm (to delete container) 10. Docker rmi (to delete image) What is Ansible? Ansible is one of the configuration Management Tools. It is a method through we automate system admin tasks. Configuration refers to each and every minute details of a system. If we do any changes in system means we are changing the configuration of a machine. That means we are changing the configuration of the machine. All windows/Linux system administrators manage the configuration of a machine manually. All DevOps engineers are managing this configuration automatic way by using some tools which are available in the market. One such tool is Ansible. That\u2019s why we call Ansible as configuration management tool. Working process of Ansible? Here we crate file called playbook and inside playbook we write script in YAML format to create infrastructure. Once we execute this playbook, automatically code will be converted into Infrastructure. We call this process as IAC (Infrastructure as Code). We have open source and enterprise editions of Ansible. Enterprise edition we call Ansible Tower. The architecture of Ansible? We create Ansible server by installing Ansible package in it. Python is pre-requisite to install ansible. We need not to install ansible package in nodes. Because, communication establishes from server to node through \u201cssh\u201d client. By default all Linux machine will have \u201cssh\u201d client. Server is going to push the code to nodes that we write in playbooks. So Ansible follows pushing mechanism. Ansible components? 1. Server:\u2013 It is the place where we create playbooks and write code in YML format 2. Node:\u2013 It is the place where we apply code to create infrastructure. Server pushes code to nodes. 3. SSH:\u2013 It is an agent through ansible server pushes code to nodes. 4. Setup:\u2013 It is a module in ansible which gathers nodes information. 5. Inventory file:- In this file we keep IP/DNS of nodes. Disadvantages in other SCM (Source Code Management) tools? \u2000 - Huge overhead of Infrastructure setup \u2000 - Complicated setup \u2000 - Pull mechanism \u2000 - Lot of learning required Advantages of Ansible over other SCM (Source Code Management) tools? \u2000 - Agentless \u2000 - Relies on \u201cssh\u201d \u2000 - Uses python \u2000 - Push mechanism How does Ansible work? We give nodes IP addresses in hosts file by creating any group in ansible server why because, ansible doesn\u2019t recognize individual IP addresses of nodes. We create playbook and write code in YAML script. The group name we have to mention in a playbook and then we execute the playbook. By default, playbook will be executed in all those nodes which are under this group. This is how ansible converts code into infrastructure. What do you mean by Ad-Hoc commands in Ansible? These are simple one liner Linux commands we use to meet temporary requirements without actually saving for later. Here we don\u2019t use ansible modules. So there, Idempotency will not work with Ad-Hoc commands. If at all we don\u2019t get required YAML module to write to create infrastructure, then we go for it. Without using playbooks we can use these Ad-Hoc commands for temporary purpose. Differences between Chef and Ansible? Ansible Chef Playbook Recipe Module Resource Host Node Setup Ohai SSH Knife mechanism: Push mechanism: Pull What is Playbook in Ansible? Playbook is a file where we write YAML script to create infrastructure in nodes. Here, we use modules to create infrastructure. We create so many sections in playbook. We mention all modules in task section. You can create any no of playbooks. There is no limit. Each playbook defines one scenario. All sections begin with \u201c-\u201d & its attributes & parameters beneath it. Mention some list of sections that we mention in Playbook? 1. Target section 2. Task section 3. Variable section 4. Handler section What is Target section in Ansible playbook? This is one of the important sections in Playbook. In this section, we mention the group name which contains either IP addresses or Hostnames of nodes. When we execute playbook, then code will be pushed too all nodes which are there in the group that we mention in Target section. We use \u201call\u201d key word to refer all groups. What is Task section in Ansible playbook? This is second most important section in playbook after target section. In this section, we are going to mention list of all modules. All tasks we mention in this task section. We can mention any no of modules in one playbook. There is no limit. If there is only one task, then instead of going with big playbook, simply we can go with arbitrary command where we can use one module at a time. If more than one module, then there is no option except going with big playbook. What is Variable section? In this section we are going to mention variables. Instead of hard coding, we can mention as variables so that during runtime it pulls the actual value in place of key. We have this concept in each and every programming language and scripting language. We use \u201cvars\u201d key word to use variables. What is Handler section? All tasks we mention in tasks section. But some tasks where dependency is there, we should not mention in tasks section. That is not good practice. For example, installing package is one task and starting service is one more task. But there is dependency between them. I.e. after installing package only, we have to start service. Otherwise it throws error. These kind of tasks, we mention in handler section. In above example, package task we mention in task section and service task we mention in handler section so that after installing task only service will be started. What is Dry run in playbook? Dry run is to test playbook. Before executing playbook in nodes, we can test whether the code in playbook is written properly or not. Dry run won\u2019t actually executes playbook, but it shows output as` if it executed playbook. Then by seeing the output, we can come to know whether the playbook is written properly or not. It checks whether the playbook is formatted correctly or not. It tests how the playbook is going to behave without running the tasks. Why are we using loops concept in Ansible? Sometimes we might need to deal with multiple tasks. For instance, Installing multiple packages, Creating many users, creation many groups..etc. In this case, mentioning module for every task is complex process. So, to address this issue, we have a concept of loops. We have to use variables in combination with loops. Where do we use conditionals in Playbooks? Sometimes, your nodes could be mixture of different flavors of Linux OS. Linux commands vary in different Linux operating systems. In this case, we can\u2019t execute common set of commands in all machines, at the same time, we can\u2019t execute different commands in each node separately. To address this issue, we have conditionals concept where commands will be executed based up on certain condition that we give. What is Ansible vault? Sometimes, we use sensitive information in playbooks like passwords, keys \u2026etc. So any one can open these playbooks and get to know about this sensitive information. So we have to protect our playbooks from being read by others. So by using Ansible vault, we encrypt playbooks so that, those who ever is having password, only those can read this information. It is the way of protecting playbooks by encrypting them. What do you mean by Roles in Ansible? Adding more & more functionality to the playbooks will make it difficult to maintain in a single file. To address this issue, we organize playbooks into a directory structure called \u201croles\u201d. We create separate file to each section and we just mention the names of those sections in playbook instead of mentioning all modules in main playbook. When you call main playbook, main playbook will call all sections files respectively in the order whatever order you mention in playbook. So, by using this Roles, we can maintain small playbook without any complexity. Write a sample playbook to install any package? \u2014 # My First YAML playbook \u2013 hosts: demo user: ansible become: yes connection: ssh tasks: \u2013 name: Install HTTPD on centos 7 action: yum name=httpd state=installed Write a sample playbook by mentioning variables instead of hard coding? \u2014 # My First YAML playbook \u2013 hosts: demo user: nightwolf become: yes connection: ssh vars: pkgname: httpd tasks: \u2013 name: Install HTTPD server on centos 7 action: yum name=\u2018{{pkgname}}\u2019 state=installed What is CI & CD? CI means Continues Integration and CD means Continues Delivery/Deploy. Whenever developers write code, we integrate all that code of all developers at that point of time and we build, test and deliver/deploy to the client. This process we call CI & CD. Jenkins helps in achieving this. So instead of doing night builds, build as and when commit occurs by integrating all code in SCM tool, build, test and checking the quality of that code is what we call Continues Integration. Key terminology that we use in Jenkins? - Integrate: Combine all code written by developers till some point of time. - Build: Compile the code and make a small executable package. - Test: Test in all environments whether application is working properly or not. - Archived: Stored in an artifactory so that in future we may use/deliver again. - Deliver: Handing the product to Client Deploy: Installing product in client\u2019s machines. What is Jenkins Workflow? We attach Git, Maven, Selenium & Artifactory plug-ins to Jenkins. Once Developers put the code in Git, Jenkins pulls that code and send to Maven for build. Once build is done, Jenkins pulls that built code and send to selenium for testing. Once testing is done, then Jenkins will pull that code and send to Artifactory as per requirement and finally we can deliver the end product to client we call Continues delivery. We can also deploy with Jenkins into clients machine directly as per the requirement. This is what Jenkins work flow. What are the ways through which we can do Continues Integration? There are total three ways through which we can do Continues Integration: 1. Manually:\u2013 Manually write code, then do build manually and then test manually by writing test cases and deploy manually into clients machine. 2. Scripts:\u2013 Can do above process by writing scripts so that these scripts do CI&CD automatically. here complexity is, writing script is not so easy. 3. Tool:\u2013 Using tools like Jenkins is very handy. Everything is preconfigured in these type of tools. So less manual intervention. This is the most preferred way. Benefits of CI? 1. Detects bugs as soon as possible, so that bug will be rectified fast and development happens fast. 2. Complete automation. No need manual intervention. 3. We can intervene manually whenever we want i.e. we can stop any stage at any point of time so have better control. 4. Can establish complete and continues work flow. Why only Jenkins? \u2000 - It has so many plug-ins. - You can write your own plug-in \u2000 - You can use community plug-ins \u2000 - Jenkins is not just a tool. It is a framework i.e. you can do what ever you want. All you need is plugins. - We can attach slaves to Jenkins master. It instructs others(slaves) to do Job. - If slaves are not available, Jenkins itself does the job. \u2000 - Jenkins also acts as cron server replacement i.e. can do repeated tasks automatically. \u2000 - Running some scripts regularly E.g.: Automatic daily alarm. \u2000 - Can create Labels (Group of slaves) (Can restrict where the project has to run). What is Jenkins Architecture? Jenkins architecture is Client-Server model. Where ever, we install Jenkins, we call that server is Jenkins master. We can also create slaves in Jenkins, so that server load will be distributed to slaves. Jenkins Master randomly assigns tasks to slaves. But if you want to restrict any job to run in particular slave, then we can do it, so that particular job will be executed in that slave only. We can group some slaves by using \u201cLabel\u201d. How to install Jenkins? - You can install Jenkins in any OS. All OSs supports Jenkins. We access Jenkins through web page only. That\u2019s why it doesn\u2019t make any difference whether you install Jenkins in Windows or Linux. \u2000 - Choose Long Term Support release version, so that you will get support from Jenkins community. If you are using Jenkins for testing purpose, you can choose weekly release. But for production environments, we prefer Long Term Support release version. - Need to install JAVA. Java is pre-requisite to install Jenkins. \u2000 - Need to install web package. Because, we are going to access Jenkins through web page only. Does Jenkins open source? There are two editions in Jenkins 1. Open source 2. Enterprise edition Open source edition we call Jenkins. Here we get support from community if we need it. Enterprise edition we call Hudson. Here Jenkins company will provide support. How many types of configurations in Jenkins? There are total 3 types of configurations in Jenkins: 1. Global:\u2013 Here, whatever configuration changes we do, applicable to whole Jenkins including jobs as well as nodes. This configuration has high priority. 2. Job:\u2013 These configurations applicable to only Jobs. Jobs also we call as projects or items in Jenkins. 3. Node:\u2013 These configurations applicable to only nodes. Also we call Slaves. These are kind of helpers to Jenkins master to distribute the excessive load. What do you mean by workspace in Jenkins? The workspace is the location on your computer where Jenkins places all files related to the Jenkins project. By default each project or job is assigned a workspace location and it contains Jenkins-specific project metadata, temporary files like logs and any build artifacts, including transient build files. Jenkins web page acts like a window through which we are actually doing work in workspace. List of Jenkins services? \u2000 - localhost:8080/restart (to restart Jenkins).\u2000 - localhost:8080/stop (to stop Jenkins). - localhost:8080/start (to start Jenkins). How to create a free style project in Jenkins? \u2000 - Create project by giving any name \u2000 - Select Free style project \u2000 - Click on build \u2000 - Select execute windows batch command \u2000 - Give any command (echo \u201cHello Dear Students!!\u201d) \u2000 - Select Save \u2000 - Click on Build now \u2000 - Finally can see Console output What do you mean by Plugins in Jenkins? \u2000 - With Jenkins, nearly everything is a plugin and that nearly all functionality is provided by plugins. You can think of Jenkins as little more than an executor of plugins. \u2000 - Plugins are small libraries that add new abilities to Jenkins and can provide integration points to other tools. \u2000 - Since nearly everything Jenkins does is because of a plugin, Jenkins ships with a small set of default plugins, some of which can be upgraded independently of Jenkins. How to create Maven Project? \u2000 - Select new item - Copy the git hub maven project link and paste in git section in Jenkins \u2000 - Select build \u2000 - Click on clean package \u2000 - Select save \u2000 - Click on Build now \u2000 - Verify workspace contents with GitHub sideSee console output How can we Schedule projects? Sometimes, we might need some jobs to be executed after frequent intervals. To schedule a job: - Click on any project - Click on Configure \u2000 - Select on Build triggers \u2000 - Click on Build periodically \u2000 - Give timing (In format : * * * * * ) \u2000 - Select Save \u2000 - Can see automatic builds every 1 min \u2000 - You can manually trigger build as well if you want. What do you mean by Upstream and Downstream projects? We can also call them as linked projects. These are the ways through which, we connect jobs one with other. In Upstream jobs, first job will trigger second job after build is over. In Downstream jobs, second job will wait till first job finishes its build. As and when first job finishes its work, then second job will be triggered automatically. In Upstream, first job will be active. In Downstream jobs, second job will be active. We can use any one type to link multiple jobs. What is view in Jenkins? We can customize view as per our needs. We can modify Jenkins home page. We can segregate jobs as per the type of jobs like free style jobs and maven jobs and so on. To create custom view: - Select List of Related Projects \u2000 - Select Default views \u2000 - Click on All \u2000 - Click on + and select Freestyle - Select List Views \u2000 - Select Job filter \u2000 - Select required jobs to be segregated \u2000 - Now, you can see different view What is User Administration in Jenkins? In Jenkins, we can create users, groups and can assign limited privileges to them so that, we can have better control on Jenkins. Users will not install Jenkins in their machines. They access Jenkins as a user. Here we can\u2019t assign permissions directly to users. Instead we create \u201cRoles\u201d and assign permissions to those roles. These roles we attach to users so that users get the permissions whatever we assign to those roles. What is Global tool configuration in Jenkins? We install Java, Maven, Git and many other tools in our server. Whenever Jenkins need those tools, by default Jenkins will install them automatically every time. But it\u2019s not a good practice. That\u2019s why we give installed path of all these tools in Jenkins so that whenever Jenkins need them, automatically Jenkins pull them form local machine instead of downloading every time. This way of giving path of these tools in Jenkins we call \u201cGlobal tool configuration\u201d What is Build? Build means, Compile the source code, assembling of all class files and finally creating deliverable Compile:\u2013 Convert Source code into machine-readable format Assembly (Linking):\u2013 Grouping all class files Deliverable:\u2013 .war, .jar The above process is same for any type of code. This process we call Build. What is Maven? Maven is one of the Build tools. It is the most advance build tool in the market. In this, everything is already pre-configured. Maven belongs to Apache Company. We use maven to build Java code only. We can\u2019t build other codes by using Maven. By default, we get so many plugins with Maven. You can write your own plugins as well. Maven\u2019s local repository is \u201c.M2\u201d where we can get required compilers and dependencies. Maven\u2019s main configuration file is \u201cpom.xml\u201d where we keep all instructions to build. Advantages of Maven? \u2000 - Automated tasks (Mention all in pom.xml) \u2000 - Multiple Tasks at a time \u2000 - Quality product \u2000 - Minimize bad builds - Keep history - Save time \u2013 Save money \u2000 - Gives set of standards - Gives define project life cycle (Goals) \u2000 - Manage all dependencies \u2000 - Uniformity in all projects \u2000 - Re-usability List of Build tools available in Market? \u2000 - C and C++: Make file \u2000 - .Net: Visual studio \u2000 - Java: Ant, Maven What is the architecture of Maven? Maven main configuration file is pom.xml. For one project, there will be one workspace and one pom.xml. Requirements for build:\u2013 - Source code (Will be pulled from Git hub) - Compiler (Pulls from remote repo and then put them in local repo, from there, maven pulls into Workspace) - Dependencies (Pulls from remote repo and then put them in local repo, from there, maven pulls into Workspace) What is Maven\u2019s Build Life Cycle? In maven, we have different goals. These are: - Generate resources (Dependencies) - Compile code - Unit test - Package (Build) - Install (in to local repo & artifactory) - Deploy (to servers) - Clean (delete all run time files) What does POM.XML contains? POM.XML is maven\u2019s main configuration file where we keep all details related to project. It contains: - Metadata about that project - Dependencies required to build the project - The kind of project - Kind of output you want (.jar, .war) - Description about that project What is Multi-Module Project in Maven? - Dividing big project into small modules, we call Multi Module Project. - Each module must have its own SRC folder & pom.xml so that build will happen separately. - To build all modules with one command, there should be a parent pom.xml file. This calls all child pom.xml files automatically. - In parent pom.xml file, need to mention the child pom.xml files in an order. What is Nagios? Nagios is one of the monitoring tools. By using Nagios we can monitor and give alerts. Where ever you install Nagios that becomes Nagios server. Monitoring is important, because we need to make sure that our servers should never go down. If at all in some exceptional cases server goes down, immediately we need alert in the form of intimation so that we can take required action to bring the server up immediately. So for this purpose, we use Nagios. Why do we have to use Nagios? There are many advantages in using Nagios: - It is oldest & Latest (every now and then, it is getting upgraded as per current market requirements) - Stable (we have been using this since so many years and it is performing well) - By default, we get so many Plug-ins - It is having its own Database. - Nagios is both Monitoring & Alerting tool. How does Nagios works? - We mention all details in configuration files what data to be collected from which machine. - Nagios daemon reads those details about what data to be collected. - Daemon use NRPE (Nagios Remote Plug-in Executer) plug-in to collect data form nodes and stores in its own database. - Finally displays in Nagios dashboard. What is the Directory structure of Nagios? /usr/local/nagios/bin \u2013 binary files /usr/local/nagios/sbin \u2013 CGI files (to get web page) /usr/local/nagios/libexec \u2013 plugins /usr/local/nagios/share \u2013 PHP Files /usr/local/nagios/etc \u2013 configuration files /usr/local/nagios/var \u2013 logs /usr/local/nagios/var/status.dat(file) \u2013 database What are the Important Configuration files in Nagios? Nagios main configuration file is /usr/local/nagios/etc/nagios.cfg /usr/local/nagios/etc/objects/localhost.cfg (where we keep hosts information) /usr/local/nagios/etc/objects/contacts.cfg (whom to be informed (emails)) /usr/local/nagios/etc/objects/timeperiods.cfg (at what time to monitor) /usr/local/nagios/etc/objects/commands.cfg (plugins to use) /usr/local/nagios/etc/objects/templates.cfg (sample templates) You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"DevOps Interview Questions for Freshers and Experianced - 2"},{"location":"nightwolf-cotribution/devops_interview_questions-2/#devops-interview-question-and-answered-for-freshers-and-experianced-2","text":"We have consolidated a list of frequently asked DevOps interview questions for Freshers and Experianced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. List of Docker components? 1. Docker image: \u2013 Contains OS (very small) (almost negligible) + soft wares 2. Docker Container: \u2013 Container like a machine which is created from Docker image. 3. Docker file: \u2013 Describes steps to create a docker image. 4. Docker hub/registry: \u2013 Stores all docker images publicly. 5. Docker daemon: \u2013 Docker service runs at back end Above five components we call as Docker components What is Docker workflow? First we create Docker file by mentioning instructions to build docker image. Form this Docker image, we are going to create Docker container. This Docker image we can push to docker hub as well. This image can be pulled by others to create docker containers. We can create docker images from docker containers. Like this we can create Docker images form either docker file or docker containers. We can create docker containers from docker images. This is the work flow of docker. Sample Docker file instructions? FROM ubuntu WORKDIR /tmp RUN echo \u201cHello\u201d > /tmp/testfile ENV myname user1 COPY testfile1 /tmp ADD test.tar.gz /tmp What is the importance of volumes in Docker? 1. Volume is a directory inside your container \u2000 2. First declare directory as a volume and then share volume \u2000 3. Even if we stop container, still we can access volume \u2000 4. Volume will be created in one container \u2000 5. You can share one volume across any no of containers \u2000 6. Volume will not be included when you update an image \u2000 7. Map volumes in two ways \u2000 8. Share host \u2013 container \u2000 9. Share container \u2013 container What do you mean by port mapping in Docker? Suppose if you want to make any container as web server by installing web package in it, you need to provide containers IP address to public in order to access website which is running inside docker container. But Docker containers don\u2019t have an IP address. So, to address this issue, we have a concept called Docker port mapping. We map host port with container port and customers use public IP of host machine. Then their request will be routed from host port to container\u2019s port and will be loaded webpage which is running inside docker container. This is how we can access website which is running inside container through port mapping. What is Registry server in Docker? Registry server is our own docker hub created to store private docker images instead of storing in public Docker hub. Registry server is one of the docker containers. We create this Registry server from \u201cregistry\u201d image, especially provided by docker to create private docker hub. We can store any no of private docker images in this Registry server. We can give access to others, so that, they also can store their docker images whomever you provide access. Whenever we want, we can pull these images and can create containers out of these images. Important docker commands? 1. Docker ps (to see list of running containers) 2. Docker ps -a (to see list of all containers) 3. Docker images (to see list of all images) 4. Docker run (to create docker container) 5. Docker attach (to go inside container) 6. Docker stop (to stop container) 7. Docker start (to start container) 8. Docker commit (to create image out of docker file) 9. Docker rm (to delete container) 10. Docker rmi (to delete image) What is Ansible? Ansible is one of the configuration Management Tools. It is a method through we automate system admin tasks. Configuration refers to each and every minute details of a system. If we do any changes in system means we are changing the configuration of a machine. That means we are changing the configuration of the machine. All windows/Linux system administrators manage the configuration of a machine manually. All DevOps engineers are managing this configuration automatic way by using some tools which are available in the market. One such tool is Ansible. That\u2019s why we call Ansible as configuration management tool. Working process of Ansible? Here we crate file called playbook and inside playbook we write script in YAML format to create infrastructure. Once we execute this playbook, automatically code will be converted into Infrastructure. We call this process as IAC (Infrastructure as Code). We have open source and enterprise editions of Ansible. Enterprise edition we call Ansible Tower. The architecture of Ansible? We create Ansible server by installing Ansible package in it. Python is pre-requisite to install ansible. We need not to install ansible package in nodes. Because, communication establishes from server to node through \u201cssh\u201d client. By default all Linux machine will have \u201cssh\u201d client. Server is going to push the code to nodes that we write in playbooks. So Ansible follows pushing mechanism. Ansible components? 1. Server:\u2013 It is the place where we create playbooks and write code in YML format 2. Node:\u2013 It is the place where we apply code to create infrastructure. Server pushes code to nodes. 3. SSH:\u2013 It is an agent through ansible server pushes code to nodes. 4. Setup:\u2013 It is a module in ansible which gathers nodes information. 5. Inventory file:- In this file we keep IP/DNS of nodes. Disadvantages in other SCM (Source Code Management) tools? \u2000 - Huge overhead of Infrastructure setup \u2000 - Complicated setup \u2000 - Pull mechanism \u2000 - Lot of learning required Advantages of Ansible over other SCM (Source Code Management) tools? \u2000 - Agentless \u2000 - Relies on \u201cssh\u201d \u2000 - Uses python \u2000 - Push mechanism How does Ansible work? We give nodes IP addresses in hosts file by creating any group in ansible server why because, ansible doesn\u2019t recognize individual IP addresses of nodes. We create playbook and write code in YAML script. The group name we have to mention in a playbook and then we execute the playbook. By default, playbook will be executed in all those nodes which are under this group. This is how ansible converts code into infrastructure. What do you mean by Ad-Hoc commands in Ansible? These are simple one liner Linux commands we use to meet temporary requirements without actually saving for later. Here we don\u2019t use ansible modules. So there, Idempotency will not work with Ad-Hoc commands. If at all we don\u2019t get required YAML module to write to create infrastructure, then we go for it. Without using playbooks we can use these Ad-Hoc commands for temporary purpose. Differences between Chef and Ansible? Ansible Chef Playbook Recipe Module Resource Host Node Setup Ohai SSH Knife mechanism: Push mechanism: Pull What is Playbook in Ansible? Playbook is a file where we write YAML script to create infrastructure in nodes. Here, we use modules to create infrastructure. We create so many sections in playbook. We mention all modules in task section. You can create any no of playbooks. There is no limit. Each playbook defines one scenario. All sections begin with \u201c-\u201d & its attributes & parameters beneath it. Mention some list of sections that we mention in Playbook? 1. Target section 2. Task section 3. Variable section 4. Handler section What is Target section in Ansible playbook? This is one of the important sections in Playbook. In this section, we mention the group name which contains either IP addresses or Hostnames of nodes. When we execute playbook, then code will be pushed too all nodes which are there in the group that we mention in Target section. We use \u201call\u201d key word to refer all groups. What is Task section in Ansible playbook? This is second most important section in playbook after target section. In this section, we are going to mention list of all modules. All tasks we mention in this task section. We can mention any no of modules in one playbook. There is no limit. If there is only one task, then instead of going with big playbook, simply we can go with arbitrary command where we can use one module at a time. If more than one module, then there is no option except going with big playbook. What is Variable section? In this section we are going to mention variables. Instead of hard coding, we can mention as variables so that during runtime it pulls the actual value in place of key. We have this concept in each and every programming language and scripting language. We use \u201cvars\u201d key word to use variables. What is Handler section? All tasks we mention in tasks section. But some tasks where dependency is there, we should not mention in tasks section. That is not good practice. For example, installing package is one task and starting service is one more task. But there is dependency between them. I.e. after installing package only, we have to start service. Otherwise it throws error. These kind of tasks, we mention in handler section. In above example, package task we mention in task section and service task we mention in handler section so that after installing task only service will be started. What is Dry run in playbook? Dry run is to test playbook. Before executing playbook in nodes, we can test whether the code in playbook is written properly or not. Dry run won\u2019t actually executes playbook, but it shows output as` if it executed playbook. Then by seeing the output, we can come to know whether the playbook is written properly or not. It checks whether the playbook is formatted correctly or not. It tests how the playbook is going to behave without running the tasks. Why are we using loops concept in Ansible? Sometimes we might need to deal with multiple tasks. For instance, Installing multiple packages, Creating many users, creation many groups..etc. In this case, mentioning module for every task is complex process. So, to address this issue, we have a concept of loops. We have to use variables in combination with loops. Where do we use conditionals in Playbooks? Sometimes, your nodes could be mixture of different flavors of Linux OS. Linux commands vary in different Linux operating systems. In this case, we can\u2019t execute common set of commands in all machines, at the same time, we can\u2019t execute different commands in each node separately. To address this issue, we have conditionals concept where commands will be executed based up on certain condition that we give. What is Ansible vault? Sometimes, we use sensitive information in playbooks like passwords, keys \u2026etc. So any one can open these playbooks and get to know about this sensitive information. So we have to protect our playbooks from being read by others. So by using Ansible vault, we encrypt playbooks so that, those who ever is having password, only those can read this information. It is the way of protecting playbooks by encrypting them. What do you mean by Roles in Ansible? Adding more & more functionality to the playbooks will make it difficult to maintain in a single file. To address this issue, we organize playbooks into a directory structure called \u201croles\u201d. We create separate file to each section and we just mention the names of those sections in playbook instead of mentioning all modules in main playbook. When you call main playbook, main playbook will call all sections files respectively in the order whatever order you mention in playbook. So, by using this Roles, we can maintain small playbook without any complexity. Write a sample playbook to install any package? \u2014 # My First YAML playbook \u2013 hosts: demo user: ansible become: yes connection: ssh tasks: \u2013 name: Install HTTPD on centos 7 action: yum name=httpd state=installed Write a sample playbook by mentioning variables instead of hard coding? \u2014 # My First YAML playbook \u2013 hosts: demo user: nightwolf become: yes connection: ssh vars: pkgname: httpd tasks: \u2013 name: Install HTTPD server on centos 7 action: yum name=\u2018{{pkgname}}\u2019 state=installed What is CI & CD? CI means Continues Integration and CD means Continues Delivery/Deploy. Whenever developers write code, we integrate all that code of all developers at that point of time and we build, test and deliver/deploy to the client. This process we call CI & CD. Jenkins helps in achieving this. So instead of doing night builds, build as and when commit occurs by integrating all code in SCM tool, build, test and checking the quality of that code is what we call Continues Integration. Key terminology that we use in Jenkins? - Integrate: Combine all code written by developers till some point of time. - Build: Compile the code and make a small executable package. - Test: Test in all environments whether application is working properly or not. - Archived: Stored in an artifactory so that in future we may use/deliver again. - Deliver: Handing the product to Client Deploy: Installing product in client\u2019s machines. What is Jenkins Workflow? We attach Git, Maven, Selenium & Artifactory plug-ins to Jenkins. Once Developers put the code in Git, Jenkins pulls that code and send to Maven for build. Once build is done, Jenkins pulls that built code and send to selenium for testing. Once testing is done, then Jenkins will pull that code and send to Artifactory as per requirement and finally we can deliver the end product to client we call Continues delivery. We can also deploy with Jenkins into clients machine directly as per the requirement. This is what Jenkins work flow. What are the ways through which we can do Continues Integration? There are total three ways through which we can do Continues Integration: 1. Manually:\u2013 Manually write code, then do build manually and then test manually by writing test cases and deploy manually into clients machine. 2. Scripts:\u2013 Can do above process by writing scripts so that these scripts do CI&CD automatically. here complexity is, writing script is not so easy. 3. Tool:\u2013 Using tools like Jenkins is very handy. Everything is preconfigured in these type of tools. So less manual intervention. This is the most preferred way. Benefits of CI? 1. Detects bugs as soon as possible, so that bug will be rectified fast and development happens fast. 2. Complete automation. No need manual intervention. 3. We can intervene manually whenever we want i.e. we can stop any stage at any point of time so have better control. 4. Can establish complete and continues work flow. Why only Jenkins? \u2000 - It has so many plug-ins. - You can write your own plug-in \u2000 - You can use community plug-ins \u2000 - Jenkins is not just a tool. It is a framework i.e. you can do what ever you want. All you need is plugins. - We can attach slaves to Jenkins master. It instructs others(slaves) to do Job. - If slaves are not available, Jenkins itself does the job. \u2000 - Jenkins also acts as cron server replacement i.e. can do repeated tasks automatically. \u2000 - Running some scripts regularly E.g.: Automatic daily alarm. \u2000 - Can create Labels (Group of slaves) (Can restrict where the project has to run). What is Jenkins Architecture? Jenkins architecture is Client-Server model. Where ever, we install Jenkins, we call that server is Jenkins master. We can also create slaves in Jenkins, so that server load will be distributed to slaves. Jenkins Master randomly assigns tasks to slaves. But if you want to restrict any job to run in particular slave, then we can do it, so that particular job will be executed in that slave only. We can group some slaves by using \u201cLabel\u201d. How to install Jenkins? - You can install Jenkins in any OS. All OSs supports Jenkins. We access Jenkins through web page only. That\u2019s why it doesn\u2019t make any difference whether you install Jenkins in Windows or Linux. \u2000 - Choose Long Term Support release version, so that you will get support from Jenkins community. If you are using Jenkins for testing purpose, you can choose weekly release. But for production environments, we prefer Long Term Support release version. - Need to install JAVA. Java is pre-requisite to install Jenkins. \u2000 - Need to install web package. Because, we are going to access Jenkins through web page only. Does Jenkins open source? There are two editions in Jenkins 1. Open source 2. Enterprise edition Open source edition we call Jenkins. Here we get support from community if we need it. Enterprise edition we call Hudson. Here Jenkins company will provide support. How many types of configurations in Jenkins? There are total 3 types of configurations in Jenkins: 1. Global:\u2013 Here, whatever configuration changes we do, applicable to whole Jenkins including jobs as well as nodes. This configuration has high priority. 2. Job:\u2013 These configurations applicable to only Jobs. Jobs also we call as projects or items in Jenkins. 3. Node:\u2013 These configurations applicable to only nodes. Also we call Slaves. These are kind of helpers to Jenkins master to distribute the excessive load. What do you mean by workspace in Jenkins? The workspace is the location on your computer where Jenkins places all files related to the Jenkins project. By default each project or job is assigned a workspace location and it contains Jenkins-specific project metadata, temporary files like logs and any build artifacts, including transient build files. Jenkins web page acts like a window through which we are actually doing work in workspace. List of Jenkins services? \u2000 - localhost:8080/restart (to restart Jenkins).\u2000 - localhost:8080/stop (to stop Jenkins). - localhost:8080/start (to start Jenkins). How to create a free style project in Jenkins? \u2000 - Create project by giving any name \u2000 - Select Free style project \u2000 - Click on build \u2000 - Select execute windows batch command \u2000 - Give any command (echo \u201cHello Dear Students!!\u201d) \u2000 - Select Save \u2000 - Click on Build now \u2000 - Finally can see Console output What do you mean by Plugins in Jenkins? \u2000 - With Jenkins, nearly everything is a plugin and that nearly all functionality is provided by plugins. You can think of Jenkins as little more than an executor of plugins. \u2000 - Plugins are small libraries that add new abilities to Jenkins and can provide integration points to other tools. \u2000 - Since nearly everything Jenkins does is because of a plugin, Jenkins ships with a small set of default plugins, some of which can be upgraded independently of Jenkins. How to create Maven Project? \u2000 - Select new item - Copy the git hub maven project link and paste in git section in Jenkins \u2000 - Select build \u2000 - Click on clean package \u2000 - Select save \u2000 - Click on Build now \u2000 - Verify workspace contents with GitHub sideSee console output How can we Schedule projects? Sometimes, we might need some jobs to be executed after frequent intervals. To schedule a job: - Click on any project - Click on Configure \u2000 - Select on Build triggers \u2000 - Click on Build periodically \u2000 - Give timing (In format : * * * * * ) \u2000 - Select Save \u2000 - Can see automatic builds every 1 min \u2000 - You can manually trigger build as well if you want. What do you mean by Upstream and Downstream projects? We can also call them as linked projects. These are the ways through which, we connect jobs one with other. In Upstream jobs, first job will trigger second job after build is over. In Downstream jobs, second job will wait till first job finishes its build. As and when first job finishes its work, then second job will be triggered automatically. In Upstream, first job will be active. In Downstream jobs, second job will be active. We can use any one type to link multiple jobs. What is view in Jenkins? We can customize view as per our needs. We can modify Jenkins home page. We can segregate jobs as per the type of jobs like free style jobs and maven jobs and so on. To create custom view: - Select List of Related Projects \u2000 - Select Default views \u2000 - Click on All \u2000 - Click on + and select Freestyle - Select List Views \u2000 - Select Job filter \u2000 - Select required jobs to be segregated \u2000 - Now, you can see different view What is User Administration in Jenkins? In Jenkins, we can create users, groups and can assign limited privileges to them so that, we can have better control on Jenkins. Users will not install Jenkins in their machines. They access Jenkins as a user. Here we can\u2019t assign permissions directly to users. Instead we create \u201cRoles\u201d and assign permissions to those roles. These roles we attach to users so that users get the permissions whatever we assign to those roles. What is Global tool configuration in Jenkins? We install Java, Maven, Git and many other tools in our server. Whenever Jenkins need those tools, by default Jenkins will install them automatically every time. But it\u2019s not a good practice. That\u2019s why we give installed path of all these tools in Jenkins so that whenever Jenkins need them, automatically Jenkins pull them form local machine instead of downloading every time. This way of giving path of these tools in Jenkins we call \u201cGlobal tool configuration\u201d What is Build? Build means, Compile the source code, assembling of all class files and finally creating deliverable Compile:\u2013 Convert Source code into machine-readable format Assembly (Linking):\u2013 Grouping all class files Deliverable:\u2013 .war, .jar The above process is same for any type of code. This process we call Build. What is Maven? Maven is one of the Build tools. It is the most advance build tool in the market. In this, everything is already pre-configured. Maven belongs to Apache Company. We use maven to build Java code only. We can\u2019t build other codes by using Maven. By default, we get so many plugins with Maven. You can write your own plugins as well. Maven\u2019s local repository is \u201c.M2\u201d where we can get required compilers and dependencies. Maven\u2019s main configuration file is \u201cpom.xml\u201d where we keep all instructions to build. Advantages of Maven? \u2000 - Automated tasks (Mention all in pom.xml) \u2000 - Multiple Tasks at a time \u2000 - Quality product \u2000 - Minimize bad builds - Keep history - Save time \u2013 Save money \u2000 - Gives set of standards - Gives define project life cycle (Goals) \u2000 - Manage all dependencies \u2000 - Uniformity in all projects \u2000 - Re-usability List of Build tools available in Market? \u2000 - C and C++: Make file \u2000 - .Net: Visual studio \u2000 - Java: Ant, Maven What is the architecture of Maven? Maven main configuration file is pom.xml. For one project, there will be one workspace and one pom.xml. Requirements for build:\u2013 - Source code (Will be pulled from Git hub) - Compiler (Pulls from remote repo and then put them in local repo, from there, maven pulls into Workspace) - Dependencies (Pulls from remote repo and then put them in local repo, from there, maven pulls into Workspace) What is Maven\u2019s Build Life Cycle? In maven, we have different goals. These are: - Generate resources (Dependencies) - Compile code - Unit test - Package (Build) - Install (in to local repo & artifactory) - Deploy (to servers) - Clean (delete all run time files) What does POM.XML contains? POM.XML is maven\u2019s main configuration file where we keep all details related to project. It contains: - Metadata about that project - Dependencies required to build the project - The kind of project - Kind of output you want (.jar, .war) - Description about that project What is Multi-Module Project in Maven? - Dividing big project into small modules, we call Multi Module Project. - Each module must have its own SRC folder & pom.xml so that build will happen separately. - To build all modules with one command, there should be a parent pom.xml file. This calls all child pom.xml files automatically. - In parent pom.xml file, need to mention the child pom.xml files in an order. What is Nagios? Nagios is one of the monitoring tools. By using Nagios we can monitor and give alerts. Where ever you install Nagios that becomes Nagios server. Monitoring is important, because we need to make sure that our servers should never go down. If at all in some exceptional cases server goes down, immediately we need alert in the form of intimation so that we can take required action to bring the server up immediately. So for this purpose, we use Nagios. Why do we have to use Nagios? There are many advantages in using Nagios: - It is oldest & Latest (every now and then, it is getting upgraded as per current market requirements) - Stable (we have been using this since so many years and it is performing well) - By default, we get so many Plug-ins - It is having its own Database. - Nagios is both Monitoring & Alerting tool. How does Nagios works? - We mention all details in configuration files what data to be collected from which machine. - Nagios daemon reads those details about what data to be collected. - Daemon use NRPE (Nagios Remote Plug-in Executer) plug-in to collect data form nodes and stores in its own database. - Finally displays in Nagios dashboard. What is the Directory structure of Nagios? /usr/local/nagios/bin \u2013 binary files /usr/local/nagios/sbin \u2013 CGI files (to get web page) /usr/local/nagios/libexec \u2013 plugins /usr/local/nagios/share \u2013 PHP Files /usr/local/nagios/etc \u2013 configuration files /usr/local/nagios/var \u2013 logs /usr/local/nagios/var/status.dat(file) \u2013 database What are the Important Configuration files in Nagios? Nagios main configuration file is /usr/local/nagios/etc/nagios.cfg /usr/local/nagios/etc/objects/localhost.cfg (where we keep hosts information) /usr/local/nagios/etc/objects/contacts.cfg (whom to be informed (emails)) /usr/local/nagios/etc/objects/timeperiods.cfg (at what time to monitor) /usr/local/nagios/etc/objects/commands.cfg (plugins to use) /usr/local/nagios/etc/objects/templates.cfg (sample templates) You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"DevOps Interview Question and Answered for Freshers and Experianced - 2"},{"location":"nightwolf-cotribution/devops_interview_questions/","text":"DevOps Interview Question and Answered for Freshers and Experianced \uf0c1 We have consolidated a list of frequently asked DevOps interview questions for Freshers and Experianced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. What is Source Code Management? It is a process through which we can store and manage any code. Developers write code, Testers write test cases and DevOps engineers write scripts. This code, we can store and manage in Source Code Management. Different teams can store code simultaneously. It saves all changes separately. We can retrieve this code at any point of time. What are the Advantages of Source Code Management ? - Helps in Achieving teamwork. - Can work on different features simultaneously. - Acts like pipeline b/w offshore & onshore teams. - Track changes (Minute level). - Different people from the same team, as well as different teams, can store code simultaneously. Available Source Code Management tools in the market? There are so many Source Code Management tools available in the market. Those are - Git - SVN - Perforce - Clear case Out of all these tools, Git is the most advanced tool in the market where we are getting so many advantages compared to other Source Code Management tools. What is Git? Git is one of the Source Code Management tools where we can store any type of code. Git is the most advanced tool in the market now. We also call Git is version control system because every update stored as a new version. At any point of time, we can get any previous version. We can go back to previous versions. Every version will have a unique number. That number we call commit-ID. By using this commit ID, we can track each change i.e. who did what at what time. For every version, it takes incremental backup instead of taking the whole backup. That\u2019s why Git occupies less space. Since it is occupying less space, it is very fast. What are the advantages of Git? Speed:- Git stores every update in the form of versions. For every version, it takes incremental backup instead of taking the whole backup. Since it is taking less space, Git is very fast. That incremental backup we call \u201cSnapshot\u201d. Parallel branching:- We can create any number of branches as per our requirement. No need to take prior permission from any one, unlike other Source Code Management tools. Branching is for parallel development. Git branches allow us to work simultaneously on multiple features. Fully Distributed:- A backup copy is available in multiple locations in each and everyone\u2019s server instead of keeping in one central location, unlike other Source Code Management tools. So even if we lose data from one server, we can recover it easily. That\u2019s why we call GIT as DVCS (Distributed Version Control System) What are the stages in Git? There are total of 4 stages in Git 1. Workspace:- It is the place where we can create files physically and modify. Being a Git user, we work in this work space. 2. Staging area/Indexing area:- In this area, Git takes a snapshot for every version. It is a buffer zone between workspace and local repository. We can\u2019t see this region because it is virtual. 3. Local repository:- It is the place where Git stores all commit locally. It is a hidden directory so that no one can delete it accidentally. Every commit will have unique commit ID. 4. Central repository:- It is the place where Git stores all commit centrally. It belongs to everyone who is working in your project. Git Hub is one of the central repositories. Used for storing the code and sharing the code to others in the team. What is the common branching strategy in Git? - Product is the same, so one repo. But different features. - Each feature has one separate branch - Finally, merge (code) all branches - For Parallel development - Can create any no of branches - Can create one branch on the basis of another branch - Changes are personal to that particular branch - Can put files only in branches (not in repo directly) - The default branch is \u201cMaster\u201d - Files created in a workspace will be visible in any of the branch workspaces until you commit. Once you commit, then that file belongs to that particular branch. How many types of repositories available in Git? There are two types of repositories available in Git Bare Repositories (Central) These repositories are only for Storing & Sharing the code All central repositories are bare repositories Non \u2013 Bare Repositories (Local) In these repositories, we can modify the files All local /user repositories are Bare Repositories Can you elaborate commit in Git? - Storing file permanently in the local repository we call commit. - For every commit, we get one commit ID. - It contains 40 long Alpha-numeric characters. - It uses the concept \u201cCheck some\u201d (It\u2019s a tool in Linux, generates binary value equal to the data present in file) - Even if you change one dot, Commit-ID will get changed. - Helps in tracking the changes What do you mean by \u201cSnapshot\u201d in Git? - It is a backup copy for each version git stores in a repository. - Snapshot is an incremental backup copy (only backup for new changes) - Snapshot represents some data of particular time so that, we can get data of particular time by taking that particular snapshot - This snapshot will be taken in Staging area in Git which is present between Git workspace and Git local repository. What is GitHub? Git hub is central git repository where we can store code centrally. Git hub belongs to Microsoft Company. We can create any number of repositories in Git hub. All public repositories are free and can be accessible by everyone. Private repositories are not free and can restrict public access for security. We can copy the repository from one account to other accounts also. This process we call as \u201cFork\u201d. In this repository also we can create branches. The default branch is \u201cMaster\u201d. What is Git merge? By default, we get one branch in git local repository called \u201cMaster\u201d. We can create any no of branches for parallel development. We write code for each feature in each branch so that development happens separately. Finally, we merge code off all branches in to Master and push to central repository. We can merge code to any other branch as well. But merging code into master is standard practice that being followed widely. Sometimes, while merging, conflict occurs. When same file is in different branches with different code, when try to merge those branches, conflict occurs. We need to resolve that conflict manually by rearranging the code. What is Git stash? We create multiple branches to work simultaneously on multiple features. But to work on multiple tasks simultaneously in one branch (i.e. on one feature), we use git stash. Stash is a temporary repository where we can store our content and bring it back whenever we want to continue with our work with that stored content. It removes content inside file from working directory and puts in stashing store and gives clean working directory so that we can start new work freshly. Later on you can bring back that stashed items to working directory and can resume your work on that file. Git stash applicable to modified files. Not new files. Once we finish our work, we can remove all stashed items form stash repository. What is Git Reset? Git Reset command is used to remove changes form staging area. This is bringing back file form staging area to work directory. We use this command before commit. Often we go with git add accidentally. In this case if we commit, that file will be committed. Once you commit, commit ID will be generated and it will be in the knowledge of everyone. So to avoid this one, we use Git reset. If you add \u201c\u2013hard\u201d flag to git reset command, in one go, file will be removed from staging area as well as working directory. We generally go with this one if we fell that something wrong in the file itself. What is Git Revert? Git Revert command is used to remove changes from all 3 stages (work directory, staging area and local repository). We use this command after commit. Sometimes, we commit accidentally and later on we realize that we shouldn\u2019t have done that. For this we use Git revert. This operation will generate new commit ID with some meaningful message to ignore previous commit where mistake is there. But, here we can\u2019t completely eliminate the commit where mistake is there. Because Git tracks each and every change. Difference between Git pull and Git clone? We use these two commands to get changes from central repository. For the first time if you want whole central repository in your local server, we use git clone. It brings entire repository to your local server. Next time onwards you might want only changes instead of whole repository. In this case, we use Git pull. - Git clone is to get whole copy of central repository - Git pull is to get only new changes from central repository (Incremental data) What is the difference between Git pull and Fetch? We use Git pull command to get changes from central repository. In this operation, internally two commands will get executed. One is Git fetch and another one is Git merge. Git fetch means, only bringing changes from central repo to local repo. But these changes will not be integrated to local repo which is there in your server. Git merge means, merging changes to your local repository which is there in your server. Then only you can see these changes. So Git pull is the combination of Git pull and Git merge. What is the difference between Git merge and rebase? We often use these commands to merge code in multiple branches. Both are almost same but few differences. When you run Git merge, one new merge commit will be generated which is having the history of both development branches. It preserves the history of both branches. By seeing this merge commit, everyone will come to know that we merged two branches. If you do Git rebase, commits in new branch will be applied on top of base branch tip. There won\u2019t be any merge commit here. It appears that you started working in one single branch form the beginning. This operation will not preserves the history of new branch. What is Git Bisect? Git Bisect we use to pick bad commit out of all good commits. Often developers do some mistakes. For them it is very difficult to pick that commit where mistake is there. They go with building all commits one by one to pick bad commit. But Git bisect made their lives easy. Git bisect divides all commits equally in to two parts (bisecting equally). Now instead of building each commit, they go with building both parts. Where ever bad commit is there, that part build will be failed. We do operation many times till we get bad commit. So Git bisect allows you to find a bad commit out of good commits. You don\u2019t have to trace down the bad commit by hand; git-bisect will do that for you. What is Git squash? To move multiple commits into its parent so that you end up with one commit. If you repeat this process multiple times, you can reduce \u201cn\u201d number of commits to a single one. Finally we will end up with only one parent commit. We use this operation just to reduce number of commits. What is Git hooks? We often call this as web hooks as well. By default we get some configuration files when you install git. These files we use to set some permissions and notification purpose. We have different types of hooks (pre commit hooks & post commit hooks) Pre-commit hooks:- Sometimes you would want every member in your team to follow certain pattern while giving commit message. Then only it should allow them to commit. These type of restrictions we call pre-commit hooks. Post-commit hooks:- Sometimes, being a manager you would want an email notification regarding every commit occurs in a central repository. This kind of things we call post-commit hooks. In simple terms, hooks are nothing but scripts to put some restrictions. What is Git cherry-pick? When you go with git merge, all commits which are there in new development branch will be merged into current branch where you are. But sometimes, requirement will be in such that you would want to get only one commit form development branch instead of merging all commits. In this case we go with git cherry-pick. Git cherry-pick will pick only one commit whatever you select and merges with commits which are there in your current branch. So picking particular commit and merging into your current branch we call git cherry-pick. What is the difference between Git and SVN? SVN:- It is centralized version control system (CVCS) where back up copy will be placed in only one central repository. There is no branching strategy in SVN. You can\u2019t create branches. So no parallel development. There is no local repository. So can\u2019t save anything locally. Every time after writing code you need to push that code to central repository immediately to save changes. Git:- It is a Distributed version control system where back up copy is available in everyone\u2019s machine\u2019s local repository as well as a central repository. We can create any no of branches as we want. So we can go in parallel development simultaneously. Every Git repository will have its own local repository. So we can save changes locally. At the end of our work finally, we can push code to a central repository. What is the commit message in Git? Every time we commit, while committing, we have to give commit message just to identify each commit. We can\u2019t remember to commit numbers because they contain 40 long alphanumeric characters. So, to remember commits easily, we give commit message. The format of commit message differs from company to company and individual to individual. We have one more way to identify commits. That is giving \u201cTags\u201d. Tag is a kind of meaningful name to a particular commit. Instead of referring to commit ID, we can refer to tags. Internally tag will refer to respective commit ID. These are the ways to get a particular commit easily. What is Configuration Management? It is a method through we automate admin tasks. Each and every minute details of a system, we call configuration details. If we do any change here means we are changing the configuration of a machine. That means we are managing the configuration of the machine. System administrators used to manage the configuration of machine through manually. DevOps engineers are managing this configuration through automated way by using some tools which are available in the market. That\u2019s why we call these tools as configuration management tools. What is IAC? IAC means Infrastructure As Code. It is the process through which we automate all admin tasks. Here we write code in Ruby script in chef. When you apply this code, automatically code will be converted into Infrastructure. So here we are getting so many advantages in writing the code. Those are: 1. Code is Testable (Testing code is easy compare to Infrastructure) 2. Code is Repeatable (Can re-use the same code again and again) 3. Code is Versionable (Can store in versions so that can get any previous versions at any time) What do you mean by IT Infrastructure?? IT Infrastructure is a composite of the following things 1. Software 2. Network 3. People 4. Process What are the problems that system admins used to face earlier when there were no configuration management tools? 1. Managing users & Groups is big hectic thing (create users and groups, delete, edit\u2026\u2026) 2. Dealing with packages (Installing, Upgrading & Uninstalling) 3. Taking backups on regular basis manually 4. Deploying all kinds of applications in servers 5. Configure services (Starting, stopping and restarting services) These are some problems that system administrators used to face earlier in their manual process of managing configuration of any machine. Why should we go with Configuration Management Tool? 1. By using the Configuration Management Tool, we can automate almost each and every admin task. 2. We can increase uptime so that can provide maximum user satisfaction. 3. Improve the performance of systems. 4. Ensure compliance 5. Prevent errors as tools won\u2019t do any errors 6. Reduce cost (Buy tool once and use 24/7) How this Configuration Management Tool works? Whatever system admins (Linux/windows) used to do manually, now we are automating all those tasks by using any Configuration Management Tool. We can use this tool whether your servers are in on-premises or in the cloud. It turns your code into infrastructure. So your code is versionable, repeatable and testable. You only need to tell what the desired configuration should be, not how to achieve it. Through automation, we get our desired state of server. This is unique feature of Configuration Management Tool. What is the architecture of Chef? Chef is an administration tool. In this we have total 3 stages: 1. Chef Workstation (It is the place where we write code) 2. Chef Server (It is the place where we store code) 3. Chef Node (It is the place where we apply code) We need to establish communication among workstation, server and nodes. You can have any no of nodes. There is no limit. Chef can manage any no of nodes effectively. Components of Chef? 1. Chef Workstation: Where you write the code 2. Chef Server: Where you upload the code 3. Chef Node: Where you apply the code 4. Knife: Tool to establish communication among workstation, server & node. 5. Chef-client: Tool runs on every chef node to pull code from chef server 6. Ohai: Maintains current state information of chef node (System Discovery Tool) 7. Idempotency: Tracking the state of system resources to ensure that the changes should not re-apply repeatedly. 8. Chef Supermarket: Where you get custom code How does Chef Works? We need to install chef package in workstation, server and nodes. We create cookbook in workstation. Inside cookbook, there will be a default recipe where you write code in ruby script. You can create any no of recipes. There is no limit. After writing code in recipe, we upload whole cookbook to chef server. Chef server acts as central hub storing code. Then, we need to add this cookbook\u2019s recipe to nodes run-list. Chef-client tool will be there in each and every chef node. It runs frequently. Chef-client comes to chef server and take that code and applies that code in node. This is how code will be converted into infrastructure. What is Idempotency? It is unique feature in all configuration management tools. It ensures that changes should not re-apply repeatedly. Once chef-client converted code into Infrastructure, then even chef-client runs again, it will not take any action. It won\u2019t do the same task again and again. If any new changes are there in that code, then only chef-client is going to take action. So it doesn\u2019t make any difference ever if you run chef-client any no of times. So tracking the system details to not to reapply changes again and again, we call Idempotency. What is Ohai and how does it works?? Ohai we call \u201cSystem Discovery Tool\u201d. It stores system information. It captures each and every minute details of system and updates it then and there if any new changes are there. Whenever chef-client converts code in infrastructure in node, immediately Ohai store will be updated. Next time onwards, before chef-client runs, it verifies in Ohai store to know about current state of information. So chef-client will come to know the current state of server. Then chef-client acts accordingly. If new changes are there, then only it will take action. If there are no new changes, then it won\u2019t take any action. Ohai tool helps in achieving this. How many types of chef server? Total there are 3 ways through which we can manage chef server. 1. Directly we can take chef server from Chef Company itself. In this case, everything will be managed by Chef Company. You will get support from chef. This type of server we call Managed/Hosted chef. This is completely Graphical User Interface (GUI). It\u2019s not free. We need to pay to Chef Company after exceeding free tier limit. 2. We can launch one server and we need to install chef server package. It is completely free package. It\u2019s GUI (Graphical User interface) 3. We can launch one server and we need to install chef server package. It is completely free package. It\u2019s CLI (Command Line Interface). What is there inside cookbook? Below mentioned files and folders will be there inside cookbook when you first create it: Chefignore: like .gitignore (to ignore files and folders) Kitchen.yml: for testing of cookbook Metadata.rb: name, author, version\u2026. etc of cookbook Readme.md: information about usage of cookbook Recipe: It is a file where you write code Spec: for unit test Test: for integration test What is Attributes concept in chef? Sometimes we might need to deploy web applications to in nodes and for that we need to know some host specific details of each server like IP Address, Hostname etc. Because we need to mention that in configuration files of each server. These files we call as Configuration files. This information will be vary from system to system. These host specific details that we mention in Configuration files,we call \u201cAttributes\u201d. Chef-client tool gathers these Attributes from Ohai store and puts in configuration files. Instead of hard coding these attributes, we mention as variables so that every time, file will be updated with latest details of their respective nodes. What is Run-list in Chef? This is an ordered list of recipes that we are going to apply to nodes. We mention all recipes in cookbook and then we upload that cookbook to chef server. Then, we attach all recipes to nodes run-list in sequence order. When chef-client runs, it applies all recipes to nodes in the same order whatever the order you mention in run-list. Because sometimes order is important especially when we deal with dependent recipes. What is bootstrap? It is the process of adding chef node to chef server or we can call, bringing any machine into chef environment. In this bootstrapping process total three action will be performed automatically. 1. Node gets connected to chef server. 2. Chef server will install chef package in chef node. 3. Cookbooks will be applied to chef node. It is only one time effort. As and when we purchase any new machine in company, immediately we add that server to chef server. At a time, we can only bootstrap one machine, not multiple machines. What is the workflow of Chef? We connect chef workstation, chef server and chef node with each other. After that, we create cookbook in chef workstation and inside that cookbook, we write code in recipe w.r.t. the infrastructure to be created. Then we upload entire cookbook to chef server and attach that cookbook\u2019s recipe to nodes run-list. Now we automate chef-client which will be there in all chef nodes. Chef-client runs frequently towards chef server for new code. So chef-client will get that code from server and finally applies to chef node. This is how, code is converted into infrastructure. If no changes are there in code, even if chef-client runs any no of time, it won\u2019t take any action until it finds some changes in code. This is what we call Idempotency. How does we connect Chef Workstation to Chef Server? First we download started kit from chef server. This will be downloaded in the form of zip file. If we extract this zip file, we will get chef-repo folder. This chef-repo folder we need to place in chef workstation. Inside chef-repo folder, we can see total three folders. They are .chef, cookbooks and roles. Out of these three, .chef folder is responsible to establish communication between chef server and chef workstation. Because, inside .chef folder, we can see two files. They are knife.rb and organization.pem. Inside kinfe.rb, there will be the url (address) of chef server. Because of this url, communication will be established between chef server and chef workstation. This is how we connect Chef Workstation to Chef Server. How does the chef-client runs automatically? By default, chef-client runs manually. So we need to automate this manually. For this, we use \u201ccron tool\u201d which is the default tool in all Linux machines use to schedule tasks to be executed automatically at frequent intervals. So in this \u201ccrontab\u201d file, we give chef-client command and we need to set the timing as per our requirement. Then onwards chef-client runs automatically after every frequent intervals. It is only one time effort. When we purchase any new server in company, along with bootstrap, we automate chef-client then and there. What is chef supermarket? Chef supermarket is the place where we get custom cookbooks. Every time we need not to create cookbooks and need not to write code from scratch. We can go with custom cookbooks which are available in chef supermarket being provided by chef organization and community. We can download these cookbooks and modify as per our needs. We get almost each and every cookbook from chef supermarket. They are safe to use. What is wrapper cookbook? Either we can download those chef supermarket cookbooks or without downloading, we can call these supermarket cookbooks during run time so that every time we get updates automatically for that cookbook if any new updates are there. Here, we use our own cookbook to call chef supermarket cookbook. This process of calling cookbook by using another cookbook, we call wrapper cookbook. Especially, we use this concept to automate chef-client. What is \u201croles\u201d in chef? Roles are nothing but a Custom run-list. We create role & upload to chef server & assign them to nodes. If we have so many nodes, need to add cookbook to run-list of all those nodes, it is very difficult to attach to all nodes run-list. So, we create role & attach that role to all those nodes once. Next time onwards, add cookbook to that role. Automatically, that cookbook will be attached to all those nodes. So role is one time effort. Instead of adding cookbooks to each & every node\u2019s run-list always, just create a role & attach that role to nodes. When we add cookbook to that role, it will be automatically applied to all nodes those assigned with that role. What is include_recipe in chef? By default, we can call one recipe at a time in one cookbook. But if you want to call multiple recipes from same cookbook, we use include_recipe concept. Here, we take default recipe and we mention all recipes to be called in this default recipe in an order. If we call default recipe, automatically default recipe will call all other recipes which are there inside default recipe. By using one recipe, we can call any number of recipes. This process of calling one recipe by using other recipe, we call as include_recipe. Here condition is we can call recipes from same cookbook, but not from different cookbooks. How to deploy a web server by using chef? package \u2018httpd\u2019 do action :install end file \u2018/var/www/html/index.html\u2019 do content \u2018Hello nightwolf Students!!\u2019 action :create end service \u2018httpd\u2019 do action [ :enable, :start ] end How to write ruby code to create file, directory? file \u2018/myfile\u2019 do content \u2018This is my second file\u2019 action :create owner \u2018root\u2019 group \u2018root\u2019 end directory \u2018/mydir\u2019 do action :create owner \u2018root\u2019 group \u2018root\u2019 end How to write ruby code to create user, group and install package? user \u2018user1\u2019 do action: create end group \u2018group1\u2019 do action :create members \u2018user1\u2019 append true end package \u2018httpd\u2019 do action: install end What is container? The container is like a virtual machine in which we can deploy any type of applications, softwares and libraries. It\u2019s a light weight virtual machine which uses OS in the form of image, which is having less in size compare to traditional VMware and oracle virtual box OS images. Container word has been taken from shipping containers. It has everything to run an application. What is virtualization? Logically dividing big machine into multiple virtual machines so that each virtual machine acts as new server and we can deploy any kind of applications in it. For this first we install any virtualization software on top of base OS. This virtualization software will divide base machine resources in to logical components. In a simple terms, logically dividing one machine into multiple machines we call virtualization. What is Docker? Docker is a tool by using which, we create containers in less time. Docker uses light weight OS in the form of docker images that we will get from docker hub. Docker is open source now. It became so popular because of its unique virtualization concept called \u201cContainerization\u201d which is not there in other tools. We can use docker in both windows and Linux machines. What do you mean by docker image? Docker image is light weight OS provided by docker company. We can get any type of docker image form docker hub. We use these docker images to create docker containers. This docker images may contain only OS or OS + other softwares as well. Each software in docker image, will be stored in the form of layer. Advantage of using docker images is, we can replicate the same environment any no of times. What are the ways through which we can create docker images? There are three ways through which we can create docker images. 1. We can take any type of docker image directly from docker hub being provided by docker company and docker community. 2. We can create our own docker images form our own docker containers i.e. first we create container form base docker image taken form docker hub and then by going inside container, we install all required soft wares and then create docker image from our own docker container. 3. We can create docker image form docker file. It is the most preferred way of creating docker images. What is docker file and why do we use it? It is a just normal text file with instructions in it to build docker image. It is the automated way of creating docker images. Once you build docker image, automatically docker file will be created. In this file, we mention required OS image and all required soft wares in the form of instructions. Once we build docker file, back end, docker container will be created and then docker image will be crated from that container and that container will be destroyed automatically. Difference between docker and VM Ware? VM Ware uses complete OS which contains GBs in size. But docker image size is MBs only. So it takes less size. That\u2019s why it takes less base machine resources. This docker image is compressed version of OS. The second advantage of docker is, there is no pre-allocation of RAM. During run time, it takes RAM as pre requirement from base machine and one\u2019s job is done, it release RAM. But in VM Ware, pre-allocation of RAM is there and it blocked whether it uses or not. So need more RAM for base machine if you want to use VM Ware unlike Docker. What is OS-Lever Virtualization? It is the unique feature of Docker which is not available in other virtualization soft wares. Docker takes most of UNIX features form host machine OS and it only takes extra layers of required OS in the form of docker image. So docker image contains only extra layers of required OS. For core UNIX kernel, it depends upon host OS, why because UNIX kernel is same in any of the UNIX and Linux flavors. In a simple terms, docker takes host OS virtually. That\u2019s why we call this concept as OS-Lever Virtualization. What is Layered file system/Union file system? Inside docker container, wheat ever we do, that forms as a new layer. For instance, creating files, directories, installing packages etc. This is what we call as layered file system. Each layer takes less space. We can create docker image form this container. In that docker image also we get all these layers and forms unity. That\u2019s why we also call Union File System. If we create container out of docker image, you can able to see all those files, directories and packages. This is what replication of same environment. What are the benefits of Docker? \u2000 1. Containerization (OS level virtualization) (No need guest OS) \u2000 2. No pre-allocation of RAM \u20003. Can replicate same environment \u2000 4. Less cost \u2000 5. Less weight (MB\u2019s in size) \u2000 6. Fast to fire up \u2000 7. Can run on physical/virtual/cloud \u2000 8. Can re-use (same image) \u2000 9. Can create containers in less time Next Page You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"DevOps Interview Questions for Freshers and Experianced"},{"location":"nightwolf-cotribution/devops_interview_questions/#devops-interview-question-and-answered-for-freshers-and-experianced","text":"We have consolidated a list of frequently asked DevOps interview questions for Freshers and Experianced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. What is Source Code Management? It is a process through which we can store and manage any code. Developers write code, Testers write test cases and DevOps engineers write scripts. This code, we can store and manage in Source Code Management. Different teams can store code simultaneously. It saves all changes separately. We can retrieve this code at any point of time. What are the Advantages of Source Code Management ? - Helps in Achieving teamwork. - Can work on different features simultaneously. - Acts like pipeline b/w offshore & onshore teams. - Track changes (Minute level). - Different people from the same team, as well as different teams, can store code simultaneously. Available Source Code Management tools in the market? There are so many Source Code Management tools available in the market. Those are - Git - SVN - Perforce - Clear case Out of all these tools, Git is the most advanced tool in the market where we are getting so many advantages compared to other Source Code Management tools. What is Git? Git is one of the Source Code Management tools where we can store any type of code. Git is the most advanced tool in the market now. We also call Git is version control system because every update stored as a new version. At any point of time, we can get any previous version. We can go back to previous versions. Every version will have a unique number. That number we call commit-ID. By using this commit ID, we can track each change i.e. who did what at what time. For every version, it takes incremental backup instead of taking the whole backup. That\u2019s why Git occupies less space. Since it is occupying less space, it is very fast. What are the advantages of Git? Speed:- Git stores every update in the form of versions. For every version, it takes incremental backup instead of taking the whole backup. Since it is taking less space, Git is very fast. That incremental backup we call \u201cSnapshot\u201d. Parallel branching:- We can create any number of branches as per our requirement. No need to take prior permission from any one, unlike other Source Code Management tools. Branching is for parallel development. Git branches allow us to work simultaneously on multiple features. Fully Distributed:- A backup copy is available in multiple locations in each and everyone\u2019s server instead of keeping in one central location, unlike other Source Code Management tools. So even if we lose data from one server, we can recover it easily. That\u2019s why we call GIT as DVCS (Distributed Version Control System) What are the stages in Git? There are total of 4 stages in Git 1. Workspace:- It is the place where we can create files physically and modify. Being a Git user, we work in this work space. 2. Staging area/Indexing area:- In this area, Git takes a snapshot for every version. It is a buffer zone between workspace and local repository. We can\u2019t see this region because it is virtual. 3. Local repository:- It is the place where Git stores all commit locally. It is a hidden directory so that no one can delete it accidentally. Every commit will have unique commit ID. 4. Central repository:- It is the place where Git stores all commit centrally. It belongs to everyone who is working in your project. Git Hub is one of the central repositories. Used for storing the code and sharing the code to others in the team. What is the common branching strategy in Git? - Product is the same, so one repo. But different features. - Each feature has one separate branch - Finally, merge (code) all branches - For Parallel development - Can create any no of branches - Can create one branch on the basis of another branch - Changes are personal to that particular branch - Can put files only in branches (not in repo directly) - The default branch is \u201cMaster\u201d - Files created in a workspace will be visible in any of the branch workspaces until you commit. Once you commit, then that file belongs to that particular branch. How many types of repositories available in Git? There are two types of repositories available in Git Bare Repositories (Central) These repositories are only for Storing & Sharing the code All central repositories are bare repositories Non \u2013 Bare Repositories (Local) In these repositories, we can modify the files All local /user repositories are Bare Repositories Can you elaborate commit in Git? - Storing file permanently in the local repository we call commit. - For every commit, we get one commit ID. - It contains 40 long Alpha-numeric characters. - It uses the concept \u201cCheck some\u201d (It\u2019s a tool in Linux, generates binary value equal to the data present in file) - Even if you change one dot, Commit-ID will get changed. - Helps in tracking the changes What do you mean by \u201cSnapshot\u201d in Git? - It is a backup copy for each version git stores in a repository. - Snapshot is an incremental backup copy (only backup for new changes) - Snapshot represents some data of particular time so that, we can get data of particular time by taking that particular snapshot - This snapshot will be taken in Staging area in Git which is present between Git workspace and Git local repository. What is GitHub? Git hub is central git repository where we can store code centrally. Git hub belongs to Microsoft Company. We can create any number of repositories in Git hub. All public repositories are free and can be accessible by everyone. Private repositories are not free and can restrict public access for security. We can copy the repository from one account to other accounts also. This process we call as \u201cFork\u201d. In this repository also we can create branches. The default branch is \u201cMaster\u201d. What is Git merge? By default, we get one branch in git local repository called \u201cMaster\u201d. We can create any no of branches for parallel development. We write code for each feature in each branch so that development happens separately. Finally, we merge code off all branches in to Master and push to central repository. We can merge code to any other branch as well. But merging code into master is standard practice that being followed widely. Sometimes, while merging, conflict occurs. When same file is in different branches with different code, when try to merge those branches, conflict occurs. We need to resolve that conflict manually by rearranging the code. What is Git stash? We create multiple branches to work simultaneously on multiple features. But to work on multiple tasks simultaneously in one branch (i.e. on one feature), we use git stash. Stash is a temporary repository where we can store our content and bring it back whenever we want to continue with our work with that stored content. It removes content inside file from working directory and puts in stashing store and gives clean working directory so that we can start new work freshly. Later on you can bring back that stashed items to working directory and can resume your work on that file. Git stash applicable to modified files. Not new files. Once we finish our work, we can remove all stashed items form stash repository. What is Git Reset? Git Reset command is used to remove changes form staging area. This is bringing back file form staging area to work directory. We use this command before commit. Often we go with git add accidentally. In this case if we commit, that file will be committed. Once you commit, commit ID will be generated and it will be in the knowledge of everyone. So to avoid this one, we use Git reset. If you add \u201c\u2013hard\u201d flag to git reset command, in one go, file will be removed from staging area as well as working directory. We generally go with this one if we fell that something wrong in the file itself. What is Git Revert? Git Revert command is used to remove changes from all 3 stages (work directory, staging area and local repository). We use this command after commit. Sometimes, we commit accidentally and later on we realize that we shouldn\u2019t have done that. For this we use Git revert. This operation will generate new commit ID with some meaningful message to ignore previous commit where mistake is there. But, here we can\u2019t completely eliminate the commit where mistake is there. Because Git tracks each and every change. Difference between Git pull and Git clone? We use these two commands to get changes from central repository. For the first time if you want whole central repository in your local server, we use git clone. It brings entire repository to your local server. Next time onwards you might want only changes instead of whole repository. In this case, we use Git pull. - Git clone is to get whole copy of central repository - Git pull is to get only new changes from central repository (Incremental data) What is the difference between Git pull and Fetch? We use Git pull command to get changes from central repository. In this operation, internally two commands will get executed. One is Git fetch and another one is Git merge. Git fetch means, only bringing changes from central repo to local repo. But these changes will not be integrated to local repo which is there in your server. Git merge means, merging changes to your local repository which is there in your server. Then only you can see these changes. So Git pull is the combination of Git pull and Git merge. What is the difference between Git merge and rebase? We often use these commands to merge code in multiple branches. Both are almost same but few differences. When you run Git merge, one new merge commit will be generated which is having the history of both development branches. It preserves the history of both branches. By seeing this merge commit, everyone will come to know that we merged two branches. If you do Git rebase, commits in new branch will be applied on top of base branch tip. There won\u2019t be any merge commit here. It appears that you started working in one single branch form the beginning. This operation will not preserves the history of new branch. What is Git Bisect? Git Bisect we use to pick bad commit out of all good commits. Often developers do some mistakes. For them it is very difficult to pick that commit where mistake is there. They go with building all commits one by one to pick bad commit. But Git bisect made their lives easy. Git bisect divides all commits equally in to two parts (bisecting equally). Now instead of building each commit, they go with building both parts. Where ever bad commit is there, that part build will be failed. We do operation many times till we get bad commit. So Git bisect allows you to find a bad commit out of good commits. You don\u2019t have to trace down the bad commit by hand; git-bisect will do that for you. What is Git squash? To move multiple commits into its parent so that you end up with one commit. If you repeat this process multiple times, you can reduce \u201cn\u201d number of commits to a single one. Finally we will end up with only one parent commit. We use this operation just to reduce number of commits. What is Git hooks? We often call this as web hooks as well. By default we get some configuration files when you install git. These files we use to set some permissions and notification purpose. We have different types of hooks (pre commit hooks & post commit hooks) Pre-commit hooks:- Sometimes you would want every member in your team to follow certain pattern while giving commit message. Then only it should allow them to commit. These type of restrictions we call pre-commit hooks. Post-commit hooks:- Sometimes, being a manager you would want an email notification regarding every commit occurs in a central repository. This kind of things we call post-commit hooks. In simple terms, hooks are nothing but scripts to put some restrictions. What is Git cherry-pick? When you go with git merge, all commits which are there in new development branch will be merged into current branch where you are. But sometimes, requirement will be in such that you would want to get only one commit form development branch instead of merging all commits. In this case we go with git cherry-pick. Git cherry-pick will pick only one commit whatever you select and merges with commits which are there in your current branch. So picking particular commit and merging into your current branch we call git cherry-pick. What is the difference between Git and SVN? SVN:- It is centralized version control system (CVCS) where back up copy will be placed in only one central repository. There is no branching strategy in SVN. You can\u2019t create branches. So no parallel development. There is no local repository. So can\u2019t save anything locally. Every time after writing code you need to push that code to central repository immediately to save changes. Git:- It is a Distributed version control system where back up copy is available in everyone\u2019s machine\u2019s local repository as well as a central repository. We can create any no of branches as we want. So we can go in parallel development simultaneously. Every Git repository will have its own local repository. So we can save changes locally. At the end of our work finally, we can push code to a central repository. What is the commit message in Git? Every time we commit, while committing, we have to give commit message just to identify each commit. We can\u2019t remember to commit numbers because they contain 40 long alphanumeric characters. So, to remember commits easily, we give commit message. The format of commit message differs from company to company and individual to individual. We have one more way to identify commits. That is giving \u201cTags\u201d. Tag is a kind of meaningful name to a particular commit. Instead of referring to commit ID, we can refer to tags. Internally tag will refer to respective commit ID. These are the ways to get a particular commit easily. What is Configuration Management? It is a method through we automate admin tasks. Each and every minute details of a system, we call configuration details. If we do any change here means we are changing the configuration of a machine. That means we are managing the configuration of the machine. System administrators used to manage the configuration of machine through manually. DevOps engineers are managing this configuration through automated way by using some tools which are available in the market. That\u2019s why we call these tools as configuration management tools. What is IAC? IAC means Infrastructure As Code. It is the process through which we automate all admin tasks. Here we write code in Ruby script in chef. When you apply this code, automatically code will be converted into Infrastructure. So here we are getting so many advantages in writing the code. Those are: 1. Code is Testable (Testing code is easy compare to Infrastructure) 2. Code is Repeatable (Can re-use the same code again and again) 3. Code is Versionable (Can store in versions so that can get any previous versions at any time) What do you mean by IT Infrastructure?? IT Infrastructure is a composite of the following things 1. Software 2. Network 3. People 4. Process What are the problems that system admins used to face earlier when there were no configuration management tools? 1. Managing users & Groups is big hectic thing (create users and groups, delete, edit\u2026\u2026) 2. Dealing with packages (Installing, Upgrading & Uninstalling) 3. Taking backups on regular basis manually 4. Deploying all kinds of applications in servers 5. Configure services (Starting, stopping and restarting services) These are some problems that system administrators used to face earlier in their manual process of managing configuration of any machine. Why should we go with Configuration Management Tool? 1. By using the Configuration Management Tool, we can automate almost each and every admin task. 2. We can increase uptime so that can provide maximum user satisfaction. 3. Improve the performance of systems. 4. Ensure compliance 5. Prevent errors as tools won\u2019t do any errors 6. Reduce cost (Buy tool once and use 24/7) How this Configuration Management Tool works? Whatever system admins (Linux/windows) used to do manually, now we are automating all those tasks by using any Configuration Management Tool. We can use this tool whether your servers are in on-premises or in the cloud. It turns your code into infrastructure. So your code is versionable, repeatable and testable. You only need to tell what the desired configuration should be, not how to achieve it. Through automation, we get our desired state of server. This is unique feature of Configuration Management Tool. What is the architecture of Chef? Chef is an administration tool. In this we have total 3 stages: 1. Chef Workstation (It is the place where we write code) 2. Chef Server (It is the place where we store code) 3. Chef Node (It is the place where we apply code) We need to establish communication among workstation, server and nodes. You can have any no of nodes. There is no limit. Chef can manage any no of nodes effectively. Components of Chef? 1. Chef Workstation: Where you write the code 2. Chef Server: Where you upload the code 3. Chef Node: Where you apply the code 4. Knife: Tool to establish communication among workstation, server & node. 5. Chef-client: Tool runs on every chef node to pull code from chef server 6. Ohai: Maintains current state information of chef node (System Discovery Tool) 7. Idempotency: Tracking the state of system resources to ensure that the changes should not re-apply repeatedly. 8. Chef Supermarket: Where you get custom code How does Chef Works? We need to install chef package in workstation, server and nodes. We create cookbook in workstation. Inside cookbook, there will be a default recipe where you write code in ruby script. You can create any no of recipes. There is no limit. After writing code in recipe, we upload whole cookbook to chef server. Chef server acts as central hub storing code. Then, we need to add this cookbook\u2019s recipe to nodes run-list. Chef-client tool will be there in each and every chef node. It runs frequently. Chef-client comes to chef server and take that code and applies that code in node. This is how code will be converted into infrastructure. What is Idempotency? It is unique feature in all configuration management tools. It ensures that changes should not re-apply repeatedly. Once chef-client converted code into Infrastructure, then even chef-client runs again, it will not take any action. It won\u2019t do the same task again and again. If any new changes are there in that code, then only chef-client is going to take action. So it doesn\u2019t make any difference ever if you run chef-client any no of times. So tracking the system details to not to reapply changes again and again, we call Idempotency. What is Ohai and how does it works?? Ohai we call \u201cSystem Discovery Tool\u201d. It stores system information. It captures each and every minute details of system and updates it then and there if any new changes are there. Whenever chef-client converts code in infrastructure in node, immediately Ohai store will be updated. Next time onwards, before chef-client runs, it verifies in Ohai store to know about current state of information. So chef-client will come to know the current state of server. Then chef-client acts accordingly. If new changes are there, then only it will take action. If there are no new changes, then it won\u2019t take any action. Ohai tool helps in achieving this. How many types of chef server? Total there are 3 ways through which we can manage chef server. 1. Directly we can take chef server from Chef Company itself. In this case, everything will be managed by Chef Company. You will get support from chef. This type of server we call Managed/Hosted chef. This is completely Graphical User Interface (GUI). It\u2019s not free. We need to pay to Chef Company after exceeding free tier limit. 2. We can launch one server and we need to install chef server package. It is completely free package. It\u2019s GUI (Graphical User interface) 3. We can launch one server and we need to install chef server package. It is completely free package. It\u2019s CLI (Command Line Interface). What is there inside cookbook? Below mentioned files and folders will be there inside cookbook when you first create it: Chefignore: like .gitignore (to ignore files and folders) Kitchen.yml: for testing of cookbook Metadata.rb: name, author, version\u2026. etc of cookbook Readme.md: information about usage of cookbook Recipe: It is a file where you write code Spec: for unit test Test: for integration test What is Attributes concept in chef? Sometimes we might need to deploy web applications to in nodes and for that we need to know some host specific details of each server like IP Address, Hostname etc. Because we need to mention that in configuration files of each server. These files we call as Configuration files. This information will be vary from system to system. These host specific details that we mention in Configuration files,we call \u201cAttributes\u201d. Chef-client tool gathers these Attributes from Ohai store and puts in configuration files. Instead of hard coding these attributes, we mention as variables so that every time, file will be updated with latest details of their respective nodes. What is Run-list in Chef? This is an ordered list of recipes that we are going to apply to nodes. We mention all recipes in cookbook and then we upload that cookbook to chef server. Then, we attach all recipes to nodes run-list in sequence order. When chef-client runs, it applies all recipes to nodes in the same order whatever the order you mention in run-list. Because sometimes order is important especially when we deal with dependent recipes. What is bootstrap? It is the process of adding chef node to chef server or we can call, bringing any machine into chef environment. In this bootstrapping process total three action will be performed automatically. 1. Node gets connected to chef server. 2. Chef server will install chef package in chef node. 3. Cookbooks will be applied to chef node. It is only one time effort. As and when we purchase any new machine in company, immediately we add that server to chef server. At a time, we can only bootstrap one machine, not multiple machines. What is the workflow of Chef? We connect chef workstation, chef server and chef node with each other. After that, we create cookbook in chef workstation and inside that cookbook, we write code in recipe w.r.t. the infrastructure to be created. Then we upload entire cookbook to chef server and attach that cookbook\u2019s recipe to nodes run-list. Now we automate chef-client which will be there in all chef nodes. Chef-client runs frequently towards chef server for new code. So chef-client will get that code from server and finally applies to chef node. This is how, code is converted into infrastructure. If no changes are there in code, even if chef-client runs any no of time, it won\u2019t take any action until it finds some changes in code. This is what we call Idempotency. How does we connect Chef Workstation to Chef Server? First we download started kit from chef server. This will be downloaded in the form of zip file. If we extract this zip file, we will get chef-repo folder. This chef-repo folder we need to place in chef workstation. Inside chef-repo folder, we can see total three folders. They are .chef, cookbooks and roles. Out of these three, .chef folder is responsible to establish communication between chef server and chef workstation. Because, inside .chef folder, we can see two files. They are knife.rb and organization.pem. Inside kinfe.rb, there will be the url (address) of chef server. Because of this url, communication will be established between chef server and chef workstation. This is how we connect Chef Workstation to Chef Server. How does the chef-client runs automatically? By default, chef-client runs manually. So we need to automate this manually. For this, we use \u201ccron tool\u201d which is the default tool in all Linux machines use to schedule tasks to be executed automatically at frequent intervals. So in this \u201ccrontab\u201d file, we give chef-client command and we need to set the timing as per our requirement. Then onwards chef-client runs automatically after every frequent intervals. It is only one time effort. When we purchase any new server in company, along with bootstrap, we automate chef-client then and there. What is chef supermarket? Chef supermarket is the place where we get custom cookbooks. Every time we need not to create cookbooks and need not to write code from scratch. We can go with custom cookbooks which are available in chef supermarket being provided by chef organization and community. We can download these cookbooks and modify as per our needs. We get almost each and every cookbook from chef supermarket. They are safe to use. What is wrapper cookbook? Either we can download those chef supermarket cookbooks or without downloading, we can call these supermarket cookbooks during run time so that every time we get updates automatically for that cookbook if any new updates are there. Here, we use our own cookbook to call chef supermarket cookbook. This process of calling cookbook by using another cookbook, we call wrapper cookbook. Especially, we use this concept to automate chef-client. What is \u201croles\u201d in chef? Roles are nothing but a Custom run-list. We create role & upload to chef server & assign them to nodes. If we have so many nodes, need to add cookbook to run-list of all those nodes, it is very difficult to attach to all nodes run-list. So, we create role & attach that role to all those nodes once. Next time onwards, add cookbook to that role. Automatically, that cookbook will be attached to all those nodes. So role is one time effort. Instead of adding cookbooks to each & every node\u2019s run-list always, just create a role & attach that role to nodes. When we add cookbook to that role, it will be automatically applied to all nodes those assigned with that role. What is include_recipe in chef? By default, we can call one recipe at a time in one cookbook. But if you want to call multiple recipes from same cookbook, we use include_recipe concept. Here, we take default recipe and we mention all recipes to be called in this default recipe in an order. If we call default recipe, automatically default recipe will call all other recipes which are there inside default recipe. By using one recipe, we can call any number of recipes. This process of calling one recipe by using other recipe, we call as include_recipe. Here condition is we can call recipes from same cookbook, but not from different cookbooks. How to deploy a web server by using chef? package \u2018httpd\u2019 do action :install end file \u2018/var/www/html/index.html\u2019 do content \u2018Hello nightwolf Students!!\u2019 action :create end service \u2018httpd\u2019 do action [ :enable, :start ] end How to write ruby code to create file, directory? file \u2018/myfile\u2019 do content \u2018This is my second file\u2019 action :create owner \u2018root\u2019 group \u2018root\u2019 end directory \u2018/mydir\u2019 do action :create owner \u2018root\u2019 group \u2018root\u2019 end How to write ruby code to create user, group and install package? user \u2018user1\u2019 do action: create end group \u2018group1\u2019 do action :create members \u2018user1\u2019 append true end package \u2018httpd\u2019 do action: install end What is container? The container is like a virtual machine in which we can deploy any type of applications, softwares and libraries. It\u2019s a light weight virtual machine which uses OS in the form of image, which is having less in size compare to traditional VMware and oracle virtual box OS images. Container word has been taken from shipping containers. It has everything to run an application. What is virtualization? Logically dividing big machine into multiple virtual machines so that each virtual machine acts as new server and we can deploy any kind of applications in it. For this first we install any virtualization software on top of base OS. This virtualization software will divide base machine resources in to logical components. In a simple terms, logically dividing one machine into multiple machines we call virtualization. What is Docker? Docker is a tool by using which, we create containers in less time. Docker uses light weight OS in the form of docker images that we will get from docker hub. Docker is open source now. It became so popular because of its unique virtualization concept called \u201cContainerization\u201d which is not there in other tools. We can use docker in both windows and Linux machines. What do you mean by docker image? Docker image is light weight OS provided by docker company. We can get any type of docker image form docker hub. We use these docker images to create docker containers. This docker images may contain only OS or OS + other softwares as well. Each software in docker image, will be stored in the form of layer. Advantage of using docker images is, we can replicate the same environment any no of times. What are the ways through which we can create docker images? There are three ways through which we can create docker images. 1. We can take any type of docker image directly from docker hub being provided by docker company and docker community. 2. We can create our own docker images form our own docker containers i.e. first we create container form base docker image taken form docker hub and then by going inside container, we install all required soft wares and then create docker image from our own docker container. 3. We can create docker image form docker file. It is the most preferred way of creating docker images. What is docker file and why do we use it? It is a just normal text file with instructions in it to build docker image. It is the automated way of creating docker images. Once you build docker image, automatically docker file will be created. In this file, we mention required OS image and all required soft wares in the form of instructions. Once we build docker file, back end, docker container will be created and then docker image will be crated from that container and that container will be destroyed automatically. Difference between docker and VM Ware? VM Ware uses complete OS which contains GBs in size. But docker image size is MBs only. So it takes less size. That\u2019s why it takes less base machine resources. This docker image is compressed version of OS. The second advantage of docker is, there is no pre-allocation of RAM. During run time, it takes RAM as pre requirement from base machine and one\u2019s job is done, it release RAM. But in VM Ware, pre-allocation of RAM is there and it blocked whether it uses or not. So need more RAM for base machine if you want to use VM Ware unlike Docker. What is OS-Lever Virtualization? It is the unique feature of Docker which is not available in other virtualization soft wares. Docker takes most of UNIX features form host machine OS and it only takes extra layers of required OS in the form of docker image. So docker image contains only extra layers of required OS. For core UNIX kernel, it depends upon host OS, why because UNIX kernel is same in any of the UNIX and Linux flavors. In a simple terms, docker takes host OS virtually. That\u2019s why we call this concept as OS-Lever Virtualization. What is Layered file system/Union file system? Inside docker container, wheat ever we do, that forms as a new layer. For instance, creating files, directories, installing packages etc. This is what we call as layered file system. Each layer takes less space. We can create docker image form this container. In that docker image also we get all these layers and forms unity. That\u2019s why we also call Union File System. If we create container out of docker image, you can able to see all those files, directories and packages. This is what replication of same environment. What are the benefits of Docker? \u2000 1. Containerization (OS level virtualization) (No need guest OS) \u2000 2. No pre-allocation of RAM \u20003. Can replicate same environment \u2000 4. Less cost \u2000 5. Less weight (MB\u2019s in size) \u2000 6. Fast to fire up \u2000 7. Can run on physical/virtual/cloud \u2000 8. Can re-use (same image) \u2000 9. Can create containers in less time","title":"DevOps Interview Question and Answered for Freshers and Experianced"},{"location":"nightwolf-cotribution/docker_interview_questions/","text":"Docker interview questions and Answers \uf0c1 We have consolidated a list of frequently asked Docker interview questions for Freshers and Experianced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. Docker Basic Interview Questions \uf0c1 Can you tell something about docker container? - In simplest terms, docker containers consist of applications and all their dependencies. - They share the kernel and system resources with other containers and run asisolated systems in the host operating system. - The main aim of docker containers is to get rid of the infrastructure dependency while deploying and running applications. This means that any containerized application can run on any platform irrespective of the infrastructure being used beneath. - Technically, they are just the runtime instances of docker images. What are docker images? They are executable packages(bundled with application code & dependencies, soware packages, etc.) for the purpose of creating containers. Docker images can be deployed to any docker environment and the containers can be spun up there to run the application. What is a DockerFile? It is a text file that has all commands which need to be run for building a given image. Can you tell what is the functionality of a hypervisor? A hypervisor is a so\u2000\u2000ware that makes virtualization happen because of which is sometimes referred to as the Virtual Machine Monitor. This divides the resources of the host system and allocates them to each guest environment installed. - This means that multiple OS can be installed on a single host system. Hypervisors are of 2 types: 1. Native Hypervisor: This type is also called a Bare-metal Hypervisor and runs directly on the underlying host system which also ensures direct access to the host hardware which is why it does not require base OS. 2. Hosted Hypervisor: This type makes use of the underlying host operating system which has the existing OS installed. What can you tell about Docker Compose? It is a YAML file consisting of all the details regarding various services, networks, and volumes that are needed for setting up the Docker-based application. So, dockercompose is used for creating multiple containers, host them and establish communication between them. For the purpose of communication amongst the containers, ports are exposed by each and every container. Can you tell something about docker namespace? A namespace is basically a Linux feature that ensures OS resources partition in a mutually exclusive manner. This forms the core concept behind containerization as namespaces introduce a layer of isolation amongst the containers. In docker, the namespaces ensure that the containers are portable and they don't affect the underlying host. Examples for namespace types that are currently being supported by Docker \u2013 PID, Mount, User, Network, IPC. What is the docker command that lists the status of all docker containers? In order to get the status of all the containers, we run the below command: docker # ps -a On what circumstances will you lose data stored in a container? The data of a container remains in it until and unless you delete the container. What is docker image registry? - A Docker image registry, in simple terms, is an area where the docker images are stored. Instead of converting the applications to containers each and every time, a developer can directly use the images stored in the registry. - This image registry can either be public or private and Docker hub is the most popular and famous public registry available. How many Docker components are there? There are three docker components, they are - Docker Client, Docker Host, and Docker Registry. - Docker Client: This component performs \u201cbuild\u201d and \u201crun\u201d operations for the purpose of opening communication with the docker host. - Docker Host: This component has the main docker daemon and hosts containers and their associated images. The daemon establishes a connection with the docker registry. - Docker Registry: This component stores the docker images. There can be a public registry or a private one. The most famous public registries are Docker Hub and Docker Cloud. What is a Docker Hub? - It is a public cloud-based registry provided by Docker for storing public images of the containers along with the provision of finding and sharing them. - The images can be pushed to Docker Hub through the docker push command. What command can you run to export a docker image as an archive? This can be done using the docker save command and the syntax is: # docker save -o <exported_name>.tar <container-name> What command can be run to import a pre-exported Docker image into another Docker host? This can be done using the docker load command and the syntax is: # docker load -i <export_image_name>.tar Can a paused container be removed from Docker? No, it is not possible! A container MUST be in the stopped state before we can remove it. What command is used to check for the version of docker client and server? - The command used to get all version information of the client and server is the \"docker version\". - To get only the server version details, we can run: # docker version --format '{{.Server.Version}}' Docker Intermediate Interview Questions \uf0c1 Differentiate between virtualization and containerization. The question indirectly translates to explaining the difference between virtual machines and Docker containers. Virtualization Containerization This helps developers to run and host multiple OS on the hardware of a single physical server. This helps developers to deploy multiple applications using the same operating system on a single virtual machine or server. Hypervisors provide overall virtual machines to the guest operating systems. Containers ensure isolated environment/ user spaces are provided for running the applications. Any changes done within the container do not reflect on the host or other containers of the same host. These virtual machines form an abstraction of the system hardware layer this means that each virtual machine on the host acts like a physical machine. Containers form abstraction of the application layer which means that each container constitutes a different application. Differentiate between COPY and ADD commands that are used in a Dockerfile? Both the commands have similar functionality, but 'COPY' is more preferred because of its higher transparency level than that of 'ADD'. 'COPY' provides just the basic support of copying local files into the container whereas 'ADD' provides additional features like remote URL and tar extraction support. Can a container restart by itself? - Yes, it is possible only while using certain docker-defined policies while using the docker run command. Following are the available policies: 1. Off: In this, the container won\u2019t be restarted in case it is stopped or it fails. 2. On-failure: Here, the container restarts by itself only when it experiences failures not associated with the user. 3. Unless-stopped: Using this policy, ensures that a container can restart only when the command is executed to stop it by the user. 4. Always: Irrespective of the failure or stopping, the container always gets restarted in this type of policy. These policies can be used as: # docker run -dit \u2014 restart [restart-policy-value] [container_name] Can you tell the differences between a docker Image and Layer? Image: This is built up from a series of read-only layers of instructions. An image corresponds to the docker container and is used for speedy operation due to the caching mechanism of each step. Layer: Each layer corresponds to an instruction of the image\u2019s Dockerfile. In simple words, the layer is also an image but it is the image of the instructions run. Consider the example Dockerfile below. FROM ubuntu:18.04 COPY . /myapp RUN make /myapp CMD python /myapp/app.py Importantly, each layer is only a set of differences from the layer before it. - The result of building this docker file is an image. Whereas the instructions present in this file add the layers to the image. The layers can be thought of as intermediate images. In the example above, there are 4 instructions, hence 4 layers are added to the resultant image. What is the purpose of the volume parameter in a docker run command? - The syntax of docker run when using the volumes is: # docker run -v host_path:docker_path <container_name> - The volume parameter is used for syncing a directory of a container with any of the host directories. Consider the below command as an example: # docker run -v /data/app:usr/src/app myapp The above command mounts the directory /data/app in the host to the usr/src/app directory. We can sync the container with the data files from the host without having the need to restart it. - This also ensures data security in cases of container deletion. This ensures that even if the container is deleted, the data of the container exists in the volume mapped host location making it the easiest way to store the container data. Where are docker volumes stored in docker? Volumes are created and managed by Docker and cannot be accessed by non-docker entities. They are stored in Docker host filesystem at '/var/lib/docker/volumes/'. What does the docker info command do? The command gets detailed information about Docker installed on the host system. The information can be like what is the number of containers or images and in what state they are running and hardware specifications like total memory allocated, speed of the processor, kernel version, etc. Can you tell the what are the purposes of up, run, and start commands of docker compose? - Using the up command for keeping a docker-compose up (ideally at all times), we can start or restart all the networks, services, and drivers associated with the app that are specified in the docker-compose.yml file. Now if we are running the docker-compose up in the \u201cattached\u201d mode then all the logs from the containers would be accessible to us. In case the docker-compose is run in the \u201cdetached\u201d mode, then once the containers are started, it just exits and shows no logs. - Using the run command, the docker-compose can run one-off or ad-hoc tasks based on the business requirements. Here, the service name has to be provided and the docker starts only that specific service and also the other services to which the target service is dependent (if any). - This command is helpful for testing the containers and also performing tasks such as adding or removing data to the container volumes etc. - Using the start command, only those containers can be restarted which were already created and then stopped. This is not useful for creating new containers on its own. What are the basic requirements for the docker to run on any system? Docker can run on both Windows and Linux platforms. - For the Windows platform, docker atleast needs Windows 10 64bit with 2GB RAM space. For the lower versions, docker can be installed by taking help of the toolbox. Docker can be downloaded from https://docs.docker.com/docker-forwindows/ website. - For Linux platforms, Docker can run on various Linux flavors such as Ubuntu >=12.04, Fedora >=19, RHEL >=6.5, CentOS >=6 etc. Can you tell the approach to login to the docker registry? Using the docker login command credentials to log in to their own cloud repositories can be entered and accessed. List the most commonly used instructions in Dockerfile? - FROM: This is used to set the base image for upcoming instructions. A docker file is considered to be valid if it starts with the FROM instruction. - LABEL: This is used for the image organization based on projects, modules, or licensing. It also helps in automation as we specify a key-value pair while defining a label that can be later accessed and handled programmatically. - RUN: This command is used to execute instructions following it on the top of the current image in a new layer. Note that with each RUN command execution, we add layers on top of the image and then use that in subsequent steps. - CMD: This command is used to provide default values of an executing container. In cases of multiple CMD commands the last instruction would be considered. Can you differentiate between Daemon Logging and Container Logging? - In docker, logging is supported at 2 levels and they are logging at the Daemon level or logging at the Container level. - Daemon Level: This kind of logging has four levels- Debug, Info, Error, and Fatal. => Debug has all the data that happened during the execution of the daemon process. => Info carries all the information along with the error information during the execution of the daemon process. => Errors have those errors that occurred during the execution of the daemon process. => Fatal has the fatal errors that occurred during the execution. - Container Level: => Container level logging can be done using the command: # sudo docker run \u2013it <container_name> /bin/bash => In order to check for the container level logs, we can run the command: # sudo docker logs <container_id> What is the way to establish communication between docker host and Linux host? This can be done using networking by identifying the \u201cipconfig\u201d on the docker host. This command ensures that an ethernet adapter is created as long as the docker is present in the host. What is the best way of deleting a container? We need to follow the following two steps for deleting a container: # docker stop <container_id> # docker rm <container_id> Can you tell the difference between CMD and ENTRYPOINT? - CMD command provides executable defaults for an executing container. In case the executable has to be omitted then the usage of ENTRYPOINT instruction along with the JSON array format has to be incorporated. - ENTRYPOINT specifies that the instruction within it will always be run when the container starts. This command provides an option to configure the parameters and the executables. If the DockerFile does not have this command, then it would still get inherited from the base image mentioned in the FROM instruction. => The most commonly used ENTRYPOINT is /bin/sh or /bin/bash for most of the base images. - As part of good practices, every DockerFile should have at least one of these two commands. Docker Advanced Interview Questions \uf0c1 Can we use JSON instead of YAML while developing dockercompose file in Docker? Yes! It can be used. In order to run docker-compose with JSON: # docker-compose -f docker-compose.json up can be used. How many containers you can run in docker and what are the factors influencing this limit? There is no clearly defined limit to the number of containers that can be run within docker. But it all depends on the limitations - more specifically hardware restrictions. The size of the app and the CPU resources available are 2 important factors influencing this limit. In case your application is not very big and you have abundant CPU resources, then we can run a huge number of containers. Describe the lifecycle of Docker Container? The different stages of the docker container from the start of creating it to its end are called the docker container life cycle. The most important stages are: - Created: This is the state where the container has just been created new but not started yet. - Running: In this state, the container would be running with all its associated processes. - Paused: This state happens when the running container has been paused. - Stopped: This state happens when the running container has been stopped. - Deleted: In this, the container is in a dead state. How to use docker for multiple application environments? - Docker-compose feature of docker will come to help here. In the dockercompose file, we can define multiple services, networks, and containers along with the volume mapping in a clean manner, and then we can just call the command \u201cdocker-compose up\u201d. - When there are multiple environments involved - it can be either dev, staging, uat, or production servers, we would want to define the server-specific dependencies and processes for running the application. In this case, we can go ahead with creating environment-specific docker-compose files of the name \u201cdocker-compose.{environment}.yml\u201d and then based on the environment, we can set up and run the application. How will you ensure that a container 1 runs before container 2 while using docker compose? Docker-compose does not wait for any container to be \u201cready\u201d before going ahead with the next containers. In order to achieve the order of execution, we can use: - The \u201cdepends_on\u201d which got added in version 2 of docker-compose can be used as shown in a sample docker-compose.yml file below: version: \"2.4\" services: backend: build: . depends_on: - db db: image: postgres The introduction of service dependencies has various causes and effects: - The docker-compose up command starts and runs the services in the dependency order specified. For the above example, the DB container is started before the backend. - docker-compose up SERVICE_NAME by default includes the dependencies associated with the service. In the given example, running docker-compose up backend creates and starts DB (dependency of backend). - Finally, the command docker-compose stop also stops the services in the order of the dependency specified. For the given example, the backend service is stopped before the DB service.","title":"Docker Interview Questions"},{"location":"nightwolf-cotribution/docker_interview_questions/#docker-interview-questions-and-answers","text":"We have consolidated a list of frequently asked Docker interview questions for Freshers and Experianced. This will help DevOps Engineers in their preparations for Interview. You will find these questions very helpful in your DevOps interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome.","title":"Docker interview questions and Answers"},{"location":"nightwolf-cotribution/docker_interview_questions/#docker-basic-interview-questions","text":"Can you tell something about docker container? - In simplest terms, docker containers consist of applications and all their dependencies. - They share the kernel and system resources with other containers and run asisolated systems in the host operating system. - The main aim of docker containers is to get rid of the infrastructure dependency while deploying and running applications. This means that any containerized application can run on any platform irrespective of the infrastructure being used beneath. - Technically, they are just the runtime instances of docker images. What are docker images? They are executable packages(bundled with application code & dependencies, soware packages, etc.) for the purpose of creating containers. Docker images can be deployed to any docker environment and the containers can be spun up there to run the application. What is a DockerFile? It is a text file that has all commands which need to be run for building a given image. Can you tell what is the functionality of a hypervisor? A hypervisor is a so\u2000\u2000ware that makes virtualization happen because of which is sometimes referred to as the Virtual Machine Monitor. This divides the resources of the host system and allocates them to each guest environment installed. - This means that multiple OS can be installed on a single host system. Hypervisors are of 2 types: 1. Native Hypervisor: This type is also called a Bare-metal Hypervisor and runs directly on the underlying host system which also ensures direct access to the host hardware which is why it does not require base OS. 2. Hosted Hypervisor: This type makes use of the underlying host operating system which has the existing OS installed. What can you tell about Docker Compose? It is a YAML file consisting of all the details regarding various services, networks, and volumes that are needed for setting up the Docker-based application. So, dockercompose is used for creating multiple containers, host them and establish communication between them. For the purpose of communication amongst the containers, ports are exposed by each and every container. Can you tell something about docker namespace? A namespace is basically a Linux feature that ensures OS resources partition in a mutually exclusive manner. This forms the core concept behind containerization as namespaces introduce a layer of isolation amongst the containers. In docker, the namespaces ensure that the containers are portable and they don't affect the underlying host. Examples for namespace types that are currently being supported by Docker \u2013 PID, Mount, User, Network, IPC. What is the docker command that lists the status of all docker containers? In order to get the status of all the containers, we run the below command: docker # ps -a On what circumstances will you lose data stored in a container? The data of a container remains in it until and unless you delete the container. What is docker image registry? - A Docker image registry, in simple terms, is an area where the docker images are stored. Instead of converting the applications to containers each and every time, a developer can directly use the images stored in the registry. - This image registry can either be public or private and Docker hub is the most popular and famous public registry available. How many Docker components are there? There are three docker components, they are - Docker Client, Docker Host, and Docker Registry. - Docker Client: This component performs \u201cbuild\u201d and \u201crun\u201d operations for the purpose of opening communication with the docker host. - Docker Host: This component has the main docker daemon and hosts containers and their associated images. The daemon establishes a connection with the docker registry. - Docker Registry: This component stores the docker images. There can be a public registry or a private one. The most famous public registries are Docker Hub and Docker Cloud. What is a Docker Hub? - It is a public cloud-based registry provided by Docker for storing public images of the containers along with the provision of finding and sharing them. - The images can be pushed to Docker Hub through the docker push command. What command can you run to export a docker image as an archive? This can be done using the docker save command and the syntax is: # docker save -o <exported_name>.tar <container-name> What command can be run to import a pre-exported Docker image into another Docker host? This can be done using the docker load command and the syntax is: # docker load -i <export_image_name>.tar Can a paused container be removed from Docker? No, it is not possible! A container MUST be in the stopped state before we can remove it. What command is used to check for the version of docker client and server? - The command used to get all version information of the client and server is the \"docker version\". - To get only the server version details, we can run: # docker version --format '{{.Server.Version}}'","title":"Docker Basic Interview Questions"},{"location":"nightwolf-cotribution/docker_interview_questions/#docker-intermediate-interview-questions","text":"Differentiate between virtualization and containerization. The question indirectly translates to explaining the difference between virtual machines and Docker containers. Virtualization Containerization This helps developers to run and host multiple OS on the hardware of a single physical server. This helps developers to deploy multiple applications using the same operating system on a single virtual machine or server. Hypervisors provide overall virtual machines to the guest operating systems. Containers ensure isolated environment/ user spaces are provided for running the applications. Any changes done within the container do not reflect on the host or other containers of the same host. These virtual machines form an abstraction of the system hardware layer this means that each virtual machine on the host acts like a physical machine. Containers form abstraction of the application layer which means that each container constitutes a different application. Differentiate between COPY and ADD commands that are used in a Dockerfile? Both the commands have similar functionality, but 'COPY' is more preferred because of its higher transparency level than that of 'ADD'. 'COPY' provides just the basic support of copying local files into the container whereas 'ADD' provides additional features like remote URL and tar extraction support. Can a container restart by itself? - Yes, it is possible only while using certain docker-defined policies while using the docker run command. Following are the available policies: 1. Off: In this, the container won\u2019t be restarted in case it is stopped or it fails. 2. On-failure: Here, the container restarts by itself only when it experiences failures not associated with the user. 3. Unless-stopped: Using this policy, ensures that a container can restart only when the command is executed to stop it by the user. 4. Always: Irrespective of the failure or stopping, the container always gets restarted in this type of policy. These policies can be used as: # docker run -dit \u2014 restart [restart-policy-value] [container_name] Can you tell the differences between a docker Image and Layer? Image: This is built up from a series of read-only layers of instructions. An image corresponds to the docker container and is used for speedy operation due to the caching mechanism of each step. Layer: Each layer corresponds to an instruction of the image\u2019s Dockerfile. In simple words, the layer is also an image but it is the image of the instructions run. Consider the example Dockerfile below. FROM ubuntu:18.04 COPY . /myapp RUN make /myapp CMD python /myapp/app.py Importantly, each layer is only a set of differences from the layer before it. - The result of building this docker file is an image. Whereas the instructions present in this file add the layers to the image. The layers can be thought of as intermediate images. In the example above, there are 4 instructions, hence 4 layers are added to the resultant image. What is the purpose of the volume parameter in a docker run command? - The syntax of docker run when using the volumes is: # docker run -v host_path:docker_path <container_name> - The volume parameter is used for syncing a directory of a container with any of the host directories. Consider the below command as an example: # docker run -v /data/app:usr/src/app myapp The above command mounts the directory /data/app in the host to the usr/src/app directory. We can sync the container with the data files from the host without having the need to restart it. - This also ensures data security in cases of container deletion. This ensures that even if the container is deleted, the data of the container exists in the volume mapped host location making it the easiest way to store the container data. Where are docker volumes stored in docker? Volumes are created and managed by Docker and cannot be accessed by non-docker entities. They are stored in Docker host filesystem at '/var/lib/docker/volumes/'. What does the docker info command do? The command gets detailed information about Docker installed on the host system. The information can be like what is the number of containers or images and in what state they are running and hardware specifications like total memory allocated, speed of the processor, kernel version, etc. Can you tell the what are the purposes of up, run, and start commands of docker compose? - Using the up command for keeping a docker-compose up (ideally at all times), we can start or restart all the networks, services, and drivers associated with the app that are specified in the docker-compose.yml file. Now if we are running the docker-compose up in the \u201cattached\u201d mode then all the logs from the containers would be accessible to us. In case the docker-compose is run in the \u201cdetached\u201d mode, then once the containers are started, it just exits and shows no logs. - Using the run command, the docker-compose can run one-off or ad-hoc tasks based on the business requirements. Here, the service name has to be provided and the docker starts only that specific service and also the other services to which the target service is dependent (if any). - This command is helpful for testing the containers and also performing tasks such as adding or removing data to the container volumes etc. - Using the start command, only those containers can be restarted which were already created and then stopped. This is not useful for creating new containers on its own. What are the basic requirements for the docker to run on any system? Docker can run on both Windows and Linux platforms. - For the Windows platform, docker atleast needs Windows 10 64bit with 2GB RAM space. For the lower versions, docker can be installed by taking help of the toolbox. Docker can be downloaded from https://docs.docker.com/docker-forwindows/ website. - For Linux platforms, Docker can run on various Linux flavors such as Ubuntu >=12.04, Fedora >=19, RHEL >=6.5, CentOS >=6 etc. Can you tell the approach to login to the docker registry? Using the docker login command credentials to log in to their own cloud repositories can be entered and accessed. List the most commonly used instructions in Dockerfile? - FROM: This is used to set the base image for upcoming instructions. A docker file is considered to be valid if it starts with the FROM instruction. - LABEL: This is used for the image organization based on projects, modules, or licensing. It also helps in automation as we specify a key-value pair while defining a label that can be later accessed and handled programmatically. - RUN: This command is used to execute instructions following it on the top of the current image in a new layer. Note that with each RUN command execution, we add layers on top of the image and then use that in subsequent steps. - CMD: This command is used to provide default values of an executing container. In cases of multiple CMD commands the last instruction would be considered. Can you differentiate between Daemon Logging and Container Logging? - In docker, logging is supported at 2 levels and they are logging at the Daemon level or logging at the Container level. - Daemon Level: This kind of logging has four levels- Debug, Info, Error, and Fatal. => Debug has all the data that happened during the execution of the daemon process. => Info carries all the information along with the error information during the execution of the daemon process. => Errors have those errors that occurred during the execution of the daemon process. => Fatal has the fatal errors that occurred during the execution. - Container Level: => Container level logging can be done using the command: # sudo docker run \u2013it <container_name> /bin/bash => In order to check for the container level logs, we can run the command: # sudo docker logs <container_id> What is the way to establish communication between docker host and Linux host? This can be done using networking by identifying the \u201cipconfig\u201d on the docker host. This command ensures that an ethernet adapter is created as long as the docker is present in the host. What is the best way of deleting a container? We need to follow the following two steps for deleting a container: # docker stop <container_id> # docker rm <container_id> Can you tell the difference between CMD and ENTRYPOINT? - CMD command provides executable defaults for an executing container. In case the executable has to be omitted then the usage of ENTRYPOINT instruction along with the JSON array format has to be incorporated. - ENTRYPOINT specifies that the instruction within it will always be run when the container starts. This command provides an option to configure the parameters and the executables. If the DockerFile does not have this command, then it would still get inherited from the base image mentioned in the FROM instruction. => The most commonly used ENTRYPOINT is /bin/sh or /bin/bash for most of the base images. - As part of good practices, every DockerFile should have at least one of these two commands.","title":"Docker Intermediate Interview Questions"},{"location":"nightwolf-cotribution/docker_interview_questions/#docker-advanced-interview-questions","text":"Can we use JSON instead of YAML while developing dockercompose file in Docker? Yes! It can be used. In order to run docker-compose with JSON: # docker-compose -f docker-compose.json up can be used. How many containers you can run in docker and what are the factors influencing this limit? There is no clearly defined limit to the number of containers that can be run within docker. But it all depends on the limitations - more specifically hardware restrictions. The size of the app and the CPU resources available are 2 important factors influencing this limit. In case your application is not very big and you have abundant CPU resources, then we can run a huge number of containers. Describe the lifecycle of Docker Container? The different stages of the docker container from the start of creating it to its end are called the docker container life cycle. The most important stages are: - Created: This is the state where the container has just been created new but not started yet. - Running: In this state, the container would be running with all its associated processes. - Paused: This state happens when the running container has been paused. - Stopped: This state happens when the running container has been stopped. - Deleted: In this, the container is in a dead state. How to use docker for multiple application environments? - Docker-compose feature of docker will come to help here. In the dockercompose file, we can define multiple services, networks, and containers along with the volume mapping in a clean manner, and then we can just call the command \u201cdocker-compose up\u201d. - When there are multiple environments involved - it can be either dev, staging, uat, or production servers, we would want to define the server-specific dependencies and processes for running the application. In this case, we can go ahead with creating environment-specific docker-compose files of the name \u201cdocker-compose.{environment}.yml\u201d and then based on the environment, we can set up and run the application. How will you ensure that a container 1 runs before container 2 while using docker compose? Docker-compose does not wait for any container to be \u201cready\u201d before going ahead with the next containers. In order to achieve the order of execution, we can use: - The \u201cdepends_on\u201d which got added in version 2 of docker-compose can be used as shown in a sample docker-compose.yml file below: version: \"2.4\" services: backend: build: . depends_on: - db db: image: postgres The introduction of service dependencies has various causes and effects: - The docker-compose up command starts and runs the services in the dependency order specified. For the above example, the DB container is started before the backend. - docker-compose up SERVICE_NAME by default includes the dependencies associated with the service. In the given example, running docker-compose up backend creates and starts DB (dependency of backend). - Finally, the command docker-compose stop also stops the services in the order of the dependency specified. For the given example, the backend service is stopped before the DB service.","title":"Docker Advanced Interview Questions"},{"location":"nightwolf-cotribution/git/","text":"Top 50 GIT Interview Questions and Answers for DevOps Roles - Solved \uf0c1 We have consolidated a list of frequently asked GIT interview questions for Freshers and Experianced DevOps Engineer. You will find these questions very helpful in your interviews for DevOps Engineer. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. Q1) What is a version control system (VCS)? Version control systems are software tools that help a team manage changes to source code/documents over time. Q2) Why are version control systems (VCS) necessary? They allow you to 1. Keep track of code changes. 2. Can help team memembers to synchronize the code to the latest version easily. 3. It helps teams to develop products faster. 4. Helps teams to collaborate with each other easily. 5. It acts as a backup for your code base. Q3) What are distributed version control systems (DVCS)? Distributed revision control synchronizes repositories by transferring patches from peer to peer. There is no single central version of the codebase instead, each user can download a working copy and full change history. Hence not requiring to be connected to the server all the time. Example : GIT Q4) What are centralized version control systems (CVCS)? It is a version control where there is a single central repository hosted on a server. This server is expected to have the latest code and expects all its clients to contribute by being connected to the server always. Q5) How does git maintain the data internally. Git mainly uses three different types of objects to hold information about a repository BLOB binary form of the actual data TREE It contains pointers to the objects COMMIT Commit object contains information about the author, date, ha Q6) Can git be used locally without using GIT server or any SAAS providers like git, bitbucket &etc Yes, if a person is willing to work on the project alone he can use git to maintain the state of the project. However the full potential of git is unutilized. Q7) How do you find a list of files impacted with a particular commit hash? git diff-tree -r {hash} Q8) What is a conflict in git. Is it necessary? A conflict in git arises when branches are merged with new commits which has changes on same file(s). In cases like this git cannot take precendence of changes hen Q9) What is the difference between git pull and git fetch Git fetch will download new commits from a remote repository to a local repository git pull does the same as git fetch and also merges the same into your local working files Q10) How are conflicts solved? The files in conflict must be edited and fixed. Then add the resolved files by running 1. git add . 2. git commit Q11) What is the command that defines the author email to be used for all the commits performed by the current user? # git config global user.email <email> Q12) GIT belongs to which generation of version control tools 3rd Q13) What is GIT stash? When changes are made to the working directory but you don t want to comattempting to recreate a clean working directory. GIT stash is used where all changes in working directory and Index are are pushed into a stack. Q14) How do you create a new branches # git branch <branch name> Q15) How do you checkout to a particular branch # git checkout Q17) What languages were used to build the git? C was the major language althought few parts were also written using shell, perl, tcl and python Q18) What is git config? git config allows you to configure git installation. example includes setting user name , password, email etc Q19) What is git clone? Git clone lets users to copy an exising git repository that resides in the server. It is the easiest way that a new developer can start using and contributing to the project Q20) What is branching? Branching is analogus to a storyline on which changes are made. The changes can resides in different branches so that each branch can make changes independent of each other. Q21) What is the command line environment used to perform git operations? # git bash Q22) What is git cherry pick? cherry pick allows you to pick a particular commit from a branch and insert to another branch. This is different from git merge in that, git merge will bring in all commits from branches while cherry pick picks a specific commit only. Q23) How to return a commit that has been pushed and made open? # git revert HEAD~2.HEAD Q24) What is gitflow workflow? It is a workflow that can be used to maintain large projects and it mainly consists of The master branch is always ready for live release with everything production-ready. The Hotflix branches help in quick patching of production releases. The Develop branch helps in merging of all feature branches and also performs all the tests. The Feature branch implies a unique branch for every new feature. The feature branch could be pushed to the development branch just like their parent branch. Q25) What is the syntax for rebasing ? # git rebase [new-commit] Q26) What are git webhooks? When plannning to cascade/notify new activities on git server to a different tool like jenkins, git webhook is used. the webhook contains information about the activity (ex: push on the server). Q27) What is git instaweb Its a command that helps in directing a web browser and running a web server with an interface to the local repository. Q28) What hash is used in git ? sha1 Q29) sha1 is now considered unsecured. does that mean git is under threat. No. since git uses sha1 to only hash data to compare files/maintain state agit Q30) What is the max number of heads can be used in git? unlimited Q31) How many characters are used in sha1 name 40 chars Q32)What is a commit message. It is the message assigned to a commit made. When a commit is made a hash is assigned automatically but for humans it becomes difficult to make anything out of this hash at later point of time. Hence a commit message is assigned along for his future reference Q33)Name few graphical git clients git cola, git gui Q34)What is git diff? Git diff represents the changes between the commits and changes between working tree and commits. Q35)What is git pull origin? The command git pull origin master tells git to perform a pull operation (download a copy of repository) where the origin represents the server url (alias) and master is the name of the branch Q36) What is gitlog? It is basically a command that can be executed when it comes to finding the history of a project according to the date, changes made, the developer who handled it and usefulness of the same. Q37) How can a developer update his changes to the git server. Perform git push from the branch he is currently checked out to. Q38) How do you make git not to consider few files/directories track changes. need to add them in .gitignore file Q39)What is the difference between git add . git add -all They are same Q40) What is git bisect? Its the command that uses a binary search algorithm to find which commit \u00fe\u00ffa bug.Before the bug is introduced into the commit, the commit is referred Q41) What is git stash drop? Its the command that helps you remove the last entry made to the stash list or it can also help to eliminate any stash entry. Q42) How do you delete a branch locally? git branch -d <branch name> Q43) What is git clean? It cleans a repository by removing the files that are not currently tracked by git recursively. Q44) What is git pop? It is the command that helps you retrieve the changes pushed on the stack. Q45) What is git fork? A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project. Generally open source projects follow fork workflow to allow users to contribute to the project. GIT Cheat Sheet You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 Jenkins Interview Questions for Experianced DevOps Engineer","title":"GIT Interview Questions"},{"location":"nightwolf-cotribution/git/#top-50-git-interview-questions-and-answers-for-devops-roles-solved","text":"We have consolidated a list of frequently asked GIT interview questions for Freshers and Experianced DevOps Engineer. You will find these questions very helpful in your interviews for DevOps Engineer. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. Q1) What is a version control system (VCS)? Version control systems are software tools that help a team manage changes to source code/documents over time. Q2) Why are version control systems (VCS) necessary? They allow you to 1. Keep track of code changes. 2. Can help team memembers to synchronize the code to the latest version easily. 3. It helps teams to develop products faster. 4. Helps teams to collaborate with each other easily. 5. It acts as a backup for your code base. Q3) What are distributed version control systems (DVCS)? Distributed revision control synchronizes repositories by transferring patches from peer to peer. There is no single central version of the codebase instead, each user can download a working copy and full change history. Hence not requiring to be connected to the server all the time. Example : GIT Q4) What are centralized version control systems (CVCS)? It is a version control where there is a single central repository hosted on a server. This server is expected to have the latest code and expects all its clients to contribute by being connected to the server always. Q5) How does git maintain the data internally. Git mainly uses three different types of objects to hold information about a repository BLOB binary form of the actual data TREE It contains pointers to the objects COMMIT Commit object contains information about the author, date, ha Q6) Can git be used locally without using GIT server or any SAAS providers like git, bitbucket &etc Yes, if a person is willing to work on the project alone he can use git to maintain the state of the project. However the full potential of git is unutilized. Q7) How do you find a list of files impacted with a particular commit hash? git diff-tree -r {hash} Q8) What is a conflict in git. Is it necessary? A conflict in git arises when branches are merged with new commits which has changes on same file(s). In cases like this git cannot take precendence of changes hen Q9) What is the difference between git pull and git fetch Git fetch will download new commits from a remote repository to a local repository git pull does the same as git fetch and also merges the same into your local working files Q10) How are conflicts solved? The files in conflict must be edited and fixed. Then add the resolved files by running 1. git add . 2. git commit Q11) What is the command that defines the author email to be used for all the commits performed by the current user? # git config global user.email <email> Q12) GIT belongs to which generation of version control tools 3rd Q13) What is GIT stash? When changes are made to the working directory but you don t want to comattempting to recreate a clean working directory. GIT stash is used where all changes in working directory and Index are are pushed into a stack. Q14) How do you create a new branches # git branch <branch name> Q15) How do you checkout to a particular branch # git checkout Q17) What languages were used to build the git? C was the major language althought few parts were also written using shell, perl, tcl and python Q18) What is git config? git config allows you to configure git installation. example includes setting user name , password, email etc Q19) What is git clone? Git clone lets users to copy an exising git repository that resides in the server. It is the easiest way that a new developer can start using and contributing to the project Q20) What is branching? Branching is analogus to a storyline on which changes are made. The changes can resides in different branches so that each branch can make changes independent of each other. Q21) What is the command line environment used to perform git operations? # git bash Q22) What is git cherry pick? cherry pick allows you to pick a particular commit from a branch and insert to another branch. This is different from git merge in that, git merge will bring in all commits from branches while cherry pick picks a specific commit only. Q23) How to return a commit that has been pushed and made open? # git revert HEAD~2.HEAD Q24) What is gitflow workflow? It is a workflow that can be used to maintain large projects and it mainly consists of The master branch is always ready for live release with everything production-ready. The Hotflix branches help in quick patching of production releases. The Develop branch helps in merging of all feature branches and also performs all the tests. The Feature branch implies a unique branch for every new feature. The feature branch could be pushed to the development branch just like their parent branch. Q25) What is the syntax for rebasing ? # git rebase [new-commit] Q26) What are git webhooks? When plannning to cascade/notify new activities on git server to a different tool like jenkins, git webhook is used. the webhook contains information about the activity (ex: push on the server). Q27) What is git instaweb Its a command that helps in directing a web browser and running a web server with an interface to the local repository. Q28) What hash is used in git ? sha1 Q29) sha1 is now considered unsecured. does that mean git is under threat. No. since git uses sha1 to only hash data to compare files/maintain state agit Q30) What is the max number of heads can be used in git? unlimited Q31) How many characters are used in sha1 name 40 chars Q32)What is a commit message. It is the message assigned to a commit made. When a commit is made a hash is assigned automatically but for humans it becomes difficult to make anything out of this hash at later point of time. Hence a commit message is assigned along for his future reference Q33)Name few graphical git clients git cola, git gui Q34)What is git diff? Git diff represents the changes between the commits and changes between working tree and commits. Q35)What is git pull origin? The command git pull origin master tells git to perform a pull operation (download a copy of repository) where the origin represents the server url (alias) and master is the name of the branch Q36) What is gitlog? It is basically a command that can be executed when it comes to finding the history of a project according to the date, changes made, the developer who handled it and usefulness of the same. Q37) How can a developer update his changes to the git server. Perform git push from the branch he is currently checked out to. Q38) How do you make git not to consider few files/directories track changes. need to add them in .gitignore file Q39)What is the difference between git add . git add -all They are same Q40) What is git bisect? Its the command that uses a binary search algorithm to find which commit \u00fe\u00ffa bug.Before the bug is introduced into the commit, the commit is referred Q41) What is git stash drop? Its the command that helps you remove the last entry made to the stash list or it can also help to eliminate any stash entry. Q42) How do you delete a branch locally? git branch -d <branch name> Q43) What is git clean? It cleans a repository by removing the files that are not currently tracked by git recursively. Q44) What is git pop? It is the command that helps you retrieve the changes pushed on the stack. Q45) What is git fork? A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project. Generally open source projects follow fork workflow to allow users to contribute to the project.","title":"Top 50 GIT Interview Questions and Answers for DevOps Roles - Solved"},{"location":"nightwolf-cotribution/git_cheatsheet/","text":"GIT Cheatsheet What is Git ? \uf0c1 Git is the most commonly used Version Control System. Git tracks the changes you make to files, so you have a record of what has been done, and you can revert to specific versions should you ever need to. Git also makes the collaborations easier, allowing changes by multiple people to all be merge into one source. SETUP \uf0c1 Configuring user information used across all local repositories: S.N. Command Description 1 git config --global user.name \u201cFirstname Lastname\u201d Configuring the name 2 git config --global user.email \"User's Email id\u201d Configuring email GIT BASICS \uf0c1 Configuring user information, initializing and cloning repositories S.N. Command Description 1 git init Initialize an existing directory as a Git repository 2 git clone [url] Retrieve an entire repository from a hosted location via URL STAGE & SNAPSHOT \uf0c1 Working with snapshots and the Git staging area S.N. Command Description 1 git status Show the modified files in the working directory,staged for your next commit 2 git add [file] Add a file as it looks now to your next commit (stage) 3 git reset [file] Unstage a file while retaining the changes in working directory 4 git diff Diff of what is changed but not staged 5 git diff --staged Diff of what is staged but not yet committed 6 git commit -m \u201c[descriptive message]\u201d Commit your staged content as a new commit snapshot BRANCH & MERGE \uf0c1 Isolating work in branches, changing context, and integrating changes S.N. Command Description 1 git branch List your branches. a * will appear next to the currently active branch 2 git branch [branch-name] Create a new branch at the current commit 3 git checkout Switch to another branch and check it out into your working directory 4 git merge [branch] Merge the specified branch\u2019s history into the current one 5 git log Show all commits in the current branch\u2019s history INSPECT & COMPARE \uf0c1 Examining logs, diffs and object information S.N. Command Description 1 git log Show the commit history for the currently active branch 2 git log branch-B..branch-A Show the commits on branchA that are not on branchB 3 git log --follow [file] Show the commits that changed file, even across renames 4 git show [SHA] Show SHA object in Git in human-readable format 5 git diff branchB...branchA Show the diff of what is in branchA that is not in branchB SHARE & UPDATE \uf0c1 Retrieving updates from another repository and updating local repos S.N. Command Description 1 git remote add [alias] [url] Add a git URL as an alias 2 git fetch [alias] Fetch down all the branches from that Git remote 3 git merge [alias]/[branch] Merge a remote branch into your current branch to bring it up to date 4 git push [alias] [branch] Transmit local branch commits to the remote repository branch 5 git pull Fetch and merge any commits from the tracking remote branch TRACKING PATH CHANGES \uf0c1 Versioning file removes and path changes S.N. Command Description 1 git rm [file] Delete the file from project and stage the removal for commit 2 git mv [existing-path] [new-path] Change an existing file path and stage the move 3 git log --stat -M Show all commit logs with indication of any paths that moved REWRITE HISTORY \uf0c1 Rewriting branches, updating commits and clearing history S.N. Command Description 1 git rebase [branch] Apply any commits of current branch ahead of specified one 2 git reset --hard [commit] Clear staging area, rewrite working tree from specific commit TEMPORARY COMMITS \uf0c1 Temporarily store modified, tracked files in order to change branches S.N. Command Description 1 git stash Save modified and staged changes 2 git stash list List stack-order of stashed file changes 3 git stash pop Write working from top of stash stack 4 git stash drop Discard the changes from top of stash stack You may also refer to other articles on this website: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 Jenkins Interview Questions for Experianced DevOps Engineer","title":"GIT CheatSheet"},{"location":"nightwolf-cotribution/git_cheatsheet/#what-is-git","text":"Git is the most commonly used Version Control System. Git tracks the changes you make to files, so you have a record of what has been done, and you can revert to specific versions should you ever need to. Git also makes the collaborations easier, allowing changes by multiple people to all be merge into one source.","title":"What is Git ?"},{"location":"nightwolf-cotribution/git_cheatsheet/#setup","text":"Configuring user information used across all local repositories: S.N. Command Description 1 git config --global user.name \u201cFirstname Lastname\u201d Configuring the name 2 git config --global user.email \"User's Email id\u201d Configuring email","title":"SETUP"},{"location":"nightwolf-cotribution/git_cheatsheet/#git-basics","text":"Configuring user information, initializing and cloning repositories S.N. Command Description 1 git init Initialize an existing directory as a Git repository 2 git clone [url] Retrieve an entire repository from a hosted location via URL","title":"GIT BASICS"},{"location":"nightwolf-cotribution/git_cheatsheet/#stage-snapshot","text":"Working with snapshots and the Git staging area S.N. Command Description 1 git status Show the modified files in the working directory,staged for your next commit 2 git add [file] Add a file as it looks now to your next commit (stage) 3 git reset [file] Unstage a file while retaining the changes in working directory 4 git diff Diff of what is changed but not staged 5 git diff --staged Diff of what is staged but not yet committed 6 git commit -m \u201c[descriptive message]\u201d Commit your staged content as a new commit snapshot","title":"STAGE &amp; SNAPSHOT"},{"location":"nightwolf-cotribution/git_cheatsheet/#branch-merge","text":"Isolating work in branches, changing context, and integrating changes S.N. Command Description 1 git branch List your branches. a * will appear next to the currently active branch 2 git branch [branch-name] Create a new branch at the current commit 3 git checkout Switch to another branch and check it out into your working directory 4 git merge [branch] Merge the specified branch\u2019s history into the current one 5 git log Show all commits in the current branch\u2019s history","title":"BRANCH &amp; MERGE"},{"location":"nightwolf-cotribution/git_cheatsheet/#inspect-compare","text":"Examining logs, diffs and object information S.N. Command Description 1 git log Show the commit history for the currently active branch 2 git log branch-B..branch-A Show the commits on branchA that are not on branchB 3 git log --follow [file] Show the commits that changed file, even across renames 4 git show [SHA] Show SHA object in Git in human-readable format 5 git diff branchB...branchA Show the diff of what is in branchA that is not in branchB","title":"INSPECT &amp; COMPARE"},{"location":"nightwolf-cotribution/git_cheatsheet/#share-update","text":"Retrieving updates from another repository and updating local repos S.N. Command Description 1 git remote add [alias] [url] Add a git URL as an alias 2 git fetch [alias] Fetch down all the branches from that Git remote 3 git merge [alias]/[branch] Merge a remote branch into your current branch to bring it up to date 4 git push [alias] [branch] Transmit local branch commits to the remote repository branch 5 git pull Fetch and merge any commits from the tracking remote branch","title":"SHARE &amp; UPDATE"},{"location":"nightwolf-cotribution/git_cheatsheet/#tracking-path-changes","text":"Versioning file removes and path changes S.N. Command Description 1 git rm [file] Delete the file from project and stage the removal for commit 2 git mv [existing-path] [new-path] Change an existing file path and stage the move 3 git log --stat -M Show all commit logs with indication of any paths that moved","title":"TRACKING PATH CHANGES"},{"location":"nightwolf-cotribution/git_cheatsheet/#rewrite-history","text":"Rewriting branches, updating commits and clearing history S.N. Command Description 1 git rebase [branch] Apply any commits of current branch ahead of specified one 2 git reset --hard [commit] Clear staging area, rewrite working tree from specific commit","title":"REWRITE HISTORY"},{"location":"nightwolf-cotribution/git_cheatsheet/#temporary-commits","text":"Temporarily store modified, tracked files in order to change branches S.N. Command Description 1 git stash Save modified and staged changes 2 git stash list List stack-order of stashed file changes 3 git stash pop Write working from top of stash stack 4 git stash drop Discard the changes from top of stash stack You may also refer to other articles on this website: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 Jenkins Interview Questions for Experianced DevOps Engineer","title":"TEMPORARY COMMITS"},{"location":"nightwolf-cotribution/jenkins/","text":"Top Jenkins Interview Questions & answers for Experianced DevOps Engineer - Solved \uf0c1 We have consolidated Jenkins frequently asked interview questions in DevOps interviews. You will find these questions very helpful in your interviews for DevOps roles. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. Q#1) What is Jenkins? Answer: Jenkins is a free open source Continuous Integration tool and automation server to monitor continuous integration and delivery. It is written in Java. It is known as an automated Continuous Delivery tool that helps to build and test the software system with easy integration of changes to the system. Jenkins follows Groovy Scripting. Also, it enables developers to continuously check in their code and also analyze the postbuild actions. The automation testers can use to run their tests as soon as the new code is added or code is modified. Q#2) What are the features of Jenkins? Answer: Jenkins comes with the following features: 1. Free open source. 2. Easy installation on various operating systems. 3. Build Pipeline Support. 4. Workflow Plugin. 5. Test harness built around JUnit. 6. Easy upgrades. 7. Rapid release cycle. 8. Easy configuration setup. 9. Extensible with the use of third-party plugins. Q#3) What are the advantages of Jenkins? Why we use Jenkins? Answer: Jenkins is used to continuously monitor the large code base in real-time. It enables developers to find bugs in their code and fix them. Email notifications are made to the developers regarding their check-ins as a post-build action. Advantages of Jenkins are as follows: - Build failures are cached during the integration stage. - Notifies the developers about build report status using LDAP (Lightweight Directory Access Protocol) mail server. - Maven release project is automated with simple steps. - Easy bug tracking. - Automatic changes get updated in the build report with notification. - Supports Continuous Integration in agile development and test-driven development. Q#4) Mention some of the important plugins in Jenkins? Answer: Plugins in Jenkins includes: - Gits - Maven 2 Project - HTML Publisher - Copy Artcraft - Join - Green Balls - Amazon EC2 Q#5) What is Continuous Integration in Jenkins? Answer: Continuous integration is the process of continuously checking-in the developer code into a version control system and triggering the build to check and identify bugs in the written code. This is a very quick process and also gives them a chance to fix the bugs. Jenkins is one such continuous integration tool. In software development, multiple developers work on different software modules. While performing integration testing all the modules are being integrated together. It is considered as the development practice to integrate the code into the source repository Whenever the programmer/developer makes any change to the current code, then it automatically gets integrated with the system running on the tester\u2019s machine and makes the testing task easy and speedy for the system testers. Continuous Integration comprises of: - Development and Compilation - Database Integration - Unit Testing - Production Deployment - Code Labeling - Functional Testing - Generating and Analyzing Reports Q#6) What is the difference between Hudson and Jenkins? Answer: There is no difference between Hudson and Jenkins. Hudson was the former name of Jenkins, after going through several issues the name was changed to Jenkins. Q#7) What is Groovy in Jenkins? Answer: Groovy is the default scripting language that is being used in the development of JMeter Version 3.1. Currently Apache Groovy is the dynamic object-oriented programming language that is used as a scripting language for the Java platform. Apache Groovy comes with some useful features such as Java Compatibility and Development Support. Q#8) Which command is used to start Jenkins? Answer: You can follow the below-mentioned steps to start Jenkins: 1. Open Command Prompt 2. From the Command Prompt browse the directory where Jenkins. war resides 3. Run the command given below: D:\\>Java \u2013jar Jenkins.war Q#9) What is Jenkinsfile? Answer: The text file where all the definitions of pipelines are defined is called Jenkinsfile. It is being checked in the source control repository. Q#10) What is the difference between Continuous Integration, Continuous Delivery, and Continuous Deployment? Answer: The diagrammatic representation given below can elaborate on the differences between Continuous Integration, Continuous Delivery, and Continuous Deployment more precisely. Continuous Integration: (It involves keeping the latest copy of the source code at a commonly shared hub where all the developers can check to fetch out the latest change in order to avoid conflict.) Continuous Delivery: (Manual Deployment to Production. It does not involve every change to be deployed.) Continuous Deployment: (Automated Deployment to Production. Involves every change to be deployed automatically.) Q#11) What is Jenkins Pipeline? What is a CI CD pipeline? Answer: The pipeline can be defined as the suite of plugins supporting the implementation and integration of continuous delivery pipelines in Jenkins. Continuous integration or continuous delivery pipeline consists of build, deploy, test, release pipeline. The pipeline feature saves a lot of time and error in maintaining the builds. Basically, a pipeline is a group of build jobs that are chained and integrated in sequence. Q#12) What are Scripted Pipelines in Jenkins? Answer: Scripted Pipeline follows Groovy Syntax as given below: Node { } In the above syntax, the node is a part of the Jenkins distributed mode architecture, where there are two types of node, Master which handle all the tasks in the development environment and the Agent is being used to handle multiple tasks individually. Q #13) What are Declarative Pipelines in Jenkins? Answer: Declarative Pipelines are the newest additions to Jenkins that simplify the groovy syntax of Jenkins pipelines (top-level pipeline) with some exceptions, such as: No semicolon to be used as a statement separator. The top-level pipeline should be enclosed within block viz; The common syntax is: pipeline { /* Declarative Pipeline */ } Blocks must contain Sections, Directives, steps or assignments. pipeline { agent any stages { stage(\u2018Build\u2019) { steps { // Statements\u2026 } } stage (\u2018Test\u2019) { steps { // Statements\u2026 } } } } The above code has 3 major elements - Pipeline: The block of script contents. - Agent: Defines where the pipeline will start running from. - Stage: The pipelines contain several steps enclosed in the block called Stage. Q#14) What is SCM? Which SCM tools are supported in Jenkins? Answer: - SCM stands for Source Control Management. - SCM module specifies the source code location. - The entry point to SCM is being specified as jenkins_jobs.scm. - The job specified with \u2018scm\u2019 attribute accepts multiple numbers of SCM definitions. The SCM can be defined as: scm: name: eloc \u2013 scm scm: git: url: ssh://Jenkins.org/eloc.git Jenkins supported SCM tools include: - CVS - Git - Perforce - AccuRev - Subversion - Clearcase - RTC - Mercurial Q#15) Which CI Tools are used in Jenkin? Answer: Jenkins supported the following CI tools: 1. Jenkins 2. GitLab CI 3. Travis CI 4. CircleCI 5. Codeship 6. Go CD 7. TeamCity 8. Bamboo Q#16) Which commands can be used to start Jenkins manually? Answer: You can use the following commands to start Jenkins manually: 1. (Jenkins_url)/restart: To force restart without waiting for build completion. 2. (Jenkin_url)/safeRestart: Waits until all the build gets completed before restarting. Q#17) Which Environmental Directives are used in Jenkins? Answer: Environmental Directives is the sequence that specifies pairs of the key-values called Environmental Variables for the steps in the pipeline. Q#18) What are Triggers? Answer: Trigger in Jenkins defines the way in which the pipeline should be executed frequently. PollSCM, Cron, etc are the currently available Triggers. Q#19) What is Agent Directive in Jenkins? Answer: The Agent is the section that specifies the execution point for the entire pipeline or any specific stage in the pipeline. This section is being specified at the top-level inside the pipeline block. Q#20) How to make sure that your project build does not break in Jenkins? Answer: You need to follow the below-mentioned steps to make sure that the Project build does not break: 1. Clean and successful installation of Jenkins on your local machine with all unit tests. 2. All code changes are reflected successfully. 3. Checking for repository synchronization to make sure that all the differences and changes related to config and other settings are saved in the repository. Q#21) What is the difference between Maven, Ant, and Jenkins? Answer: Maven vs Jenkins: Maven is a build tool like Ant. It consists of a pom.xml file which is specified in Jenkins to run the code. Whereas, Jenkins is used as a continuous integration tool and automates the deployment process. The reports of the builds can be used to set a mark for continuous delivery as well. Q#22) How will you define Post in Jenkins? Answer: Post is a section that contains several additional steps that might execute after the completion of the pipeline. The execution of all the steps within the condition block depends upon the completion status of the pipeline. The condition block includes the following conditions \u2013 changed success, always, failure, unstable and aborted. Q#23) What are Parameters in Jenkins? Answer: Parameters are supported by the Agent section and are used to support various use-cases pipelines. Parameters are defined at the top-level of the pipeline or inside an individual stage directive. Q#24) How you can set up a Jenkins job? Answer: Setting up a new job in Jenkins is elaborated below with snapshots: Step 1: Go to the Jenkins Dashboard and log in with your registered login credentials. Step 2: Click on the New Item that is shown in the left panel of the page. Step 3: Click on the Freestyle Project from the given list on the upcoming page and specify the item name in the text box. Step 4: Add the URL to the Git Repository. Step 5: Go to the Build section and click on the Add build step => Execute Windows batch command. Step 6: Enter the command in the command window as shown below. Step 7: After saving all the settings and changes click on Build Now. Step 8: To see the status of the build click on Console Output. Q#25) What are the two components (pre-requisites) that Jenkins is mainly integrated with? Answer: Jenkins integrates with: 1. Build tools/ Build working script like Maven script. 2. Version control system/Accessible source code repository like Git repository. Q#26) How can You Clone a Git Repository via Jenkins? Answer: To create a clone repository via Jenkins you need to use your login credentials in the Jenkins System. To achieve the same you need to enter the Jenkins job directory and execute the git config command. Q#27) How can you secure Jenkins? Answer: Securing Jenkins is a little lengthy process, and there are two aspects of securing Jenkins: (i) Access Control which includes authenticating users and giving them an appropriate set of permissions, which can be done in 2 ways. - Security Realm determines a user or a group of users with their passwords. - Authorization Strategy defines what should be accessible to which user. In this case, there might be different types of security based on the permissions granted to the user such as Quick and simple security with easy setup, Standard security setup, Apache front-end security, etc. (ii) Protecting Jenkins users from outside threats. Q#28) How to create a backup and copy files in Jenkins? Answer: In Jenkins, all the settings, build logs and configurations are stored in the JENKINS_HOME directory. Whenever you want to create a backup of your Jenkins you can back up JENKINS_HOME directory frequently. It consists of all the job configurations and slave node configurations. Hence, regularly copying this directory allows us to keep a backup of Jenkins. You can maintain a separate backfile and copy it whenever you need the same. If you want to copy the Jenkins job, then you can do so by simply replicating the job directory. Q#29) What is the use of Backup Plugin in Jenkins? How to use it? Answer: Jenkins Backup Plugin is used to back up the critical configurations and settings in order to use them in the future in case of any failure or as per the need of time. The following steps are followed to back up your settings by using the Backup Plugin. Step 1: Go to the Jenkins Dashboard and click on Manage Jenkins. Step 2: Click on Manage Plugins that appears on the next page. Step 3: Go to Available Tab on the next page and search for ThinBackup. Step 4: Once you choose the available option, it will start installing. Step 5: Once it is installed the following screen will appear, from there choose Settings. Step 6: Enter the necessary details like backup directory along with other options as shown on the below screen and save the settings. The backup will be saved to the specified Backup Directory. Step 7: Go to the previous page to test whether the backup is happening or not by clicking on Backup Now as shown in the below image. Step 8: At last, you can check the Backup Directory specified in the ThinBackup Settings. (Step 6) to check the whole backup Q#30) What is Flow Control in Jenkins? Answer: In Jenkins, flow control follows the pipeline structure (scripted pipeline) that are being executed from the top to bottom of the Jenkins file. Q#31) What is the solution if you find a broken build for your project? Answer: To resolve the broken build follow the below-mentioned steps: - Open console output for the build and check if any file change has missed. OR - Clean and update your local workspace to replicate the problem on the local system and try to resolve it (In case you couldn\u2019t find out the issue in the console output). Q#32) What are the basic requirements for installing Jenkins? Answer: For installing Jenkins you need the following system configuration: 1. Java 7 or above. 2. Servlet 3.1 3. RAM ranging from 200 MB to 70+ GB depending on the project build needs. 4. 2 MB or more of memory. Q#33) How can you define a Continuous Delivery Workflow? Answer: The flowchart below shows the Continuous Delivery Workflow. Hope it will be much easier to understand with visuals. Q#34) What are the various ways in which the build can be scheduled in Jenkins? Answer: The build can be triggered in the following ways: 1. After the completion of other builds. 2. By source code management (modifications) commit. 3. At a specific time. 4. By requesting manual builds. Q#35) Why is Jenkins called a Continuous Delivery Tool? Answer: We have seen the Continuous Delivery workflow in the previous question, now let\u2019s see the step by step process of why Jenkins is being called as a Continuous Delivery Tool: 1. Developers work on their local environment for making changes in the source code and push it into the code repository. 2. When a change is detected, Jenkins performs several tests and code standards to check whether the changes are good to deploy or not. 3. Upon a successful build, it is being viewed by the developers. 4. Then the change is deployed manually on a staging environment where the client can have a look at it. 5. When all the changes get approved by the developers, testers, and clients, the final outcome is saved manually on the production server to be used by the end-users of the product. In this way, Jenkins follows a Continuous Delivery approach and is called the Continuous Delivery Tool. Q#36) Give any simple example of Jenkins script. Answer: This is a Jenkins declarative pipeline code for Java: pipeline { agent stages { stage('Building your first asset') { agent steps { echo 'Build asset' } } stage('Test') { agent steps { echo 'Building project 1' } } } } You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles","title":"Jenkins Interview Questions"},{"location":"nightwolf-cotribution/jenkins/#top-jenkins-interview-questions-answers-for-experianced-devops-engineer-solved","text":"We have consolidated Jenkins frequently asked interview questions in DevOps interviews. You will find these questions very helpful in your interviews for DevOps roles. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. Q#1) What is Jenkins? Answer: Jenkins is a free open source Continuous Integration tool and automation server to monitor continuous integration and delivery. It is written in Java. It is known as an automated Continuous Delivery tool that helps to build and test the software system with easy integration of changes to the system. Jenkins follows Groovy Scripting. Also, it enables developers to continuously check in their code and also analyze the postbuild actions. The automation testers can use to run their tests as soon as the new code is added or code is modified. Q#2) What are the features of Jenkins? Answer: Jenkins comes with the following features: 1. Free open source. 2. Easy installation on various operating systems. 3. Build Pipeline Support. 4. Workflow Plugin. 5. Test harness built around JUnit. 6. Easy upgrades. 7. Rapid release cycle. 8. Easy configuration setup. 9. Extensible with the use of third-party plugins. Q#3) What are the advantages of Jenkins? Why we use Jenkins? Answer: Jenkins is used to continuously monitor the large code base in real-time. It enables developers to find bugs in their code and fix them. Email notifications are made to the developers regarding their check-ins as a post-build action. Advantages of Jenkins are as follows: - Build failures are cached during the integration stage. - Notifies the developers about build report status using LDAP (Lightweight Directory Access Protocol) mail server. - Maven release project is automated with simple steps. - Easy bug tracking. - Automatic changes get updated in the build report with notification. - Supports Continuous Integration in agile development and test-driven development. Q#4) Mention some of the important plugins in Jenkins? Answer: Plugins in Jenkins includes: - Gits - Maven 2 Project - HTML Publisher - Copy Artcraft - Join - Green Balls - Amazon EC2 Q#5) What is Continuous Integration in Jenkins? Answer: Continuous integration is the process of continuously checking-in the developer code into a version control system and triggering the build to check and identify bugs in the written code. This is a very quick process and also gives them a chance to fix the bugs. Jenkins is one such continuous integration tool. In software development, multiple developers work on different software modules. While performing integration testing all the modules are being integrated together. It is considered as the development practice to integrate the code into the source repository Whenever the programmer/developer makes any change to the current code, then it automatically gets integrated with the system running on the tester\u2019s machine and makes the testing task easy and speedy for the system testers. Continuous Integration comprises of: - Development and Compilation - Database Integration - Unit Testing - Production Deployment - Code Labeling - Functional Testing - Generating and Analyzing Reports Q#6) What is the difference between Hudson and Jenkins? Answer: There is no difference between Hudson and Jenkins. Hudson was the former name of Jenkins, after going through several issues the name was changed to Jenkins. Q#7) What is Groovy in Jenkins? Answer: Groovy is the default scripting language that is being used in the development of JMeter Version 3.1. Currently Apache Groovy is the dynamic object-oriented programming language that is used as a scripting language for the Java platform. Apache Groovy comes with some useful features such as Java Compatibility and Development Support. Q#8) Which command is used to start Jenkins? Answer: You can follow the below-mentioned steps to start Jenkins: 1. Open Command Prompt 2. From the Command Prompt browse the directory where Jenkins. war resides 3. Run the command given below: D:\\>Java \u2013jar Jenkins.war Q#9) What is Jenkinsfile? Answer: The text file where all the definitions of pipelines are defined is called Jenkinsfile. It is being checked in the source control repository. Q#10) What is the difference between Continuous Integration, Continuous Delivery, and Continuous Deployment? Answer: The diagrammatic representation given below can elaborate on the differences between Continuous Integration, Continuous Delivery, and Continuous Deployment more precisely. Continuous Integration: (It involves keeping the latest copy of the source code at a commonly shared hub where all the developers can check to fetch out the latest change in order to avoid conflict.) Continuous Delivery: (Manual Deployment to Production. It does not involve every change to be deployed.) Continuous Deployment: (Automated Deployment to Production. Involves every change to be deployed automatically.) Q#11) What is Jenkins Pipeline? What is a CI CD pipeline? Answer: The pipeline can be defined as the suite of plugins supporting the implementation and integration of continuous delivery pipelines in Jenkins. Continuous integration or continuous delivery pipeline consists of build, deploy, test, release pipeline. The pipeline feature saves a lot of time and error in maintaining the builds. Basically, a pipeline is a group of build jobs that are chained and integrated in sequence. Q#12) What are Scripted Pipelines in Jenkins? Answer: Scripted Pipeline follows Groovy Syntax as given below: Node { } In the above syntax, the node is a part of the Jenkins distributed mode architecture, where there are two types of node, Master which handle all the tasks in the development environment and the Agent is being used to handle multiple tasks individually. Q #13) What are Declarative Pipelines in Jenkins? Answer: Declarative Pipelines are the newest additions to Jenkins that simplify the groovy syntax of Jenkins pipelines (top-level pipeline) with some exceptions, such as: No semicolon to be used as a statement separator. The top-level pipeline should be enclosed within block viz; The common syntax is: pipeline { /* Declarative Pipeline */ } Blocks must contain Sections, Directives, steps or assignments. pipeline { agent any stages { stage(\u2018Build\u2019) { steps { // Statements\u2026 } } stage (\u2018Test\u2019) { steps { // Statements\u2026 } } } } The above code has 3 major elements - Pipeline: The block of script contents. - Agent: Defines where the pipeline will start running from. - Stage: The pipelines contain several steps enclosed in the block called Stage. Q#14) What is SCM? Which SCM tools are supported in Jenkins? Answer: - SCM stands for Source Control Management. - SCM module specifies the source code location. - The entry point to SCM is being specified as jenkins_jobs.scm. - The job specified with \u2018scm\u2019 attribute accepts multiple numbers of SCM definitions. The SCM can be defined as: scm: name: eloc \u2013 scm scm: git: url: ssh://Jenkins.org/eloc.git Jenkins supported SCM tools include: - CVS - Git - Perforce - AccuRev - Subversion - Clearcase - RTC - Mercurial Q#15) Which CI Tools are used in Jenkin? Answer: Jenkins supported the following CI tools: 1. Jenkins 2. GitLab CI 3. Travis CI 4. CircleCI 5. Codeship 6. Go CD 7. TeamCity 8. Bamboo Q#16) Which commands can be used to start Jenkins manually? Answer: You can use the following commands to start Jenkins manually: 1. (Jenkins_url)/restart: To force restart without waiting for build completion. 2. (Jenkin_url)/safeRestart: Waits until all the build gets completed before restarting. Q#17) Which Environmental Directives are used in Jenkins? Answer: Environmental Directives is the sequence that specifies pairs of the key-values called Environmental Variables for the steps in the pipeline. Q#18) What are Triggers? Answer: Trigger in Jenkins defines the way in which the pipeline should be executed frequently. PollSCM, Cron, etc are the currently available Triggers. Q#19) What is Agent Directive in Jenkins? Answer: The Agent is the section that specifies the execution point for the entire pipeline or any specific stage in the pipeline. This section is being specified at the top-level inside the pipeline block. Q#20) How to make sure that your project build does not break in Jenkins? Answer: You need to follow the below-mentioned steps to make sure that the Project build does not break: 1. Clean and successful installation of Jenkins on your local machine with all unit tests. 2. All code changes are reflected successfully. 3. Checking for repository synchronization to make sure that all the differences and changes related to config and other settings are saved in the repository. Q#21) What is the difference between Maven, Ant, and Jenkins? Answer: Maven vs Jenkins: Maven is a build tool like Ant. It consists of a pom.xml file which is specified in Jenkins to run the code. Whereas, Jenkins is used as a continuous integration tool and automates the deployment process. The reports of the builds can be used to set a mark for continuous delivery as well. Q#22) How will you define Post in Jenkins? Answer: Post is a section that contains several additional steps that might execute after the completion of the pipeline. The execution of all the steps within the condition block depends upon the completion status of the pipeline. The condition block includes the following conditions \u2013 changed success, always, failure, unstable and aborted. Q#23) What are Parameters in Jenkins? Answer: Parameters are supported by the Agent section and are used to support various use-cases pipelines. Parameters are defined at the top-level of the pipeline or inside an individual stage directive. Q#24) How you can set up a Jenkins job? Answer: Setting up a new job in Jenkins is elaborated below with snapshots: Step 1: Go to the Jenkins Dashboard and log in with your registered login credentials. Step 2: Click on the New Item that is shown in the left panel of the page. Step 3: Click on the Freestyle Project from the given list on the upcoming page and specify the item name in the text box. Step 4: Add the URL to the Git Repository. Step 5: Go to the Build section and click on the Add build step => Execute Windows batch command. Step 6: Enter the command in the command window as shown below. Step 7: After saving all the settings and changes click on Build Now. Step 8: To see the status of the build click on Console Output. Q#25) What are the two components (pre-requisites) that Jenkins is mainly integrated with? Answer: Jenkins integrates with: 1. Build tools/ Build working script like Maven script. 2. Version control system/Accessible source code repository like Git repository. Q#26) How can You Clone a Git Repository via Jenkins? Answer: To create a clone repository via Jenkins you need to use your login credentials in the Jenkins System. To achieve the same you need to enter the Jenkins job directory and execute the git config command. Q#27) How can you secure Jenkins? Answer: Securing Jenkins is a little lengthy process, and there are two aspects of securing Jenkins: (i) Access Control which includes authenticating users and giving them an appropriate set of permissions, which can be done in 2 ways. - Security Realm determines a user or a group of users with their passwords. - Authorization Strategy defines what should be accessible to which user. In this case, there might be different types of security based on the permissions granted to the user such as Quick and simple security with easy setup, Standard security setup, Apache front-end security, etc. (ii) Protecting Jenkins users from outside threats. Q#28) How to create a backup and copy files in Jenkins? Answer: In Jenkins, all the settings, build logs and configurations are stored in the JENKINS_HOME directory. Whenever you want to create a backup of your Jenkins you can back up JENKINS_HOME directory frequently. It consists of all the job configurations and slave node configurations. Hence, regularly copying this directory allows us to keep a backup of Jenkins. You can maintain a separate backfile and copy it whenever you need the same. If you want to copy the Jenkins job, then you can do so by simply replicating the job directory. Q#29) What is the use of Backup Plugin in Jenkins? How to use it? Answer: Jenkins Backup Plugin is used to back up the critical configurations and settings in order to use them in the future in case of any failure or as per the need of time. The following steps are followed to back up your settings by using the Backup Plugin. Step 1: Go to the Jenkins Dashboard and click on Manage Jenkins. Step 2: Click on Manage Plugins that appears on the next page. Step 3: Go to Available Tab on the next page and search for ThinBackup. Step 4: Once you choose the available option, it will start installing. Step 5: Once it is installed the following screen will appear, from there choose Settings. Step 6: Enter the necessary details like backup directory along with other options as shown on the below screen and save the settings. The backup will be saved to the specified Backup Directory. Step 7: Go to the previous page to test whether the backup is happening or not by clicking on Backup Now as shown in the below image. Step 8: At last, you can check the Backup Directory specified in the ThinBackup Settings. (Step 6) to check the whole backup Q#30) What is Flow Control in Jenkins? Answer: In Jenkins, flow control follows the pipeline structure (scripted pipeline) that are being executed from the top to bottom of the Jenkins file. Q#31) What is the solution if you find a broken build for your project? Answer: To resolve the broken build follow the below-mentioned steps: - Open console output for the build and check if any file change has missed. OR - Clean and update your local workspace to replicate the problem on the local system and try to resolve it (In case you couldn\u2019t find out the issue in the console output). Q#32) What are the basic requirements for installing Jenkins? Answer: For installing Jenkins you need the following system configuration: 1. Java 7 or above. 2. Servlet 3.1 3. RAM ranging from 200 MB to 70+ GB depending on the project build needs. 4. 2 MB or more of memory. Q#33) How can you define a Continuous Delivery Workflow? Answer: The flowchart below shows the Continuous Delivery Workflow. Hope it will be much easier to understand with visuals. Q#34) What are the various ways in which the build can be scheduled in Jenkins? Answer: The build can be triggered in the following ways: 1. After the completion of other builds. 2. By source code management (modifications) commit. 3. At a specific time. 4. By requesting manual builds. Q#35) Why is Jenkins called a Continuous Delivery Tool? Answer: We have seen the Continuous Delivery workflow in the previous question, now let\u2019s see the step by step process of why Jenkins is being called as a Continuous Delivery Tool: 1. Developers work on their local environment for making changes in the source code and push it into the code repository. 2. When a change is detected, Jenkins performs several tests and code standards to check whether the changes are good to deploy or not. 3. Upon a successful build, it is being viewed by the developers. 4. Then the change is deployed manually on a staging environment where the client can have a look at it. 5. When all the changes get approved by the developers, testers, and clients, the final outcome is saved manually on the production server to be used by the end-users of the product. In this way, Jenkins follows a Continuous Delivery approach and is called the Continuous Delivery Tool. Q#36) Give any simple example of Jenkins script. Answer: This is a Jenkins declarative pipeline code for Java: pipeline { agent stages { stage('Building your first asset') { agent steps { echo 'Build asset' } } stage('Test') { agent steps { echo 'Building project 1' } } } } You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles","title":"Top Jenkins Interview Questions &amp; answers for Experianced DevOps Engineer - Solved"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/","text":"Kubernetes Cheatsheet Creating Objects \uf0c1 Name Command Create resource kubectl apply -f file_name.yaml Create from multiple files kubectl apply -f file_name_1.yaml -f file_name_2.yaml Create all files in directory kubectl apply -f directory_name Create from url kubectl apply -f https://url/uri Create pod kubectl run pod_name --image image_name Create pod, then expose it as service kubectl run pod_name --image image_name --port port --expose Create pod yaml file kubectl run pod_name --image image_name --dry-run=client -o yaml > file_name.yaml Create deployment kubectl create deployment deployment_name --image image_name Create deployment yaml file kubectl create deployment deployment_name --image image_name --dry-run=client -o yaml > file_name.yaml Create service kubectl create service service-type service_name --tcp=port:target_port Create service yaml file kubectl create service service-type service_name --tcp=port:target_port --dry-run=client -o yaml > file_name.yaml Expose service from pod/deployment kubectl expose deployment {pod/deployment_name} --type=service-type --port port --target-port target_port Create config map from key-value kubectl create configmap configmap_name --from-literal={key:value} --from-literal={key:value} Create config map from file kubectl create configmap configmap_name --from-file=file_name Create config map from env file kubectl create configmap configmap_name --from-env-file=file_name Create secret from key-value kubectl create secret generic secret_name --from-literal={key:value} --from-literal={key:value} Create secret from file kubectl create secret generic secret_name --from-file=file_name Create job kubectl create job job_name --image=image_name Create job from cronjob kubectl create job job_name --from=cronjob/cronjob-name Create cronjob kubectl create cronjob --image=image_name --schedule='cron-syntax' -- {command} {args} Monitoring Usage Commands \uf0c1 Name Command Get node cpu and memory utilization kubectl top node node_name Get pod cpu and memory utilization kubectl top pods pod_name Node Commands \uf0c1 Name Command Describe node kubectl describe node node_name Get node in yaml kubectl get node node_name -o yaml Get node kubectl get node node_name Drain node kubectl drain node node_name Cordon node kubectl cordon node node_name Uncordon node kubectl uncordon node node_name Pod Commands \uf0c1 Name Command Get pod kubectl get pod pod_name Get pod in yaml kubectl get pod pod_name -o yaml Get pod wide information kubectl get pod pod_name -o wide Get pod with watch kubectl get pod pod_name -w Edit pod kubectl edit pod pod_name Describe pod kubectl describe pod pod_name Delete pod kubectl delete pod pod_name Log pod kubectl logs pod pod_name Tail -f pod kubectl logs pod -f pod_name Execute into pod kubectl exec -it pod pod_name /bin/bash Running Temporary Image kubectl run pod_name --image=curlimages/curl --rm -it --restart=Never -- curl {destination Deployment Commands \uf0c1 Name Command Get deployment kubectl get deployment deployment_name Get deployment in yaml kubectl get deployment deployment_name -o yaml Get deployment wide information kubectl get deployment deployment_name -o wide Edit deployment kubectl edit deployment deployment_name Describe deployment kubectl describe deployment deployment_name Delete deployment kubectl delete deployment deployment_name Log deployment kubectl logs deployment/deployment_name -f Update image kubectl set image deployment deployment_name container_name=new_image_name Scale deployment with replicas kubectl scale deployment deployment_name --replicas replicas_name Service Commands \uf0c1 Name Command Get service kubectl get service service_name Get service in yaml Get service wide information kubectl get service service_name -o wide Edit service kubectl edit service service_name Describe service kubectl describe service service_name Delete service kubectl delete service service_name Endpoints Commands \uf0c1 Name Command Get endpoints kubectl get endpoints endpoints_name Ingress Commands \uf0c1 Name Command Get ingress kubectl get ingress Get ingress in yaml kubectl get ingress -o yaml Get ingress wide information kubectl get ingress -o wide Edit ingress kubectl edit ingress ingress_name Describe ingress kubectl describe ingress ingress_name Delete ingress kubectl delete ingress ingress_name DaemonSet Commands \uf0c1 Name Command Get daemonset kubectl get daemonset daemonset_name Get daemonset in yaml kubectl get daemonset daemonset_name -o yaml Edit daemonset kubectl edit daemonset daemonset_name Describe daemonset kubectl describe daemonset daemonset_name Delete daemonset kubectl delete deployment daemonset_name StatefulSet Commands \uf0c1 Name Command Get statefulset kubectl get statefulset statefulset_name Get statefulset in yaml kubectl get statefulset statefulset_name -o yaml Edit statefulset kubectl edit statefulset statefulset_name Describe statefulset kubectl describe statefulset statefulset_name Delete statefuleset kubectl delete statefulset statefulset_name ConfigMaps Commands \uf0c1 Name Command Get configmap kubectl get configmap configmap_name Get configmap in yaml kubectl get configmap configmap_name -o yaml Edit configmap kubectl edit configmap configmap_name Describe configmap kubectl describe configmap configmap_name Delete configmap kubectl delete configmap configmap_name Secret Commands \uf0c1 Name Command Get secret kubectl get secret secret_name Get secret in yaml kubectl get secret secret_name -o yaml Edit secret kubectl edit secret secret_name Describe secret kubectl describe secret secret_name Delete secret kubectl delete secret secret_name Rollout Commands \uf0c1 Name Command Restart deployment kubectl rollout restart deployment deployment_name Undo deployment with the latest revision kubectl rollout undo deployment deployment_name Undo deployment with specified revision kubectl rollout undo deployment deployment_name --to-revision revision_number Get all revisions of deployment kubectl rollout history deployment deployment_name Get specified revision of deployment kubectl rollout history deployment deployment_name --revision= revision_number Job Commands \uf0c1 Name Command Get job kubectl get job job_name Get job in yaml kubectl get job job_name -o yaml Edit job in yaml kubectl edit job job_name Describe job kubectl describe job job_name Delete job kubectl delete job job_name Cronjob Commands \uf0c1 Name Command Get cronjob kubectl get cronjob cronjob_name Get cronjob in yaml kubectl get cronjob cronjob_name -o yaml Edit cronjob kubectl edit cronjob cronjob_name Describe cronjob kubectl describe cronjob cronjob_name Delete cronjob kubectl delete cronjob cronjob_name Network Policy Commands \uf0c1 Name Command Get networkpolicy kubectl get networkpolicy networkpolicy_name Get networkpolicy in yaml kubectl get networkpolicy networkpolicy_name -o yaml Get networkpolicy wide information kubectl get networkpolicy networkpolicy_name -o wide Edit networkpolicy kubectl edit networkpolicy networkpolicy_name Describe networkpolicy kubectl describe networkpolicy networkpolicy_name Delete networkpolicy kubectl delete networkpolicy networkpolicy_name Labels and Selectors Commands \uf0c1 Name Command Show labels of node,pod and deployment kubectl get {node/pod/deployment} --show-labels Attach labels to {node/pod/deployment} kubectl label {node/pod/deployment} pod_name {key}={value} Remove labels from {node/pod/deployment} kubectl label {node/pod/deployment} {pod_name} {key}- Select node,pod and deployment by using labels kubectl get {node/pod/deployment} -l {key}={value} You may also refer to other articles on this website: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 Jenkins Interview Questions for Experianced DevOps Engineer","title":"Kubernetes CheatSheet"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#creating-objects","text":"Name Command Create resource kubectl apply -f file_name.yaml Create from multiple files kubectl apply -f file_name_1.yaml -f file_name_2.yaml Create all files in directory kubectl apply -f directory_name Create from url kubectl apply -f https://url/uri Create pod kubectl run pod_name --image image_name Create pod, then expose it as service kubectl run pod_name --image image_name --port port --expose Create pod yaml file kubectl run pod_name --image image_name --dry-run=client -o yaml > file_name.yaml Create deployment kubectl create deployment deployment_name --image image_name Create deployment yaml file kubectl create deployment deployment_name --image image_name --dry-run=client -o yaml > file_name.yaml Create service kubectl create service service-type service_name --tcp=port:target_port Create service yaml file kubectl create service service-type service_name --tcp=port:target_port --dry-run=client -o yaml > file_name.yaml Expose service from pod/deployment kubectl expose deployment {pod/deployment_name} --type=service-type --port port --target-port target_port Create config map from key-value kubectl create configmap configmap_name --from-literal={key:value} --from-literal={key:value} Create config map from file kubectl create configmap configmap_name --from-file=file_name Create config map from env file kubectl create configmap configmap_name --from-env-file=file_name Create secret from key-value kubectl create secret generic secret_name --from-literal={key:value} --from-literal={key:value} Create secret from file kubectl create secret generic secret_name --from-file=file_name Create job kubectl create job job_name --image=image_name Create job from cronjob kubectl create job job_name --from=cronjob/cronjob-name Create cronjob kubectl create cronjob --image=image_name --schedule='cron-syntax' -- {command} {args}","title":"Creating Objects"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#monitoring-usage-commands","text":"Name Command Get node cpu and memory utilization kubectl top node node_name Get pod cpu and memory utilization kubectl top pods pod_name","title":"Monitoring Usage Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#node-commands","text":"Name Command Describe node kubectl describe node node_name Get node in yaml kubectl get node node_name -o yaml Get node kubectl get node node_name Drain node kubectl drain node node_name Cordon node kubectl cordon node node_name Uncordon node kubectl uncordon node node_name","title":"Node Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#pod-commands","text":"Name Command Get pod kubectl get pod pod_name Get pod in yaml kubectl get pod pod_name -o yaml Get pod wide information kubectl get pod pod_name -o wide Get pod with watch kubectl get pod pod_name -w Edit pod kubectl edit pod pod_name Describe pod kubectl describe pod pod_name Delete pod kubectl delete pod pod_name Log pod kubectl logs pod pod_name Tail -f pod kubectl logs pod -f pod_name Execute into pod kubectl exec -it pod pod_name /bin/bash Running Temporary Image kubectl run pod_name --image=curlimages/curl --rm -it --restart=Never -- curl {destination","title":"Pod Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#deployment-commands","text":"Name Command Get deployment kubectl get deployment deployment_name Get deployment in yaml kubectl get deployment deployment_name -o yaml Get deployment wide information kubectl get deployment deployment_name -o wide Edit deployment kubectl edit deployment deployment_name Describe deployment kubectl describe deployment deployment_name Delete deployment kubectl delete deployment deployment_name Log deployment kubectl logs deployment/deployment_name -f Update image kubectl set image deployment deployment_name container_name=new_image_name Scale deployment with replicas kubectl scale deployment deployment_name --replicas replicas_name","title":"Deployment Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#service-commands","text":"Name Command Get service kubectl get service service_name Get service in yaml Get service wide information kubectl get service service_name -o wide Edit service kubectl edit service service_name Describe service kubectl describe service service_name Delete service kubectl delete service service_name","title":"Service Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#endpoints-commands","text":"Name Command Get endpoints kubectl get endpoints endpoints_name","title":"Endpoints Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#ingress-commands","text":"Name Command Get ingress kubectl get ingress Get ingress in yaml kubectl get ingress -o yaml Get ingress wide information kubectl get ingress -o wide Edit ingress kubectl edit ingress ingress_name Describe ingress kubectl describe ingress ingress_name Delete ingress kubectl delete ingress ingress_name","title":"Ingress Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#daemonset-commands","text":"Name Command Get daemonset kubectl get daemonset daemonset_name Get daemonset in yaml kubectl get daemonset daemonset_name -o yaml Edit daemonset kubectl edit daemonset daemonset_name Describe daemonset kubectl describe daemonset daemonset_name Delete daemonset kubectl delete deployment daemonset_name","title":"DaemonSet Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#statefulset-commands","text":"Name Command Get statefulset kubectl get statefulset statefulset_name Get statefulset in yaml kubectl get statefulset statefulset_name -o yaml Edit statefulset kubectl edit statefulset statefulset_name Describe statefulset kubectl describe statefulset statefulset_name Delete statefuleset kubectl delete statefulset statefulset_name","title":"StatefulSet Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#configmaps-commands","text":"Name Command Get configmap kubectl get configmap configmap_name Get configmap in yaml kubectl get configmap configmap_name -o yaml Edit configmap kubectl edit configmap configmap_name Describe configmap kubectl describe configmap configmap_name Delete configmap kubectl delete configmap configmap_name","title":"ConfigMaps Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#secret-commands","text":"Name Command Get secret kubectl get secret secret_name Get secret in yaml kubectl get secret secret_name -o yaml Edit secret kubectl edit secret secret_name Describe secret kubectl describe secret secret_name Delete secret kubectl delete secret secret_name","title":"Secret Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#rollout-commands","text":"Name Command Restart deployment kubectl rollout restart deployment deployment_name Undo deployment with the latest revision kubectl rollout undo deployment deployment_name Undo deployment with specified revision kubectl rollout undo deployment deployment_name --to-revision revision_number Get all revisions of deployment kubectl rollout history deployment deployment_name Get specified revision of deployment kubectl rollout history deployment deployment_name --revision= revision_number","title":"Rollout Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#job-commands","text":"Name Command Get job kubectl get job job_name Get job in yaml kubectl get job job_name -o yaml Edit job in yaml kubectl edit job job_name Describe job kubectl describe job job_name Delete job kubectl delete job job_name","title":"Job Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#cronjob-commands","text":"Name Command Get cronjob kubectl get cronjob cronjob_name Get cronjob in yaml kubectl get cronjob cronjob_name -o yaml Edit cronjob kubectl edit cronjob cronjob_name Describe cronjob kubectl describe cronjob cronjob_name Delete cronjob kubectl delete cronjob cronjob_name","title":"Cronjob Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#network-policy-commands","text":"Name Command Get networkpolicy kubectl get networkpolicy networkpolicy_name Get networkpolicy in yaml kubectl get networkpolicy networkpolicy_name -o yaml Get networkpolicy wide information kubectl get networkpolicy networkpolicy_name -o wide Edit networkpolicy kubectl edit networkpolicy networkpolicy_name Describe networkpolicy kubectl describe networkpolicy networkpolicy_name Delete networkpolicy kubectl delete networkpolicy networkpolicy_name","title":"Network Policy Commands"},{"location":"nightwolf-cotribution/kubernetes_cheatsheet/#labels-and-selectors-commands","text":"Name Command Show labels of node,pod and deployment kubectl get {node/pod/deployment} --show-labels Attach labels to {node/pod/deployment} kubectl label {node/pod/deployment} pod_name {key}={value} Remove labels from {node/pod/deployment} kubectl label {node/pod/deployment} {pod_name} {key}- Select node,pod and deployment by using labels kubectl get {node/pod/deployment} -l {key}={value} You may also refer to other articles on this website: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 Jenkins Interview Questions for Experianced DevOps Engineer","title":"Labels and Selectors Commands"},{"location":"nightwolf-cotribution/linux_L1/","text":"Linux Admin Interview Questions and Answers for Freshers and Experianced - Solved \uf0c1 We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. 1. What did you learn yesterday/this week? 2. Describe the general file system hierarchy of a Linux system. 3. What is the name and the UID of the administrator user in Linux/Unix? Hint => name of user is root and uid is 1. 4. How to list all files, including hidden ones, in a directory? 5. What is the Unix/Linux command to remove a directory and its contents? Hint => rm -rf DIR_NAME 6. Which command will show you free/used memory? Does free memory exist on Linux? Hint => free -m 7. How to search for the string \"my konfu is the best\" in files of a directory recursively? Hint => grep -rin \"my konfu is the best\" DIR_NAME 8. How to connect to a remote server or what is SSH? Hint => We can use ssh command to connect to remote server. command : ssh USER@10.0.0.1 9. How to get all environment variables and how can you use them? Hint => env command print all the environment variables associated with user. 10. I get \"command not found\" when I run ifconfig -a. What can be wrong? Hint => a). Either ifconfig command binary is not present in /usr/sbin/ or /sbin. b). Or PATH environment for user variable do not have /usr/sbin/ and /sbin paths. 11. What happens if I type TAB-TAB? 12. What command will show the available disk space on the Unix/Linux system? Hint => df -h 13. What commands do you know that can be used to check DNS records? Hint => a) nslookup b) dig c) host 14. What Unix/Linux commands will alter a files ownership, files permissions? Hint => a) To change file ownership: chown b) To change file permissions: chmod 15. What does chmod +x FILENAME do? Hint => It will provide shell executable permissions for FILENAME to everybody. 16. What does the permission 0750 on a file mean? 17. What does the permission 0750 on a directory mean? 18. How to add a new system user without login permissions? Hint => useradd -s /sbin/nologin tony OR useradd -M tony usermod -L tony 19. How to add/remove a group from a user? Hint => # gpasswd -d user_name group_name 20. What is a bash alias? Hint => An alias is a (usually short) name that the shell translates into another (usually longer) name or command. Aliases allow you to define new commands by substituting a string for the first token of a simple command 21. How do you set the mail address of the root/a user? Hint => The system administrator can define email aliases in the file /etc/aliases. This file contains lines like root: cwd@mailhost.example.com, /root/mailbox; the effect is the same as having cwd@mailhost.example.com, root/mailbox in ~root/.forward. You may need to run a program such as newaliases after changing /etc/aliases. 22. What does CTRL-c do? Hint => It sends the SIGINT signal to the process, which is technically just a request\u2014most processes will honor it, but some may ignore it. 23. What does CTRL-d do? Hint => Close the bash shell. This sends an EOF (End-of-file) marker to bash, and bash exits. 24. What does CTRL-z do? Hint => Suspend the current foreground process running in bash. This sends the SIGTSTP signal to the process. To return the process to the foreground later, use the fg process_name command. 25. What is in /etc/services? Hint => /etc/services contains port number mapping with service name. 26. How to redirect STDOUT and STDERR in bash? Hint => a) for all output : &> /dev/null b) To redirect the standard error (stderr): 2> /dev/null c) To redirect the standard output(stdout): 1> /dev/null 27. What is the difference between UNIX and Linux. 28. What is the difference between Telnet and SSH? 29. Which difference have between public and private SSH keys? 30. Explain the three load averages and what do they indicate. What command can be used to view the load averages? 31. Can you name a lower-case letter that is not a valid option for GNU ls? 32. What is a Linux kernel module? 33. Walk me through the steps in booting into single user mode to troubleshoot a problem. 34. Walk me through the steps you'd take to troubleshoot a 404 error on a web application you administer. 35. What is ICMP protocol? Why do you need to use? 36. What do the following commands do and how would you use them? tee awk tr cut tac curl wget watch head tail less cat touch sar netstat tcpdump lsof 37. What does an & after a command do? 38. What does & disown after a command do? 39. What is a packet filter and how does it work? 40. What is Virtual Memory? 41. What is swap and what is it used for? 42. What is an A record, an NS record, a PTR record, a CNAME record, an MX record? 43. Are there any other RRs and what are they used for? 44. What is a Split-Horizon DNS? 45. What is the sticky bit? 46. What does the immutable bit do to a file? 47. What is the difference between hardlinks and symlinks? What happens when you remove the source to a symlink/hardlink? 48. What is an inode and what fields are stored in an inode? 49. How to force/trigger a file system check on next reboot? 50. What is SNMP and what is it used for? 51. What is a runlevel and how to get the current runlevel? 52. What is SSH port forwarding? 53. What is the difference between local and remote port forwarding? 54. What are the steps to add a user to a system without using useradd/adduser? 55. What is MAJOR and MINOR numbers of special files? 56. Describe the mknod command and when you would use it. 57. Describe a scenario when you get a \"filesystem is full\" error, but \"df\" shows there is free space. 58. Describe a scenario when deleting a file, but \"df\" not showing the space being freed. 59. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. 60. Different types of file systems? Hint => ext2, ext3, ext4, xfs, vfat etc. 61. Difference between ext4 and xfs? 62. When v create user which files are referred? 63. Differnce between passwd and shadow file? Hint => /etc/passwd contains details abou User like home directory, shell etc. /etc/shadow conatains User password hashes. 64. What are the contents in passwd file? Hint => root:x:0:0:root:/root:/bin/bash \u2022 login name \u2022 optional encrypted password \u2022 numerical user ID \u2022 numerical group ID \u2022 user name or comment field \u2022 user home directory \u2022 optional user command interpreter 65. List all users who are not given passwd? with cmd Hint => cat /etc/shadow| awk -F \":\" '$2 ~ /[a-z]/ {print $1}' 66. How to check what processess are running on which port? Hint => Using \"netstat\" and \"ss\" command we can relate processes with port. 67. How to extend partition in lvm? and indepth questions in lvm ? Hint => 1. First attach extra storage/disk to server. 2. Create a new partition on the disk using \"fdisk\" or \"parted\" command. 3. Create a new Physical Volume(PV) on that new partition. e.g. \"pvcreate /dev/sdb1\" 4. Exetend existing Volume Group(VG) using new PV. e.g. \"vgextend vg_name /dev/sdb1\" 5. Now extend the LV. e.g . \"lvextend -l 100%FREE /dev/mapper/vg_name-lv_name\" 6. now execute \"resize2fs\" or \"xfs_growfs\". 68. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses, 69. What function does DNS play on a network? 70. On which port dns works? Hint => DNS works on port 53. 71. What is HTTP? 72. What is an HTTP proxy and how does it work? 73. Describe briefly how HTTPS works. 74. What is SMTP? Give the basic scenario of how a mail message is delivered via SMTP. 75. What is RAID? What is RAID0, RAID1, RAID5, RAID10? 76. What is a level 0 backup? What is an incremental backup? Next Page You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"Linux Interview Questions for Freshers and Experianced - L1"},{"location":"nightwolf-cotribution/linux_L1/#linux-admin-interview-questions-and-answers-for-freshers-and-experianced-solved","text":"We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. 1. What did you learn yesterday/this week? 2. Describe the general file system hierarchy of a Linux system. 3. What is the name and the UID of the administrator user in Linux/Unix? Hint => name of user is root and uid is 1. 4. How to list all files, including hidden ones, in a directory? 5. What is the Unix/Linux command to remove a directory and its contents? Hint => rm -rf DIR_NAME 6. Which command will show you free/used memory? Does free memory exist on Linux? Hint => free -m 7. How to search for the string \"my konfu is the best\" in files of a directory recursively? Hint => grep -rin \"my konfu is the best\" DIR_NAME 8. How to connect to a remote server or what is SSH? Hint => We can use ssh command to connect to remote server. command : ssh USER@10.0.0.1 9. How to get all environment variables and how can you use them? Hint => env command print all the environment variables associated with user. 10. I get \"command not found\" when I run ifconfig -a. What can be wrong? Hint => a). Either ifconfig command binary is not present in /usr/sbin/ or /sbin. b). Or PATH environment for user variable do not have /usr/sbin/ and /sbin paths. 11. What happens if I type TAB-TAB? 12. What command will show the available disk space on the Unix/Linux system? Hint => df -h 13. What commands do you know that can be used to check DNS records? Hint => a) nslookup b) dig c) host 14. What Unix/Linux commands will alter a files ownership, files permissions? Hint => a) To change file ownership: chown b) To change file permissions: chmod 15. What does chmod +x FILENAME do? Hint => It will provide shell executable permissions for FILENAME to everybody. 16. What does the permission 0750 on a file mean? 17. What does the permission 0750 on a directory mean? 18. How to add a new system user without login permissions? Hint => useradd -s /sbin/nologin tony OR useradd -M tony usermod -L tony 19. How to add/remove a group from a user? Hint => # gpasswd -d user_name group_name 20. What is a bash alias? Hint => An alias is a (usually short) name that the shell translates into another (usually longer) name or command. Aliases allow you to define new commands by substituting a string for the first token of a simple command 21. How do you set the mail address of the root/a user? Hint => The system administrator can define email aliases in the file /etc/aliases. This file contains lines like root: cwd@mailhost.example.com, /root/mailbox; the effect is the same as having cwd@mailhost.example.com, root/mailbox in ~root/.forward. You may need to run a program such as newaliases after changing /etc/aliases. 22. What does CTRL-c do? Hint => It sends the SIGINT signal to the process, which is technically just a request\u2014most processes will honor it, but some may ignore it. 23. What does CTRL-d do? Hint => Close the bash shell. This sends an EOF (End-of-file) marker to bash, and bash exits. 24. What does CTRL-z do? Hint => Suspend the current foreground process running in bash. This sends the SIGTSTP signal to the process. To return the process to the foreground later, use the fg process_name command. 25. What is in /etc/services? Hint => /etc/services contains port number mapping with service name. 26. How to redirect STDOUT and STDERR in bash? Hint => a) for all output : &> /dev/null b) To redirect the standard error (stderr): 2> /dev/null c) To redirect the standard output(stdout): 1> /dev/null 27. What is the difference between UNIX and Linux. 28. What is the difference between Telnet and SSH? 29. Which difference have between public and private SSH keys? 30. Explain the three load averages and what do they indicate. What command can be used to view the load averages? 31. Can you name a lower-case letter that is not a valid option for GNU ls? 32. What is a Linux kernel module? 33. Walk me through the steps in booting into single user mode to troubleshoot a problem. 34. Walk me through the steps you'd take to troubleshoot a 404 error on a web application you administer. 35. What is ICMP protocol? Why do you need to use? 36. What do the following commands do and how would you use them? tee awk tr cut tac curl wget watch head tail less cat touch sar netstat tcpdump lsof 37. What does an & after a command do? 38. What does & disown after a command do? 39. What is a packet filter and how does it work? 40. What is Virtual Memory? 41. What is swap and what is it used for? 42. What is an A record, an NS record, a PTR record, a CNAME record, an MX record? 43. Are there any other RRs and what are they used for? 44. What is a Split-Horizon DNS? 45. What is the sticky bit? 46. What does the immutable bit do to a file? 47. What is the difference between hardlinks and symlinks? What happens when you remove the source to a symlink/hardlink? 48. What is an inode and what fields are stored in an inode? 49. How to force/trigger a file system check on next reboot? 50. What is SNMP and what is it used for? 51. What is a runlevel and how to get the current runlevel? 52. What is SSH port forwarding? 53. What is the difference between local and remote port forwarding? 54. What are the steps to add a user to a system without using useradd/adduser? 55. What is MAJOR and MINOR numbers of special files? 56. Describe the mknod command and when you would use it. 57. Describe a scenario when you get a \"filesystem is full\" error, but \"df\" shows there is free space. 58. Describe a scenario when deleting a file, but \"df\" not showing the space being freed. 59. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. 60. Different types of file systems? Hint => ext2, ext3, ext4, xfs, vfat etc. 61. Difference between ext4 and xfs? 62. When v create user which files are referred? 63. Differnce between passwd and shadow file? Hint => /etc/passwd contains details abou User like home directory, shell etc. /etc/shadow conatains User password hashes. 64. What are the contents in passwd file? Hint => root:x:0:0:root:/root:/bin/bash \u2022 login name \u2022 optional encrypted password \u2022 numerical user ID \u2022 numerical group ID \u2022 user name or comment field \u2022 user home directory \u2022 optional user command interpreter 65. List all users who are not given passwd? with cmd Hint => cat /etc/shadow| awk -F \":\" '$2 ~ /[a-z]/ {print $1}' 66. How to check what processess are running on which port? Hint => Using \"netstat\" and \"ss\" command we can relate processes with port. 67. How to extend partition in lvm? and indepth questions in lvm ? Hint => 1. First attach extra storage/disk to server. 2. Create a new partition on the disk using \"fdisk\" or \"parted\" command. 3. Create a new Physical Volume(PV) on that new partition. e.g. \"pvcreate /dev/sdb1\" 4. Exetend existing Volume Group(VG) using new PV. e.g. \"vgextend vg_name /dev/sdb1\" 5. Now extend the LV. e.g . \"lvextend -l 100%FREE /dev/mapper/vg_name-lv_name\" 6. now execute \"resize2fs\" or \"xfs_growfs\". 68. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses, 69. What function does DNS play on a network? 70. On which port dns works? Hint => DNS works on port 53. 71. What is HTTP? 72. What is an HTTP proxy and how does it work? 73. Describe briefly how HTTPS works. 74. What is SMTP? Give the basic scenario of how a mail message is delivered via SMTP. 75. What is RAID? What is RAID0, RAID1, RAID5, RAID10? 76. What is a level 0 backup? What is an incremental backup?","title":"Linux Admin Interview Questions and Answers for Freshers and Experianced - Solved"},{"location":"nightwolf-cotribution/linux_L2/","text":"Linux Interview Questions and Answers for Experianced Linux Admins - Solved \uf0c1 We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. Most of these are scenario based Linux interview questions for Experianced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. 1. Please explain Linux booting processing? 2. Difference between RHEL6 and RHEL7 booting process. 3. Difference between systemd and initd. Hint => \"Systemd\" is an enhanced version of \"init\". \"sysytemd\" was launched with RHEL 7.x has capability to start the services in parallel. Please refer to below table for more information. 4. What are the inodes and how will you free up them? 5. How to check the loaded kernel modules. Hint => using \"lsmod\" command 6. How to load kernel module. any commands ? Hint => We can use \"modprobe\" or \"insmod\" commands to load kernel module. 7. How to blacklist a module. 8. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- 9. Explain few examples of kernel panic. Hint => A kernel panic is one of several Linux boot issues. In basic terms, it is a situation when the kernel can't load properly and therefore the system fails to boot. During the boot process, the kernel does not load directly. Instead, initramfs loads in RAM, then it points to the kernel (vmlinuz), and then the operating system boots. If initramfs gets corrupted or deleted at this stage because of recent OS patching, updates, or other causes, then we face a kernel panic. Causes of Kernel panics: a). If the initramfs file gets corrupted. b). If initramfs is not created properly for the specified kernel. Every kernel version has its own corresponding initramfs. c). If the installed kernel is not supported or not installed correctly. d). If recent patches have some flaws. e). If a module has been installed from online or another source, but the initrd image is not created with the latest installed module. 10. What is an oops in terms of Kernel? Hint => An oops indicates a kernel bug and should always be reported and fixed. When an oops occurs, the system will print out information that is relevent to debugging the problem, like the contents of all the CPU registers, and the location of page descriptor tables. In particular, the contents of the EIP (instruction pointer) is printed. Like this: EIP: 0010:[<00000000>] Call Trace: [<c010b860>] An oops is not a kernel panic. In a panic, the kernel cannot continue; the system grinds to a halt and must be restarted. An oops may cause a panic if a vital part of the system is destroyed. An oops in a device driver, for example, will almost never cause a panic. 11. What is \"nohup\" used for? Hint => nohup is a POSIX command which means \"no hang up\". Its purpose is to execute a command such that it ignores the HUP (hangup) signal and therefore does not stop when the user logs out. Output that would normally go to the terminal goes to a file called nohup. ## nuhup Syntax: nohup command [command-argument ...] 12. How to troubleshoot a issue where a client not able to access a server? 13. What are outputs when you execute the NATCAT (nc) command and explain each ? Hint => - Connected - Connection timeout - Connection refused(if port / server is blocking by the firewall) 14. How can we check the packet flow in our system? Hint => We can use \"tcpdump\" command to check the incoming and outgoing packets. 15. what is the steal value in top command? 16. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses. Files used by tcpwrappr for access control are /etc/hosts.allow and /etc/hosts.deny. 17. Describe how 'ps' works. Hint => On Linux, the ps command works by reading files in the proc filesystem. The directory '/proc/PID' contains various files that provide information about process PID. The content of these files is generated on the fly by the kernel when a process reads them. You can use 'strace' command to actually see how ps works. 18. Which Linux file types do you know? 19. What is \"nohup\" used for? 20. What is the difference between these two commands? myvar=hello & export myvar=hello 21. What happens to a child process that dies and has no parent process to wait for it and what\u2019s bad about this? 22. Explain briefly each one of the process states. 23. How to know which process listens on a specific port? 24. What is a zombie process and what could be the cause of it? 25. You run a bash script and you want to see its output on your terminal and save it to a file at the same time. How can you achieve it? Hint => We can use \"tee\" command to do this. Below is the command expamle: /sbin/ifconfig | tee FILE_NAME 26. Explain what echo \"1\" > /proc/sys/net/ipv4/ip_forward does. 27. What is CPU load ? How to calculate the load average on the system? 28. What the Zombie and Orphan process? How to kill zombie process? 29. What could be the impacts on the system if there are many zombie process are available? 30. Explain the TOP, IOSTAT,VMSTAT,IOTOP commands. 31. You need to upgrade kernel at 100-1000 servers, how you would do this? 32. How can you tell if the httpd package was already installed? 33. How can you list the contents of a package? 34. How can you determine which package is better: openssh-server-5.3p1-118.1.el6_8.x86_64 or openssh-server-6.6p1-1.el6.x86_64 ? 35. Describe briefly the steps you need to take in order to create and install a valid certificate for the site https://www.nightwolf.in. 36. Can you have several HTTPS virtual hosts sharing the same IP? 37. What is a wildcard certificate? 38. Which Linux file types do you know? 39. How can you get Host, Channel, ID, LUN of SCSI disk? 40. Can you explain to me the difference between block based, and object based storage? 41. What is a tarpipe (or, how would you go about copying everything, including hardlinks and special files, from one server to another)? 42. How many NTP servers would you configure in your local ntp.conf? 43. What does the column 'reach' mean in ntpq -p output? 44. What is bash quick substitution/caret replace(^x^y)? 45. Do you know of any alternative shells? If so, have you used any? 46. How can you limit process memory usage? 47. How do you troubleshoot memory performance issue. Please explain the details. 48. Which tools do you use to troubleshoot high Memory troubleshooting. 49. What are zombie process and how to kill/reclaim them. 50. What are D-State processes and what causes these. 51. If Disk is causing D-state processes, what you can check and can do to fix the issue. 52. How to troubleshoot the system performance if any Linux system is facing slowness? 53. How to troubleshoot high memory usageissue on Linux system. Next Page You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"Linux Interview Questions for Experianced Linux Admins - L2"},{"location":"nightwolf-cotribution/linux_L2/#linux-interview-questions-and-answers-for-experianced-linux-admins-solved","text":"We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. Most of these are scenario based Linux interview questions for Experianced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. 1. Please explain Linux booting processing? 2. Difference between RHEL6 and RHEL7 booting process. 3. Difference between systemd and initd. Hint => \"Systemd\" is an enhanced version of \"init\". \"sysytemd\" was launched with RHEL 7.x has capability to start the services in parallel. Please refer to below table for more information. 4. What are the inodes and how will you free up them? 5. How to check the loaded kernel modules. Hint => using \"lsmod\" command 6. How to load kernel module. any commands ? Hint => We can use \"modprobe\" or \"insmod\" commands to load kernel module. 7. How to blacklist a module. 8. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- 9. Explain few examples of kernel panic. Hint => A kernel panic is one of several Linux boot issues. In basic terms, it is a situation when the kernel can't load properly and therefore the system fails to boot. During the boot process, the kernel does not load directly. Instead, initramfs loads in RAM, then it points to the kernel (vmlinuz), and then the operating system boots. If initramfs gets corrupted or deleted at this stage because of recent OS patching, updates, or other causes, then we face a kernel panic. Causes of Kernel panics: a). If the initramfs file gets corrupted. b). If initramfs is not created properly for the specified kernel. Every kernel version has its own corresponding initramfs. c). If the installed kernel is not supported or not installed correctly. d). If recent patches have some flaws. e). If a module has been installed from online or another source, but the initrd image is not created with the latest installed module. 10. What is an oops in terms of Kernel? Hint => An oops indicates a kernel bug and should always be reported and fixed. When an oops occurs, the system will print out information that is relevent to debugging the problem, like the contents of all the CPU registers, and the location of page descriptor tables. In particular, the contents of the EIP (instruction pointer) is printed. Like this: EIP: 0010:[<00000000>] Call Trace: [<c010b860>] An oops is not a kernel panic. In a panic, the kernel cannot continue; the system grinds to a halt and must be restarted. An oops may cause a panic if a vital part of the system is destroyed. An oops in a device driver, for example, will almost never cause a panic. 11. What is \"nohup\" used for? Hint => nohup is a POSIX command which means \"no hang up\". Its purpose is to execute a command such that it ignores the HUP (hangup) signal and therefore does not stop when the user logs out. Output that would normally go to the terminal goes to a file called nohup. ## nuhup Syntax: nohup command [command-argument ...] 12. How to troubleshoot a issue where a client not able to access a server? 13. What are outputs when you execute the NATCAT (nc) command and explain each ? Hint => - Connected - Connection timeout - Connection refused(if port / server is blocking by the firewall) 14. How can we check the packet flow in our system? Hint => We can use \"tcpdump\" command to check the incoming and outgoing packets. 15. what is the steal value in top command? 16. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses. Files used by tcpwrappr for access control are /etc/hosts.allow and /etc/hosts.deny. 17. Describe how 'ps' works. Hint => On Linux, the ps command works by reading files in the proc filesystem. The directory '/proc/PID' contains various files that provide information about process PID. The content of these files is generated on the fly by the kernel when a process reads them. You can use 'strace' command to actually see how ps works. 18. Which Linux file types do you know? 19. What is \"nohup\" used for? 20. What is the difference between these two commands? myvar=hello & export myvar=hello 21. What happens to a child process that dies and has no parent process to wait for it and what\u2019s bad about this? 22. Explain briefly each one of the process states. 23. How to know which process listens on a specific port? 24. What is a zombie process and what could be the cause of it? 25. You run a bash script and you want to see its output on your terminal and save it to a file at the same time. How can you achieve it? Hint => We can use \"tee\" command to do this. Below is the command expamle: /sbin/ifconfig | tee FILE_NAME 26. Explain what echo \"1\" > /proc/sys/net/ipv4/ip_forward does. 27. What is CPU load ? How to calculate the load average on the system? 28. What the Zombie and Orphan process? How to kill zombie process? 29. What could be the impacts on the system if there are many zombie process are available? 30. Explain the TOP, IOSTAT,VMSTAT,IOTOP commands. 31. You need to upgrade kernel at 100-1000 servers, how you would do this? 32. How can you tell if the httpd package was already installed? 33. How can you list the contents of a package? 34. How can you determine which package is better: openssh-server-5.3p1-118.1.el6_8.x86_64 or openssh-server-6.6p1-1.el6.x86_64 ? 35. Describe briefly the steps you need to take in order to create and install a valid certificate for the site https://www.nightwolf.in. 36. Can you have several HTTPS virtual hosts sharing the same IP? 37. What is a wildcard certificate? 38. Which Linux file types do you know? 39. How can you get Host, Channel, ID, LUN of SCSI disk? 40. Can you explain to me the difference between block based, and object based storage? 41. What is a tarpipe (or, how would you go about copying everything, including hardlinks and special files, from one server to another)? 42. How many NTP servers would you configure in your local ntp.conf? 43. What does the column 'reach' mean in ntpq -p output? 44. What is bash quick substitution/caret replace(^x^y)? 45. Do you know of any alternative shells? If so, have you used any? 46. How can you limit process memory usage? 47. How do you troubleshoot memory performance issue. Please explain the details. 48. Which tools do you use to troubleshoot high Memory troubleshooting. 49. What are zombie process and how to kill/reclaim them. 50. What are D-State processes and what causes these. 51. If Disk is causing D-state processes, what you can check and can do to fix the issue. 52. How to troubleshoot the system performance if any Linux system is facing slowness? 53. How to troubleshoot high memory usageissue on Linux system.","title":"Linux Interview Questions and Answers for Experianced Linux Admins - Solved"},{"location":"nightwolf-cotribution/linux_L3/","text":"Advanced Linux Interview Questions for Experianced Linux Admins - Solved \uf0c1 We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. Most of these are scenario based Linux interview questions for Experianced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. 1. Explain the below error: ------------------------------------------------------------------------------------------- find: page allocation failure. order:5, mode:0x0 Pid: 22938, comm: find Not tainted 2.6.32-279.el6.i686 #1 Call Trace: [<c04f22bc>] ? __alloc_pages_nodemask+0x6bc/0x870 [<c051f1bc>] ? cache_alloc_refill+0x2bc/0x510 [<c083f035>] ? apic_timer_interrupt+0x31/0x38 [<c051f552>] ? __kmalloc+0x142/0x180 [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb2f0>] ? rpc_wait_bit_killable+0x0/0x80 [sunrpc] ------------------------------------------------------------------------------------------- Hint => Above error is kernel dump due to kmalloc buffer exhaustion. The system is out of memory and cannot satisfy the allocation request at all. This is more likely to happen on swap-less systems that are unable to purge application pages to free up memory. The system may have sufficient free pages in total but not enough that are physically contiguous to satisfy the allocation request. This is referred to as memory fragmentation. - The phrase \"page allocation failure\" - this defines the error that should be used for error search. - The process name \"find\" - often page allocation failures are related to code that gets executed by specific threads, try searching KCS with and without this term. If the allocation request was performed by an interrupt service routine then the process name will be irrelevant. - The order of the allocation \"order:5\" and mode \"mode:0x0\" can often be the same for every instance of a particular problem, try searching KCS with and without these terms. - The function requesting the memory allocation \"rpc_malloc()\" is likely to be the same for every instance of this particular problem so include that in the KCS search. - Do not search for the CPU number, the process id or function offsets since they will change and may prevent finding a match. For more details https://access.redhat.com/articles/1360023 2. What is order:5, mode:0x0 in above output. 3. Difference between kernel panic due to Memory crunch and page allocation failure. 4. Difference between VMalloc and KMalloc Hint => The kmalloc() & vmalloc() functions are a simple interface for obtaining kernel memory in byte-sized chunks. - The kmalloc() function guarantees that the pages are physically contiguous (and virtually contiguous). - The vmalloc() function works in a similar fashion to kmalloc(), except it allocates memory that is only virtually contiguous and not necessarily physically contiguous. 5. How do you troubleshoot memory performance issue. Please explain the details. 6. Which tools do you use to troubleshoot high Memory troubleshooting. 7. What are zombie process and how to kill/reclaim them. 8. What are system calls. 9. What is strace used for. 10. Difference between fork and exec in Linux. 11. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- 12. Explain few examples of kernel panic. 13. How to blacklist a module. 14. What are D-State processes and what causes these. 15. If Disk is causing D-state processes, what you can check and can do to fix the issue. 16. Explain blk_trace. 17. If you are receiving \"Connection reset by peer\" when trying to connect a server, what can be root cause of this and how you can fix this. 18. How will you troubleshoot high load avg. Which tools will you use and why. Explain in details. 19. What can be reason of high load avg in kernel space i.e. %sy in top. 20. What is CPU affinity . How to check it. 21. How to check all cores stat in top. Hint: by pressing 1 or using mpstat 22. What can be the reasons for server to stuck at boot prompt. and how to troubleshoot further ? 23. What is backporting in Linux. Hint: its related to packages. 24. There are two systems and one is taking less booting time and other than taking more than booting time. What can be difference in both ? 25. Please explain few types of kernel errors? 26. How to troubleshoot the system performance if any Linux system is facing slowness? 27. How to troubleshoot high memory usageissue on Linux system. 28. What is CPU load ? How to calculate the load average on the system? 29. What the Zombie and Orphan process? How to kill zombie process? 30. What could be the impacts on the system if there are many zombie process are available? 31. What is affinity in Linux? 1. Binding a process with a CPU core. 2. # taskset < command > 32. What is paging and swapping in Linux? Please explain Page fault ? 33. What is difference between cache and buffer? 34. Explain what is NUMA. You may know this as \"Node Interleaving\u201d. Give an example where NUMA is beneficial, and another where it's detrimental. Hints => a). NUMA: Non Uniform Memory Access (NUMA) is a design where the memory is split into groups by CPU, making the memory access faster for each CPU to their memory group. b). Beneficial: Multiple processes with smaller memory footprints. Good for applications that fork processes, like Apache. Detrimental: Monolithic, memory-heavy, single processes. May be threaded, example MySQL, Redis, Java-everything, Memcached. Follow-Up Q. - what if there's only a single physical proc. 35. Customer complains that their CPUs aren't running as fast as they're rated. How would you Troubleshoot ? Hints => a). Could be heat damage - check /proc/cpuinfo for \"cpu MHz\". b). Could be the wrong CPUs installed. Check the CPU specs in /proc/cpuinfo. OR dmidecode. c). Change system profile to MaximumPerformance with omconfig, then reboot. 36. Can we check System logs on Hardware layer from OS ?How ? Hints => a). Yes, in dmesg logs or kern.log b). By using respective Hardware CLI commands. c). Specific answers hpasmcli -s \"show iml\" ==> For HP servers omreport system [alertlog | esmlog] ==> For Dell Servers 37. What can be done to improve the performance of an I/O-heavy system? Hints => a). Check RAID Level, if server using any. b). Partition aligned, i.e. starting a partition on sector 2048. c). Mount Options like: noatime - (covers norelatime/nodiratime) Can be enabled only via fstab. d). RAID Controller settings - read/write policy, e.g. write-through(good to ensure data has been changed on disk. write-back(relys on cache, battery). e). nobarrier - depends on whether or not the PERC Controller is making use of a battery or not. f). Adjust the filesystem journaling method - the default mount option is in the middle, data=ordered. This indicates the data is written out to the filesystem before the metadata in the journal is updated. The filesystem can be mounted with data=writeback which allows data to be written to the journal and disk at the same time, with the risk of some data possibly being written to the disk after the metadata that cannot be recovered after a crash. Additionally, the journal can be placed on an external device with another mount option, allowing separation of the writes to increase throughput performance for each write operation. g). discard - For SSDs. Also called fs TRIM. Extends life and improves performance of SSD drives. TRIM enables the controller to cache the expected SSD write cycles to collapse them into a \"do only only\" mechanism to prevent burning out the SSD cells, which can only be written to once with voltage. Note: For LVM, set issue_discards=1 in lvm.conf. Note: On Dell, TRIM/discard is only available on RAID controllers >= PERC H710 h). For SSD, you don't want controller caching for performance reasons. By default HPs turn caching off. Dell: Cut-through IO (CTIO) is an IO accelerator for SSD arrays that boosts the throughput of devices connected to the PERC Controller. It is enabled through disabling the write-back cache (enable writethrough cache) and disabling Read Ahead. HP: On servers with SSD drives, write caching appears to be disabled by default for performance reasons (there is no need for write caching on SSD drives which are fast enough). Writes to SSD drives will not be acknowledged until the data is actually written to disk: i). On HP, HP SSD Smart Path - this is enabled by default by the RAID creation process in the HP RAID controller itself, and can be checked with the standard commands using hpssacli to view controller details and RAID logical device details. The theory is this special method of SSD writing replaces traditional caching techniques with a special HP invented method for improving SSD performance and longevity. j). Disk IO Scheduler deadline (recommended for DBs) noop (recommended for VMs) 38. Why would I choose to use RAID-5 instead of RAID-10? Or vice versa? Hints => RAID-5: Pro: Space efficiency - you only sacrifice 1x HDD to redundancy. Pro: Possible with a minimum of 3x drives, as opposed to RAID-10's minimum-4. Con: Slow writes. Note: Reads are still quick though, so you'd be fine on a read-heavy application. Con: If two drives fail, you lose everything. RAID-10: Pro: Great performance with both reads and writes. Good for write-heavy applications. Pro: Less risk of a catastrophic failure, since more than 1 drive can fail safely (Note: Depends which drives). Con: Too much wastage of Space. Only 50% capacity is uasable. 39. How snapshots work ? Can you please explain. In terms of VMWare or LVM(in case someone asks). Hints => As soon as you create a snapshot, LVM creates a pool of blocks. This pool also contains a full copy of the LVM metadata of the volume. When writes happen to the main volume such as updating an inode, the block being overwritten is copied to this new pool and the new block is written to the main volume. This is the 'copy-on-write'. Because of this, the more data that gets changed between when a snapshot was taken and the current state of the main volume, the more space will get consumed by that snapshot pool. When you mount the snapshot, the meta-data written when the snapshot was taken allows the mapping of snapshot pool blocks over changed blocks in the volume (or higher level snapshot). This way when an access comes for a specific block, LVM knows which block access. As far as the filesystem on that volume is concerned, there are no snapshots. 40. In which scenario we need to use vgcfgrestore and vgcfgbackup commands ? When /etc/lvm/archive contents are written (when does backup happen ?) Hints => a). To restore VG configuration. b). Backup is taken whenever we make change to LVM (create/remove LV/VG) and we can use it to restore to the prior state. 41. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. 42. What is deadlock ? How to identify if process is in deadlock in Linux/Unix ? Hint => Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process 43. Difference between insmod and modprobe commands in Linux ? 44. Difference between ext4 and xfs? 45. When we create user which files are referred? 46. What is the difference between a process and a thread? And parent and child processes after a fork system call? 47. What is the difference between exec and fork? 48. How can you limit process memory usage? 49. What is a tunnel and how you can bypass a http proxy? 50. What is the difference between IDS and IPS? 51. What shortcuts do you use on a regular basis? 52. What is the Linux Standard Base? 53. What is an atomic operation? 54. Your freshly configured http server is not running after a restart, what can you do? 55. What kind of keys are in ~/.ssh/authorized_keys and what it is this file used for? 56. I've added my public ssh key into authorized_keys but I'm still getting a password prompt, what can be wrong? 57. Did you ever create RPM's, DEB's or solaris pkg's? 58. What does :(){ :|:& };: do on your system? 59. How do you catch a Linux signal on a script? 60. Can you catch a SIGKILL? 61. What's happening when the Linux kernel is starting the OOM killer and how does it choose which process to kill first? 62. Describe the linux boot process with as much detail as possible, starting from when the system is powered on and ending when you get a prompt. 63. What's a chroot jail? 64. When trying to umount a directory it says it's busy, how to find out which PID holds the directory? 65. What's LD_PRELOAD and when it's used? 66. You ran a binary and nothing happened. How would you debug this? 67. What are cgroups? Can you specify a scenario where you could use them? 68. How can you remove/delete a file with file-name consisting of only non-printable/non-type-able characters? 69. How can you increase or decrease the priority of a process in Linux? 70. A running process gets EAGAIN: Resource temporarily unavailable on reading a socket. How can you close this bad socket/file descriptor without killing the process? 71. What do you control with swapiness? 72. How do you change TCP stack buffers? How do you calculate it? 73. What is Huge Tables? Why isn't it enabled by default? Why and when use it? 74. What is LUKS? How to use it? Next Page You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"Advanced Linux Interview Questions for Experianced Admins - L3"},{"location":"nightwolf-cotribution/linux_L3/#advanced-linux-interview-questions-for-experianced-linux-admins-solved","text":"We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Infrastructure specialist), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. Most of these are scenario based Linux interview questions for Experianced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. 1. Explain the below error: ------------------------------------------------------------------------------------------- find: page allocation failure. order:5, mode:0x0 Pid: 22938, comm: find Not tainted 2.6.32-279.el6.i686 #1 Call Trace: [<c04f22bc>] ? __alloc_pages_nodemask+0x6bc/0x870 [<c051f1bc>] ? cache_alloc_refill+0x2bc/0x510 [<c083f035>] ? apic_timer_interrupt+0x31/0x38 [<c051f552>] ? __kmalloc+0x142/0x180 [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb2f0>] ? rpc_wait_bit_killable+0x0/0x80 [sunrpc] ------------------------------------------------------------------------------------------- Hint => Above error is kernel dump due to kmalloc buffer exhaustion. The system is out of memory and cannot satisfy the allocation request at all. This is more likely to happen on swap-less systems that are unable to purge application pages to free up memory. The system may have sufficient free pages in total but not enough that are physically contiguous to satisfy the allocation request. This is referred to as memory fragmentation. - The phrase \"page allocation failure\" - this defines the error that should be used for error search. - The process name \"find\" - often page allocation failures are related to code that gets executed by specific threads, try searching KCS with and without this term. If the allocation request was performed by an interrupt service routine then the process name will be irrelevant. - The order of the allocation \"order:5\" and mode \"mode:0x0\" can often be the same for every instance of a particular problem, try searching KCS with and without these terms. - The function requesting the memory allocation \"rpc_malloc()\" is likely to be the same for every instance of this particular problem so include that in the KCS search. - Do not search for the CPU number, the process id or function offsets since they will change and may prevent finding a match. For more details https://access.redhat.com/articles/1360023 2. What is order:5, mode:0x0 in above output. 3. Difference between kernel panic due to Memory crunch and page allocation failure. 4. Difference between VMalloc and KMalloc Hint => The kmalloc() & vmalloc() functions are a simple interface for obtaining kernel memory in byte-sized chunks. - The kmalloc() function guarantees that the pages are physically contiguous (and virtually contiguous). - The vmalloc() function works in a similar fashion to kmalloc(), except it allocates memory that is only virtually contiguous and not necessarily physically contiguous. 5. How do you troubleshoot memory performance issue. Please explain the details. 6. Which tools do you use to troubleshoot high Memory troubleshooting. 7. What are zombie process and how to kill/reclaim them. 8. What are system calls. 9. What is strace used for. 10. Difference between fork and exec in Linux. 11. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- 12. Explain few examples of kernel panic. 13. How to blacklist a module. 14. What are D-State processes and what causes these. 15. If Disk is causing D-state processes, what you can check and can do to fix the issue. 16. Explain blk_trace. 17. If you are receiving \"Connection reset by peer\" when trying to connect a server, what can be root cause of this and how you can fix this. 18. How will you troubleshoot high load avg. Which tools will you use and why. Explain in details. 19. What can be reason of high load avg in kernel space i.e. %sy in top. 20. What is CPU affinity . How to check it. 21. How to check all cores stat in top. Hint: by pressing 1 or using mpstat 22. What can be the reasons for server to stuck at boot prompt. and how to troubleshoot further ? 23. What is backporting in Linux. Hint: its related to packages. 24. There are two systems and one is taking less booting time and other than taking more than booting time. What can be difference in both ? 25. Please explain few types of kernel errors? 26. How to troubleshoot the system performance if any Linux system is facing slowness? 27. How to troubleshoot high memory usageissue on Linux system. 28. What is CPU load ? How to calculate the load average on the system? 29. What the Zombie and Orphan process? How to kill zombie process? 30. What could be the impacts on the system if there are many zombie process are available? 31. What is affinity in Linux? 1. Binding a process with a CPU core. 2. # taskset < command > 32. What is paging and swapping in Linux? Please explain Page fault ? 33. What is difference between cache and buffer? 34. Explain what is NUMA. You may know this as \"Node Interleaving\u201d. Give an example where NUMA is beneficial, and another where it's detrimental. Hints => a). NUMA: Non Uniform Memory Access (NUMA) is a design where the memory is split into groups by CPU, making the memory access faster for each CPU to their memory group. b). Beneficial: Multiple processes with smaller memory footprints. Good for applications that fork processes, like Apache. Detrimental: Monolithic, memory-heavy, single processes. May be threaded, example MySQL, Redis, Java-everything, Memcached. Follow-Up Q. - what if there's only a single physical proc. 35. Customer complains that their CPUs aren't running as fast as they're rated. How would you Troubleshoot ? Hints => a). Could be heat damage - check /proc/cpuinfo for \"cpu MHz\". b). Could be the wrong CPUs installed. Check the CPU specs in /proc/cpuinfo. OR dmidecode. c). Change system profile to MaximumPerformance with omconfig, then reboot. 36. Can we check System logs on Hardware layer from OS ?How ? Hints => a). Yes, in dmesg logs or kern.log b). By using respective Hardware CLI commands. c). Specific answers hpasmcli -s \"show iml\" ==> For HP servers omreport system [alertlog | esmlog] ==> For Dell Servers 37. What can be done to improve the performance of an I/O-heavy system? Hints => a). Check RAID Level, if server using any. b). Partition aligned, i.e. starting a partition on sector 2048. c). Mount Options like: noatime - (covers norelatime/nodiratime) Can be enabled only via fstab. d). RAID Controller settings - read/write policy, e.g. write-through(good to ensure data has been changed on disk. write-back(relys on cache, battery). e). nobarrier - depends on whether or not the PERC Controller is making use of a battery or not. f). Adjust the filesystem journaling method - the default mount option is in the middle, data=ordered. This indicates the data is written out to the filesystem before the metadata in the journal is updated. The filesystem can be mounted with data=writeback which allows data to be written to the journal and disk at the same time, with the risk of some data possibly being written to the disk after the metadata that cannot be recovered after a crash. Additionally, the journal can be placed on an external device with another mount option, allowing separation of the writes to increase throughput performance for each write operation. g). discard - For SSDs. Also called fs TRIM. Extends life and improves performance of SSD drives. TRIM enables the controller to cache the expected SSD write cycles to collapse them into a \"do only only\" mechanism to prevent burning out the SSD cells, which can only be written to once with voltage. Note: For LVM, set issue_discards=1 in lvm.conf. Note: On Dell, TRIM/discard is only available on RAID controllers >= PERC H710 h). For SSD, you don't want controller caching for performance reasons. By default HPs turn caching off. Dell: Cut-through IO (CTIO) is an IO accelerator for SSD arrays that boosts the throughput of devices connected to the PERC Controller. It is enabled through disabling the write-back cache (enable writethrough cache) and disabling Read Ahead. HP: On servers with SSD drives, write caching appears to be disabled by default for performance reasons (there is no need for write caching on SSD drives which are fast enough). Writes to SSD drives will not be acknowledged until the data is actually written to disk: i). On HP, HP SSD Smart Path - this is enabled by default by the RAID creation process in the HP RAID controller itself, and can be checked with the standard commands using hpssacli to view controller details and RAID logical device details. The theory is this special method of SSD writing replaces traditional caching techniques with a special HP invented method for improving SSD performance and longevity. j). Disk IO Scheduler deadline (recommended for DBs) noop (recommended for VMs) 38. Why would I choose to use RAID-5 instead of RAID-10? Or vice versa? Hints => RAID-5: Pro: Space efficiency - you only sacrifice 1x HDD to redundancy. Pro: Possible with a minimum of 3x drives, as opposed to RAID-10's minimum-4. Con: Slow writes. Note: Reads are still quick though, so you'd be fine on a read-heavy application. Con: If two drives fail, you lose everything. RAID-10: Pro: Great performance with both reads and writes. Good for write-heavy applications. Pro: Less risk of a catastrophic failure, since more than 1 drive can fail safely (Note: Depends which drives). Con: Too much wastage of Space. Only 50% capacity is uasable. 39. How snapshots work ? Can you please explain. In terms of VMWare or LVM(in case someone asks). Hints => As soon as you create a snapshot, LVM creates a pool of blocks. This pool also contains a full copy of the LVM metadata of the volume. When writes happen to the main volume such as updating an inode, the block being overwritten is copied to this new pool and the new block is written to the main volume. This is the 'copy-on-write'. Because of this, the more data that gets changed between when a snapshot was taken and the current state of the main volume, the more space will get consumed by that snapshot pool. When you mount the snapshot, the meta-data written when the snapshot was taken allows the mapping of snapshot pool blocks over changed blocks in the volume (or higher level snapshot). This way when an access comes for a specific block, LVM knows which block access. As far as the filesystem on that volume is concerned, there are no snapshots. 40. In which scenario we need to use vgcfgrestore and vgcfgbackup commands ? When /etc/lvm/archive contents are written (when does backup happen ?) Hints => a). To restore VG configuration. b). Backup is taken whenever we make change to LVM (create/remove LV/VG) and we can use it to restore to the prior state. 41. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. 42. What is deadlock ? How to identify if process is in deadlock in Linux/Unix ? Hint => Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process 43. Difference between insmod and modprobe commands in Linux ? 44. Difference between ext4 and xfs? 45. When we create user which files are referred? 46. What is the difference between a process and a thread? And parent and child processes after a fork system call? 47. What is the difference between exec and fork? 48. How can you limit process memory usage? 49. What is a tunnel and how you can bypass a http proxy? 50. What is the difference between IDS and IPS? 51. What shortcuts do you use on a regular basis? 52. What is the Linux Standard Base? 53. What is an atomic operation? 54. Your freshly configured http server is not running after a restart, what can you do? 55. What kind of keys are in ~/.ssh/authorized_keys and what it is this file used for? 56. I've added my public ssh key into authorized_keys but I'm still getting a password prompt, what can be wrong? 57. Did you ever create RPM's, DEB's or solaris pkg's? 58. What does :(){ :|:& };: do on your system? 59. How do you catch a Linux signal on a script? 60. Can you catch a SIGKILL? 61. What's happening when the Linux kernel is starting the OOM killer and how does it choose which process to kill first? 62. Describe the linux boot process with as much detail as possible, starting from when the system is powered on and ending when you get a prompt. 63. What's a chroot jail? 64. When trying to umount a directory it says it's busy, how to find out which PID holds the directory? 65. What's LD_PRELOAD and when it's used? 66. You ran a binary and nothing happened. How would you debug this? 67. What are cgroups? Can you specify a scenario where you could use them? 68. How can you remove/delete a file with file-name consisting of only non-printable/non-type-able characters? 69. How can you increase or decrease the priority of a process in Linux? 70. A running process gets EAGAIN: Resource temporarily unavailable on reading a socket. How can you close this bad socket/file descriptor without killing the process? 71. What do you control with swapiness? 72. How do you change TCP stack buffers? How do you calculate it? 73. What is Huge Tables? Why isn't it enabled by default? Why and when use it? 74. What is LUKS? How to use it?","title":"Advanced Linux Interview Questions for Experianced Linux Admins - Solved"},{"location":"nightwolf-cotribution/linux_basic/","text":"Linux Interview Questions and Answers for Freshers - Solved: \uf0c1 We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux Admin, Amazon interview questions for Linux admin and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. 1.What is two types of Linux User Mode ? Command Line GUI 2.What is command for created multiple files at a time? touch 3.What is INODE and How to Identify? Its unique identification code for files and directories, its was generate automatically while create new file and directories ls -i filename ls -ldi directoryname 4.List of Permissions and Users Read, Write and Execute Owner, Group Owners and Others 5.List of Special Permissions and numerical value. Set User ID = 4 Set Group ID = 2 Stickybit = 1 6. What command to use see Process list in Hierarchical Structure along with PID? pstree -P 7. What is use of \u201ctop\u201d command and how to sort Memory and User wise? Its used to real time monitor hardware utilization of linux machine. Press M to sort Memory wise result Press U to sort User wise result 8. What is command for to force close one particular process kill -9 Processid 9.What is command to refresh NIC ? Service network restart 10.Tell me two types of IP Address configuration Static IP Address Dynamic IP Address 11.How do Enable / Disable Ethernet Device Open and Edit this file #vi /etc/sysconfig/network-scripts/devicename For enable ONBOOT = yes For disable ONBOOT =no 12.What is command to change Hostname without System Restart hostname newhostname hostnamectl 13.What is File Path of Network Configuration ? /etc/sysconfig/network-scripts 14.What is File Path of DNS Configuration ? /etc/resolv.conf 15.How to Update locate DB ? cd/var/lib/mlocate updatedb 16. How to edit and save file using editors? The following commands are used to exit from vi editors. 1. :wq saves the current work and exits the VI. 2. :q! exits the VI without saving current work. 17.What is command for Zip and Unzip files 1. gzip = Compress File 2. gunzip = Uncompress File 18.What is file path of Alias name set by Permanent? /etc/bashrc 19.What is MBR in linux? Its Master Boot Recorder to help booting operating system. 20.What is Two Types of Mount in linux? Temporary Mount Permanent Mount 21.What is command for delete Partition? #umount #palimpsest & OR #parted OR #fdisk 22.What is command for Refresh Partition? mount -a 23.What is SWAP? Linux uses swap space to increase the amount of virtual memory available to a host. It can use one or more dedicated swap partitions or a swap file on a regular filesystem or logical volume. 24.What are types can set SWAP? Temporary set Permanent set 25. What command use for Filesystem Error checking and Error Fixing fsck and e2fsck 26. What is PV, VG, and LVM PV = Physical Volume VG = Volume Group LVM = Logical Volume 27. What is LVM LVM is used to create logical partitions and during run time we can resize particular partition without data loss. Empty partitions only can do LVM creation. 28. What are common commands used for Physical Volume pvcreate pvs pvdisplay 29. What command is used for create Volume Group vgcreate vgs 30. What is Syntax for LVM Create? #lvcreate -L partitionsize -n userdefinename volumegroupname 31. What types of Installation Tools in REDHAT? RPM = Redhat Package Manager YUM = Yellow Dog Updated Modifier 32. Tell me Linux Boot Sequence Floow? BIOS \u2192 MBR \u2192 Boot Loader \u2192 Kernal \u2192 Runlevel 33. Types of Zone in DNS? Forward lookup zone Reverse lookup zone 34. What are inbuilt firwall in Linux ? IP Tables Selinux TCPwrappers 35. What command to Execute disable IPTables permanently? #iptables -F service iptables save 36. What is SELinux? Its one type of firewall in linux To block particular service in a Protocol 37. File to disable SELinux permanently: /etc/selinux/config 38. What is command to check selinux status ? getenforce 39. What is LDAP The Lightweight Directory Access Protocol (LDAP) is a set of open protocols used to access centrally stored information over a network. 40. Which Configuration File Is Required For Ldap Clients? ldap.conf 41. What Is The Name Of Main Configuration File Name For Ldap Server? slapd.conf 42. How Will You Verify Ldap Configuration File? slaptest -u 43. What is command package install using YUM without ask Prompt? yum install packagename -y 44. What is command Uninstall package? yum remove packagename 45. What is command package re-install using YUM without ask Prompt? yum reinstall packagename -y 46. Location of Cron file in linux? /var/spool/cron 47. What is command for to see Particular user Job Schedule ? crontab -lu username 48. What is command for restart cron service? service crond restart 49. What is command for restart postfix service? service postfix restart 50.What is command for FTP service on and restart? chkconfig vsftpd on service vsftpd restart 50. What is Kernel un Unix Operating system? Kernel is the heart of operating system. It interacts with shell and executes the machine level language. 51. How can I save my input and output commands and see them when required? At the beginning of the session if you use 'script' command then the details of the input and output commands will be saved in a file called typescript and we can view it any time using \u201ccat typescript\u201d command. This is very useful to track what user is doing what. HISTORY command will not work because it shows data only for the current session. 52. How to create a file in Unix? There are multiple way to create files in unix, but the simple way to create a file is using \u201ccat\u201d and \u201cTouch\u201d command Syntax: cat > File name touch file name 53. How can I check which processes are running in my machine? To check process which are running in my machine I can use two commands. (a) TOP and (b) PS 54. What is the difference between TOP and PS command? Top command gives the dynamic view of the processes are running in the server and generally the dynamic change happens in every 3 second. Whereas PS commands gives the static view of the processes. 55. You used TOP command and without aborting the TOP process I need to kill one process. Is it possible to kill? Yes TOP command it self has a command prompt. Type K then it will ask you for the PID of the process to kill. Hit the PID and enter, it will kill the process. 56. What is the difference between creating a file in cat and in touch command? cat command creates a file and we can save some data inside the file but touch command by default will create a blank file. 57. How can I create multiple directories at a time? Say I want to create a directory D1 and inside that D2 and inside that D3. Is it possible? If yes how ? Yes, creating multiple directories is possible. In this scenario the below command works. #mkdir \u2013p D1/D2/D3 58. I want to create D1, under that D2 and D3. Inside D2 I want D4 and inside D3 I want D5 to be created. How is it possible? The below command will work for it. #mkdir \u2013p D1/D2/D4 D1/D3/D5 59. How can I check in which directory I am in ? Use PWD command to check which directory you are in. 60. We are using so many commands and getting output. Have you ever wondered how the commands are executing and getting you the output? Yes, every command in Unix is a C program in the backend. When we type a command and hit enter the program runs in the backend and gives you the output. We can view the C program as well as below. type <Command Name> ->hit enter, it will give you a path where the program the command is located. You can view the program by doing cat and the path name. it will open a C program file in decrypted mode. 61. How can I list the directories and the files ? Using ls command. I can view the directories and files of the system. 62. How can I view hidden files in a system ? Using ls \u2013a command I can view the hidden files of the sytem 63. In real time environment many people use \u201cll\u201d command instead of ls. So is there any command called \u201cll\u201d exits? No, there is no such command called \u201cll\u201d. It\u2019s just the alias of ls command. We can check it by typing alias command. 64. What is a shell ? Description of shell is huge, but yes commonly we explain it as the interpreter between the user and the machine. 65. Describe the usage of rm \u2013r* command in unix and shall we use it in real time environment? rm \u2013r* will remove all the file entries in the current directory. It is not advisable to use this command in real time environment. Specifically in production. Because we have huge files which are necessary to be accessed by other users. 66. What is symbolic link ? The second name of a file is called a link, it\u2019s assigned to create another link to the current file. 67. What is absolute path and relative path in unix ? Absolute path refers to the path starting from the root directory and the path continues with a sequence starting from Root. Whereas relative path is the current path. 68. How can I check the system IP ? type hostname command or else you can use ifconfig as well. 69. How can I check if a server is up and running or not ? you can use ping \u2013t command for this. Ping \u2013t <hostname> or <IP address> 70. How can I append some lines in an existing file ? #cat >> file name and hit enter. You can append lines below the existing lines of the file. And do a ctrl D to save and exit. 71. What is FIFO and LIFO in unix ? FIFO is first in first out and LIFO is last in last out. 72. What is PATH variable ? PATH is an environmental variable which contains the path of the command files and we can change the paths inside the PATH variable. 73. How can I kill a process in unix? first use PS \u2013ef command and get the PID of the process you want to kill. Then use kill -9 <PID_Number> command to kill the process. 74. How can I check the memory size of a linux/unix machine ? Use Free \u2013m or free \u2013G command to check the memory size of a linux machine. 75. How to check disk utilization of a linux server? Use du command to check the disk utilization. 76. How to check the disk free of all the mount points in unix ? use df \u2013h command, it will show the disk free of linux machine. 77. How can I check who are the users logged in my system? use users command. It will how the details of the users logged in to the system. 78. I have a file Mantu.txt which contains multiple lines and few of the lines has a particular pattern as \u201cIndia\u201d. I want to print only those lines. How can I ? I will ue grep command here. And the syntax will be as below. #grep -i \u201cIndia\u201d Mantu.txt grep command is used for pattern earching. 79. What do you mean by a super user ? A admin while giving permission to the users usually give normal access permission but few of the user having special permission then normal user, they are called super user. 80. What is the syntax to move to a super user? sudo su \u2013 <user name> 81. How can I change the permission of a file? Using chmod command I can change the permissions of a file. 82. How can I give all permission to a user? Use the below command to give all read, write and execute permission. chmod 777 <file name> 83. What is a process group in unix ? Collection of more than one process is called as a process group in unix.the function getpgrp returns the process group id. 84. How many numbers are used with kill while killing a process ? There are 64 numbers which can be used with kill command but generaly we use kill -9 85. What are different types of files available in unix? There are multiple type of files available in unix, few of among them are : \u2022 Regular file \u2022 Image file \u2022 Binary file \u2022 Linked file 86. What are cmp and different command in unix? cmp command compares the two files byte by byte and gives the output what is not common in between them. Diff command through the output which is not matching between the two file immediately rather comparing bit by bit. 87. What is pipe command ? why it is used for ? Pipe symbol interlinks two commands. It stores the output of the first command and give it to the second command as input. #cat emp.lst | mantu.txt 88. How can I number the lines of a file in VI editor? Open the file using vi <filename> Then go to command prompt and type set number. The numbers will be set before every line of the file. 89. What is the command to check all the options and detail information of a command in unix ? We can use man <command name>. it will show you all the possible way to use the command. 90. What is head command used for ? head command is used to view the top portions of the file. Say if you want to view top 5 lines of a file then you can use the below command. #cat <filename> | head -5 91. What is tail command used for ? tail command is used to view the bottom of the lines of a file. Say if I want to view bottom 5 lines of a file then I can use the below command. #cat <filename> | tail -5 92. What are the other commands used for pattern searching ? grep, awk and sed are the main command used for pattern searching. 93. How can I search a pattern in vi editor ? Open the file with vi. Use /pattern name , then hit enter, it will show you the matching patterns in VI. 94. How can I delete one line in Vi editor ? Use dd in command mode to delete one line of a file in vi. 95. What is the command used for copying a file? Use cp command while copying a file in unix. #cp <sourcepath of the file> <destination path of the file> 96. What is SCP in unix ? scp stands for secure copy in unix. The files which get copied by using scp command are decrypted so we need not be worry of hacking of the file system. 97. What is mv command in unix? We can move a file or rename a file using this command. General purpose of using mv command is to use it for reaming purpose. 98. What does a touch command do apart from creating a blank file? touch command is used to change the access and modification time of the file. 99. Explain the advantages of executing a process in background We use \u201c&\u201d symbol to execute a job in back ground. When we execute a job or process in unix it starts executing in the prompt itself and we can\u2019t do other stuffs in the command prompt at that time. So until unless the process gets executed we have to seat idle. So for continuous interaction with the command prompt we prefer executing the jobs or processes in back ground. 100. How do you protect file deletion in ext4? You change any attributes of the file to read only. The command is: #chattr +i filename And to disable it: #chattr -i filename 101. How do you list the kerenel modules which is already loaded ? List Currently Loaded Modules \u2013 lsmod List Available Kernel Modules \u2013 modprobe -l Install New modules into Linux Kernel \u2013 modprobe vmhghs Remove the Currently Loaded Modul \u2013 modprobe -r vmhghs 102. What will happen in chkconfig? issuing the command \u201cchkconfig sendmail on\u201d will create symlinks(softlinks) /etc/rd1.d/K30sendmail /etc/rd2.d/S80sendmail /etc/rd3.d/S80sendmail /etc/rd4.d/S80sendmail /etc/rd5.d/S80sendmail /etc/rd6.d/K30sendmail 103. How To rebuild Corrupted RPM Database ? [root@tecmint]# cd /var/lib [root@tecmint]# rm __db* [root@tecmint]# rpm \u2013rebuilddb [root@tecmint]# rpmdb_verify Packages 104. what resize2fs do at back end? Mounted, Extending The kernel then begins writing additional filesystem metadata on the newly available storage. Unmounted,Shrinking resize2fs makes the filesystem use only the first size bytes of the storage. It does this by moving both filesystem metadata and your data around. After the completes, there will be unused storage at the end of the block device, unused by the filesystem. 105. Special Permissions in linux Sticky bit \u2013 Only created user and root can able to delete the file #chmod o+t nightwolf.txt #chmod +t nightwolf.txt #chmod 1777 nightwolf.txt #ls -ld nightwolf.txt drwxrwxrwt 2 root root 4096 Mar 24 12:19 nightwolf.txt SUID \u2013 Ging permission for all users like root chmod u+s /bin/ls \u2013 ls can be used for all users as like root # chmod 4555 [path_to_file] #ls -l /bin/ls -rwsr-xr-x-x 1 root user 16384 Jan 12 2014 /bin/ls SGID \u2013 SGUID :- chmod g+s /dir \u2013> all subdirectories and files created inside will get same group ownership as the main directory, it doesn\u2019t matter who is creating. #chmod 2555 [dir] #ls -l /usr/bin/write -r-xr-sr-x 1 root tty 11484 Jan 15 17:55 /usr/bin/write 106. Password never expire linux? # chage -M -1 nightwolf \u2013> set the max passwd age to -1 # passwd -x -1 nightwolf # chage -m 0 -M 99999 -I -1 -E -1 nightwolf 107. What files are created/modified when adding a user (useradd) in linux? /etc/passwd and /etc/shadow files from /etc/skel are typically copied into the new user\u2019s home directory 108. How to see and get info about RAM in your system # free # cat /proc/meminfo 109. How will you suspend a running process and put it in the background? Ctrl+z 110. Name the Daemon responsible for tracking System Event on your Linux box? Syslogd 111. To see tar file without extracting? tar -tvf 112. How to check dependencies of RPM Package on before Installing ? # rpm -qpR BitTorrent-5.2.2-1-Python2.4.noarch.rpm /usr/bin/python2.4 python >= 2.3 python(abi) = 2.4 python-crypto >= 2.0 python-psyco python-twisted >= 2.0 python-zopeinterface rpmlib(CompressedFileNames) = 2.6 q : Query a package p : List capabilities this package provides. R: List capabilities on which this package depends. 113. How can we increase disk read performance in single commands? To see the current read performance, # blockdev \u2013getra /dev/sdb 256 # time dd if=/tmp/disk.iso of=/dev/null bs=256k 2549+1 records in 2549+1 records out copied, 6,84256 seconds, 97,7 MB/s real 0m6.845s user 0m0.004s sys 0m0.865s # After test # blockdev \u2013setra 1024 /dev/sdb # time dd if=/tmp/disk.iso of=/dev/null bs=256k 2435+1 records in 2435+1 records out copied, 0,364251 seconds, 1,8 GB/s real 0m0.370s user 0m0.001s sys 0m0.370s 114. How Many Run Levels present in Linux? There are seven run levels, with each having its own properties. \u2022 Halt the system \u2022 Single-user mode \u2022 Multiuser mode without networking(NFS) \u2022 Multi-user mode with text login \u2022 Not used \u2022 Multi-user mode with graphical login \u2022 Reboot 115. How do i check which NFS version ? rpcinfo -p localhost | grep -i nfs rpm -qa | grep nfs rpm -qi nfs nfs-utils 116. Use find command to delete file by inode? Find and remove file using find command follows: # find . -inum 782263 -exec rm -i {} \\; 117. Check if any user is using the file system? Check to the what users are currently using the file system: # fuser -cu /dev/hdc1 /opt/backup: 2337c(root) 118. Explain ntsysv or chkconfig command Both are similar want all services to start in different runlevel # ntsysv \u2013level <level> # chkconfig \u2013list <service name> # chkconfig <service name> on # chkconfig <service name> \u2013level 3 119. Explained BOOT LOADER? The boot loader is then responsible for loading the kernel. A boot loader finds the kernel image on the disk, loads it into memory, starts it. Stage 1 boot loader: First stage the primary boot loader is to find and load the secondary boot loader. It will find by looking through the partition table for an active partition. This is verified methos to the active partition\u2019s boot record is read from the device into RAM and executed. Stage 2 boot loader The second-stage, boot loader called the kernel loader. The first- and second-stage boot loaders combined are calledGRand Unified Bootloader. With stage 2 loaded, GRUB can display a list of available kernels You can select a kernel parameters. 120. Explained about File System Labels? File system labels are useful where you need to address the file system that is on the device. The file system label is set, you can use it when mounting the device. The name replace to device by LABEL=labelname to do this To add a lable on ext3 filesystems # mkfs.ext4 -L mylabel /dev/sda2 To add a lable on exitsting filesystems # tune2fs -L mylabel /dev/sda2 121. To convert ext2 to ext3 filesystem? # tune2fs -j /dev/hda4 => Do it on your own risk. I would recommend to create a new filesystem of ext3 and copy the data from ext2 to ext3. 122. To convert ext3 to ext2 filesystem? # tune2fs -O^has-journal /dev/hda1 => Do it on your own risk. I would recommend to create a new filesystem of ext2 and copy the data from ext3 to ext2. 123. To convert ext2 to ext4 filesystem? # tune2fs -O dir_index,has_journal,uninit_bg /dev/hdXX # e2fsck -pf /dev/hdXX 124. To convert ext3 to ext4 filesystem? umount /dev/sda2 tune2fs -O extents,uninit_bg,dir_index /dev/sda2 e2fsck -pf /dev/sda2 mount /dev/sda2 /home 125. Explained Hash Tables? The bash shell maintains a hash table for each command which has been run. The reason, why it does so is, making the commands run faster. Whenever, a user runs a command on the shell, it first has to search the command executable as to where is it located. whenever the first time bash shell, finds the location of a command executable, it adds it to a hash table. The next time, same command is run, the path is taken from the hash table rather than searched again making the commands run faster. # hash hitscommand 7 /bin/grep 1 /usr/bin/which 1 /usr/bin/touch Reset the hash table # hash -r Delete the corresponding entry # hash -d myprint 126. Following the program will not affected by this shell /sbin/nologin? FTP clients mail clients sudo many setuid programs telnet/login gdm/kdm/xdm (graphical login) su 127. So how do I find out zombie process? # ps aux | awk \u2018{ print $8 \u201d \u201d $2 }\u2019 | grep -w Z Output: Z 4104 Z 5320 Z 2945 128. How do I kill zombie process? ps axo ppid,stat | grep Z | awk \u2018{print $1}\u2019 | xargs kill -HUP kill -HUP $(ps -A -ostat,ppid | grep -e \u2018[zZ]\u2019| awk \u2018{ print $2 }\u2019) kill -9 $(ps -A -ostat,ppid | grep -e \u2018[zZ]\u2019| awk \u2018{ print $2 }\u2019) 129. Details about Backup? \u2022 full \u2013 as the name implies, this is a backup of everything \u2022 differential \u2013 this is a backup of everything since the last full backup \u2022 incremental \u2013 this is a backup of everything since the last _incremental_ backup This command will create a backup of /home and put that in the file /tmp/home.tar # tar -cvf /tmp/home.tar /home Create a backup of the directories /home /var /root and write that to the file /tmp/system-backup.tar # tar -cvf /tmp/system-backup.tar /home /var /root The following command makes a backup of /home and writes that to the /dev/mt0 device # tar -cvf /dev/mt0 /home 130. To create a compressed archive of the directory /home # tar -zcvf /tmp/home.tar.gz /home # tar -jcvf /tmp/home.tar.bz2 /home 131. Extracts the contents of the compressed file # tar -zxvf /file.tar.gz # tar -jxvf /file.tar.bz2 132. To check the contents of a tar file # tar -tvf file.tgz 133. Making Device Backups Using dd # dd if=/etc/hosts of=/home/somefile # dd if=/etc/passwd of=/home/file1 # dd if=/dev/sda of=/dev/sdb bs=4096 # dd if=backup.tar.gz of=/dev/mt0 134. To save MBR file backup as boot files in tmp directory # dd if=/dev/sda of=/tmp/bootfiles bs=512 count=1 135. Determining Filesystem Usage: To determine how much disk space is being used for a given partition, logical volume, or NFS mount, use the df command.To display the output in \u201chuman readable\u201d format, use the -h argument to df. The du command displays the disk usage totals for each subdirectory and finally the total usage for the current directory.Values are in kilobytes. # du -hs /etc # du -h /vol1/group1/examplefile 136. Reporting Disk Performance: For example, if the access time for a drive suddenly drops, an administrator must quickly start troubleshooting the problem to determine if it is a software or hardware issue or simply due to lack of free space on the disk. 137. Displaying Memory Usage with free # free -m The free command tells you about current memory usage. Two types of system memory exist: physical and virtual. To display the amount of free and used memory, both physical and virtual (swap), use the free command 138. Monitoring and Tuning the Kernel: Using the /proc Directory Instead of executing utilities such as free and top to determine the status of system resources or fdisk to view disk partitions, an administrator can gather system information directly from the kernel through the /proc filesystem. When you view the contents of files in /proc, you are really asking the kernel what the current state is for that particular device or subsystem. To view the contents of a special file in /proc, use the cat, less or more file viewing utilities. 139. Network Information Service (NIS) NIS can have only one authoritative server where the original data files are kept This authoritative server is called the master NIS server. If your organization is large enough, you may need to distribute the load across more than one machine. This can be done by setting up one or more secondary (slave) NIS servers. # echo \u201cNISDOMAIN=nis.nightwolf.in\u201d >> /etc/sysconfig/network # ypserv This daemon runs on the NIS server. It listens for queries from clients and responds with answers to those queries. # ypxfrd This daemon is used for propagating and transferring the NIS databases to slave servers. # ypbind This is the client-side component of NIS. It is responsible for finding an NIS server to be queried for information. The ypbind daemon binds NIS clients to an NIS domain. It must be running on any machines running NIS client programs. 140. Main config file for Yum Server in linux # cat /etc/yum.conf [main] cachedir=/var/cache/yum keepcache=0 debuglevel=2 logfile=/var/log/yum.log pkgpolicy=newest distroverpkg=redhat-release tolerant=1 exactarch=1 obsoletes=1 gpgcheck=1 plugins=1 metadata_expire=1800 141. Change User with noLogin Shell: # useradd -s /sbin/nologin nightwolf 142. Add a User with Home Directory, Custom Shell, Custom Comment and UID/GID # useradd -m -d /var/www/nightwolf -s /bin/zsh -c \u201cnightwolf web user\u201d -u 1000 -g 1000 nightwolf 143. Creating a user along with encrypted password in linux Encrypt your password using below command # openssl passwd -crypt myPa55w0rd 4VU5GpOSRWbeo Now you can use the encrypted the password for your new user # useradd -p 4VU5GpOSRWbeo nightwolf 144. Adding Information to User Account # usermod -c \u201cnightwolf user account\u201d nightwolf 145. Change User Home Directory # usermod -d /var/www/ nightwolf 146. Set User Account Expiry Date # usermod -e 2015-03-15 nightwolf 147. Change User Primary Group # usermod -g group_name nightwolf set the group_name group as a primary group to the user nightwolf 148. Adding Group to an Existing User # usermod -G nightwolf_test nightwolf \u2018nightwolf\u2018 user is added to group called \u2018nightwolf_test\u2018 149. Change User Login Name # usermod -l nightwolf_login_name nightwolf 150. Lock User Account # usermod -L nightwolf nightwolf:!$33341$HEWdPIJ.$qX/RbB.TPGcyerAVDlF4g.:12830:0:99999:7::: you will see a ! added before the encrypted password in /etc/shadow file, means password disabled. 151. Unlock User Account # usermod -U nightwolf nightwolf:$33341$HEWdPIJ.$qX/RbB.TPGcyerAVDlF4g.:12830:0:99999:7::: 152. Change User Shell # usermod -s /bin/sh nightwolf 153. Change UID and GID of a User # usermod -u 666 -g 777 nightwolf 154. To check on the status of our RAID device # mdadm \u2013query \u2013detail /dev/md0 # cat /proc/mdstat 155. To create RAID disk # mdadm \u2013create /dev/md0 -l 1 -n 3 /dev/sda{5,6,7} # mdadm \u2013create /dev/md0 -l 1 -n 3 /dev/sda /dev/sdb /dev/sdc -x 2 /dev/sdd 156. To create RAID disk with spare disk # mdadm \u2013create /dev/md0 -l 1 -n 2 /dev/sda{5,6,} -x 1 /dev/sda7 # mdadm \u2013manage /dev/md0 \u2013stop # mdadm \u2013create /dev/md0 -l 5 -n 3 /dev/sda{5,6,7} -x 1 /dev/sda8 x\u2014\u2014\u2014> spare-devices 157. To create LVM on RAID 1 disk # pvcreate /dev/md0 # vgcreate datavg /dev/md0 # lvcreate -L +1G -n /dev/datavg/datalv 158. Disk Failure on RAID To simulate a disk failure, we\u2019ll use mdadm to tell the kernel that /dev/sdb1 has failed # mdadm \u2013manage /dev/md0 \u2013fail /dev/sda7 # cat /proc/mdstat sda7 [F] 159. How do remove failed disk from the RAID array # mdadm \u2013manage /dev/md0 \u2013remove /dev/sda7 160. To add raid device # mdadm \u2013manage /dev/md0 \u2013add /dev/sda9 161. To quickly check the state of all your RAID arrays # cat /proc/mdstat 162. userlist_enable vsftpd Will load a list of usernames from the filename specified by the userlist_file directive when this option is enabled. And if a user tries to log in using a name in this file, that user will be denied access before even being prompted for a password. The default value is NO. 163. userlist_deny This option is examined if the userlist_enable option is active. When its value is set to NO, users will be denied login, unless they are explicitly listed in the file specified by userlist_file. When login is denied, the denial is issued before the user is asked for a password; this helps prevent users from sending clear text across the network. The default value is YES. 163. userlist_file This option specifies the name of the file to be loaded when the userlist_enable option is active. The default value is vsftpd.user_list. 164. download_enable If set to NO, all download requests will be denied permission. The default value is YES. 165. write_enable This option controls whether any FTP commands that change the file system are allowed. These commands are used STOR, DELE, RNFR, RNTO, MKD, RMD, APPE, and SITE. The default value is NO. 166. UserDir This directive defines the subdirectory within each user\u2019s home directory, where users can place personal content that they want to make accessible via the web server. This directory is usually named public_html and is usually stored under each user\u2019s home directory. This option is, of course, dependent on the availability of the mod_userdir module in the web server setup. A sample usage of this option in the httpd.conf file is UserDir disable UserDir public_html 167. ErrorDocument The ErrorDocuments directive lets you specify what happens when a client asks for nonexistent document. Specifies a file that the server sends when an error of a specific type occurs. You can also provide a text message for an error. Here are some examples: ErrorDocument 403 \u201cSorry, you cannot access this directory\u201d ErrorDocument 403 /error/noindex.html ErrorDocument 404 /cgi-bin/bad_link.pl ErrorDocument 401 /new_subscriber.htm \u2022 400: Bad Request \u2022 401: Unauthorized \u2022 402: Payment Required \u2022 403: Forbidden \u2022 404: Not Found \u2022 405: Method Not Allowed \u2022 406: Not Acceptable \u2022 407: Proxy Authentication Required \u2022 408: Request Timeout \u2022 409: Conflict \u2022 410: Gone \u2022 411: Length Required \u2022 412: Precondition Failed \u2022 413: Request Entity Too Large \u2022 414: Request-URI Too Long \u2022 415: Unsupported Media Type \u2022 416: Requested Range Not Satisfiable \u2022 417: Expectation Failed \u2022 500: Internal Server Error \u2022 501: Not Implemented \u2022 502: Bad Gateway \u2022 503: Service Unavailable \u2022 504: Gateway Timeout \u2022 505: HTTP Version Not Supported 168. How to connect to a specific share using smbclient, use the following: # smbclient //<servername>/<sharename> -U <username> # smbclient //192.168.10.10/data -U nightwolf # vim /etc/samba/smb.conf workgroup=WORKGROUP hosts allow = <IP addresses> valid users: List of Samba users allowed access to the share. invalid users: List of Samba users denied access the share. If a user is listed in the valid users and the invalid users list, the user is denied access. public: If set to yes, password authentication is not required. Access is granted through the guest user with guest privileges. (default=no) read only: If set to yes, client users can not create, modify, or delete files in the share.(default=yes) printable: If set to yes, client users can open, write to, and submit spool files on the shared directory (default=no) hosts allow: List of clients allowed access to share. Use the command man 5 hosts_access for details on valid IP address formats. browseable: If set to no, the share will not be visible by a net view or a browse list. 169. Find Files Using Name in Current Directory? # find . -name nightwolf.txt 170. Find Files Under Home Directory? # find /home -name nightwolf.txt 171. Find all PHP Files in Directory? # find . -type f -name \u201c*.php\u201d 172. Find Files Without 777 Permissions? # find / -type f ! -perm 777 173. Find SGID Files with 644 Permissions? # find / -perm 2644 174. Find Sticky Bit Files with 551 Permissions? # find / -perm 1551 175. Find SUID Files? # find / -perm /u=s 176. Find SGID Files? # find / -perm /g=s 177. Find Read Only Files? # find / -perm /u=r 178. Find Executable Files? # find / -perm /a=x 179. Find all Empty Files? # find /tmp -type f -empty 180. Find all Empty Directories? # find /tmp -type d -empty 181. File all Hidden Files? # find /tmp -type f -name \u201c.*\u201d 182. Find Single File Based on User? # find / -user root -name nightwolf.txt 183. Find all Files Based on User? # find /home -user nightwolf 184. Find all Files Based on Group? # find /home -group developer 185. Find Last 50 Days Modified Files? # find / -mtime -50 186. Find Last 50 Days Accessed Files? # find / -atime -50 187. Find Last 50-100 Days Modified Files? # find / -mtime +50 \u2013mtime -100 188. Find Changed Files in Last 1 Hour? # find / -cmin -60 189. Find Modified Files in Last 1 Hour? # find / -mmin -60 190. Find Accessed Files in Last 1 Hour? # find / -amin -60 191. Find 50MB Files? # find / -size 50M 192. Find using inode number? find . -inum 27492358 -exec rm -i {} \\; 193. Main configuration file of Apache server ? /etc/httpd/conf/httpd.conf 194. Main configuration file of Apache server ? /etc/httpd/conf/httpd.conf 195. Specify number of maximum open files in a single login based on the amount of system RAM. # echo \u201c1599383\u201d > /proc/sys/fs/file-max This can also be done by using sysctl sysctl command is used to change Kernel Parameters at run-time # sysctl -w fs.file-max=1599383 Kernel Parameters can also be changed by making changes in the below file:/etc/sysctl.conf Append the below line in the /etc/sysctl.conf file fs.file-max = 1599383 After making the above change run the below command for changes to reflect, loads the sysctl settings # sysctl -p 196. Increase the local port range, by default the port range is small? # echo \u201c1024 65535\u2033 > /proc/sys/net/ipv4/ip_local_port_rangeThis can also be done using sysctl command # sysctl -w net.ipv4.ip_local_port_range=\u201d1024 65535\u201d Append the below line in the /etc/sysctl.conf file net.ipv4.ip_local_port_range = 1024 61000 After making the above change run the below command for changes to reflect, loads the sysctl settings # sysctl \u2013p 197. Disable packet_forwarding(routing)? net.ipv4.ip_forward = 0 # cat /proc/sys/net/ipv4/ip_forward 198. Mount file systems with noatime options? noatime option means it will not update the file and directory access time. Main advantage is I/O performance will increase. 199. Which command is use to extend a logical volume? # lvextend \u2013size +<addsize> /dev/<vgname>/<lvname> # resize2fs /dev/<vgname>/<lvname> # lvextend -L +1G /dev/VolGroup/LogVol1 This will extend the partition size by +1 GB # resize2fs /dev/VolGroup/LogVol1 200. ServerAdmin : Email address This is the e-mail address that the server includes in error messages sent to the client. Defines the e-mail address that is shown when the server generates an error page. The e-mail address that the Web server provides to clients in case any errors occur. 201. What is the use of SCP command in Linux? SCP command stands for secure copy. It is used to copy/download data from one machine to another machine. 202. What is telnet and what does it do? telnet command is used to check the connectivity to other servers. It helps you to check whether you are able to talk to another server or now. Ex: telnet 192.0.0.1 22 where 22 is the port number. 203. What is a bastion host? A bastion host is also known as a jump server. It is used to connect from one machine to another machine securely. Bastion hosts are used to connecting to private servers securely. 204. What is the command to find the IP address of the host machine in linux? You can use ifconfig/ ipaddr show command to find the IP address of the host machine. 205. Name some of the text editors that are available in Linux? Some of the common text editors that are available in Linux are vi/vim, nano, subl, gedit, atom, emacs. Vi is the default editor that you have in Linux machines. 206. What are the different zip files formats that are available in linux? The different zip formats in Linux are zip, gzip and bzip. 207. What is the difference between cp and mv command? cp command stands for copy and is used to copy data from one location to another. mv stands for the move and is used to move data from one location to another. 208. How can you run a process in the background in Linux? You can run a process in the background by pressing ctrl+z command. 209. What is the use of \u2018chown\u2019 command ? chown stands for \u2018change ownership\u2019 and is used to change the ownership of a file or directory. Eg: chown username.username <filename>. 210. What is the use of \u2018chmod\u2019 command? chmod stands for \u2018change mode\u2019 and is used to change the permissions on files or directories. Eg: # chmod a+w <filename> 211. What is the command to create a zip file in linux? To create a zip file you can use tar command with -cvzf arguments. Eg: tar -cvzf test.tar.gz <file names to be included in the zip> 212. What is the command to unzip the file in linux? To unzip a file you can tar command with -xvzf arguments. Eg: tar -xvzf test.tar.gz 213. What is the command to show the contents of a zip file ? To see the contents of the zip file you can use tar -tvzf arguments. Eg: tar -tvzf test.tar.gz 214. What is a soft link in Linux? A soft link is used to create a shortcut in Linux. This is similar to creating a shortcut in windows systems. 215. What is the command used to create a soft link? To create a soft link you can use ln command with -s arguments. Eg: ln -s /var/www/html html, where /var/www/html is the source file and HTML is the destination of the shortcut. 216. What is the command to remove the soft link in Linux? To remove the soft link in Linux you can use unlink command. Eg: unlink <filename> 217. What is the use of whereis command in linux? Whereis command is used to find the binaries and libraries files of an application in linux. 218. What is the use of man pages? Man pages stand for manual pages. It is the documentation about and helps you to understand the commands and how to use the commands. Eg: man wget. 219. what does \u201c2>\u201d indicate in redirection? This means that output will be shown on the screen and the errors will be written to a file that you specify. Eg: # ls /etc/test 2> error.txt 220. What are the different type of users that you have in Linux? You have 2 types of users in linux. They are \u2022 root user \u2022 standard users. 221. How To check Memory stats and CPU stats ? free & vmstat commands. 222. What is the purpose of runlevels ? Useful for debugging purpose. Basic idea is each runlevel has some services operational and depending on need can enable different runlevels to test which services are running 223. You are able to ping with a numeric IP address, but not by name. How will you debug ? First check /etc/resolv.conf file for DNS Server Entry 224. Generally setting up services in Linux require \u2014\u2014\u2014\u2014\u2014 and \u2014\u2014\u2014\u2014\u2013 Update the associated configuration file and ensure the appropriate daemon is started 225. What is the purpose of iptables command ? Basically allows rule creation to filter packets according to established criteria. Network Address Translation function is also done 226. You are noticing mails are not sent by sendmail. Where can you find the error log to see what happened ? /var/spool/mail/ 227. How can you findout the current runlevel the system is in ? # who -r 228. Linux system has crashed and keeps getting to Debug Prompt. How do you bring it to normal login prompt. Likely due to file system inconsistency, run fsck to check and accept inode repairs. 229. How can you customise startup settings for your login in bash shell You can use file .bashrc for this customisation. 230. What is the first process started by the Schedulerin RHEL 6. init 231. How can you find the current status of Virtual memory in the Linux System ? # cat /proc/meminfo * Note the very useful /proc filesystem 232. Hardware devices are identified as special files in Linux. Name the types. Character,block and Network 233. Name 3 Environment variables SHELL,HOME,PATH 234. Where is my vmlinuz executable loaded from ? /boot 235. As System Administrator you have to apply a patch that is in .tar.gz format. How would you use it ? Get the file and apply tar -zxvf <filename.tar.gz> 236. What are Kernel types and which one is Linux ? Monolithic and Micro. Linux is a Monolithic Kernel. 237. As System Admin, log files are monitored as they grow. How is this achieved ? tail -f 238. Automatic mounting of new file system at boot-time can be done by Adding an entry in /etc/fstab file 239. You recently ran an install, the command for which you need to recall. How do you get this? # history 240. In writing a Bash Shell script, special character\u2019 meaning has to be altered. How is it done? Escape sequence. Precede the special character with a \u2018\\\u2019 241. Linux associates devices with File Descriptors. List them. 0 \u2013 Standard Input 1- Standard Output 2-Standard Error 242. How do you set a mask to stop certain permissions from being granted by default on file creation? # umask <value> 243. You want to try out a Distribution before installation. Which image would suit? LiveCD 244. System Administrators monitor load averages on System for analysis. How is this done? # top # uptime # w 245. What is the behaviour of the following very useful grep command ? # grep [abc] file1 : Looks for matches in file1 containing either an a or b or c character. 246. You need to make a file with only read permissions for yourself, group and all. How can you ? #chmod 444 file1 247. What does su \u2013 do ? Give an example of when you would use this. Gets the switch to root account. Need to be done before Software installation. 248. Pipe is a very important concept in process communication. Give an example # cat file1 |grep xyz : Pattern matching by grep happens on the displayed file1 249. In Shell scripting command return codes are checked prior to proceeding. Explain These are Exit status codes. Linux follows 0 for success and nonzero codes for failures 250. Signals is one way that process communication happens in Linux. What does ctrl C do ? Generates a SIGINT signal that stops the current process running in the shell. 251. To set a password to the boot loader: # grub grub> md5crypt Password: ************ Encrypted: $1$3yQFp$MEDEglsxOvuTWzWaztRly. grub> quit Next, add this to your grub.conf file like so: default=1 timeout=10 splashimage=(hd0,0)/grub/splash.xpm.gz password \u2013md5 $1$3yQFp$MEDEglsxOvuTWzWaztRly. Next Page You may also refer to: Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"Linux Interview Questions for Freshers"},{"location":"nightwolf-cotribution/linux_basic/#linux-interview-questions-and-answers-for-freshers-solved","text":"We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux Admin, Amazon interview questions for Linux admin and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. 1.What is two types of Linux User Mode ? Command Line GUI 2.What is command for created multiple files at a time? touch 3.What is INODE and How to Identify? Its unique identification code for files and directories, its was generate automatically while create new file and directories ls -i filename ls -ldi directoryname 4.List of Permissions and Users Read, Write and Execute Owner, Group Owners and Others 5.List of Special Permissions and numerical value. Set User ID = 4 Set Group ID = 2 Stickybit = 1 6. What command to use see Process list in Hierarchical Structure along with PID? pstree -P 7. What is use of \u201ctop\u201d command and how to sort Memory and User wise? Its used to real time monitor hardware utilization of linux machine. Press M to sort Memory wise result Press U to sort User wise result 8. What is command for to force close one particular process kill -9 Processid 9.What is command to refresh NIC ? Service network restart 10.Tell me two types of IP Address configuration Static IP Address Dynamic IP Address 11.How do Enable / Disable Ethernet Device Open and Edit this file #vi /etc/sysconfig/network-scripts/devicename For enable ONBOOT = yes For disable ONBOOT =no 12.What is command to change Hostname without System Restart hostname newhostname hostnamectl 13.What is File Path of Network Configuration ? /etc/sysconfig/network-scripts 14.What is File Path of DNS Configuration ? /etc/resolv.conf 15.How to Update locate DB ? cd/var/lib/mlocate updatedb 16. How to edit and save file using editors? The following commands are used to exit from vi editors. 1. :wq saves the current work and exits the VI. 2. :q! exits the VI without saving current work. 17.What is command for Zip and Unzip files 1. gzip = Compress File 2. gunzip = Uncompress File 18.What is file path of Alias name set by Permanent? /etc/bashrc 19.What is MBR in linux? Its Master Boot Recorder to help booting operating system. 20.What is Two Types of Mount in linux? Temporary Mount Permanent Mount 21.What is command for delete Partition? #umount #palimpsest & OR #parted OR #fdisk 22.What is command for Refresh Partition? mount -a 23.What is SWAP? Linux uses swap space to increase the amount of virtual memory available to a host. It can use one or more dedicated swap partitions or a swap file on a regular filesystem or logical volume. 24.What are types can set SWAP? Temporary set Permanent set 25. What command use for Filesystem Error checking and Error Fixing fsck and e2fsck 26. What is PV, VG, and LVM PV = Physical Volume VG = Volume Group LVM = Logical Volume 27. What is LVM LVM is used to create logical partitions and during run time we can resize particular partition without data loss. Empty partitions only can do LVM creation. 28. What are common commands used for Physical Volume pvcreate pvs pvdisplay 29. What command is used for create Volume Group vgcreate vgs 30. What is Syntax for LVM Create? #lvcreate -L partitionsize -n userdefinename volumegroupname 31. What types of Installation Tools in REDHAT? RPM = Redhat Package Manager YUM = Yellow Dog Updated Modifier 32. Tell me Linux Boot Sequence Floow? BIOS \u2192 MBR \u2192 Boot Loader \u2192 Kernal \u2192 Runlevel 33. Types of Zone in DNS? Forward lookup zone Reverse lookup zone 34. What are inbuilt firwall in Linux ? IP Tables Selinux TCPwrappers 35. What command to Execute disable IPTables permanently? #iptables -F service iptables save 36. What is SELinux? Its one type of firewall in linux To block particular service in a Protocol 37. File to disable SELinux permanently: /etc/selinux/config 38. What is command to check selinux status ? getenforce 39. What is LDAP The Lightweight Directory Access Protocol (LDAP) is a set of open protocols used to access centrally stored information over a network. 40. Which Configuration File Is Required For Ldap Clients? ldap.conf 41. What Is The Name Of Main Configuration File Name For Ldap Server? slapd.conf 42. How Will You Verify Ldap Configuration File? slaptest -u 43. What is command package install using YUM without ask Prompt? yum install packagename -y 44. What is command Uninstall package? yum remove packagename 45. What is command package re-install using YUM without ask Prompt? yum reinstall packagename -y 46. Location of Cron file in linux? /var/spool/cron 47. What is command for to see Particular user Job Schedule ? crontab -lu username 48. What is command for restart cron service? service crond restart 49. What is command for restart postfix service? service postfix restart 50.What is command for FTP service on and restart? chkconfig vsftpd on service vsftpd restart 50. What is Kernel un Unix Operating system? Kernel is the heart of operating system. It interacts with shell and executes the machine level language. 51. How can I save my input and output commands and see them when required? At the beginning of the session if you use 'script' command then the details of the input and output commands will be saved in a file called typescript and we can view it any time using \u201ccat typescript\u201d command. This is very useful to track what user is doing what. HISTORY command will not work because it shows data only for the current session. 52. How to create a file in Unix? There are multiple way to create files in unix, but the simple way to create a file is using \u201ccat\u201d and \u201cTouch\u201d command Syntax: cat > File name touch file name 53. How can I check which processes are running in my machine? To check process which are running in my machine I can use two commands. (a) TOP and (b) PS 54. What is the difference between TOP and PS command? Top command gives the dynamic view of the processes are running in the server and generally the dynamic change happens in every 3 second. Whereas PS commands gives the static view of the processes. 55. You used TOP command and without aborting the TOP process I need to kill one process. Is it possible to kill? Yes TOP command it self has a command prompt. Type K then it will ask you for the PID of the process to kill. Hit the PID and enter, it will kill the process. 56. What is the difference between creating a file in cat and in touch command? cat command creates a file and we can save some data inside the file but touch command by default will create a blank file. 57. How can I create multiple directories at a time? Say I want to create a directory D1 and inside that D2 and inside that D3. Is it possible? If yes how ? Yes, creating multiple directories is possible. In this scenario the below command works. #mkdir \u2013p D1/D2/D3 58. I want to create D1, under that D2 and D3. Inside D2 I want D4 and inside D3 I want D5 to be created. How is it possible? The below command will work for it. #mkdir \u2013p D1/D2/D4 D1/D3/D5 59. How can I check in which directory I am in ? Use PWD command to check which directory you are in. 60. We are using so many commands and getting output. Have you ever wondered how the commands are executing and getting you the output? Yes, every command in Unix is a C program in the backend. When we type a command and hit enter the program runs in the backend and gives you the output. We can view the C program as well as below. type <Command Name> ->hit enter, it will give you a path where the program the command is located. You can view the program by doing cat and the path name. it will open a C program file in decrypted mode. 61. How can I list the directories and the files ? Using ls command. I can view the directories and files of the system. 62. How can I view hidden files in a system ? Using ls \u2013a command I can view the hidden files of the sytem 63. In real time environment many people use \u201cll\u201d command instead of ls. So is there any command called \u201cll\u201d exits? No, there is no such command called \u201cll\u201d. It\u2019s just the alias of ls command. We can check it by typing alias command. 64. What is a shell ? Description of shell is huge, but yes commonly we explain it as the interpreter between the user and the machine. 65. Describe the usage of rm \u2013r* command in unix and shall we use it in real time environment? rm \u2013r* will remove all the file entries in the current directory. It is not advisable to use this command in real time environment. Specifically in production. Because we have huge files which are necessary to be accessed by other users. 66. What is symbolic link ? The second name of a file is called a link, it\u2019s assigned to create another link to the current file. 67. What is absolute path and relative path in unix ? Absolute path refers to the path starting from the root directory and the path continues with a sequence starting from Root. Whereas relative path is the current path. 68. How can I check the system IP ? type hostname command or else you can use ifconfig as well. 69. How can I check if a server is up and running or not ? you can use ping \u2013t command for this. Ping \u2013t <hostname> or <IP address> 70. How can I append some lines in an existing file ? #cat >> file name and hit enter. You can append lines below the existing lines of the file. And do a ctrl D to save and exit. 71. What is FIFO and LIFO in unix ? FIFO is first in first out and LIFO is last in last out. 72. What is PATH variable ? PATH is an environmental variable which contains the path of the command files and we can change the paths inside the PATH variable. 73. How can I kill a process in unix? first use PS \u2013ef command and get the PID of the process you want to kill. Then use kill -9 <PID_Number> command to kill the process. 74. How can I check the memory size of a linux/unix machine ? Use Free \u2013m or free \u2013G command to check the memory size of a linux machine. 75. How to check disk utilization of a linux server? Use du command to check the disk utilization. 76. How to check the disk free of all the mount points in unix ? use df \u2013h command, it will show the disk free of linux machine. 77. How can I check who are the users logged in my system? use users command. It will how the details of the users logged in to the system. 78. I have a file Mantu.txt which contains multiple lines and few of the lines has a particular pattern as \u201cIndia\u201d. I want to print only those lines. How can I ? I will ue grep command here. And the syntax will be as below. #grep -i \u201cIndia\u201d Mantu.txt grep command is used for pattern earching. 79. What do you mean by a super user ? A admin while giving permission to the users usually give normal access permission but few of the user having special permission then normal user, they are called super user. 80. What is the syntax to move to a super user? sudo su \u2013 <user name> 81. How can I change the permission of a file? Using chmod command I can change the permissions of a file. 82. How can I give all permission to a user? Use the below command to give all read, write and execute permission. chmod 777 <file name> 83. What is a process group in unix ? Collection of more than one process is called as a process group in unix.the function getpgrp returns the process group id. 84. How many numbers are used with kill while killing a process ? There are 64 numbers which can be used with kill command but generaly we use kill -9 85. What are different types of files available in unix? There are multiple type of files available in unix, few of among them are : \u2022 Regular file \u2022 Image file \u2022 Binary file \u2022 Linked file 86. What are cmp and different command in unix? cmp command compares the two files byte by byte and gives the output what is not common in between them. Diff command through the output which is not matching between the two file immediately rather comparing bit by bit. 87. What is pipe command ? why it is used for ? Pipe symbol interlinks two commands. It stores the output of the first command and give it to the second command as input. #cat emp.lst | mantu.txt 88. How can I number the lines of a file in VI editor? Open the file using vi <filename> Then go to command prompt and type set number. The numbers will be set before every line of the file. 89. What is the command to check all the options and detail information of a command in unix ? We can use man <command name>. it will show you all the possible way to use the command. 90. What is head command used for ? head command is used to view the top portions of the file. Say if you want to view top 5 lines of a file then you can use the below command. #cat <filename> | head -5 91. What is tail command used for ? tail command is used to view the bottom of the lines of a file. Say if I want to view bottom 5 lines of a file then I can use the below command. #cat <filename> | tail -5 92. What are the other commands used for pattern searching ? grep, awk and sed are the main command used for pattern searching. 93. How can I search a pattern in vi editor ? Open the file with vi. Use /pattern name , then hit enter, it will show you the matching patterns in VI. 94. How can I delete one line in Vi editor ? Use dd in command mode to delete one line of a file in vi. 95. What is the command used for copying a file? Use cp command while copying a file in unix. #cp <sourcepath of the file> <destination path of the file> 96. What is SCP in unix ? scp stands for secure copy in unix. The files which get copied by using scp command are decrypted so we need not be worry of hacking of the file system. 97. What is mv command in unix? We can move a file or rename a file using this command. General purpose of using mv command is to use it for reaming purpose. 98. What does a touch command do apart from creating a blank file? touch command is used to change the access and modification time of the file. 99. Explain the advantages of executing a process in background We use \u201c&\u201d symbol to execute a job in back ground. When we execute a job or process in unix it starts executing in the prompt itself and we can\u2019t do other stuffs in the command prompt at that time. So until unless the process gets executed we have to seat idle. So for continuous interaction with the command prompt we prefer executing the jobs or processes in back ground. 100. How do you protect file deletion in ext4? You change any attributes of the file to read only. The command is: #chattr +i filename And to disable it: #chattr -i filename 101. How do you list the kerenel modules which is already loaded ? List Currently Loaded Modules \u2013 lsmod List Available Kernel Modules \u2013 modprobe -l Install New modules into Linux Kernel \u2013 modprobe vmhghs Remove the Currently Loaded Modul \u2013 modprobe -r vmhghs 102. What will happen in chkconfig? issuing the command \u201cchkconfig sendmail on\u201d will create symlinks(softlinks) /etc/rd1.d/K30sendmail /etc/rd2.d/S80sendmail /etc/rd3.d/S80sendmail /etc/rd4.d/S80sendmail /etc/rd5.d/S80sendmail /etc/rd6.d/K30sendmail 103. How To rebuild Corrupted RPM Database ? [root@tecmint]# cd /var/lib [root@tecmint]# rm __db* [root@tecmint]# rpm \u2013rebuilddb [root@tecmint]# rpmdb_verify Packages 104. what resize2fs do at back end? Mounted, Extending The kernel then begins writing additional filesystem metadata on the newly available storage. Unmounted,Shrinking resize2fs makes the filesystem use only the first size bytes of the storage. It does this by moving both filesystem metadata and your data around. After the completes, there will be unused storage at the end of the block device, unused by the filesystem. 105. Special Permissions in linux Sticky bit \u2013 Only created user and root can able to delete the file #chmod o+t nightwolf.txt #chmod +t nightwolf.txt #chmod 1777 nightwolf.txt #ls -ld nightwolf.txt drwxrwxrwt 2 root root 4096 Mar 24 12:19 nightwolf.txt SUID \u2013 Ging permission for all users like root chmod u+s /bin/ls \u2013 ls can be used for all users as like root # chmod 4555 [path_to_file] #ls -l /bin/ls -rwsr-xr-x-x 1 root user 16384 Jan 12 2014 /bin/ls SGID \u2013 SGUID :- chmod g+s /dir \u2013> all subdirectories and files created inside will get same group ownership as the main directory, it doesn\u2019t matter who is creating. #chmod 2555 [dir] #ls -l /usr/bin/write -r-xr-sr-x 1 root tty 11484 Jan 15 17:55 /usr/bin/write 106. Password never expire linux? # chage -M -1 nightwolf \u2013> set the max passwd age to -1 # passwd -x -1 nightwolf # chage -m 0 -M 99999 -I -1 -E -1 nightwolf 107. What files are created/modified when adding a user (useradd) in linux? /etc/passwd and /etc/shadow files from /etc/skel are typically copied into the new user\u2019s home directory 108. How to see and get info about RAM in your system # free # cat /proc/meminfo 109. How will you suspend a running process and put it in the background? Ctrl+z 110. Name the Daemon responsible for tracking System Event on your Linux box? Syslogd 111. To see tar file without extracting? tar -tvf 112. How to check dependencies of RPM Package on before Installing ? # rpm -qpR BitTorrent-5.2.2-1-Python2.4.noarch.rpm /usr/bin/python2.4 python >= 2.3 python(abi) = 2.4 python-crypto >= 2.0 python-psyco python-twisted >= 2.0 python-zopeinterface rpmlib(CompressedFileNames) = 2.6 q : Query a package p : List capabilities this package provides. R: List capabilities on which this package depends. 113. How can we increase disk read performance in single commands? To see the current read performance, # blockdev \u2013getra /dev/sdb 256 # time dd if=/tmp/disk.iso of=/dev/null bs=256k 2549+1 records in 2549+1 records out copied, 6,84256 seconds, 97,7 MB/s real 0m6.845s user 0m0.004s sys 0m0.865s # After test # blockdev \u2013setra 1024 /dev/sdb # time dd if=/tmp/disk.iso of=/dev/null bs=256k 2435+1 records in 2435+1 records out copied, 0,364251 seconds, 1,8 GB/s real 0m0.370s user 0m0.001s sys 0m0.370s 114. How Many Run Levels present in Linux? There are seven run levels, with each having its own properties. \u2022 Halt the system \u2022 Single-user mode \u2022 Multiuser mode without networking(NFS) \u2022 Multi-user mode with text login \u2022 Not used \u2022 Multi-user mode with graphical login \u2022 Reboot 115. How do i check which NFS version ? rpcinfo -p localhost | grep -i nfs rpm -qa | grep nfs rpm -qi nfs nfs-utils 116. Use find command to delete file by inode? Find and remove file using find command follows: # find . -inum 782263 -exec rm -i {} \\; 117. Check if any user is using the file system? Check to the what users are currently using the file system: # fuser -cu /dev/hdc1 /opt/backup: 2337c(root) 118. Explain ntsysv or chkconfig command Both are similar want all services to start in different runlevel # ntsysv \u2013level <level> # chkconfig \u2013list <service name> # chkconfig <service name> on # chkconfig <service name> \u2013level 3 119. Explained BOOT LOADER? The boot loader is then responsible for loading the kernel. A boot loader finds the kernel image on the disk, loads it into memory, starts it. Stage 1 boot loader: First stage the primary boot loader is to find and load the secondary boot loader. It will find by looking through the partition table for an active partition. This is verified methos to the active partition\u2019s boot record is read from the device into RAM and executed. Stage 2 boot loader The second-stage, boot loader called the kernel loader. The first- and second-stage boot loaders combined are calledGRand Unified Bootloader. With stage 2 loaded, GRUB can display a list of available kernels You can select a kernel parameters. 120. Explained about File System Labels? File system labels are useful where you need to address the file system that is on the device. The file system label is set, you can use it when mounting the device. The name replace to device by LABEL=labelname to do this To add a lable on ext3 filesystems # mkfs.ext4 -L mylabel /dev/sda2 To add a lable on exitsting filesystems # tune2fs -L mylabel /dev/sda2 121. To convert ext2 to ext3 filesystem? # tune2fs -j /dev/hda4 => Do it on your own risk. I would recommend to create a new filesystem of ext3 and copy the data from ext2 to ext3. 122. To convert ext3 to ext2 filesystem? # tune2fs -O^has-journal /dev/hda1 => Do it on your own risk. I would recommend to create a new filesystem of ext2 and copy the data from ext3 to ext2. 123. To convert ext2 to ext4 filesystem? # tune2fs -O dir_index,has_journal,uninit_bg /dev/hdXX # e2fsck -pf /dev/hdXX 124. To convert ext3 to ext4 filesystem? umount /dev/sda2 tune2fs -O extents,uninit_bg,dir_index /dev/sda2 e2fsck -pf /dev/sda2 mount /dev/sda2 /home 125. Explained Hash Tables? The bash shell maintains a hash table for each command which has been run. The reason, why it does so is, making the commands run faster. Whenever, a user runs a command on the shell, it first has to search the command executable as to where is it located. whenever the first time bash shell, finds the location of a command executable, it adds it to a hash table. The next time, same command is run, the path is taken from the hash table rather than searched again making the commands run faster. # hash hitscommand 7 /bin/grep 1 /usr/bin/which 1 /usr/bin/touch Reset the hash table # hash -r Delete the corresponding entry # hash -d myprint 126. Following the program will not affected by this shell /sbin/nologin? FTP clients mail clients sudo many setuid programs telnet/login gdm/kdm/xdm (graphical login) su 127. So how do I find out zombie process? # ps aux | awk \u2018{ print $8 \u201d \u201d $2 }\u2019 | grep -w Z Output: Z 4104 Z 5320 Z 2945 128. How do I kill zombie process? ps axo ppid,stat | grep Z | awk \u2018{print $1}\u2019 | xargs kill -HUP kill -HUP $(ps -A -ostat,ppid | grep -e \u2018[zZ]\u2019| awk \u2018{ print $2 }\u2019) kill -9 $(ps -A -ostat,ppid | grep -e \u2018[zZ]\u2019| awk \u2018{ print $2 }\u2019) 129. Details about Backup? \u2022 full \u2013 as the name implies, this is a backup of everything \u2022 differential \u2013 this is a backup of everything since the last full backup \u2022 incremental \u2013 this is a backup of everything since the last _incremental_ backup This command will create a backup of /home and put that in the file /tmp/home.tar # tar -cvf /tmp/home.tar /home Create a backup of the directories /home /var /root and write that to the file /tmp/system-backup.tar # tar -cvf /tmp/system-backup.tar /home /var /root The following command makes a backup of /home and writes that to the /dev/mt0 device # tar -cvf /dev/mt0 /home 130. To create a compressed archive of the directory /home # tar -zcvf /tmp/home.tar.gz /home # tar -jcvf /tmp/home.tar.bz2 /home 131. Extracts the contents of the compressed file # tar -zxvf /file.tar.gz # tar -jxvf /file.tar.bz2 132. To check the contents of a tar file # tar -tvf file.tgz 133. Making Device Backups Using dd # dd if=/etc/hosts of=/home/somefile # dd if=/etc/passwd of=/home/file1 # dd if=/dev/sda of=/dev/sdb bs=4096 # dd if=backup.tar.gz of=/dev/mt0 134. To save MBR file backup as boot files in tmp directory # dd if=/dev/sda of=/tmp/bootfiles bs=512 count=1 135. Determining Filesystem Usage: To determine how much disk space is being used for a given partition, logical volume, or NFS mount, use the df command.To display the output in \u201chuman readable\u201d format, use the -h argument to df. The du command displays the disk usage totals for each subdirectory and finally the total usage for the current directory.Values are in kilobytes. # du -hs /etc # du -h /vol1/group1/examplefile 136. Reporting Disk Performance: For example, if the access time for a drive suddenly drops, an administrator must quickly start troubleshooting the problem to determine if it is a software or hardware issue or simply due to lack of free space on the disk. 137. Displaying Memory Usage with free # free -m The free command tells you about current memory usage. Two types of system memory exist: physical and virtual. To display the amount of free and used memory, both physical and virtual (swap), use the free command 138. Monitoring and Tuning the Kernel: Using the /proc Directory Instead of executing utilities such as free and top to determine the status of system resources or fdisk to view disk partitions, an administrator can gather system information directly from the kernel through the /proc filesystem. When you view the contents of files in /proc, you are really asking the kernel what the current state is for that particular device or subsystem. To view the contents of a special file in /proc, use the cat, less or more file viewing utilities. 139. Network Information Service (NIS) NIS can have only one authoritative server where the original data files are kept This authoritative server is called the master NIS server. If your organization is large enough, you may need to distribute the load across more than one machine. This can be done by setting up one or more secondary (slave) NIS servers. # echo \u201cNISDOMAIN=nis.nightwolf.in\u201d >> /etc/sysconfig/network # ypserv This daemon runs on the NIS server. It listens for queries from clients and responds with answers to those queries. # ypxfrd This daemon is used for propagating and transferring the NIS databases to slave servers. # ypbind This is the client-side component of NIS. It is responsible for finding an NIS server to be queried for information. The ypbind daemon binds NIS clients to an NIS domain. It must be running on any machines running NIS client programs. 140. Main config file for Yum Server in linux # cat /etc/yum.conf [main] cachedir=/var/cache/yum keepcache=0 debuglevel=2 logfile=/var/log/yum.log pkgpolicy=newest distroverpkg=redhat-release tolerant=1 exactarch=1 obsoletes=1 gpgcheck=1 plugins=1 metadata_expire=1800 141. Change User with noLogin Shell: # useradd -s /sbin/nologin nightwolf 142. Add a User with Home Directory, Custom Shell, Custom Comment and UID/GID # useradd -m -d /var/www/nightwolf -s /bin/zsh -c \u201cnightwolf web user\u201d -u 1000 -g 1000 nightwolf 143. Creating a user along with encrypted password in linux Encrypt your password using below command # openssl passwd -crypt myPa55w0rd 4VU5GpOSRWbeo Now you can use the encrypted the password for your new user # useradd -p 4VU5GpOSRWbeo nightwolf 144. Adding Information to User Account # usermod -c \u201cnightwolf user account\u201d nightwolf 145. Change User Home Directory # usermod -d /var/www/ nightwolf 146. Set User Account Expiry Date # usermod -e 2015-03-15 nightwolf 147. Change User Primary Group # usermod -g group_name nightwolf set the group_name group as a primary group to the user nightwolf 148. Adding Group to an Existing User # usermod -G nightwolf_test nightwolf \u2018nightwolf\u2018 user is added to group called \u2018nightwolf_test\u2018 149. Change User Login Name # usermod -l nightwolf_login_name nightwolf 150. Lock User Account # usermod -L nightwolf nightwolf:!$33341$HEWdPIJ.$qX/RbB.TPGcyerAVDlF4g.:12830:0:99999:7::: you will see a ! added before the encrypted password in /etc/shadow file, means password disabled. 151. Unlock User Account # usermod -U nightwolf nightwolf:$33341$HEWdPIJ.$qX/RbB.TPGcyerAVDlF4g.:12830:0:99999:7::: 152. Change User Shell # usermod -s /bin/sh nightwolf 153. Change UID and GID of a User # usermod -u 666 -g 777 nightwolf 154. To check on the status of our RAID device # mdadm \u2013query \u2013detail /dev/md0 # cat /proc/mdstat 155. To create RAID disk # mdadm \u2013create /dev/md0 -l 1 -n 3 /dev/sda{5,6,7} # mdadm \u2013create /dev/md0 -l 1 -n 3 /dev/sda /dev/sdb /dev/sdc -x 2 /dev/sdd 156. To create RAID disk with spare disk # mdadm \u2013create /dev/md0 -l 1 -n 2 /dev/sda{5,6,} -x 1 /dev/sda7 # mdadm \u2013manage /dev/md0 \u2013stop # mdadm \u2013create /dev/md0 -l 5 -n 3 /dev/sda{5,6,7} -x 1 /dev/sda8 x\u2014\u2014\u2014> spare-devices 157. To create LVM on RAID 1 disk # pvcreate /dev/md0 # vgcreate datavg /dev/md0 # lvcreate -L +1G -n /dev/datavg/datalv 158. Disk Failure on RAID To simulate a disk failure, we\u2019ll use mdadm to tell the kernel that /dev/sdb1 has failed # mdadm \u2013manage /dev/md0 \u2013fail /dev/sda7 # cat /proc/mdstat sda7 [F] 159. How do remove failed disk from the RAID array # mdadm \u2013manage /dev/md0 \u2013remove /dev/sda7 160. To add raid device # mdadm \u2013manage /dev/md0 \u2013add /dev/sda9 161. To quickly check the state of all your RAID arrays # cat /proc/mdstat 162. userlist_enable vsftpd Will load a list of usernames from the filename specified by the userlist_file directive when this option is enabled. And if a user tries to log in using a name in this file, that user will be denied access before even being prompted for a password. The default value is NO. 163. userlist_deny This option is examined if the userlist_enable option is active. When its value is set to NO, users will be denied login, unless they are explicitly listed in the file specified by userlist_file. When login is denied, the denial is issued before the user is asked for a password; this helps prevent users from sending clear text across the network. The default value is YES. 163. userlist_file This option specifies the name of the file to be loaded when the userlist_enable option is active. The default value is vsftpd.user_list. 164. download_enable If set to NO, all download requests will be denied permission. The default value is YES. 165. write_enable This option controls whether any FTP commands that change the file system are allowed. These commands are used STOR, DELE, RNFR, RNTO, MKD, RMD, APPE, and SITE. The default value is NO. 166. UserDir This directive defines the subdirectory within each user\u2019s home directory, where users can place personal content that they want to make accessible via the web server. This directory is usually named public_html and is usually stored under each user\u2019s home directory. This option is, of course, dependent on the availability of the mod_userdir module in the web server setup. A sample usage of this option in the httpd.conf file is UserDir disable UserDir public_html 167. ErrorDocument The ErrorDocuments directive lets you specify what happens when a client asks for nonexistent document. Specifies a file that the server sends when an error of a specific type occurs. You can also provide a text message for an error. Here are some examples: ErrorDocument 403 \u201cSorry, you cannot access this directory\u201d ErrorDocument 403 /error/noindex.html ErrorDocument 404 /cgi-bin/bad_link.pl ErrorDocument 401 /new_subscriber.htm \u2022 400: Bad Request \u2022 401: Unauthorized \u2022 402: Payment Required \u2022 403: Forbidden \u2022 404: Not Found \u2022 405: Method Not Allowed \u2022 406: Not Acceptable \u2022 407: Proxy Authentication Required \u2022 408: Request Timeout \u2022 409: Conflict \u2022 410: Gone \u2022 411: Length Required \u2022 412: Precondition Failed \u2022 413: Request Entity Too Large \u2022 414: Request-URI Too Long \u2022 415: Unsupported Media Type \u2022 416: Requested Range Not Satisfiable \u2022 417: Expectation Failed \u2022 500: Internal Server Error \u2022 501: Not Implemented \u2022 502: Bad Gateway \u2022 503: Service Unavailable \u2022 504: Gateway Timeout \u2022 505: HTTP Version Not Supported 168. How to connect to a specific share using smbclient, use the following: # smbclient //<servername>/<sharename> -U <username> # smbclient //192.168.10.10/data -U nightwolf # vim /etc/samba/smb.conf workgroup=WORKGROUP hosts allow = <IP addresses> valid users: List of Samba users allowed access to the share. invalid users: List of Samba users denied access the share. If a user is listed in the valid users and the invalid users list, the user is denied access. public: If set to yes, password authentication is not required. Access is granted through the guest user with guest privileges. (default=no) read only: If set to yes, client users can not create, modify, or delete files in the share.(default=yes) printable: If set to yes, client users can open, write to, and submit spool files on the shared directory (default=no) hosts allow: List of clients allowed access to share. Use the command man 5 hosts_access for details on valid IP address formats. browseable: If set to no, the share will not be visible by a net view or a browse list. 169. Find Files Using Name in Current Directory? # find . -name nightwolf.txt 170. Find Files Under Home Directory? # find /home -name nightwolf.txt 171. Find all PHP Files in Directory? # find . -type f -name \u201c*.php\u201d 172. Find Files Without 777 Permissions? # find / -type f ! -perm 777 173. Find SGID Files with 644 Permissions? # find / -perm 2644 174. Find Sticky Bit Files with 551 Permissions? # find / -perm 1551 175. Find SUID Files? # find / -perm /u=s 176. Find SGID Files? # find / -perm /g=s 177. Find Read Only Files? # find / -perm /u=r 178. Find Executable Files? # find / -perm /a=x 179. Find all Empty Files? # find /tmp -type f -empty 180. Find all Empty Directories? # find /tmp -type d -empty 181. File all Hidden Files? # find /tmp -type f -name \u201c.*\u201d 182. Find Single File Based on User? # find / -user root -name nightwolf.txt 183. Find all Files Based on User? # find /home -user nightwolf 184. Find all Files Based on Group? # find /home -group developer 185. Find Last 50 Days Modified Files? # find / -mtime -50 186. Find Last 50 Days Accessed Files? # find / -atime -50 187. Find Last 50-100 Days Modified Files? # find / -mtime +50 \u2013mtime -100 188. Find Changed Files in Last 1 Hour? # find / -cmin -60 189. Find Modified Files in Last 1 Hour? # find / -mmin -60 190. Find Accessed Files in Last 1 Hour? # find / -amin -60 191. Find 50MB Files? # find / -size 50M 192. Find using inode number? find . -inum 27492358 -exec rm -i {} \\; 193. Main configuration file of Apache server ? /etc/httpd/conf/httpd.conf 194. Main configuration file of Apache server ? /etc/httpd/conf/httpd.conf 195. Specify number of maximum open files in a single login based on the amount of system RAM. # echo \u201c1599383\u201d > /proc/sys/fs/file-max This can also be done by using sysctl sysctl command is used to change Kernel Parameters at run-time # sysctl -w fs.file-max=1599383 Kernel Parameters can also be changed by making changes in the below file:/etc/sysctl.conf Append the below line in the /etc/sysctl.conf file fs.file-max = 1599383 After making the above change run the below command for changes to reflect, loads the sysctl settings # sysctl -p 196. Increase the local port range, by default the port range is small? # echo \u201c1024 65535\u2033 > /proc/sys/net/ipv4/ip_local_port_rangeThis can also be done using sysctl command # sysctl -w net.ipv4.ip_local_port_range=\u201d1024 65535\u201d Append the below line in the /etc/sysctl.conf file net.ipv4.ip_local_port_range = 1024 61000 After making the above change run the below command for changes to reflect, loads the sysctl settings # sysctl \u2013p 197. Disable packet_forwarding(routing)? net.ipv4.ip_forward = 0 # cat /proc/sys/net/ipv4/ip_forward 198. Mount file systems with noatime options? noatime option means it will not update the file and directory access time. Main advantage is I/O performance will increase. 199. Which command is use to extend a logical volume? # lvextend \u2013size +<addsize> /dev/<vgname>/<lvname> # resize2fs /dev/<vgname>/<lvname> # lvextend -L +1G /dev/VolGroup/LogVol1 This will extend the partition size by +1 GB # resize2fs /dev/VolGroup/LogVol1 200. ServerAdmin : Email address This is the e-mail address that the server includes in error messages sent to the client. Defines the e-mail address that is shown when the server generates an error page. The e-mail address that the Web server provides to clients in case any errors occur. 201. What is the use of SCP command in Linux? SCP command stands for secure copy. It is used to copy/download data from one machine to another machine. 202. What is telnet and what does it do? telnet command is used to check the connectivity to other servers. It helps you to check whether you are able to talk to another server or now. Ex: telnet 192.0.0.1 22 where 22 is the port number. 203. What is a bastion host? A bastion host is also known as a jump server. It is used to connect from one machine to another machine securely. Bastion hosts are used to connecting to private servers securely. 204. What is the command to find the IP address of the host machine in linux? You can use ifconfig/ ipaddr show command to find the IP address of the host machine. 205. Name some of the text editors that are available in Linux? Some of the common text editors that are available in Linux are vi/vim, nano, subl, gedit, atom, emacs. Vi is the default editor that you have in Linux machines. 206. What are the different zip files formats that are available in linux? The different zip formats in Linux are zip, gzip and bzip. 207. What is the difference between cp and mv command? cp command stands for copy and is used to copy data from one location to another. mv stands for the move and is used to move data from one location to another. 208. How can you run a process in the background in Linux? You can run a process in the background by pressing ctrl+z command. 209. What is the use of \u2018chown\u2019 command ? chown stands for \u2018change ownership\u2019 and is used to change the ownership of a file or directory. Eg: chown username.username <filename>. 210. What is the use of \u2018chmod\u2019 command? chmod stands for \u2018change mode\u2019 and is used to change the permissions on files or directories. Eg: # chmod a+w <filename> 211. What is the command to create a zip file in linux? To create a zip file you can use tar command with -cvzf arguments. Eg: tar -cvzf test.tar.gz <file names to be included in the zip> 212. What is the command to unzip the file in linux? To unzip a file you can tar command with -xvzf arguments. Eg: tar -xvzf test.tar.gz 213. What is the command to show the contents of a zip file ? To see the contents of the zip file you can use tar -tvzf arguments. Eg: tar -tvzf test.tar.gz 214. What is a soft link in Linux? A soft link is used to create a shortcut in Linux. This is similar to creating a shortcut in windows systems. 215. What is the command used to create a soft link? To create a soft link you can use ln command with -s arguments. Eg: ln -s /var/www/html html, where /var/www/html is the source file and HTML is the destination of the shortcut. 216. What is the command to remove the soft link in Linux? To remove the soft link in Linux you can use unlink command. Eg: unlink <filename> 217. What is the use of whereis command in linux? Whereis command is used to find the binaries and libraries files of an application in linux. 218. What is the use of man pages? Man pages stand for manual pages. It is the documentation about and helps you to understand the commands and how to use the commands. Eg: man wget. 219. what does \u201c2>\u201d indicate in redirection? This means that output will be shown on the screen and the errors will be written to a file that you specify. Eg: # ls /etc/test 2> error.txt 220. What are the different type of users that you have in Linux? You have 2 types of users in linux. They are \u2022 root user \u2022 standard users. 221. How To check Memory stats and CPU stats ? free & vmstat commands. 222. What is the purpose of runlevels ? Useful for debugging purpose. Basic idea is each runlevel has some services operational and depending on need can enable different runlevels to test which services are running 223. You are able to ping with a numeric IP address, but not by name. How will you debug ? First check /etc/resolv.conf file for DNS Server Entry 224. Generally setting up services in Linux require \u2014\u2014\u2014\u2014\u2014 and \u2014\u2014\u2014\u2014\u2013 Update the associated configuration file and ensure the appropriate daemon is started 225. What is the purpose of iptables command ? Basically allows rule creation to filter packets according to established criteria. Network Address Translation function is also done 226. You are noticing mails are not sent by sendmail. Where can you find the error log to see what happened ? /var/spool/mail/ 227. How can you findout the current runlevel the system is in ? # who -r 228. Linux system has crashed and keeps getting to Debug Prompt. How do you bring it to normal login prompt. Likely due to file system inconsistency, run fsck to check and accept inode repairs. 229. How can you customise startup settings for your login in bash shell You can use file .bashrc for this customisation. 230. What is the first process started by the Schedulerin RHEL 6. init 231. How can you find the current status of Virtual memory in the Linux System ? # cat /proc/meminfo * Note the very useful /proc filesystem 232. Hardware devices are identified as special files in Linux. Name the types. Character,block and Network 233. Name 3 Environment variables SHELL,HOME,PATH 234. Where is my vmlinuz executable loaded from ? /boot 235. As System Administrator you have to apply a patch that is in .tar.gz format. How would you use it ? Get the file and apply tar -zxvf <filename.tar.gz> 236. What are Kernel types and which one is Linux ? Monolithic and Micro. Linux is a Monolithic Kernel. 237. As System Admin, log files are monitored as they grow. How is this achieved ? tail -f 238. Automatic mounting of new file system at boot-time can be done by Adding an entry in /etc/fstab file 239. You recently ran an install, the command for which you need to recall. How do you get this? # history 240. In writing a Bash Shell script, special character\u2019 meaning has to be altered. How is it done? Escape sequence. Precede the special character with a \u2018\\\u2019 241. Linux associates devices with File Descriptors. List them. 0 \u2013 Standard Input 1- Standard Output 2-Standard Error 242. How do you set a mask to stop certain permissions from being granted by default on file creation? # umask <value> 243. You want to try out a Distribution before installation. Which image would suit? LiveCD 244. System Administrators monitor load averages on System for analysis. How is this done? # top # uptime # w 245. What is the behaviour of the following very useful grep command ? # grep [abc] file1 : Looks for matches in file1 containing either an a or b or c character. 246. You need to make a file with only read permissions for yourself, group and all. How can you ? #chmod 444 file1 247. What does su \u2013 do ? Give an example of when you would use this. Gets the switch to root account. Need to be done before Software installation. 248. Pipe is a very important concept in process communication. Give an example # cat file1 |grep xyz : Pattern matching by grep happens on the displayed file1 249. In Shell scripting command return codes are checked prior to proceeding. Explain These are Exit status codes. Linux follows 0 for success and nonzero codes for failures 250. Signals is one way that process communication happens in Linux. What does ctrl C do ? Generates a SIGINT signal that stops the current process running in the shell. 251. To set a password to the boot loader: # grub grub> md5crypt Password: ************ Encrypted: $1$3yQFp$MEDEglsxOvuTWzWaztRly. grub> quit Next, add this to your grub.conf file like so: default=1 timeout=10 splashimage=(hd0,0)/grub/splash.xpm.gz password \u2013md5 $1$3yQFp$MEDEglsxOvuTWzWaztRly.","title":"Linux Interview Questions and Answers for Freshers - Solved:"},{"location":"nightwolf-cotribution/linux_interview_questions_for_freshers/","text":"Linux 200+ Interview Questions for Freshers - Solved: \uf0c1 We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux Admin, Amazon interview questions for Linux admin and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. When you login you get \u201c$\u201d prompt, what is the prompt for root? \uf0c1 Explain the difference between grep and egrep? Search online What is the port # for DNS, NTP and NFS? 53,123 and 111/2049 What is the configuration file name of DNS and where is it located? /etc/named.conf How many new directories will be created after running the following command mkdir {a..c}{1..3} 9 Your PC is configured with a DNS server address but not the default gateway. Can the PC access internet? No What is the difference between IP and Gateway? Search online Can you assign one static IP to 2 computers, if not then why? No because it will create IP conflict How to change IPs address to static? ifconfig x.x.x.x You are trying to ping a server by hostname and you get an error message, \u201cping: unknown host \u2026\u201d. What could be the reason and how to solve the problem so you can ping it by hostname? Check for /etc/hosts or DNS to see if it has hostname to IP entry Explain the difference between relative and absolute path? Absolute path starts from / where relative path is your current directory List 3 different methods of adding user? Search online What is the command to change file/directory ownership and group? chown and chgrp List any 3 type of filesystem? ext4,NTFS and FAT When you login you get a message on the screen. What is the name of that file and where is it located? /etc/motd What is /bin directory used for? Search online What are the different types of DNS Server Master and secondary How to change a user password? passwd username What is the version of Redhat Linux you have experience with? 7.4 List any 4 linux distributions? Redhat, CentOS, Ubuntu and SUSE How to logoff from the system? exit Give any 3 examples of operating systems? Windows, Linux and MAC How to create a directory? mkdir Where are the zone files located for DNS service? /var/named/zonefiles How to check kernel version? uname \u2013a Which directory has all the configuration files? /etc How to become root user from a regular user? su \u2013 How many mega bytes in 1 giga bytes? Search online What is the purpose of having different network ports? So the communication of each application goes through a dedicated port How to display first column of a file? cat filename | awk \u2018{print $1}\u2019 What is the name of DNS rpm package? bind What is the difference between nslookup and dig commands? Search online How to check your user id and group id? id How to check a file\u2019s permission? ls \u2013l What is the difference between \u201ckill\u201d and \u201ckill -9\u201d command? Search online What is subnet? Search online You are troubleshooting an issue with Redhat support and they have asked you to send the contents of /etc directory. How and which method you will use to transfer the contents? tar (compress) the entire /etc directory and ftp What is root home directory? /root What is rsyslogd deamon and its purpose? Search online Your company has terminated a server administrator. What is first thing as an administrator you should do to enhance the security? Change root password How to check the computer name or host name in Linux? hostname Which permission allows a user to run an executable with the permissions of the owner of that file? First 3 bits should have x What is the command to untar a tarred file? untar What is /proc directory used for? Search online What is the purpose of nsswitch.conf file It tells the system where to go to resolve hostnames List 3 basic commands to navigate the filesystem? cd, pwd and ls Which service/daemon should be running on the server that allows you to connect remotely? sshd What is the purpose of firewall? Search online List any 3 IT components? Hardware, OS and Applications Which directory has all the commands we use, e.g. ls, cd etc.? /usr/bin or /bin What is the difference between memory, virtual memory and cache? Search online Which of the following is correct? a. Hardware \u2000 Operating System \u2000 Users b. Operating System \u2000 Users \u2000 Hardware c. Database \u2000 Hardware \u2000 Users Which of the following is a communication command? o grep o mail o touch o cd How to rename a file or directory? mv How to change a hostname in Linux? Search online How to check network interfaces in Linux? ifconfig Why is \u201ctail \u2013f logfilename\u201d command used most often and what does it do? It will output all incoming logs in real time What type of hardware have you worked on? You should get yourself familiar with Dell, HP and UCS hardware by going online and check the vendor websites How to sort a file in reverse order? cat filename | sort \u2013r What is the name of operating system that runs Unix? Solaris, HP-UX etc. List all byte sizes from smallest to largest? Search online How to check the total number of partition in Linux? fdisk -l How to access a linux system from a linux system? ssh Explain the procedure of bonding 2 NICs or interfaces together? Search online What is the exact command syntax to list the 5th column of a file and cut the first 3 letters? cat filename | awk \u2018{print $5}\u2019 | cut \u2013c1-3 What is /etc/hosts file used for? To resolve hostnames with IP address List any 3 options of \u2018df\u2019 command and what they are used for? Search online What is the command to change file/directory permissions? chmod What is the purpose of pipe (|)? To combine multiple commands What is /etc directory used for? For configuration files Which command is used to list files in a directory? ls \u2013l There is a command which gives you information about other commands, please explain that command and what is it used for? man How to delete a file and a directory? rm filename and rmdir dirname What is the difference between \u201ctail\u201d and \u201ctail -10\u201d? None List 4 commands to display or read a file contents? cat, more, less, vi Which command is used to read the top 5 lines of a file? head -5 filename What are the different commands or methods to write to a file? echo > filename and vi filename What is swap space and how to check swap space? Search online What is inode and how to find an inode of a file? Search online Which file to edit for kernel tuning? Search online What is the latest version of Redhat? Search online Name the command to find specific word from a file? grep word filename You have scheduled a job using crontab but it does not run at the time you specified, what could be the reason and how would you troubleshoot? Check your system time Check your crontab entry Check /var/log/messages How to check system hardware information? dmidecode How to check network interface MAC address? ifconfig If I don\u2019t want others to read my file1, how to do that? Remove r from the last 3 bits of file permission What is the purpose of \u201cuniq\u201d and \u201csed\u201d command? Search online Which command is used to list the contents of a directory in the most recent time and in reverse order, meaning the most updated file should be listed on the bottom? ls \u2013ltr What is the difference between tar, gzip and gunzip? Search online What are the different ways to install and OS? DVD, DVD iso and network boot How to view difference between two files? diff file1 and file2 You noticed that one of the Linux servers has no disk space left, how would you troubleshoot that issue? If running LVM then add more disk and extend LVM If not running LVM then add more disk, create a new partition and link the new partition to an existing filesystem How to check Redhat version release? uname \u2013a or /etc/redhat-release What is the difference between TCP and UDP? Search online What is a zombie process? Search online How do you search for a pattern/word in a file and then replace it in an entire file? sed command Explain the purpose of \u201ctouch\u201d command? To create an empty file If a command hangs, how to stop it and get the prompt back? Ctrl C Which command is used to count words or lines? wc How to check the number of users logged in? who What is the command to view the calendar of 2011? cal 2011 Which command is used to view disk space? df \u2013h How to create a new group in Linux? groupadd What is the command to send a message to everyone who is logged into the system? wall Which command is used to check total number of disks? fdisk \u2013l What is an mail server record in DNS? MX What does the following command line do? ps -ef | awk '{print $1}' | sort | uniq List the first column of all running processes, sort them and remove duplicates You get a call that when a user goes to www.yourwebsite.com it fails and gets an error, how do you troubleshoot? Check for user internet Check to see if user computer has DNS for hostname lookup Check to see if the server is up that is running that website Check to see if the server\u2019s web service is running Check for DNS availability which is resolving that website List 4 different directories in /? /etc, /bin, /tmp, /home What is the output of the following command: $tail -10 filename | head -1 It will show the first line from the last 10 lines of a file What are the different fields in /etc/passwd file? Search online Which command is used to list the processes? ps \u2013ef What is the difference between \u201chostname\u201d and \u201cuname\u201d commands? Hostname will give you system name and uname will give you OS information How to check system load? top and uptime command How to schedule jobs? crontab and at What is the 3rd field when setting up crontab? Day of the month What is the command to create a new user? useradd What is the \u201cinit #\u201d for system reboot? 6 How to restart a service? systemctl restart servicename How to shutdown a system? shutdown or init 0 What is \u201cftp\u201d command used for? To transfer files from one computer to another Explain cron job syntax? First is minute, second is..? Min, house, day of the month, month, day of the week and command How to delete a package in Linux? rpm \u2013e packagename What is the file name where user password information is saved? /etc/shadow Which command you would use to find the location of chmod command? which chmod Which command is used to check if the other computer is online? ping othercomputer Please explain about LAN, MAN and WAN? Search online How to list hidden files in a directory? ls \u2013la What is the difference between telnet and ssh? ssh is secure where telnet is not How to run a calculator on Linux and exit out of it? bc and quit List any 4 commands to monitor system? top, df \u2013h, iostat, dmesg You are notified that your server is down, list the steps you will take to troubleshoot? Check the system physically Login through system console Ping the system Reboot or boot if possible What is difference between static and DHCP IP? Search online How to write in vi editor mode? i = insert, a = insert in next space, o = insert in new line What is the difference between \u201ccrontab\u201d and \u201cat\u201d jobs? crontab is for repetitive jobs where at is for one time job What is vCenter server in VMWare? Search online What is \u201cdmidecode\u201d command used for? To get system information What is the difference between SAN and NAS? Search online What is the location of system logs? E.g. messages /var/log directory How to setup an alias and what is it used for? alias aliasname=\u201dcommand\u201d It is used to created short-cuts for long commands What is the purpose of \u201cnetstat\u201d command? Search online What are terminal control keys, list any 3? Crtl C, D and Z Which command(s) you would run if you need to find out how many processes are running on your system? ps \u2013ef | wc \u2013l What are the different types of shells? sh, bash, ksh, csh etc. How to delete a line when in vi editor mode? dd Which is the core of the operating system? a) Shell b) Kernel c) Commands d) Script Which among the following interacts directly with system hardware? a) Shell b) Commands c) Kernel d) Applications How to save and quit from vi editor? Shift ZZ or :wq! What is the difference between a process and daemon? Search online What is the process or daemon name for NTP? ntpd What are a few commands you would run if your system is running slow? top, iostat, df \u2013h, netstat etc. How to install a package in Redhat Linux? yum install packagename What is the difference between \u201cifconfig\u201d and \u201cipconfig\u201d commands? ifconfig for Linux and ipconfig for Windows What is the first line written in a shell script? Define shell e.g. #!/bin/bash Where is the network (Ethernet) file located, please provide exact directory location and file name? /etc/sysconfig/network-scripts/ifcfg-nic Why do we use \u201clast\u201d command? To see who has logged in the system whether active or logged off What is RHEL Linux stands for? Search online To view your command history, which command is used and how to run a specific command? history and history # What is NTP and briefly explain how does it work and where is the config files and related commands of NTP? Search online How to disable firewall in Linux? Search online How to configure mail server relay for sendmail service? Edit /etc/mail/sendmail.mc file and add SMART_HOST entry Where is samba log file located? /var/log/samba What is mkfs command used for? To create a new filesystem If you create a new group, which file does it get created in? /etc/group Which file has DNS server information (e.g. DNS resolution)? /etc/resolv.conf What are the commands you would run if you need to find out the version and build date of a package (e.g. http)? rpm \u2013qi http On the file permissions? What are the first 3 bits for and who is it for? Read, write and execute. They are used for the owner of the file How to create a soft link? ln \u2013s How to write a script to delete messages in a log file older than 30 days automatically? Search online How to quit out of \u201cman\u201d command? q Which command is used to partition disk in Linux? fdisk What is the difference between \u201cshutdown\u201d and \u201chalt\u201d command? Search online What is the exact syntax of mounting NFS share on a client and also how to un-mount? Search online What experience do you have with scripting, explain? if-the, do-while, case, for loop scripts How to get information on all the packages installed on the system? rpm \u2013qa Explain VMWare? Search online You are tasked to examine a log file in order to find out why a particular application keep crashing. Log file is very lengthy, which command can you use to simplify the log search using a search string? grep for error, warning, failure etc. in /var/log/messages file What is /etc/fstab file and explain each column of this file? Search online What the latest version of Windows server? Search online What is the exact command to list only the first 2 lines of history output? history | head -2 How to upgrade Linux from 7.3 to 7.4? yum install update How to tell which shell you are in or running? $0 You have tried to \u201ccd\u201d into a directory but you have been denied. You are not the owner of that directory, what permissions do you need and where? r \u2013 x What is CNAME record in DNS? Entry for hostname to hostname What is the name of VMWare operating system? ESXi What is the client name used to connect to ESXi or vCenter server? vSphere client You get a call from a user saying that I cannot write to a file because it says, permission denied. The file is owned by that user, how do you troubleshoot? Give write permission on the first 3 bits What is the latest version of VMWare? Search online What is the name of firewall daemon in Linux? firewalld Which command syntax you can use to list only the 20th line of a file? Search online What is the difference between run level 3 and 5? 3 = Boot system with networking, 5 = boot system with networking and GUI List a few commands that are used in troubleshooting network related issue? netstat, tcpdump etc. What is the difference between domain and nameserver? Search online You open up a file and it has 3000 lines and it scrolled up really fast, which command you will use to view it one page at a time? more or less How to start a new shell. E.g. start a new ksh shell? Simply type ksh, or bash How to kill a process? kill processID How to check scheduled jobs? crontab \u2013l How to check system memory and CPU usage? free and top Which utility could you use to repair the corrupted file system? fsck What is the command to make a service start at boot? systemctl enable servicename How to combine 2 files into 1? E.g. you 3 lines in file \u201cA\u201d and 5 lines in file \u201cB\u201d, which command syntax to use that will combine into one file of 3+5 = 8 lines cat fileA >> fileB What is echo command used for? To output to a screen What does the following command do? echo This year the summer will be great > file1 It will create a new file \u201cfile1\u201d with the content as \u201cThis year the summer will be great\u201d Which file to modify to allow users to run root commands /etc/sudoers You need to modify httpd.conf file but you cannot find it, Which command line tool you can use to find file? find / -name \u201chttpd.conf\u201d Your system crashed and being restarted, but a message appears, indicating that the operating system cannot be found. What is the most likely cause of the problem? The /boot file is most likely corrupted Next Page You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"Linux Interview Questions for Freshers - 2"},{"location":"nightwolf-cotribution/linux_interview_questions_for_freshers/#linux-200-interview-questions-for-freshers-solved","text":"We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux Admin, Amazon interview questions for Linux admin and other reputed firms as well. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. When you login you get \u201c$\u201d prompt, what is the prompt for root?","title":"Linux 200+ Interview Questions for Freshers - Solved:"},{"location":"nightwolf-cotribution/linux_interview_questions_for_freshers/#_1","text":"Explain the difference between grep and egrep? Search online What is the port # for DNS, NTP and NFS? 53,123 and 111/2049 What is the configuration file name of DNS and where is it located? /etc/named.conf How many new directories will be created after running the following command mkdir {a..c}{1..3} 9 Your PC is configured with a DNS server address but not the default gateway. Can the PC access internet? No What is the difference between IP and Gateway? Search online Can you assign one static IP to 2 computers, if not then why? No because it will create IP conflict How to change IPs address to static? ifconfig x.x.x.x You are trying to ping a server by hostname and you get an error message, \u201cping: unknown host \u2026\u201d. What could be the reason and how to solve the problem so you can ping it by hostname? Check for /etc/hosts or DNS to see if it has hostname to IP entry Explain the difference between relative and absolute path? Absolute path starts from / where relative path is your current directory List 3 different methods of adding user? Search online What is the command to change file/directory ownership and group? chown and chgrp List any 3 type of filesystem? ext4,NTFS and FAT When you login you get a message on the screen. What is the name of that file and where is it located? /etc/motd What is /bin directory used for? Search online What are the different types of DNS Server Master and secondary How to change a user password? passwd username What is the version of Redhat Linux you have experience with? 7.4 List any 4 linux distributions? Redhat, CentOS, Ubuntu and SUSE How to logoff from the system? exit Give any 3 examples of operating systems? Windows, Linux and MAC How to create a directory? mkdir Where are the zone files located for DNS service? /var/named/zonefiles How to check kernel version? uname \u2013a Which directory has all the configuration files? /etc How to become root user from a regular user? su \u2013 How many mega bytes in 1 giga bytes? Search online What is the purpose of having different network ports? So the communication of each application goes through a dedicated port How to display first column of a file? cat filename | awk \u2018{print $1}\u2019 What is the name of DNS rpm package? bind What is the difference between nslookup and dig commands? Search online How to check your user id and group id? id How to check a file\u2019s permission? ls \u2013l What is the difference between \u201ckill\u201d and \u201ckill -9\u201d command? Search online What is subnet? Search online You are troubleshooting an issue with Redhat support and they have asked you to send the contents of /etc directory. How and which method you will use to transfer the contents? tar (compress) the entire /etc directory and ftp What is root home directory? /root What is rsyslogd deamon and its purpose? Search online Your company has terminated a server administrator. What is first thing as an administrator you should do to enhance the security? Change root password How to check the computer name or host name in Linux? hostname Which permission allows a user to run an executable with the permissions of the owner of that file? First 3 bits should have x What is the command to untar a tarred file? untar What is /proc directory used for? Search online What is the purpose of nsswitch.conf file It tells the system where to go to resolve hostnames List 3 basic commands to navigate the filesystem? cd, pwd and ls Which service/daemon should be running on the server that allows you to connect remotely? sshd What is the purpose of firewall? Search online List any 3 IT components? Hardware, OS and Applications Which directory has all the commands we use, e.g. ls, cd etc.? /usr/bin or /bin What is the difference between memory, virtual memory and cache? Search online Which of the following is correct? a. Hardware \u2000 Operating System \u2000 Users b. Operating System \u2000 Users \u2000 Hardware c. Database \u2000 Hardware \u2000 Users Which of the following is a communication command? o grep o mail o touch o cd How to rename a file or directory? mv How to change a hostname in Linux? Search online How to check network interfaces in Linux? ifconfig Why is \u201ctail \u2013f logfilename\u201d command used most often and what does it do? It will output all incoming logs in real time What type of hardware have you worked on? You should get yourself familiar with Dell, HP and UCS hardware by going online and check the vendor websites How to sort a file in reverse order? cat filename | sort \u2013r What is the name of operating system that runs Unix? Solaris, HP-UX etc. List all byte sizes from smallest to largest? Search online How to check the total number of partition in Linux? fdisk -l How to access a linux system from a linux system? ssh Explain the procedure of bonding 2 NICs or interfaces together? Search online What is the exact command syntax to list the 5th column of a file and cut the first 3 letters? cat filename | awk \u2018{print $5}\u2019 | cut \u2013c1-3 What is /etc/hosts file used for? To resolve hostnames with IP address List any 3 options of \u2018df\u2019 command and what they are used for? Search online What is the command to change file/directory permissions? chmod What is the purpose of pipe (|)? To combine multiple commands What is /etc directory used for? For configuration files Which command is used to list files in a directory? ls \u2013l There is a command which gives you information about other commands, please explain that command and what is it used for? man How to delete a file and a directory? rm filename and rmdir dirname What is the difference between \u201ctail\u201d and \u201ctail -10\u201d? None List 4 commands to display or read a file contents? cat, more, less, vi Which command is used to read the top 5 lines of a file? head -5 filename What are the different commands or methods to write to a file? echo > filename and vi filename What is swap space and how to check swap space? Search online What is inode and how to find an inode of a file? Search online Which file to edit for kernel tuning? Search online What is the latest version of Redhat? Search online Name the command to find specific word from a file? grep word filename You have scheduled a job using crontab but it does not run at the time you specified, what could be the reason and how would you troubleshoot? Check your system time Check your crontab entry Check /var/log/messages How to check system hardware information? dmidecode How to check network interface MAC address? ifconfig If I don\u2019t want others to read my file1, how to do that? Remove r from the last 3 bits of file permission What is the purpose of \u201cuniq\u201d and \u201csed\u201d command? Search online Which command is used to list the contents of a directory in the most recent time and in reverse order, meaning the most updated file should be listed on the bottom? ls \u2013ltr What is the difference between tar, gzip and gunzip? Search online What are the different ways to install and OS? DVD, DVD iso and network boot How to view difference between two files? diff file1 and file2 You noticed that one of the Linux servers has no disk space left, how would you troubleshoot that issue? If running LVM then add more disk and extend LVM If not running LVM then add more disk, create a new partition and link the new partition to an existing filesystem How to check Redhat version release? uname \u2013a or /etc/redhat-release What is the difference between TCP and UDP? Search online What is a zombie process? Search online How do you search for a pattern/word in a file and then replace it in an entire file? sed command Explain the purpose of \u201ctouch\u201d command? To create an empty file If a command hangs, how to stop it and get the prompt back? Ctrl C Which command is used to count words or lines? wc How to check the number of users logged in? who What is the command to view the calendar of 2011? cal 2011 Which command is used to view disk space? df \u2013h How to create a new group in Linux? groupadd What is the command to send a message to everyone who is logged into the system? wall Which command is used to check total number of disks? fdisk \u2013l What is an mail server record in DNS? MX What does the following command line do? ps -ef | awk '{print $1}' | sort | uniq List the first column of all running processes, sort them and remove duplicates You get a call that when a user goes to www.yourwebsite.com it fails and gets an error, how do you troubleshoot? Check for user internet Check to see if user computer has DNS for hostname lookup Check to see if the server is up that is running that website Check to see if the server\u2019s web service is running Check for DNS availability which is resolving that website List 4 different directories in /? /etc, /bin, /tmp, /home What is the output of the following command: $tail -10 filename | head -1 It will show the first line from the last 10 lines of a file What are the different fields in /etc/passwd file? Search online Which command is used to list the processes? ps \u2013ef What is the difference between \u201chostname\u201d and \u201cuname\u201d commands? Hostname will give you system name and uname will give you OS information How to check system load? top and uptime command How to schedule jobs? crontab and at What is the 3rd field when setting up crontab? Day of the month What is the command to create a new user? useradd What is the \u201cinit #\u201d for system reboot? 6 How to restart a service? systemctl restart servicename How to shutdown a system? shutdown or init 0 What is \u201cftp\u201d command used for? To transfer files from one computer to another Explain cron job syntax? First is minute, second is..? Min, house, day of the month, month, day of the week and command How to delete a package in Linux? rpm \u2013e packagename What is the file name where user password information is saved? /etc/shadow Which command you would use to find the location of chmod command? which chmod Which command is used to check if the other computer is online? ping othercomputer Please explain about LAN, MAN and WAN? Search online How to list hidden files in a directory? ls \u2013la What is the difference between telnet and ssh? ssh is secure where telnet is not How to run a calculator on Linux and exit out of it? bc and quit List any 4 commands to monitor system? top, df \u2013h, iostat, dmesg You are notified that your server is down, list the steps you will take to troubleshoot? Check the system physically Login through system console Ping the system Reboot or boot if possible What is difference between static and DHCP IP? Search online How to write in vi editor mode? i = insert, a = insert in next space, o = insert in new line What is the difference between \u201ccrontab\u201d and \u201cat\u201d jobs? crontab is for repetitive jobs where at is for one time job What is vCenter server in VMWare? Search online What is \u201cdmidecode\u201d command used for? To get system information What is the difference between SAN and NAS? Search online What is the location of system logs? E.g. messages /var/log directory How to setup an alias and what is it used for? alias aliasname=\u201dcommand\u201d It is used to created short-cuts for long commands What is the purpose of \u201cnetstat\u201d command? Search online What are terminal control keys, list any 3? Crtl C, D and Z Which command(s) you would run if you need to find out how many processes are running on your system? ps \u2013ef | wc \u2013l What are the different types of shells? sh, bash, ksh, csh etc. How to delete a line when in vi editor mode? dd Which is the core of the operating system? a) Shell b) Kernel c) Commands d) Script Which among the following interacts directly with system hardware? a) Shell b) Commands c) Kernel d) Applications How to save and quit from vi editor? Shift ZZ or :wq! What is the difference between a process and daemon? Search online What is the process or daemon name for NTP? ntpd What are a few commands you would run if your system is running slow? top, iostat, df \u2013h, netstat etc. How to install a package in Redhat Linux? yum install packagename What is the difference between \u201cifconfig\u201d and \u201cipconfig\u201d commands? ifconfig for Linux and ipconfig for Windows What is the first line written in a shell script? Define shell e.g. #!/bin/bash Where is the network (Ethernet) file located, please provide exact directory location and file name? /etc/sysconfig/network-scripts/ifcfg-nic Why do we use \u201clast\u201d command? To see who has logged in the system whether active or logged off What is RHEL Linux stands for? Search online To view your command history, which command is used and how to run a specific command? history and history # What is NTP and briefly explain how does it work and where is the config files and related commands of NTP? Search online How to disable firewall in Linux? Search online How to configure mail server relay for sendmail service? Edit /etc/mail/sendmail.mc file and add SMART_HOST entry Where is samba log file located? /var/log/samba What is mkfs command used for? To create a new filesystem If you create a new group, which file does it get created in? /etc/group Which file has DNS server information (e.g. DNS resolution)? /etc/resolv.conf What are the commands you would run if you need to find out the version and build date of a package (e.g. http)? rpm \u2013qi http On the file permissions? What are the first 3 bits for and who is it for? Read, write and execute. They are used for the owner of the file How to create a soft link? ln \u2013s How to write a script to delete messages in a log file older than 30 days automatically? Search online How to quit out of \u201cman\u201d command? q Which command is used to partition disk in Linux? fdisk What is the difference between \u201cshutdown\u201d and \u201chalt\u201d command? Search online What is the exact syntax of mounting NFS share on a client and also how to un-mount? Search online What experience do you have with scripting, explain? if-the, do-while, case, for loop scripts How to get information on all the packages installed on the system? rpm \u2013qa Explain VMWare? Search online You are tasked to examine a log file in order to find out why a particular application keep crashing. Log file is very lengthy, which command can you use to simplify the log search using a search string? grep for error, warning, failure etc. in /var/log/messages file What is /etc/fstab file and explain each column of this file? Search online What the latest version of Windows server? Search online What is the exact command to list only the first 2 lines of history output? history | head -2 How to upgrade Linux from 7.3 to 7.4? yum install update How to tell which shell you are in or running? $0 You have tried to \u201ccd\u201d into a directory but you have been denied. You are not the owner of that directory, what permissions do you need and where? r \u2013 x What is CNAME record in DNS? Entry for hostname to hostname What is the name of VMWare operating system? ESXi What is the client name used to connect to ESXi or vCenter server? vSphere client You get a call from a user saying that I cannot write to a file because it says, permission denied. The file is owned by that user, how do you troubleshoot? Give write permission on the first 3 bits What is the latest version of VMWare? Search online What is the name of firewall daemon in Linux? firewalld Which command syntax you can use to list only the 20th line of a file? Search online What is the difference between run level 3 and 5? 3 = Boot system with networking, 5 = boot system with networking and GUI List a few commands that are used in troubleshooting network related issue? netstat, tcpdump etc. What is the difference between domain and nameserver? Search online You open up a file and it has 3000 lines and it scrolled up really fast, which command you will use to view it one page at a time? more or less How to start a new shell. E.g. start a new ksh shell? Simply type ksh, or bash How to kill a process? kill processID How to check scheduled jobs? crontab \u2013l How to check system memory and CPU usage? free and top Which utility could you use to repair the corrupted file system? fsck What is the command to make a service start at boot? systemctl enable servicename How to combine 2 files into 1? E.g. you 3 lines in file \u201cA\u201d and 5 lines in file \u201cB\u201d, which command syntax to use that will combine into one file of 3+5 = 8 lines cat fileA >> fileB What is echo command used for? To output to a screen What does the following command do? echo This year the summer will be great > file1 It will create a new file \u201cfile1\u201d with the content as \u201cThis year the summer will be great\u201d Which file to modify to allow users to run root commands /etc/sudoers You need to modify httpd.conf file but you cannot find it, Which command line tool you can use to find file? find / -name \u201chttpd.conf\u201d Your system crashed and being restarted, but a message appears, indicating that the operating system cannot be found. What is the most likely cause of the problem? The /boot file is most likely corrupted","title":""},{"location":"nightwolf-cotribution/linux_questionairs/","text":"Top 100 Linux Interview Questions and Answers - Solved: \uf0c1 We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Technical Solutions Specialist - Infrastructure), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. Most of these are scenario based Linux interview questions for Experianced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. 1. Explain the below error: ------------------------------------------------------------------------------------------- find: page allocation failure. order:5, mode:0x0 Pid: 22938, comm: find Not tainted 2.6.32-279.el6.i686 #1 Call Trace: [<c04f22bc>] ? __alloc_pages_nodemask+0x6bc/0x870 [<c051f1bc>] ? cache_alloc_refill+0x2bc/0x510 [<c083f035>] ? apic_timer_interrupt+0x31/0x38 [<c051f552>] ? __kmalloc+0x142/0x180 [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb2f0>] ? rpc_wait_bit_killable+0x0/0x80 [sunrpc] ------------------------------------------------------------------------------------------- Hint => Above error is kernel dump due to kmalloc buffer exhaustion. The system is out of memory and cannot satisfy the allocation request at all. This is more likely to happen on swap-less systems that are unable to purge application pages to free up memory. The system may have sufficient free pages in total but not enough that are physically contiguous to satisfy the allocation request. This is referred to as memory fragmentation. - The phrase \"page allocation failure\" - this defines the error message that should be used in a KCS search. - The process name \"find\" - often page allocation failures are related to code that gets executed by specific threads, try searching KCS with and without this term. If the allocation request was performed by an interrupt service routine then the process name will be irrelevant. - The order of the allocation \"order:5\" and mode \"mode:0x0\" can often be the same for every instance of a particular problem, try searching KCS with and without these terms. - The function requesting the memory allocation \"rpc_malloc()\" is likely to be the same for every instance of this particular problem so include that in the KCS search. - Do not search for the CPU number, the process id or function offsets since they will change and may prevent finding a match. For more details https://access.redhat.com/articles/1360023 2. What is order:5, mode:0x0 in above output. 3. Difference between kernel panic due to Memory crunch and page allocation failure. 4. Difference between VMalloc and KMalloc Hint => The kmalloc() & vmalloc() functions are a simple interface for obtaining kernel memory in byte-sized chunks. - The kmalloc() function guarantees that the pages are physically contiguous (and virtually contiguous). - The vmalloc() function works in a similar fashion to kmalloc(), except it allocates memory that is only virtually contiguous and not necessarily physically contiguous. 5. How do you troubleshoot memory performance issue. Please explain the details. 6. Which tools do you use to troubleshoot high Memory troubleshooting. 7. What are zombie process and how to kill/reclaim them. 8. What are system calls. 9. What is strace used for. 10. Difference between fork and exec in Linux. 11. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- 12. Explain few examples of kernel panic. 13. How to blacklist a module. 14. What are D-State processes and what causes these. 15. If Disk is causing D-state processes, what you can check and can do to fix the issue. 16. Explain blk_trace. 17. If you are receiving \"Connection reset by peer\" when trying to connect a server, what can be root cause of this and how you can fix this. 18. How will you troubleshoot high load avg. Which tools will you use and why. Explain in details. 19. What can be reason of high load avg in kernel space i.e. %sy in top. 20. What is CPU affinity . How to check it. 21. How to check all cores stat in top. Hint: by pressing 1 or using mpstat 22. What can be the reasons for server to stuck at boot prompt. and how to troubleshoot further ? 23. What is backporting in Linux. Hint: its related to packages. 24. There are two systems and one is taking less booting time and other than taking more than booting time. 25. Please explain Linux booting processing? 26. How to check the loaded and unloaded modules 27. Please explain few types of kernel errors? 28. How to troubleshoot the system performance if any Linux system is facing slowness? 29. How to troubleshoot high memory usageissue on Linux system. 30. What is CPU load ? How to calculate the load average on the system? 31. What the Zombie and Orphan process? How to kill zombie process? 32. What could be the impacts on the system if there are many zombie process are available? 33. What is affinity in Linux? 1. Binding a process with a CPU core. 2. # taskset < command > 34. What is paging and swapping in Linux? Please explain Page fault ? 35. What is difference between cache and buffer? 36. Explain the TOP, IOSTAT,VMSTAT,IOTOP commands. 37. Difference between RHEL6 and RHEL7 booting process. 38. Difference between systemd and initd 39. How to troubleshoot a issue where a client not able to access a server? 40. What are outputs when you execute the NATCAT (nc) command and explain each ? Hint => - Connected - Connection timeout - Connection refused(if port / server is blocking by the firewall) 41. What are the inodes and how will you free up them? 42. How can we check the packet flow in our system? 43. what is the steal value in top command? 44. Explain what is NUMA. You may know this as \"Node Interleaving\u201d. Give an example where NUMA is beneficial, and another where it's detrimental. Hints => a). NUMA: Non Uniform Memory Access (NUMA) is a design where the memory is split into groups by CPU, making the memory access faster for each CPU to their memory group. b). Beneficial: Multiple processes with smaller memory footprints. Good for applications that fork processes, like Apache. Detrimental: Monolithic, memory-heavy, single processes. May be threaded, example MySQL, Redis, Java-everything, Memcached. Follow-Up Q. - what if there's only a single physical proc. 45. Customer complains that their CPUs aren't running as fast as they're rated. How would you Troubleshoot ? Hints => a). Could be heat damage - check /proc/cpuinfo for \"cpu MHz\". b). Could be the wrong CPUs installed, for that matter. Check the CPU specs in /proc/cpuinfo. OR dmidecode. c). Get 'em to tell you how to fix it (Change system profile to MaximumPerformance with omconfig, then reboot) 46. Can we check System logs on Hardware layer from OS ?How ? Hints => a). Yes, in dmesg logs or kern.log b). By using respective Hardware CLI commands. c). Specific answers hpasmcli -s \"show iml\" ==> For HP servers omreport system [alertlog | esmlog] ==> For Dell Servers 47. What can be done to improve the performance of an I/O-heavy system? Hints => a). Check RAID Level, if server using any. b). Partition aligned, i.e. starting a partition on sector 2048. c). Mount Options like: noatime - (covers norelatime/nodiratime) Can be enabled only via fstab. d). RAID Controller settings - read/write policy, e.g. write-through(good to ensure data has been changed on disk. write-back(relys on cache, battery). e). nobarrier - depends on whether or not the PERC Controller is making use of a battery or not. f). Adjust the filesystem journaling method - the default mount option is in the middle, data=ordered. This indicates the data is written out to the filesystem before the metadata in the journal is updated. The filesystem can be mounted with data=writeback which allows data to be written to the journal and disk at the same time, with the risk of some data possibly being written to the disk after the metadata that cannot be recovered after a crash. Additionally, the journal can be placed on an external device with another mount option, allowing separation of the writes to increase throughput performance for each write operation. g). discard - For SSDs. Also called fs TRIM. Extends life and improves performance of SSD drives. TRIM enables the controller to cache the expected SSD write cycles to collapse them into a \"do only once\" mechanism to prevent burning out the SSD cells, which can only be written to once with voltage. Note: For LVM, set issue_discards=1 in lvm.conf. Note: On Dell, TRIM/discard is only available on RAID controllers >= PERC H710 h). For SSD, you don't want controller caching for performance reasons. By default HPs turn caching off. Dell: Cut-through IO (CTIO) is an IO accelerator for SSD arrays that boosts the throughput of devices connected to the PERC Controller. It is enabled through disabling the write-back cache (enable writethrough cache) and disabling Read Ahead. HP: On servers with SSD drives, write caching appears to be disabled by default for performance reasons (there is no need for write caching on SSD drives which are fast enough). Writes to SSD drives will not be acknowledged until the data is actually written to disk: i). On HP, HP SSD Smart Path - this is enabled by default by the RAID creation process in the HP RAID controller itself, and can be checked with the standard commands using hpssacli to view controller details and RAID logical device details. The theory is this special method of SSD writing replaces traditional caching techniques with a special HP invented method for improving SSD performance and longevity. j). Disk IO Scheduler deadline (recommended for DBs) noop (recommended for VMs) 48. Why would I choose to use RAID-5 instead of RAID-10? Or vice versa? Hints => RAID-5: Pro: Space efficiency - you only sacrifice 1x HDD to redundancy. Pro: Possible with a minimum of 3x drives, as opposed to RAID-10's minimum-4. Con: Slow writes. Note: Reads are still quick though, so you'd be fine on a read-heavy application. Con: If two drives fail, you lose everything. RAID-10: Pro: Great performance with both reads and writes. Good for write-heavy applications. Pro: Less risk of a catastrophic failure, since more than 1 drive can fail safely (Note: Depends which drives). Con: Too much wastage of Space. Only 50% capacity is uasable. 49. How snapshots work ? Can you please explain. In terms of VMWare or LVM(in case someone asks). Hints => As soon as you create a snapshot, LVM creates a pool of blocks. This pool also contains a full copy of the LVM metadata of the volume. When writes happen to the main volume such as updating an inode, the block being overwritten is copied to this new pool and the new block is written to the main volume. This is the 'copy-on-write'. Because of this, the more data that gets changed between when a snapshot was taken and the current state of the main volume, the more space will get consumed by that snapshot pool. When you mount the snapshot, the meta-data written when the snapshot was taken allows the mapping of snapshot pool blocks over changed blocks in the volume (or higher level snapshot). This way when an access comes for a specific block, LVM knows which block access. As far as the filesystem on that volume is concerned, there are no snapshots. 50. In which scenario we need to use vgcfgrestore and vgcfgbackup commands ? When /etc/lvm/archive contents are written (when does backup happen ?) Hints => a). To restore VG configuration. b). Backup is taken whenever we make change to LVM (create/remove LV/VG) and we can use it to restore to the prior state. 51. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. 52. What is deadlock ? How to identify if process is in deadlock in Linux/Unix ? Hint => Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process 53. How to load kernel module. any commands ? Hint => We can use \"modprobe\" or \"insmod\" commands to load kernel module. 54. Difference between insmod and modprobe commands in Linux ? 55. Different types of file systems? Hint => ext2, ext3, ext4, xfs, vfat etc. 56. Difference between ext4 and xfs? 57. When v create user which files are referred? 58. Differnce between passwd and shadow file? Hint => /etc/passwd contains User's detail like home directory, shell etc. /etc/shadow conatains User's password hashes. 59. What are the contents in passwd file? Hint => root:x:0:0:root:/root:/bin/bash \u2022 login name \u2022 optional encrypted password \u2022 numerical user ID \u2022 numerical group ID \u2022 user name or comment field \u2022 user home directory \u2022 optional user command interpreter 60. List all users who are not given passwd? with cmd Hint => cat /etc/shadow| awk -F \":\" '$2 ~ /[a-z]/ {print $1}' 61. How to check what processess are running on which port? Hint => Using \"netstat\" and \"ss\" command we can relate processes with port. 62. How to extend partition in lvm? and indepth questions in lvm ? Hint => 1. First attach extra storage/disk to server. 2. Create a new partition on the disk using 'fdisk' or 'parted' command. 3. Create a new Physical Volume(PV) on that new partition. e.g. 'pvcreate /dev/sdb1' 4. Exetend existing Volume Group(VG) using new PV. e.g. 'vgextend vg_name /dev/sdb1' 5. Now extend the LV. e.g . 'lvextend -l 100%FREE /dev/mapper/vg_name-lv_name' 6. now execute 'resize2fs' or 'xfs_growfs'. 63. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses, 64. On which port dns works? Hint => DNS works on port 53. Questions from Github \uf0c1 A lot more Questions from nightwolf-cotribution github repo # General Questions: 1. What did you learn yesterday/this week? 2. Talk about your preferred development/administration environment. (OS, Editor, Browsers, Tools etc.) 3. Tell me about the last major Linux project you finished. 4. Tell me about the biggest mistake you've made in [some recent time period] and how you would do it differently today. What did you learn from this experience? 5. Why we must choose you? 6. What function does DNS play on a network? 7. What is HTTP? 8. What is an HTTP proxy and how does it work? 9. Describe briefly how HTTPS works. 10. What is SMTP? Give the basic scenario of how a mail message is delivered via SMTP. 11. What is RAID? What is RAID0, RAID1, RAID5, RAID10? 12. What is a level 0 backup? What is an incremental backup? 13. Describe the general file system hierarchy of a Linux system. 14. Which difference have between public and private SSH key? # Simple Linux Questions: 15. What is the name and the UID of the administrator user? 16. How to list all files, including hidden ones, in a directory? 17. What is the Unix/Linux command to remove a directory and its contents? 18. Which command will show you free/used memory? Does free memory exist on Linux? 19. How to search for the string \"my konfu is the best\" in files of a directory recursively? 20. How to connect to a remote server or what is SSH? 21. How to get all environment variables and how can you use them? 22. I get \"command not found\" when I run ifconfig -a. What can be wrong? 23. What happens if I type TAB-TAB? 24. What command will show the available disk space on the Unix/Linux system? 25. What commands do you know that can be used to check DNS records? 26. What Unix/Linux commands will alter a files ownership, files permissions? 27. What does chmod +x FILENAME do? 28. What does the permission 0750 on a file mean? 29. What does the permission 0750 on a directory mean? 30. How to add a new system user without login permissions? 31. How to add/remove a group from a user? 32. What is a bash alias? 33. How do you set the mail address of the root/a user? 34. What does CTRL-c do? 35. What does CTRL-d do? 36. What does CTRL-z do? 37. What is in /etc/services? 38. How to redirect STDOUT and STDERR in bash? (> /dev/null 2>&1) 39. What is the difference between UNIX and Linux. 40. What is the difference between Telnet and SSH? 41. Explain the three load averages and what do they indicate. What command can be used to view the load averages? 42. Can you name a lower-case letter that is not a valid option for GNU ls? 43. What is a Linux kernel module? 44. Walk me through the steps in booting into single user mode to troubleshoot a problem. 45. Walk me through the steps you'd take to troubleshoot a 404 error on a web application you administer. 46. What is ICMP protocol? Why do you need to use? # Medium Linux Questions: 47. What do the following commands do and how would you use them? tee awk tr cut tac curl wget watch head tail less cat touch sar netstat tcpdump lsof 48. What does an & after a command do? 49. What does & disown after a command do? 50. What is a packet filter and how does it work? 51. What is Virtual Memory? 52. What is swap and what is it used for? 53. What is an A record, an NS record, a PTR record, a CNAME record, an MX record? 54. Are there any other RRs and what are they used for? 55. What is a Split-Horizon DNS? 56. What is the sticky bit? 57. What does the immutable bit do to a file? 58. What is the difference between hardlinks and symlinks? What happens when you remove the source to a symlink/hardlink? 59. What is an inode and what fields are stored in an inode? 60. How to force/trigger a file system check on next reboot? 61. What is SNMP and what is it used for? 62. What is a runlevel and how to get the current runlevel? 63. What is SSH port forwarding? 64. What is the difference between local and remote port forwarding? 65. What are the steps to add a user to a system without using useradd/adduser? 66. What is MAJOR and MINOR numbers of special files? 67. Describe the mknod command and when you'd use it. 68. Describe a scenario when you get a \"filesystem is full\" error, but 'df' shows there is free space. 69. Describe a scenario when deleting a file, but 'df' not showing the space being freed. 70. Describe how 'ps' works. 71. What happens to a child process that dies and has no parent process to wait for it and what\u2019s bad about this? 72. Explain briefly each one of the process states. 73. How to know which process listens on a specific port? 74. What is a zombie process and what could be the cause of it? 75. You run a bash script and you want to see its output on your terminal and save it to a file at the same time. How could you do it? 76. Explain what echo \"1\" > /proc/sys/net/ipv4/ip_forward does. 77. Describe briefly the steps you need to take in order to create and install a valid certificate for the site https://foo.example.com. 78. Can you have several HTTPS virtual hosts sharing the same IP? 79. What is a wildcard certificate? 80. Which Linux file types do you know? 81. What is the difference between a process and a thread? And parent and child processes after a fork system call? 82. What is the difference between exec and fork? 83. What is \"nohup\" used for? 84. What is the difference between these two commands? myvar=hello & export myvar=hello 85. How many NTP servers would you configure in your local ntp.conf? 86. What does the column 'reach' mean in ntpq -p output? 87. You need to upgrade kernel at 100-1000 servers, how you would do this? 88. How can you get Host, Channel, ID, LUN of SCSI disk? 89. How can you limit process memory usage? 90. What is bash quick substitution/caret replace(^x^y)? 91. Do you know of any alternative shells? If so, have you used any? 92. What is a tarpipe (or, how would you go about copying everything, including hardlinks and special files, from one server to another)? 93. How can you tell if the httpd package was already installed? 94. How can you list the contents of a package? 95. How can you determine which package is better: openssh-server-5.3p1-118.1.el6_8.x86_64 or openssh-server-6.6p1-1.el6.x86_64 ? 96. Can you explain to me the difference between block based, and object based storage? # Hard Linux Questions: 97. What is a tunnel and how you can bypass a http proxy? 98. What is the difference between IDS and IPS? 99. What shortcuts do you use on a regular basis? 100. What is the Linux Standard Base? 101. What is an atomic operation? 102. Your freshly configured http server is not running after a restart, what can you do? 103. What kind of keys are in ~/.ssh/authorized_keys and what it is this file used for? 104. I've added my public ssh key into authorized_keys but I'm still getting a password prompt, what can be wrong? 105. Did you ever create RPM's, DEB's or solaris pkg's? 106. What does :(){ :|:& };: do on your system? 107. How do you catch a Linux signal on a script? 108. Can you catch a SIGKILL? 109. What's happening when the Linux kernel is starting the OOM killer and how does it choose which process to kill first? 110. Describe the linux boot process with as much detail as possible, starting from when the system is powered on and ending when you get a prompt. 111. What's a chroot jail? 112. When trying to umount a directory it says it's busy, how to find out which PID holds the directory? 113. What's LD_PRELOAD and when it's used? 114. You ran a binary and nothing happened. How would you debug this? 115. What are cgroups? Can you specify a scenario where you could use them? 116. How can you remove/delete a file with file-name consisting of only non-printable/non-type-able characters? 117. How can you increase or decrease the priority of a process in Linux? # Expert Linux Questions: 118. A running process gets EAGAIN: Resource temporarily unavailable on reading a socket. How can you close this bad socket/file descriptor without killing the process? 119. What do you control with swapiness? 120. How do you change TCP stack buffers? How do you calculate it? 121. What is Huge Tables? Why isn't it enabled by default? Why and when use it? 122. What is LUKS? How to use it? You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 Jenkins Interview Questions for Experianced DevOps Engineer","title":"Linux questionairs"},{"location":"nightwolf-cotribution/linux_questionairs/#top-100-linux-interview-questions-and-answers-solved","text":"We have prepared a set of questions to help Freshers and Experianced Linux Admins in their preparations for Interview. This list includes Google interview questions for Linux (Technical Solutions Specialist - Infrastructure), Amazon interview questions for Linux (Cloud Support Engineer - II) and other reputed firms as well. Most of these are scenario based Linux interview questions for Experianced Linux Admins. You will find these questions very helpful in your Linux Admins interviews. Prepare well and All the very best. All the feedbacks and suggestions are most welocome. 1. Explain the below error: ------------------------------------------------------------------------------------------- find: page allocation failure. order:5, mode:0x0 Pid: 22938, comm: find Not tainted 2.6.32-279.el6.i686 #1 Call Trace: [<c04f22bc>] ? __alloc_pages_nodemask+0x6bc/0x870 [<c051f1bc>] ? cache_alloc_refill+0x2bc/0x510 [<c083f035>] ? apic_timer_interrupt+0x31/0x38 [<c051f552>] ? __kmalloc+0x142/0x180 [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb1d6>] ? rpc_malloc+0x36/0x90 [sunrpc] [<f7fdb2f0>] ? rpc_wait_bit_killable+0x0/0x80 [sunrpc] ------------------------------------------------------------------------------------------- Hint => Above error is kernel dump due to kmalloc buffer exhaustion. The system is out of memory and cannot satisfy the allocation request at all. This is more likely to happen on swap-less systems that are unable to purge application pages to free up memory. The system may have sufficient free pages in total but not enough that are physically contiguous to satisfy the allocation request. This is referred to as memory fragmentation. - The phrase \"page allocation failure\" - this defines the error message that should be used in a KCS search. - The process name \"find\" - often page allocation failures are related to code that gets executed by specific threads, try searching KCS with and without this term. If the allocation request was performed by an interrupt service routine then the process name will be irrelevant. - The order of the allocation \"order:5\" and mode \"mode:0x0\" can often be the same for every instance of a particular problem, try searching KCS with and without these terms. - The function requesting the memory allocation \"rpc_malloc()\" is likely to be the same for every instance of this particular problem so include that in the KCS search. - Do not search for the CPU number, the process id or function offsets since they will change and may prevent finding a match. For more details https://access.redhat.com/articles/1360023 2. What is order:5, mode:0x0 in above output. 3. Difference between kernel panic due to Memory crunch and page allocation failure. 4. Difference between VMalloc and KMalloc Hint => The kmalloc() & vmalloc() functions are a simple interface for obtaining kernel memory in byte-sized chunks. - The kmalloc() function guarantees that the pages are physically contiguous (and virtually contiguous). - The vmalloc() function works in a similar fashion to kmalloc(), except it allocates memory that is only virtually contiguous and not necessarily physically contiguous. 5. How do you troubleshoot memory performance issue. Please explain the details. 6. Which tools do you use to troubleshoot high Memory troubleshooting. 7. What are zombie process and how to kill/reclaim them. 8. What are system calls. 9. What is strace used for. 10. Difference between fork and exec in Linux. 11. Explain the below error: ------------------------------------------------------------------------------------------- No filesystem could mount root, tried: [ 4.471120] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ------------------------------------------------------------------------------------------- 12. Explain few examples of kernel panic. 13. How to blacklist a module. 14. What are D-State processes and what causes these. 15. If Disk is causing D-state processes, what you can check and can do to fix the issue. 16. Explain blk_trace. 17. If you are receiving \"Connection reset by peer\" when trying to connect a server, what can be root cause of this and how you can fix this. 18. How will you troubleshoot high load avg. Which tools will you use and why. Explain in details. 19. What can be reason of high load avg in kernel space i.e. %sy in top. 20. What is CPU affinity . How to check it. 21. How to check all cores stat in top. Hint: by pressing 1 or using mpstat 22. What can be the reasons for server to stuck at boot prompt. and how to troubleshoot further ? 23. What is backporting in Linux. Hint: its related to packages. 24. There are two systems and one is taking less booting time and other than taking more than booting time. 25. Please explain Linux booting processing? 26. How to check the loaded and unloaded modules 27. Please explain few types of kernel errors? 28. How to troubleshoot the system performance if any Linux system is facing slowness? 29. How to troubleshoot high memory usageissue on Linux system. 30. What is CPU load ? How to calculate the load average on the system? 31. What the Zombie and Orphan process? How to kill zombie process? 32. What could be the impacts on the system if there are many zombie process are available? 33. What is affinity in Linux? 1. Binding a process with a CPU core. 2. # taskset < command > 34. What is paging and swapping in Linux? Please explain Page fault ? 35. What is difference between cache and buffer? 36. Explain the TOP, IOSTAT,VMSTAT,IOTOP commands. 37. Difference between RHEL6 and RHEL7 booting process. 38. Difference between systemd and initd 39. How to troubleshoot a issue where a client not able to access a server? 40. What are outputs when you execute the NATCAT (nc) command and explain each ? Hint => - Connected - Connection timeout - Connection refused(if port / server is blocking by the firewall) 41. What are the inodes and how will you free up them? 42. How can we check the packet flow in our system? 43. what is the steal value in top command? 44. Explain what is NUMA. You may know this as \"Node Interleaving\u201d. Give an example where NUMA is beneficial, and another where it's detrimental. Hints => a). NUMA: Non Uniform Memory Access (NUMA) is a design where the memory is split into groups by CPU, making the memory access faster for each CPU to their memory group. b). Beneficial: Multiple processes with smaller memory footprints. Good for applications that fork processes, like Apache. Detrimental: Monolithic, memory-heavy, single processes. May be threaded, example MySQL, Redis, Java-everything, Memcached. Follow-Up Q. - what if there's only a single physical proc. 45. Customer complains that their CPUs aren't running as fast as they're rated. How would you Troubleshoot ? Hints => a). Could be heat damage - check /proc/cpuinfo for \"cpu MHz\". b). Could be the wrong CPUs installed, for that matter. Check the CPU specs in /proc/cpuinfo. OR dmidecode. c). Get 'em to tell you how to fix it (Change system profile to MaximumPerformance with omconfig, then reboot) 46. Can we check System logs on Hardware layer from OS ?How ? Hints => a). Yes, in dmesg logs or kern.log b). By using respective Hardware CLI commands. c). Specific answers hpasmcli -s \"show iml\" ==> For HP servers omreport system [alertlog | esmlog] ==> For Dell Servers 47. What can be done to improve the performance of an I/O-heavy system? Hints => a). Check RAID Level, if server using any. b). Partition aligned, i.e. starting a partition on sector 2048. c). Mount Options like: noatime - (covers norelatime/nodiratime) Can be enabled only via fstab. d). RAID Controller settings - read/write policy, e.g. write-through(good to ensure data has been changed on disk. write-back(relys on cache, battery). e). nobarrier - depends on whether or not the PERC Controller is making use of a battery or not. f). Adjust the filesystem journaling method - the default mount option is in the middle, data=ordered. This indicates the data is written out to the filesystem before the metadata in the journal is updated. The filesystem can be mounted with data=writeback which allows data to be written to the journal and disk at the same time, with the risk of some data possibly being written to the disk after the metadata that cannot be recovered after a crash. Additionally, the journal can be placed on an external device with another mount option, allowing separation of the writes to increase throughput performance for each write operation. g). discard - For SSDs. Also called fs TRIM. Extends life and improves performance of SSD drives. TRIM enables the controller to cache the expected SSD write cycles to collapse them into a \"do only once\" mechanism to prevent burning out the SSD cells, which can only be written to once with voltage. Note: For LVM, set issue_discards=1 in lvm.conf. Note: On Dell, TRIM/discard is only available on RAID controllers >= PERC H710 h). For SSD, you don't want controller caching for performance reasons. By default HPs turn caching off. Dell: Cut-through IO (CTIO) is an IO accelerator for SSD arrays that boosts the throughput of devices connected to the PERC Controller. It is enabled through disabling the write-back cache (enable writethrough cache) and disabling Read Ahead. HP: On servers with SSD drives, write caching appears to be disabled by default for performance reasons (there is no need for write caching on SSD drives which are fast enough). Writes to SSD drives will not be acknowledged until the data is actually written to disk: i). On HP, HP SSD Smart Path - this is enabled by default by the RAID creation process in the HP RAID controller itself, and can be checked with the standard commands using hpssacli to view controller details and RAID logical device details. The theory is this special method of SSD writing replaces traditional caching techniques with a special HP invented method for improving SSD performance and longevity. j). Disk IO Scheduler deadline (recommended for DBs) noop (recommended for VMs) 48. Why would I choose to use RAID-5 instead of RAID-10? Or vice versa? Hints => RAID-5: Pro: Space efficiency - you only sacrifice 1x HDD to redundancy. Pro: Possible with a minimum of 3x drives, as opposed to RAID-10's minimum-4. Con: Slow writes. Note: Reads are still quick though, so you'd be fine on a read-heavy application. Con: If two drives fail, you lose everything. RAID-10: Pro: Great performance with both reads and writes. Good for write-heavy applications. Pro: Less risk of a catastrophic failure, since more than 1 drive can fail safely (Note: Depends which drives). Con: Too much wastage of Space. Only 50% capacity is uasable. 49. How snapshots work ? Can you please explain. In terms of VMWare or LVM(in case someone asks). Hints => As soon as you create a snapshot, LVM creates a pool of blocks. This pool also contains a full copy of the LVM metadata of the volume. When writes happen to the main volume such as updating an inode, the block being overwritten is copied to this new pool and the new block is written to the main volume. This is the 'copy-on-write'. Because of this, the more data that gets changed between when a snapshot was taken and the current state of the main volume, the more space will get consumed by that snapshot pool. When you mount the snapshot, the meta-data written when the snapshot was taken allows the mapping of snapshot pool blocks over changed blocks in the volume (or higher level snapshot). This way when an access comes for a specific block, LVM knows which block access. As far as the filesystem on that volume is concerned, there are no snapshots. 50. In which scenario we need to use vgcfgrestore and vgcfgbackup commands ? When /etc/lvm/archive contents are written (when does backup happen ?) Hints => a). To restore VG configuration. b). Backup is taken whenever we make change to LVM (create/remove LV/VG) and we can use it to restore to the prior state. 51. What is default signal sent by KILL command ? Hint => a). singnal 15 (SIGTERM) is sent by default if you do not specify the signal type. 52. What is deadlock ? How to identify if process is in deadlock in Linux/Unix ? Hint => Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process 53. How to load kernel module. any commands ? Hint => We can use \"modprobe\" or \"insmod\" commands to load kernel module. 54. Difference between insmod and modprobe commands in Linux ? 55. Different types of file systems? Hint => ext2, ext3, ext4, xfs, vfat etc. 56. Difference between ext4 and xfs? 57. When v create user which files are referred? 58. Differnce between passwd and shadow file? Hint => /etc/passwd contains User's detail like home directory, shell etc. /etc/shadow conatains User's password hashes. 59. What are the contents in passwd file? Hint => root:x:0:0:root:/root:/bin/bash \u2022 login name \u2022 optional encrypted password \u2022 numerical user ID \u2022 numerical group ID \u2022 user name or comment field \u2022 user home directory \u2022 optional user command interpreter 60. List all users who are not given passwd? with cmd Hint => cat /etc/shadow| awk -F \":\" '$2 ~ /[a-z]/ {print $1}' 61. How to check what processess are running on which port? Hint => Using \"netstat\" and \"ss\" command we can relate processes with port. 62. How to extend partition in lvm? and indepth questions in lvm ? Hint => 1. First attach extra storage/disk to server. 2. Create a new partition on the disk using 'fdisk' or 'parted' command. 3. Create a new Physical Volume(PV) on that new partition. e.g. 'pvcreate /dev/sdb1' 4. Exetend existing Volume Group(VG) using new PV. e.g. 'vgextend vg_name /dev/sdb1' 5. Now extend the LV. e.g . 'lvextend -l 100%FREE /dev/mapper/vg_name-lv_name' 6. now execute 'resize2fs' or 'xfs_growfs'. 63. What is tcpwrappers? Hint => tcpwrappers is a host based Network Access Control List, by which we can allow or deny a host or subnetwork IP addresses, 64. On which port dns works? Hint => DNS works on port 53.","title":"Top 100 Linux Interview Questions and Answers - Solved:"},{"location":"nightwolf-cotribution/linux_questionairs/#questions-from-github","text":"A lot more Questions from nightwolf-cotribution github repo # General Questions: 1. What did you learn yesterday/this week? 2. Talk about your preferred development/administration environment. (OS, Editor, Browsers, Tools etc.) 3. Tell me about the last major Linux project you finished. 4. Tell me about the biggest mistake you've made in [some recent time period] and how you would do it differently today. What did you learn from this experience? 5. Why we must choose you? 6. What function does DNS play on a network? 7. What is HTTP? 8. What is an HTTP proxy and how does it work? 9. Describe briefly how HTTPS works. 10. What is SMTP? Give the basic scenario of how a mail message is delivered via SMTP. 11. What is RAID? What is RAID0, RAID1, RAID5, RAID10? 12. What is a level 0 backup? What is an incremental backup? 13. Describe the general file system hierarchy of a Linux system. 14. Which difference have between public and private SSH key? # Simple Linux Questions: 15. What is the name and the UID of the administrator user? 16. How to list all files, including hidden ones, in a directory? 17. What is the Unix/Linux command to remove a directory and its contents? 18. Which command will show you free/used memory? Does free memory exist on Linux? 19. How to search for the string \"my konfu is the best\" in files of a directory recursively? 20. How to connect to a remote server or what is SSH? 21. How to get all environment variables and how can you use them? 22. I get \"command not found\" when I run ifconfig -a. What can be wrong? 23. What happens if I type TAB-TAB? 24. What command will show the available disk space on the Unix/Linux system? 25. What commands do you know that can be used to check DNS records? 26. What Unix/Linux commands will alter a files ownership, files permissions? 27. What does chmod +x FILENAME do? 28. What does the permission 0750 on a file mean? 29. What does the permission 0750 on a directory mean? 30. How to add a new system user without login permissions? 31. How to add/remove a group from a user? 32. What is a bash alias? 33. How do you set the mail address of the root/a user? 34. What does CTRL-c do? 35. What does CTRL-d do? 36. What does CTRL-z do? 37. What is in /etc/services? 38. How to redirect STDOUT and STDERR in bash? (> /dev/null 2>&1) 39. What is the difference between UNIX and Linux. 40. What is the difference between Telnet and SSH? 41. Explain the three load averages and what do they indicate. What command can be used to view the load averages? 42. Can you name a lower-case letter that is not a valid option for GNU ls? 43. What is a Linux kernel module? 44. Walk me through the steps in booting into single user mode to troubleshoot a problem. 45. Walk me through the steps you'd take to troubleshoot a 404 error on a web application you administer. 46. What is ICMP protocol? Why do you need to use? # Medium Linux Questions: 47. What do the following commands do and how would you use them? tee awk tr cut tac curl wget watch head tail less cat touch sar netstat tcpdump lsof 48. What does an & after a command do? 49. What does & disown after a command do? 50. What is a packet filter and how does it work? 51. What is Virtual Memory? 52. What is swap and what is it used for? 53. What is an A record, an NS record, a PTR record, a CNAME record, an MX record? 54. Are there any other RRs and what are they used for? 55. What is a Split-Horizon DNS? 56. What is the sticky bit? 57. What does the immutable bit do to a file? 58. What is the difference between hardlinks and symlinks? What happens when you remove the source to a symlink/hardlink? 59. What is an inode and what fields are stored in an inode? 60. How to force/trigger a file system check on next reboot? 61. What is SNMP and what is it used for? 62. What is a runlevel and how to get the current runlevel? 63. What is SSH port forwarding? 64. What is the difference between local and remote port forwarding? 65. What are the steps to add a user to a system without using useradd/adduser? 66. What is MAJOR and MINOR numbers of special files? 67. Describe the mknod command and when you'd use it. 68. Describe a scenario when you get a \"filesystem is full\" error, but 'df' shows there is free space. 69. Describe a scenario when deleting a file, but 'df' not showing the space being freed. 70. Describe how 'ps' works. 71. What happens to a child process that dies and has no parent process to wait for it and what\u2019s bad about this? 72. Explain briefly each one of the process states. 73. How to know which process listens on a specific port? 74. What is a zombie process and what could be the cause of it? 75. You run a bash script and you want to see its output on your terminal and save it to a file at the same time. How could you do it? 76. Explain what echo \"1\" > /proc/sys/net/ipv4/ip_forward does. 77. Describe briefly the steps you need to take in order to create and install a valid certificate for the site https://foo.example.com. 78. Can you have several HTTPS virtual hosts sharing the same IP? 79. What is a wildcard certificate? 80. Which Linux file types do you know? 81. What is the difference between a process and a thread? And parent and child processes after a fork system call? 82. What is the difference between exec and fork? 83. What is \"nohup\" used for? 84. What is the difference between these two commands? myvar=hello & export myvar=hello 85. How many NTP servers would you configure in your local ntp.conf? 86. What does the column 'reach' mean in ntpq -p output? 87. You need to upgrade kernel at 100-1000 servers, how you would do this? 88. How can you get Host, Channel, ID, LUN of SCSI disk? 89. How can you limit process memory usage? 90. What is bash quick substitution/caret replace(^x^y)? 91. Do you know of any alternative shells? If so, have you used any? 92. What is a tarpipe (or, how would you go about copying everything, including hardlinks and special files, from one server to another)? 93. How can you tell if the httpd package was already installed? 94. How can you list the contents of a package? 95. How can you determine which package is better: openssh-server-5.3p1-118.1.el6_8.x86_64 or openssh-server-6.6p1-1.el6.x86_64 ? 96. Can you explain to me the difference between block based, and object based storage? # Hard Linux Questions: 97. What is a tunnel and how you can bypass a http proxy? 98. What is the difference between IDS and IPS? 99. What shortcuts do you use on a regular basis? 100. What is the Linux Standard Base? 101. What is an atomic operation? 102. Your freshly configured http server is not running after a restart, what can you do? 103. What kind of keys are in ~/.ssh/authorized_keys and what it is this file used for? 104. I've added my public ssh key into authorized_keys but I'm still getting a password prompt, what can be wrong? 105. Did you ever create RPM's, DEB's or solaris pkg's? 106. What does :(){ :|:& };: do on your system? 107. How do you catch a Linux signal on a script? 108. Can you catch a SIGKILL? 109. What's happening when the Linux kernel is starting the OOM killer and how does it choose which process to kill first? 110. Describe the linux boot process with as much detail as possible, starting from when the system is powered on and ending when you get a prompt. 111. What's a chroot jail? 112. When trying to umount a directory it says it's busy, how to find out which PID holds the directory? 113. What's LD_PRELOAD and when it's used? 114. You ran a binary and nothing happened. How would you debug this? 115. What are cgroups? Can you specify a scenario where you could use them? 116. How can you remove/delete a file with file-name consisting of only non-printable/non-type-able characters? 117. How can you increase or decrease the priority of a process in Linux? # Expert Linux Questions: 118. A running process gets EAGAIN: Resource temporarily unavailable on reading a socket. How can you close this bad socket/file descriptor without killing the process? 119. What do you control with swapiness? 120. How do you change TCP stack buffers? How do you calculate it? 121. What is Huge Tables? Why isn't it enabled by default? Why and when use it? 122. What is LUKS? How to use it? You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 Jenkins Interview Questions for Experianced DevOps Engineer","title":"Questions from Github"},{"location":"nightwolf-cotribution/network/","text":"OS Networking Interview Questions \uf0c1 General questions about basic Networking + OS Networking + Troubleshooting. 1. How to check if you are receiving connection timeouts when trying to connect to a server. 2. In what condition you will receive connection refused. Hint: 1. if packets are rejected using firewall/iptables action \"REJECT\" 2. If service is not responding or is down. 3. what will you do/check if customer calls and tells you that there is packet drop when he is trying to access a remote server. 4. How will you troubleshoot packet drops? 5. What is TCP re-transmissions ? 6. What if there are no Packet drops/loss and high TCP re-transmissions on server. How will this impact server/services ? 7. What tools can be used to check bandwidth usage between 2 servers/devices. 8. When you poweron a laptop, how does it receives IP every time. Explain the process. 9. What if dhcp lease is over, will IP/connection drop from server. 10. What is APIPA. In what case Server gets APIPA ip? Hint: When server is configured to receive IP from DHCP and there is no DHCP in that network. 11. What happens in backend why you type google.com in browser and hit enter. 12. Explain recursive and Iterative DNS queries. 13. What can be the reason for receiving Request time outs when trying to resolve rackspace.com. 14. What are HOPs in network. In a packet-switching network, a hop is the trip a data packet takes from one router or intermediate point to another in the network. On the Internet (or a network that uses TCP/IP), the number of hops a packet has taken toward its destination (called the \"hop count\") is kept in the packet header. A packet with an exceedingly large hop count is discarded. 15. what is Apache 500 status code (Internel server error) and how to troubleshoot the 500 status code issue. 16. DNS working flow? What happens when we hit www.google.com in browser? 17. What is recursive and iterative query? 18. What are the dns records? Explain each and every record? 19. What is difference between below 3 domains www.google.com Google.com www.google.com. 20. What is the DORA process ? 21. TCP 3 way handshaking and 4 way termination? 22. SSL/TLS 4 way hand shaking? 23. How to check TCP buffer size in Linux system? 24. How to check packet drops in Linux? 25. Explain the OSI model and working of each and every layer. 26. Difference between TCP and UDP protocol. 27. Why we have only 13 DNS server in the world? 28. A customer complains of website slowness, what will be your troubleshooting approach in network prospective? 29. Difference between Router and switch ? 30. What configuration happens on switch and router? 31. What happens when you open a website in browser? 32. How does a client gets connected to Wifi? or What happens in the backend when client connects to wifi? 33. Remove session layer from the picture and now consider running traceroute on 2 CMD terminals. How does the request differ on both CMD terminals? 34. Can sequence number be duplicate in TCP/IP? 35. How many traceroute hops will be there when you run traceroute from server A to Server B Server A --> Switch A --> Router R1 --> Router R2 --> Switch B --> Server B 36. Please explain a recent issue that you experienced in your environment and your learning associated with it. Questions from Github \uf0c1 A lot more Questions from nightwolf-cotribution github repo 1. What is localhost and why would ping localhost fail? 2. What is the similarity between \"ping\" & \"traceroute\" ? How is traceroute able to find the hops. 3. What is the command used to show all open ports and/or socket connections on a machine? 4. Is 300.168.0.123 a valid IPv4 address? 5. Which IP ranges/subnets are \"private\" or \"non-routable\" (RFC 1918)? 6. What is a VLAN? 7. What is ARP and what is it used for? 8. What is the difference between TCP and UDP? 9. What is the purpose of a default gateway? 10. What is command used to show the routing table on a Linux box? 11. A TCP connection on a network can be uniquely defined by 4 things. What are those things? 12. When a client running a web browser connects to a web server, what is the source port and what is the destination port of the connection? 13. How do you add an IPv6 address to a specific interface? 14. You have added an IPv4 and IPv6 address to interface eth0. A ping to the v4 address is working but a ping to the v6 address gives you the response sendmsg: operation not permitted. What could be wrong? 15. What is SNAT and when should it be used? 16. Explain how could you ssh login into a Linux system that DROPs all new incoming packets using a SSH tunnel. 17. How do you stop a DDoS attack? 18. How can you see content of an ip packet? 19. What is IPoAC (RFC 1149)? 20. What will happen when you bind port 0? 21. Customer changes an A record in their DNS control panel but calls us because they're still seeing the old version of their web site. How would you troubleshoot? 22. I type \"ping nightwolf.in\" at a command-line. What things does the Linux OS do to turn \"nightwolf.in\" into an IP address? Hints => a). /etc/hosts - used in name resolution whenever the files source is being defined in the hosts database b). nameserver \u2013 IP (IPv4 or IPv6) of the server that will resolve the queries c). /etc/resolv.conf - used by the libresolv library that's used by the libnss_dns library, this is when the source used is DNS. 23. Can you still SSH to a Linux server if its default gateway is set incorrectly? How? Hints => a). You can SSH in, but only from another device in the same subnet, or in a network to which the \"broken\" server has a static route defined. b).some static route is there which can route to destination network. 24. Two servers behind the same firewall are able to communicate by their private IPs, but not via FQDN. What might be causing this, and what can be done to fix it? Hints => a). The FQDNs are resolving to public IPs, and public IPs are unrouteable within the private network behind the firewall. b). Edit /etc/hosts on the all servers behind the firewall to override DNS to point to private IP. c). The \"best\" fix would be to enable DNS doctoring/translation at the firewall. 25. What are the pros/cons of Load Balancer Health checks? 26. What is persistence (ie: sticky sessions), and what are the pros and cons of it? Hint => Persistence: If source X went to node Y within the past Z minutes, bypass all balancing algorithms and send directly to node Y again. Pros: \u2022 Avoids the breakage of per-node session information (ie: /var/lib/php/session files). Shopping carts, active logins, etc. Cons: \u2022 In general, persistence can lead to some imbalance of active sessions between nodes. If a node is temporarily unavailable (ie: rebooted), all active sessions become persistent to the remaining node(s). \u2022 With LeastConnections, this means the recovering node takes all new sessions, and is potentially overloaded until sessions equalize. \u2022 With RoundRobin, this means the recovering node has significantly fewer sessions than the others for quite a while. \u2022 If one connection is causing a large load, that load is not balanced - persistence keeps it all on one node. You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"OS Networking Interview Questions"},{"location":"nightwolf-cotribution/network/#os-networking-interview-questions","text":"General questions about basic Networking + OS Networking + Troubleshooting. 1. How to check if you are receiving connection timeouts when trying to connect to a server. 2. In what condition you will receive connection refused. Hint: 1. if packets are rejected using firewall/iptables action \"REJECT\" 2. If service is not responding or is down. 3. what will you do/check if customer calls and tells you that there is packet drop when he is trying to access a remote server. 4. How will you troubleshoot packet drops? 5. What is TCP re-transmissions ? 6. What if there are no Packet drops/loss and high TCP re-transmissions on server. How will this impact server/services ? 7. What tools can be used to check bandwidth usage between 2 servers/devices. 8. When you poweron a laptop, how does it receives IP every time. Explain the process. 9. What if dhcp lease is over, will IP/connection drop from server. 10. What is APIPA. In what case Server gets APIPA ip? Hint: When server is configured to receive IP from DHCP and there is no DHCP in that network. 11. What happens in backend why you type google.com in browser and hit enter. 12. Explain recursive and Iterative DNS queries. 13. What can be the reason for receiving Request time outs when trying to resolve rackspace.com. 14. What are HOPs in network. In a packet-switching network, a hop is the trip a data packet takes from one router or intermediate point to another in the network. On the Internet (or a network that uses TCP/IP), the number of hops a packet has taken toward its destination (called the \"hop count\") is kept in the packet header. A packet with an exceedingly large hop count is discarded. 15. what is Apache 500 status code (Internel server error) and how to troubleshoot the 500 status code issue. 16. DNS working flow? What happens when we hit www.google.com in browser? 17. What is recursive and iterative query? 18. What are the dns records? Explain each and every record? 19. What is difference between below 3 domains www.google.com Google.com www.google.com. 20. What is the DORA process ? 21. TCP 3 way handshaking and 4 way termination? 22. SSL/TLS 4 way hand shaking? 23. How to check TCP buffer size in Linux system? 24. How to check packet drops in Linux? 25. Explain the OSI model and working of each and every layer. 26. Difference between TCP and UDP protocol. 27. Why we have only 13 DNS server in the world? 28. A customer complains of website slowness, what will be your troubleshooting approach in network prospective? 29. Difference between Router and switch ? 30. What configuration happens on switch and router? 31. What happens when you open a website in browser? 32. How does a client gets connected to Wifi? or What happens in the backend when client connects to wifi? 33. Remove session layer from the picture and now consider running traceroute on 2 CMD terminals. How does the request differ on both CMD terminals? 34. Can sequence number be duplicate in TCP/IP? 35. How many traceroute hops will be there when you run traceroute from server A to Server B Server A --> Switch A --> Router R1 --> Router R2 --> Switch B --> Server B 36. Please explain a recent issue that you experienced in your environment and your learning associated with it.","title":"OS Networking Interview Questions"},{"location":"nightwolf-cotribution/network/#questions-from-github","text":"A lot more Questions from nightwolf-cotribution github repo 1. What is localhost and why would ping localhost fail? 2. What is the similarity between \"ping\" & \"traceroute\" ? How is traceroute able to find the hops. 3. What is the command used to show all open ports and/or socket connections on a machine? 4. Is 300.168.0.123 a valid IPv4 address? 5. Which IP ranges/subnets are \"private\" or \"non-routable\" (RFC 1918)? 6. What is a VLAN? 7. What is ARP and what is it used for? 8. What is the difference between TCP and UDP? 9. What is the purpose of a default gateway? 10. What is command used to show the routing table on a Linux box? 11. A TCP connection on a network can be uniquely defined by 4 things. What are those things? 12. When a client running a web browser connects to a web server, what is the source port and what is the destination port of the connection? 13. How do you add an IPv6 address to a specific interface? 14. You have added an IPv4 and IPv6 address to interface eth0. A ping to the v4 address is working but a ping to the v6 address gives you the response sendmsg: operation not permitted. What could be wrong? 15. What is SNAT and when should it be used? 16. Explain how could you ssh login into a Linux system that DROPs all new incoming packets using a SSH tunnel. 17. How do you stop a DDoS attack? 18. How can you see content of an ip packet? 19. What is IPoAC (RFC 1149)? 20. What will happen when you bind port 0? 21. Customer changes an A record in their DNS control panel but calls us because they're still seeing the old version of their web site. How would you troubleshoot? 22. I type \"ping nightwolf.in\" at a command-line. What things does the Linux OS do to turn \"nightwolf.in\" into an IP address? Hints => a). /etc/hosts - used in name resolution whenever the files source is being defined in the hosts database b). nameserver \u2013 IP (IPv4 or IPv6) of the server that will resolve the queries c). /etc/resolv.conf - used by the libresolv library that's used by the libnss_dns library, this is when the source used is DNS. 23. Can you still SSH to a Linux server if its default gateway is set incorrectly? How? Hints => a). You can SSH in, but only from another device in the same subnet, or in a network to which the \"broken\" server has a static route defined. b).some static route is there which can route to destination network. 24. Two servers behind the same firewall are able to communicate by their private IPs, but not via FQDN. What might be causing this, and what can be done to fix it? Hints => a). The FQDNs are resolving to public IPs, and public IPs are unrouteable within the private network behind the firewall. b). Edit /etc/hosts on the all servers behind the firewall to override DNS to point to private IP. c). The \"best\" fix would be to enable DNS doctoring/translation at the firewall. 25. What are the pros/cons of Load Balancer Health checks? 26. What is persistence (ie: sticky sessions), and what are the pros and cons of it? Hint => Persistence: If source X went to node Y within the past Z minutes, bypass all balancing algorithms and send directly to node Y again. Pros: \u2022 Avoids the breakage of per-node session information (ie: /var/lib/php/session files). Shopping carts, active logins, etc. Cons: \u2022 In general, persistence can lead to some imbalance of active sessions between nodes. If a node is temporarily unavailable (ie: rebooted), all active sessions become persistent to the remaining node(s). \u2022 With LeastConnections, this means the recovering node takes all new sessions, and is potentially overloaded until sessions equalize. \u2022 With RoundRobin, this means the recovering node has significantly fewer sessions than the others for quite a while. \u2022 If one connection is causing a large load, that load is not balanced - persistence keeps it all on one node. You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 GIT Interview Questions for DevOps Roles Jenkins Interview Questions for Experianced DevOps Engineer","title":"Questions from Github"},{"location":"nightwolf-cotribution/performance/","text":"Linux System Performance Troubleshooting \uf0c1 Definition: System performance is a measure of the amount of useful work done by a System in a time range. OS/CPU Load: The average amount of processes using or waiting for CPU allocation over a period of time. Usually System Performance issues can be identified by observing slowness in services offered by OS and it mostly happen due to any of the below reasons. - high CPU Usage - high Memory Usage - high Disk IO Usage - Network Performance Issues - Software Bugs like memory leaks, Kernel bugs etc. We will try to break your investigation steps into major bullet points and helps you to find the root causes of the issue. Tools helpful in your investigation: \uf0c1 Below listed tools are Linux performance monitoring tools, which will help you find out the root cause of the issue, top mpstat sar free strace iotop iostat netstat pidstat vmstat tcpdump iptraf blktrace lsof ethtool type-clip : A very good tool when you have to paste something into virtual consoles. recap : To capture historic stats of a server. This tool is almost harmless and captures a lot of information. You can add this tool to your default package list in your environment. You can install these tools very easily using below command: yum install -y lsof sysstat iptraf tcpdump procps-ng net-tools strace iotop ethtool blktrace You can start your investigation by executing small script, which will gather a lot of system stats for you: \uf0c1 bash <(curl -s https://raw.githubusercontent.com/v-nightwolf/nightwolf-cotribution/main/server_stats.sh) The output of above script will look like this: ############################################ Server Uptime: 1days 6:43:23 Load Average: Current 0.26 Load Average: 15min Average 0.26 #### Printing CPU stats: #### 01:15:26 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 01:15:26 PM all 1.47 0.00 0.48 0.02 0.00 0.12 0.00 0.00 0.00 97.91 ###RAM usage### Free Ram: 569.93MB Used RAM: 3171.23MB 15% ram left RAM ALERT: Low! TOP RAM CONSUMER: /opt/mongodb/mms/jdk/bin/mms-app RAM Usage (RSS): 1857.43 MB Total Number of RAM Processes: 2 Total RAM Usage for all /opt/mongodb/mms/jdk/bin/mms-app processes = 2287.56 MB 61.15% used by /opt/mongodb/mms/jdk/bin/mms-app ###CPU usage### Top Process: /opt/mongodb/mms/jdk/bin/mms-app CPU % for SINGLE Top Process = 4.2 number of processes this is running: 2 Total CPU % for /opt/mongodb/mms/jdk/bin/mms-app = 4.8 ############################################ Troubleshooting Linux perfomance isssue happening due to high CPU Usage \uf0c1 Every performance issue troubleshooting should starts with top command: and the most useful output of top command is: top - 16:18:32 up 2:52, 1 user, load average: 0.09, 0.20, 0.22 Tasks: 99 total, 1 running, 98 sleeping, 0 stopped, 0 zombie %Cpu(s): 3.2 us, 0.0 sy, 0.0 ni, 96.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 2889532 total, 73328 free, 2707940 used, 108264 buff/cache KiB Swap: 2097148 total, 1206268 free, 890880 used. 48052 avail Mem a). 1st line contains: UPTIME and LOAD Averages current time and length of time since last boot total number of users logged in. system load avg over the last 1, 5 and 15 minutes b). 2nd line contains: TASKs This line shows total tasks or threads, depending on the state of the Threads-mode toggle. That total is further classified as: running; sleeping; stopped; zombie c). 3rd line contains CPU state percentages: This will guide your investigation into a specific direction. As a default, percentages for these individual categories are displayed. us, user : time running un-niced user processes sy, system : time running kernel processes ni, nice : time running niced user processes id, idle : time spent in the kernel idle handler wa, IO-wait : time waiting for I/O completion hi : time spent servicing hardware interrupts si : time spent servicing software interrupts st : time stolen from this vm by the hypervisor Above stats might guide your investigation into a specific direction. Depending on above CPU stats, you will be able to decide what to check next. * If only \"%us\"(time running un-niced user processes) is high that means your application is consuming more CPU. Now you should deep dive into process stats and process trace. We will discuss this in more detail soon. * If only \"%sy\"(time running kernel processes) is high that means you kernel level system call are consuming the CPU. This usually happens when there is a bug in kernel packages. * If \"%ni\"(time running niced user processes) is high that means the prioritized processes are consuming the CPU. You should try to depriritize the process. * If \"%id\"(time spent in the kernel idle handler) is high that means your system is ideal and is not doing anything. * If \"%wa\"(time waiting for I/O completion) is high that means your most CPU time is being spent on waiting for I/O completion. These waits can be due to Disk slowness or Network slowness. * If \"%hi\"(time spent servicing hardware interrupts) is high that means CPU is very busy in servicing the Hardware interrupts. Hardware interrupt will cause CPU to stop its current processing and go handle the Hardware interrupt. Ususally hardware interrupts was generated by physical devices like disk, NIC,computer peripherals. You should check the hardwares attached to the machine and see if they are working fine. * If \"%si\"(time spent servicing software interrupts) is high that CPU is busy serving Software interrupts. Usually Software interrupt occure at kernel level. * \"%st\"(time stolen from this vm by the hypervisor) means that virtual CPU is being spent waiting for the Hypervisor to allocate CPU to virtual machine. This stat is only applicable to Virtual machines. If %us is high, that mean some process in user space is consuming the CPU. Next step would be to deep dive into process stats: 1. First identify the process consuming high CPU. This can be identified using \"top\" command. 2. Once you have identified the process, note down its process id(pid). 3. Try tracing the process using stace command. This will throw a lot output onto the screen. starce -t -p PID_OF_PROCESS => This will print wall clock time of each system call. 4. Please check if process is getting stuck at any system call and note down system call and the resource system call is using. 5. If system call is waiting for a File related I/O, then next step is to check file system call is waiting for. 6. Go to /proc/{$PID_OF_PROCESS}/ Troubleshooting Linux performance issue happening due to high Memory Usage \uf0c1 d). 4th and 5th line in 'top' output contains Memory stats in Kibibytes(kib). There is slight difference between kilibyte and kibibyte i.e 1 kB = 1000 bytes. 1 KiB = 1024 bytes. As a default, line 4 reflects physical memory, classified as: total, free, used and buff/cache line 5 reflects mostly virtual memory(swap), classified as: total, free, used and avail total: Total size of memory available to system. free: Size of memory which in un-utilized. used: Size of memory currently utilized by processes+System. buff/cache: Size of memory utilized by system for kernel buffers(i.e. buff) and page cache and slabs(i.e. cache) Troubleshooting Linux performance issue happening due to high Disk IO Usage \uf0c1 Troubleshooting Linux OS network performance Issues \uf0c1 You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 Jenkins Interview Questions for Experianced DevOps Engineer","title":"Linux-Performance"},{"location":"nightwolf-cotribution/performance/#linux-system-performance-troubleshooting","text":"Definition: System performance is a measure of the amount of useful work done by a System in a time range. OS/CPU Load: The average amount of processes using or waiting for CPU allocation over a period of time. Usually System Performance issues can be identified by observing slowness in services offered by OS and it mostly happen due to any of the below reasons. - high CPU Usage - high Memory Usage - high Disk IO Usage - Network Performance Issues - Software Bugs like memory leaks, Kernel bugs etc. We will try to break your investigation steps into major bullet points and helps you to find the root causes of the issue.","title":"Linux System Performance Troubleshooting"},{"location":"nightwolf-cotribution/performance/#tools-helpful-in-your-investigation","text":"Below listed tools are Linux performance monitoring tools, which will help you find out the root cause of the issue, top mpstat sar free strace iotop iostat netstat pidstat vmstat tcpdump iptraf blktrace lsof ethtool type-clip : A very good tool when you have to paste something into virtual consoles. recap : To capture historic stats of a server. This tool is almost harmless and captures a lot of information. You can add this tool to your default package list in your environment. You can install these tools very easily using below command: yum install -y lsof sysstat iptraf tcpdump procps-ng net-tools strace iotop ethtool blktrace","title":"Tools helpful in your investigation:"},{"location":"nightwolf-cotribution/performance/#you-can-start-your-investigation-by-executing-small-script-which-will-gather-a-lot-of-system-stats-for-you","text":"bash <(curl -s https://raw.githubusercontent.com/v-nightwolf/nightwolf-cotribution/main/server_stats.sh) The output of above script will look like this: ############################################ Server Uptime: 1days 6:43:23 Load Average: Current 0.26 Load Average: 15min Average 0.26 #### Printing CPU stats: #### 01:15:26 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 01:15:26 PM all 1.47 0.00 0.48 0.02 0.00 0.12 0.00 0.00 0.00 97.91 ###RAM usage### Free Ram: 569.93MB Used RAM: 3171.23MB 15% ram left RAM ALERT: Low! TOP RAM CONSUMER: /opt/mongodb/mms/jdk/bin/mms-app RAM Usage (RSS): 1857.43 MB Total Number of RAM Processes: 2 Total RAM Usage for all /opt/mongodb/mms/jdk/bin/mms-app processes = 2287.56 MB 61.15% used by /opt/mongodb/mms/jdk/bin/mms-app ###CPU usage### Top Process: /opt/mongodb/mms/jdk/bin/mms-app CPU % for SINGLE Top Process = 4.2 number of processes this is running: 2 Total CPU % for /opt/mongodb/mms/jdk/bin/mms-app = 4.8 ############################################","title":"You can start your investigation by executing small script, which will gather a lot of system stats for you:"},{"location":"nightwolf-cotribution/performance/#troubleshooting-linux-perfomance-isssue-happening-due-to-high-cpu-usage","text":"Every performance issue troubleshooting should starts with top command: and the most useful output of top command is: top - 16:18:32 up 2:52, 1 user, load average: 0.09, 0.20, 0.22 Tasks: 99 total, 1 running, 98 sleeping, 0 stopped, 0 zombie %Cpu(s): 3.2 us, 0.0 sy, 0.0 ni, 96.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 2889532 total, 73328 free, 2707940 used, 108264 buff/cache KiB Swap: 2097148 total, 1206268 free, 890880 used. 48052 avail Mem a). 1st line contains: UPTIME and LOAD Averages current time and length of time since last boot total number of users logged in. system load avg over the last 1, 5 and 15 minutes b). 2nd line contains: TASKs This line shows total tasks or threads, depending on the state of the Threads-mode toggle. That total is further classified as: running; sleeping; stopped; zombie c). 3rd line contains CPU state percentages: This will guide your investigation into a specific direction. As a default, percentages for these individual categories are displayed. us, user : time running un-niced user processes sy, system : time running kernel processes ni, nice : time running niced user processes id, idle : time spent in the kernel idle handler wa, IO-wait : time waiting for I/O completion hi : time spent servicing hardware interrupts si : time spent servicing software interrupts st : time stolen from this vm by the hypervisor Above stats might guide your investigation into a specific direction. Depending on above CPU stats, you will be able to decide what to check next. * If only \"%us\"(time running un-niced user processes) is high that means your application is consuming more CPU. Now you should deep dive into process stats and process trace. We will discuss this in more detail soon. * If only \"%sy\"(time running kernel processes) is high that means you kernel level system call are consuming the CPU. This usually happens when there is a bug in kernel packages. * If \"%ni\"(time running niced user processes) is high that means the prioritized processes are consuming the CPU. You should try to depriritize the process. * If \"%id\"(time spent in the kernel idle handler) is high that means your system is ideal and is not doing anything. * If \"%wa\"(time waiting for I/O completion) is high that means your most CPU time is being spent on waiting for I/O completion. These waits can be due to Disk slowness or Network slowness. * If \"%hi\"(time spent servicing hardware interrupts) is high that means CPU is very busy in servicing the Hardware interrupts. Hardware interrupt will cause CPU to stop its current processing and go handle the Hardware interrupt. Ususally hardware interrupts was generated by physical devices like disk, NIC,computer peripherals. You should check the hardwares attached to the machine and see if they are working fine. * If \"%si\"(time spent servicing software interrupts) is high that CPU is busy serving Software interrupts. Usually Software interrupt occure at kernel level. * \"%st\"(time stolen from this vm by the hypervisor) means that virtual CPU is being spent waiting for the Hypervisor to allocate CPU to virtual machine. This stat is only applicable to Virtual machines. If %us is high, that mean some process in user space is consuming the CPU. Next step would be to deep dive into process stats: 1. First identify the process consuming high CPU. This can be identified using \"top\" command. 2. Once you have identified the process, note down its process id(pid). 3. Try tracing the process using stace command. This will throw a lot output onto the screen. starce -t -p PID_OF_PROCESS => This will print wall clock time of each system call. 4. Please check if process is getting stuck at any system call and note down system call and the resource system call is using. 5. If system call is waiting for a File related I/O, then next step is to check file system call is waiting for. 6. Go to /proc/{$PID_OF_PROCESS}/","title":"Troubleshooting Linux perfomance isssue happening due to high CPU Usage"},{"location":"nightwolf-cotribution/performance/#troubleshooting-linux-performance-issue-happening-due-to-high-memory-usage","text":"d). 4th and 5th line in 'top' output contains Memory stats in Kibibytes(kib). There is slight difference between kilibyte and kibibyte i.e 1 kB = 1000 bytes. 1 KiB = 1024 bytes. As a default, line 4 reflects physical memory, classified as: total, free, used and buff/cache line 5 reflects mostly virtual memory(swap), classified as: total, free, used and avail total: Total size of memory available to system. free: Size of memory which in un-utilized. used: Size of memory currently utilized by processes+System. buff/cache: Size of memory utilized by system for kernel buffers(i.e. buff) and page cache and slabs(i.e. cache)","title":"Troubleshooting Linux performance issue happening due to high Memory Usage"},{"location":"nightwolf-cotribution/performance/#troubleshooting-linux-performance-issue-happening-due-to-high-disk-io-usage","text":"","title":"Troubleshooting Linux performance issue happening due to high Disk IO Usage"},{"location":"nightwolf-cotribution/performance/#troubleshooting-linux-os-network-performance-issues","text":"You may also refer to: Linux Interview Questions for Freshers Linux Interview Questions for Freshers-2 Linux Interview Questions for Freshers and Experianced - L1 Linux Interview Questions for Experianced Linux Admins - L2 Advanced Linux Interview Questions for Experianced Admins - L3 OS Network Interview Questions AWS interview questions for experianced professionals DevOps Interview Questions for Freshers and Experianced DevOps Interview Questions for Freshers and Experianced-2 Jenkins Interview Questions for Experianced DevOps Engineer","title":"Troubleshooting Linux OS network performance Issues"}]}